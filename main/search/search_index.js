var __index = {"config":{"lang":["en","zh"],"separator":"[\\s\\u200b\\-_,:!=\\[\\: )\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"en/index.html","title":"Home","text":"<p>Since its initial release, PaddleOCR has gained widespread acclaim across academia, industry, and research communities, thanks to its cutting-edge algorithms and proven performance in real-world applications. It\u2019s already powering popular open-source projects like Umi-OCR, OmniParser, MinerU, and RAGFlow, making it the go-to OCR toolkit for developers worldwide.</p> <p>On May 20, 2025, the PaddlePaddle team unveiled PaddleOCR 3.0, fully compatible with the official release of the PaddlePaddle 3.0 framework. This update further boosts text-recognition accuracy, adds support for multiple text-type recognition and handwriting recognition, and meets the growing demand from large-model applications for high-precision parsing of complex documents. When combined with the ERNIE 4.5 Turbo, it significantly enhances key-information extraction accuracy. PaddleOCR 3.0 also introduces support for domestic hardware platforms such as KUNLUNXIN and Ascend.</p> <p>Three Major New Features in PaddleOCR 3.0:</p> <ul> <li> <p>\ud83d\uddbc\ufe0f Universal-Scene Text Recognition Model PP-OCRv5: A single model that handles five different text types plus complex handwriting. Overall recognition accuracy has increased by 13 percentage points over the previous generation.Online Demo</p> </li> <li> <p>\ud83e\uddee General Document-Parsing Solution PP-StructureV3: Delivers high-precision parsing of multi-layout, multi-scene PDFs, outperforming many open- and closed-source solutions on public benchmarks. Online Demo</p> </li> <li> <p>\ud83d\udcc8 Intelligent Document-Understanding Solution PP-ChatOCRv4: Natively powered by the ERNIE 4.5 Turbo, achieving 15 percentage points higher accuracy than its predecessor.Online Demo</p> </li> </ul> <p>In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.</p> <p></p> <p>You can Quick Start directly, find comprehensive documentation in the PaddleOCR Docs, get support via Github Issus, and explore our OCR courses on OCR courses on AIStudio.</p>"},{"location":"en/index.html#quick-overview-of-execution-results","title":"\ud83d\udd04 Quick Overview of Execution Results","text":""},{"location":"en/index.html#community","title":"\ud83d\udc69\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66 Community","text":"<ul> <li>\ud83d\udc6b Join the PaddlePaddle Community, where you can engage with paddlepaddle developers, researchers, and enthusiasts from around the world.</li> <li>\ud83c\udf93 Learn from experts through workshops, tutorials, and Q&amp;A sessions hosted by the AI Studio.</li> <li>\ud83c\udfc6 Participate in hackathons, challenges, and competitions to showcase your skills and win exciting prizes.</li> <li>\ud83d\udce3 Stay updated with the latest news, announcements, and events by following our Twitter and WeChat).</li> </ul>"},{"location":"en/FAQ.html","title":"FAQ","text":"<ol> <li> <p>Prediction error: got an unexpected keyword argument 'gradient_clip' The installed version of paddle is incorrect. Currently, this project only supports Paddle 1.7, which will be adapted to 1.8 in the near future.</p> </li> <li> <p>Error when converting attention recognition model: KeyError: 'predict' Solved. Please update to the latest version of the code.</p> </li> <li> <p>About inference speed When there are many words in the picture, the prediction time will increase. You can use <code>--rec_batch_num</code> to set a smaller prediction batch num. The default value is 30, which can be changed to 10 or other values.</p> </li> <li> <p>Service deployment and mobile deployment It is expected that the service deployment based on Serving and the mobile deployment based on Paddle Lite will be released successively in mid-to-late June. Stay tuned for more updates.</p> </li> <li> <p>Release time of self-developed algorithm Baidu Self-developed algorithms such as SAST, SRN and end2end PSL will be released in June or July. Please be patient.</p> </li> <li> <p>How to run on Windows or Mac? PaddleOCR has completed the adaptation to Windows and MAC systems. Two points should be noted during operation:</p> <ol> <li>In Quick installation, if you do not want to install docker, you can skip the first step and start with the second step.</li> <li>When downloading the inference model, if wget is not installed, you can directly click the model link or copy the link address to the browser to download, then extract and place it in the corresponding directory.</li> </ol> </li> <li> <p>The difference between ultra-lightweight model and General OCR model At present, PaddleOCR has opensourced two Chinese models, namely 8.6M ultra-lightweight Chinese model and general Chinese OCR model. The comparison information between the two is as follows:</p> <ul> <li>Similarities: Both use the same algorithm and training data\uff1b</li> <li>Differences: The difference lies in backbone network and channel parameters, the ultra-lightweight model uses MobileNetV3 as the backbone network, the general model uses Resnet50_vd as the detection model backbone, and Resnet34_vd as the recognition model backbone. You can compare the two model training configuration files to see the differences in parameters.</li> </ul> </li> </ol> Model Backbone Detection configuration file Recognition configuration file 8.6M ultra-lightweight Chinese OCR model MobileNetV3+MobileNetV3 det_mv3_db.yml rec_chinese_lite_train.yml General Chinese OCR model Resnet50_vd+Resnet34_vd det_r50_vd_db.yml rec_chinese_common_train.yml <ol> <li> <p>Is there a plan to opensource a model that only recognizes numbers or only English + numbers? It is not planned to opensource numbers only, numbers + English only, or other vertical text models. PaddleOCR has opensourced a variety of detection and recognition algorithms for customized training. The two Chinese models are also based on the training output of the open-source algorithm library. You can prepare the data according to the tutorial, choose the appropriate configuration file, train yourselves, and we believe that you can get good result. If you have any questions during the training, you are welcome to open issues or ask in the communication group. We will answer them in time.</p> </li> <li> <p>What is the training data used by the open-source model? Can it be opensourced? At present, the open source model, dataset and magnitude are as follows:</p> <ul> <li>Detection: English dataset: ICDAR2015 Chinese dataset: LSVT street view dataset with 3w pictures</li> <li>Recognition: English dataset: MJSynth and SynthText synthetic dataset, the amount of data is tens of millions. Chinese dataset: LSVT street view dataset with cropped text area, a total of 30w images. In addition, the synthesized data based on LSVT corpus is 500w.</li> </ul> <p>Among them, the public datasets are opensourced, users can search and download by themselves, or refer to Chinese data set, synthetic data is not opensourced, users can use open-source synthesis tools to synthesize data themselves. Current available synthesis tools include text_renderer, SynthText, TextRecognitionDataGenerator, etc.</p> </li> <li> <p>Error in using the model with TPS module for prediction Error message: Input(X) dims[3] and Input(Grid) dims[2] should be equal, but received X dimension[3](108) != Grid dimension[2](100) Solution: TPS does not support variable shape. Please set --rec_image_shape='3,32,100' and --rec_char_type='en'</p> </li> <li> <p>Custom dictionary used during training, the recognition results show that words do not appear in the dictionary The used custom dictionary path is not set when making prediction. The solution is setting parameter <code>rec_char_dict_path</code> to the corresponding dictionary file.</p> </li> <li> <p>Results of cpp_infer and python_inference are very different Versions of exported inference model and inference library should be same. For example, on Windows platform, version of the inference library that PaddlePaddle provides is 1.8, but version of the inference model that PaddleOCR provides is 1.7, you should export model yourself(<code>tools/export_model.py</code>) on PaddlePaddle 1.8 and then use the exported model for inference.</p> </li> <li> <p>How to identify artistic fonts in signs or advertising images Recognizing artistic fonts in signs or advertising images is a very challenging task because the variation in individual characters is much greater compared to standard fonts. If the artistic font to be identified is within a dictionary list, each word in the dictionary can be treated as a template for recognition using a general image retrieval system. You can try using PaddleClas image recognition system.</p> </li> </ol>"},{"location":"en/quick_start.html","title":"Quick Start","text":""},{"location":"en/quick_start.html#installation","title":"Installation","text":""},{"location":"en/quick_start.html#1-install-paddlepaddle","title":"1. Install PaddlePaddle","text":"<p>Installation for CPU:</p> <pre><code>python -m pip install paddlepaddle==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cpu/\n</code></pre> <p>Installation for GPU: </p> <p>Since GPU installation requires specific CUDA versions, the following example is for installing NVIDIA GPU on the Linux platform with CUDA 11.8. For other platforms, please refer to the instructions in the PaddlePaddle official installation documentation.</p> <pre><code>python -m pip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu118/\n</code></pre>"},{"location":"en/quick_start.html#2-install-paddleocr","title":"2. Install <code>paddleocr</code>","text":"<pre><code>python -m pip install paddleocr\n</code></pre>"},{"location":"en/quick_start.html#command-line-usage","title":"Command Line Usage","text":"PP-OCRv5PP-OCRv5 Text Detection ModulePP-OCRv5 Text Recognition ModulePP-StructureV3 <pre><code>paddleocr ocr -i ./general_ocr_002.png  --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False\n</code></pre> <pre><code>paddleocr text_detection -i ./general_ocr_001.png\n</code></pre> <pre><code>paddleocr text_recognition -i ./general_ocr_rec_001.png\n</code></pre> <pre><code>paddleocr pp_structurev3 -i ./pp_structure_v3_demo.png  --use_doc_orientation_classify False --use_doc_unwarping False\n</code></pre>"},{"location":"en/quick_start.html#python-script-usage","title":"Python Script Usage","text":"PP-OCRv5PP-OCRv5 Text Detection ModulePP-OCRv5 Text Recognition ModulePP-StructureV3 <pre><code>from paddleocr import PaddleOCR\n\nocr = PaddleOCR(\n    use_doc_orientation_classify=False, \n    use_doc_unwarping=False, \n    use_textline_orientation=False) # text detection + text recognition\n# ocr = PaddleOCR(use_doc_orientation_classify=True, use_doc_unwarping=True) # text image preprocessing + text detection + textline orientation classification + text recognition\n# ocr = PaddleOCR(use_doc_orientation_classify=False, use_doc_unwarping=False) # text detection + textline orientation classification + text recognition\n# ocr = PaddleOCR(\n#     text_detection_model_name=\"PP-OCRv5_mobile_det\",\n#     text_recognition_model_name=\"PP-OCRv5_mobile_rec\",\n#     use_doc_orientation_classify=False,\n#     use_doc_unwarping=False,\n#     use_textline_orientation=False) # Switch to PP-OCRv5_mobile models\nresult = ocr.predict(\"./general_ocr_002.png\")\nfor res in result:\n    res.print()\n    res.save_to_img(\"output\")\n    res.save_to_json(\"output\")\n</code></pre> <p>Example output:</p> <pre><code>{'res': {'input_path': './general_ocr_002.png', 'page_index': None, 'model_settings': {'use_doc_preprocessor': True, 'use_textline_orientation': False}, 'doc_preprocessor_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_orientation_classify': False, 'use_doc_unwarping': False}, 'angle': -1}, 'dt_polys': array([[[  1,   4],\n    ...,\n    [  1,  33]],\n\n   ...,\n\n   [[ 99, 455],\n    ...,\n    [ 99, 480]]], dtype=int16), 'text_det_params': {'limit_side_len': 960, 'limit_type': 'max', 'thresh': 0.3, 'max_side_limit': 4000, 'box_thresh': 0.6, 'unclip_ratio': 1.5}, 'text_type': 'general', 'textline_orientation_angles': array([-1, ..., -1]), 'text_rec_score_thresh': 0.0, 'rec_texts': ['www.997788.com', '\u200b\u767b\u673a\u724c\u200b', 'BOARDING PASS', '\u200b\u8231\u4f4d\u200bCLASS', '\u200b\u5e8f\u53f7\u200b SERIAL NO.', '\u200b\u5ea7\u4f4d\u53f7\u200b', 'SEAT NO', '\u200b\u822a\u73ed\u200bFLIGHT', '\u200b\u65e5\u671f\u200b', 'DATE', 'MU 2379', '03DEC', 'W', '035', '', '\u200b\u59cb\u53d1\u5730\u200b', 'FROM', '\u200b\u767b\u673a\u53e3\u200b', 'GATE', '\u200b\u767b\u673a\u200b\u65f6\u95f4\u200bBDT', '\u200b\u76ee\u7684\u5730\u200bTO', '\u200b\u798f\u5dde\u200b', 'TAIYUAN', 'G11', 'FUZHOU', '\u200b\u8eab\u4efd\u200b\u8bc6\u522b\u200bIDNO.', '\u200b\u59d3\u540d\u200bNAME', 'ZHANGQIWEI', '\u200b\u7968\u53f7\u200bTKTNO.', '\u200b\u5f20\u797a\u4f1f\u200b', '\u200b\u7968\u4ef7\u200bFARE', 'ETKT7813699238489/1', '\u200b\u767b\u673a\u53e3\u200b\u4e8e\u200b\u8d77\u98de\u524d\u200b10\u200b\u5206\u949f\u200b\u5173\u95ed\u200b GATESCL0SE10MINUTESBEFOREDEPARTURETIME'], 'rec_scores': array([0.99684608, ..., 0.97179604]), 'rec_polys': array([[[  1,   4],\n    ...,\n    [  1,  33]],\n\n   ...,\n\n   [[ 99, 455],\n    ...,\n    [ 99, 480]]], dtype=int16), 'rec_boxes': array([[  1, ...,  33],\n   ...,\n   [ 99, ..., 480]], dtype=int16)}}\n</code></pre> <pre><code>from paddleocr import TextDetection\n\nmodel = TextDetection()\noutput = model.predict(\"general_ocr_001.png\")\nfor res in output:\n    res.print()\n    res.save_to_img(save_path=\"./output/\")\n    res.save_to_json(save_path=\"./output/res.json\")\n</code></pre> <pre><code>{'res': {'input_path': 'general_ocr_001.png', 'page_index': None, 'dt_polys': array([[[ 77, 551],\n    ...,\n    [ 78, 587]],\n\n   ...,\n\n   [[ 34, 408],\n    ...,\n    [ 36, 456]]], dtype=int16), 'dt_scores': [0.8562385635646694, 0.8818259002228059, 0.8406072284043453, 0.8855339313157491]}}\n</code></pre> <pre><code>from paddleocr import TextRecognition\n\nmodel = TextRecognition()\noutput = model.predict(input=\"general_ocr_rec_001.png\")\nfor res in output:\n    res.print()\n    res.save_to_img(save_path=\"./output/\")\n    res.save_to_json(save_path=\"./output/res.json\")\n</code></pre> <p>Example output:</p> <pre><code>{'res': {'input_path': 'general_ocr_rec_001.png', 'page_index': None, 'rec_text': '\u200b\u7eff\u6d32\u200b\u4ed5\u683c\u7ef4\u200b\u82b1\u56ed\u200b\u516c\u5bd3\u200b', 'rec_score': 0.990813672542572}}\n</code></pre> <pre><code>from paddleocr import PPStructureV3\n\npipeline = PPStructureV3(\n    use_doc_orientation_classify=False,\n    use_doc_unwarping=False\n)\noutput = pipeline.predict(\n    input=\"./pp_structure_v3_demo.png\")\nfor res in output:\n    res.print()\n    res.save_to_json(save_path=\"output\")\n    res.save_to_markdown(save_path=\"output\")\n</code></pre> <p>Example output:</p> <pre><code>{'res': {'input_path': './pp_structure_v3_demo.png', 'page_index': None, 'model_settings': {'use_doc_preprocessor': False, 'use_seal_recognition': True, 'use_table_recognition': True, 'use_formula_recognition': True, 'use_chart_recognition': False, 'use_region_detection': True}, 'layout_det_res': {'input_path': None, 'page_index': None, 'boxes': [{'cls_id': 1, 'label': 'image', 'score': 0.9864752888679504, 'coordinate': [774.821, 201.05177, 1502.1008, 685.7733]}, {'cls_id': 2, 'label': 'text', 'score': 0.9859225749969482, 'coordinate': [769.8655, 776.2446, 1121.5986, 1058.417]}, {'cls_id': 2, 'label': 'text', 'score': 0.9857110381126404, 'coordinate': [1151.98, 1112.5356, 1502.7852, 1346.3569]}, {'cls_id': 2, 'label': 'text', 'score': 0.9847239255905151, 'coordinate': [389.0322, 1136.3547, 740.2322, 1345.928]}, {'cls_id': 2, 'label': 'text', 'score': 0.9842492938041687, 'coordinate': [1152.1504, 800.1625, 1502.1265, 986.1522]}, {'cls_id': 2, 'label': 'text', 'score': 0.9840831160545349, 'coordinate': [9.158066, 848.8696, 358.5725, 1057.832]}, {'cls_id': 2, 'label': 'text', 'score': 0.9802583456039429, 'coordinate': [9.335953, 201.10046, 358.31543, 338.78876]}, {'cls_id': 2, 'label': 'text', 'score': 0.9801402688026428, 'coordinate': [389.1556, 297.4113, 740.07556, 435.41647]}, {'cls_id': 2, 'label': 'text', 'score': 0.9793564081192017, 'coordinate': [389.18976, 752.0959, 740.0832, 889.88043]}, {'cls_id': 2, 'label': 'text', 'score': 0.9793409109115601, 'coordinate': [389.02496, 896.34143, 740.7431, 1033.9465]}, {'cls_id': 2, 'label': 'text', 'score': 0.9776486754417419, 'coordinate': [8.950775, 1184.7842, 358.75067, 1297.8755]}, {'cls_id': 2, 'label': 'text', 'score': 0.9773538708686829, 'coordinate': [770.7178, 1064.5714, 1121.2249, 1177.9928]}, {'cls_id': 2, 'label': 'text', 'score': 0.9773064255714417, 'coordinate': [389.38086, 609.7071, 740.0553, 745.3206]}, {'cls_id': 2, 'label': 'text', 'score': 0.9765821099281311, 'coordinate': [1152.0112, 992.296, 1502.4927, 1106.1166]}, {'cls_id': 2, 'label': 'text', 'score': 0.9761461019515991, 'coordinate': [9.46727, 536.993, 358.2047, 651.32025]}, {'cls_id': 2, 'label': 'text', 'score': 0.975399911403656, 'coordinate': [9.353531, 1064.3059, 358.45312, 1177.8347]}, {'cls_id': 2, 'label': 'text', 'score': 0.9730532169342041, 'coordinate': [9.932312, 345.36237, 358.03476, 435.1646]}, {'cls_id': 2, 'label': 'text', 'score': 0.9722575545310974, 'coordinate': [388.91736, 200.93637, 740.00793, 290.80692]}, {'cls_id': 2, 'label': 'text', 'score': 0.9710633158683777, 'coordinate': [389.39496, 1040.3186, 740.0091, 1129.7168]}, {'cls_id': 2, 'label': 'text', 'score': 0.9696939587593079, 'coordinate': [9.6145935, 658.1123, 359.06088, 770.0288]}, {'cls_id': 2, 'label': 'text', 'score': 0.9664146900177002, 'coordinate': [770.235, 1280.4562, 1122.0927, 1346.4742]}, {'cls_id': 2, 'label': 'text', 'score': 0.9597565531730652, 'coordinate': [389.66678, 537.5609, 740.06274, 603.17725]}, {'cls_id': 2, 'label': 'text', 'score': 0.9594324827194214, 'coordinate': [10.162949, 776.86414, 359.08307, 842.1771]}, {'cls_id': 2, 'label': 'text', 'score': 0.9484634399414062, 'coordinate': [10.402863, 1304.7743, 358.9441, 1346.3749]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.9476125240325928, 'coordinate': [28.159409, 456.7627, 339.5631, 514.9665]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.9427680969238281, 'coordinate': [790.6992, 1200.3663, 1102.3799, 1259.1647]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.9424256682395935, 'coordinate': [409.02832, 456.6831, 718.8154, 515.5757]}, {'cls_id': 10, 'label': 'doc_title', 'score': 0.9376171827316284, 'coordinate': [133.77905, 36.8844, 1379.6667, 123.46869]}, {'cls_id': 2, 'label': 'text', 'score': 0.9020252823829651, 'coordinate': [584.9165, 159.1416, 927.22876, 179.01605]}, {'cls_id': 2, 'label': 'text', 'score': 0.895164430141449, 'coordinate': [1154.3364, 776.74646, 1331.8564, 794.2301]}, {'cls_id': 6, 'label': 'figure_title', 'score': 0.7892374396324158, 'coordinate': [808.9641, 704.2555, 1484.0623, 747.2296]}]}, 'overall_ocr_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_preprocessor': False, 'use_textline_orientation': False}, 'dt_polys': array([[[ 129,   42],\n        ...,\n        [ 129,  140]],\n\n    ...,\n\n    [[1156, 1330],\n        ...,\n        [1156, 1351]]], dtype=int16), 'text_det_params': {'limit_side_len': 736, 'limit_type': 'min', 'thresh': 0.3, 'max_side_limit': 4000, 'box_thresh': 0.6, 'unclip_ratio': 1.5}, 'text_type': 'general', 'textline_orientation_angles': array([-1, ..., -1]), 'text_rec_score_thresh': 0.0, 'rec_texts': ['\u200b\u52a9\u529b\u200b\u53cc\u65b9\u200b\u4ea4\u5f80\u200b', '\u200b\u642d\u5efa\u200b\u53cb\u8c0a\u200b\u6865\u6881\u200b', '\u200b\u672c\u62a5\u8bb0\u8005\u200b\u6c88\u5c0f\u6653\u200b', '\u200b\u4efb\u200b', '\u200b\u5f66\u200b', '\u200b\u9ec4\u57f9\u662d\u200b', '\u200b\u8eab\u7740\u200b\u4e2d\u56fd\u200b\u4f20\u7edf\u200b\u6c11\u65cf\u670d\u88c5\u200b\u7684\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u9752\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u9ad8\u7b49\u6559\u80b2\u200b\u4e0e\u200b\u7814\u7a76\u9662\u200b\u5408\u4f5c\u200b\u5efa\u7acb\u200b\uff0c\u200b\u5f00\u200b', '\u200b\u5e74\u200b\u4f9d\u6b21\u200b\u767b\u53f0\u200b\u8868\u6f14\u200b\u4e2d\u56fd\u200b\u6c11\u65cf\u821e\u200b\u3001\u200b\u73b0\u4ee3\u821e\u200b\u3001\u200b\u6247\u5b50\u821e\u200b', '\u200b\u8bbe\u200b\u4e86\u200b\u4e2d\u56fd\u200b\u8bed\u8a00\u200b\u8bfe\u7a0b\u200b\u548c\u200b\u4e2d\u56fd\u200b\u6587\u5316\u200b\u8bfe\u7a0b\u200b\uff0c\u200b\u6ce8\u518c\u200b\u5b66\u200b', '\u200b\u7b49\u200b\uff0c\u200b\u66fc\u5999\u200b\u7684\u200b\u821e\u59ff\u200b\u8d62\u5f97\u200b\u73b0\u573a\u200b\u89c2\u4f17\u200b\u9635\u9635\u200b\u638c\u58f0\u200b\u3002\u200b\u8fd9\u200b', '\u200b\u751f\u200b2\u200b\u4e07\u4f59\u4eba\u6b21\u200b\u300210\u200b\u4f59\u5e74\u200b\u6765\u200b\uff0c\u200b\u5384\u7279\u200b\u5b54\u9662\u200b\u5df2\u200b\u6210\u4e3a\u200b', '\u200b\u662f\u200b\u65e5\u524d\u200b\u5371\u7acb\u200b\u7279\u91cc\u4e9a\u200b\u9ad8\u7b49\u6559\u80b2\u200b\u4e0e\u200b\u7814\u7a76\u9662\u200b\u5b54\u5b50\u200b\u5b66\u200b', '\u200b\u5f53\u5730\u200b\u6c11\u4f17\u200b\u4e86\u89e3\u200b\u4e2d\u56fd\u200b\u7684\u200b\u4e00\u6247\u200b\u7a97\u53e3\u200b\u3002', '\u200b\u9662\u200b(\u200b\u4ee5\u4e0b\u200b\u7b80\u79f0\u200b\u201c\u200b\u5384\u7279\u200b\u5b54\u9662\u200b\")\u200b\u4e3e\u529e\u200b\u201c\u200b\u559c\u8fce\u200b\u65b0\u5e74\u200b\"\u200b\u4e2d\u56fd\u200b', '\u200b\u9ec4\u9e23\u98de\u200b\u8868\u793a\u200b\uff0c\u200b\u968f\u7740\u200b\u6765\u200b\u5b66\u4e60\u200b\u4e2d\u6587\u200b\u7684\u200b\u4eba\u200b\u65e5\u76ca\u200b', '\u200b\u6b4c\u821e\u200b\u6bd4\u8d5b\u200b\u7684\u200b\u573a\u666f\u200b\u3002', '\u200b\u589e\u591a\u200b\uff0c\u200b\u963f\u65af\u9a6c\u62c9\u200b\u5927\u5b66\u200b\u6559\u5b66\u70b9\u200b\u5df2\u200b\u96be\u4ee5\u200b\u6ee1\u8db3\u200b\u6559\u5b66\u200b', '\u200b\u4e2d\u56fd\u200b\u548c\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u4f20\u7edf\u53cb\u8c0a\u200b\u6df1\u539a\u200b\u3002\u200b\u8fd1\u5e74\u200b', '\u200b\u9700\u8981\u200b\u30022024\u200b\u5e74\u200b4\u200b\u6708\u200b\uff0c\u200b\u7531\u200b\u4e2d\u4f01\u200b\u8700\u9053\u200b\u96c6\u56e2\u200b\u6240\u5c5e\u200b\u56db\u200b', '\u200b\u6765\u200b\uff0c\u200b\u5728\u200b\u9ad8\u8d28\u91cf\u200b\u5171\u5efa\u200b\u201c\u200b\u4e00\u5e26\u200b\u4e00\u8def\u200b\u201d\u200b\u6846\u67b6\u200b\u4e0b\u200b\uff0c\u200b\u4e2d\u200b\u5384\u200b\u4e24\u200b', '\u200b\u5ddd\u200b\u8def\u6865\u200b\u627f\u5efa\u200b\u7684\u200b\u5b54\u9662\u200b\u6559\u5b66\u697c\u200b\u9879\u76ee\u200b\u5728\u200b\u963f\u65af\u9a6c\u62c9\u200b\u5f00\u200b', '\u200b\u56fd\u200b\u4eba\u6587\u200b\u4ea4\u6d41\u200b\u4e0d\u65ad\u200b\u6df1\u5316\u200b\uff0c\u200b\u4e92\u5229\u200b\u5408\u4f5c\u200b\u7684\u200b\u6c11\u610f\u57fa\u7840\u200b', '\u200b\u5de5\u200b\u5efa\u8bbe\u200b\uff0c\u200b\u9884\u8ba1\u200b\u4eca\u5e74\u200b\u4e0a\u534a\u5e74\u200b\u7ae3\u5de5\u200b\uff0c\u200b\u5efa\u6210\u200b\u540e\u200b\u5c06\u200b\u4e3a\u200b\u5371\u200b', '\u200b\u65e5\u76ca\u200b\u6df1\u539a\u200b\u3002', '\u200b\u7279\u5b54\u9662\u200b\u63d0\u4f9b\u200b\u5168\u65b0\u200b\u7684\u200b\u529e\u5b66\u200b\u573a\u5730\u200b\u3002', '\u201c\u200b\u5b66\u597d\u200b\u4e2d\u6587\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b', '\u201c\u200b\u5728\u200b\u4e2d\u56fd\u200b\u5b66\u4e60\u200b\u7684\u200b\u7ecf\u5386\u200b', '\u200b\u672a\u6765\u200b\u4e0d\u662f\u200b\u68a6\u200b\u201d', '\u200b\u8ba9\u200b\u6211\u200b\u770b\u5230\u200b\u66f4\u200b\u5e7f\u9614\u200b\u7684\u200b\u4e16\u754c\u200b\u201d', '\u201c\u200b\u9c9c\u82b1\u200b\u66fe\u200b\u544a\u8bc9\u200b\u6211\u200b\u4f60\u200b\u600e\u6837\u200b\u8d70\u8fc7\u200b\uff0c\u200b\u5927\u5730\u200b\u77e5\u9053\u200b\u4f60\u200b', '\u200b\u591a\u5e74\u200b\u6765\u200b\uff0c\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5e7f\u5927\u200b\u8d74\u534e\u200b\u7559\u5b66\u751f\u200b\u548c\u200b', '\u200b\u5fc3\u4e2d\u200b\u7684\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u89d2\u843d\u200b\u2026\u2026\"\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u963f\u65af\u9a6c\u62c9\u200b', '\u200b\u57f9\u8bad\u200b\u4eba\u5458\u200b\u79ef\u6781\u200b\u6295\u8eab\u200b\u56fd\u5bb6\u200b\u5efa\u8bbe\u200b\uff0c\u200b\u6210\u4e3a\u200b\u52a9\u529b\u200b\u8be5\u56fd\u200b', '\u200b\u5927\u5b66\u200b\u7efc\u5408\u697c\u200b\u4e8c\u5c42\u200b\uff0c\u200b\u4e00\u9635\u200b\u4f18\u7f8e\u200b\u7684\u200b\u6b4c\u58f0\u200b\u5728\u200b\u8d70\u5eca\u200b\u91cc\u200b\u56de\u200b', '\u200b\u53d1\u5c55\u200b\u7684\u200b\u4eba\u624d\u200b\u548c\u200b\u5384\u200b\u4e2d\u200b\u53cb\u597d\u200b\u7684\u200b\u89c1\u8bc1\u8005\u200b\u548c\u200b\u63a8\u52a8\u8005\u200b\u3002', '\u200b\u54cd\u200b\u3002\u200b\u5faa\u7740\u200b\u719f\u6089\u200b\u7684\u200b\u65cb\u5f8b\u200b\u8f7b\u8f7b\u200b\u63a8\u5f00\u200b\u4e00\u95f4\u200b\u6559\u5ba4\u200b\u7684\u200b\u95e8\u200b\uff0c', '\u200b\u5728\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5168\u56fd\u200b\u5987\u5973\u200b\u8054\u76df\u200b\u5de5\u4f5c\u200b\u7684\u200b\u7ea6\u7ff0\u200b', '\u200b\u5b66\u751f\u200b\u4eec\u200b\u6b63\u200b\u8ddf\u7740\u200b\u8001\u5e08\u200b\u5b66\u5531\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\u300a\u200b\u540c\u4e00\u9996\u6b4c\u200b\u300b\u3002', '\u200b\u5a1c\u200b\u00b7\u200b\u7279\u200b\u97e6\u5c14\u5fb7\u200b\u00b7\u200b\u51ef\u83b1\u200b\u5854\u200b\u5c31\u662f\u200b\u5176\u4e2d\u200b\u4e00\u4f4d\u200b\u3002\u200b\u5979\u200b\u66fe\u200b\u5728\u200b', '\u200b\u8fd9\u200b\u662f\u200b\u5384\u200b\u7279\u5b54\u9662\u200b\u963f\u65af\u9a6c\u62c9\u200b\u5927\u5b66\u200b\u6559\u5b66\u70b9\u200b\u7684\u200b\u4e00\u200b', '\u200b\u4e2d\u534e\u200b\u5973\u5b50\u200b\u5b66\u9662\u200b\u653b\u8bfb\u200b\u7855\u58eb\u5b66\u4f4d\u200b\uff0c\u200b\u7814\u7a76\u200b\u65b9\u5411\u200b\u662f\u200b\u5973\u200b', '\u200b\u8282\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\u8bfe\u200b\u3002\u200b\u4e3a\u4e86\u200b\u8ba9\u200b\u5b66\u751f\u200b\u4eec\u200b\u66f4\u597d\u200b\u5730\u200b\u7406\u89e3\u200b\u6b4c\u200b', '\u200b\u6027\u200b\u9886\u5bfc\u529b\u200b\u4e0e\u200b\u793e\u4f1a\u200b\u53d1\u5c55\u200b\u3002\u200b\u5176\u95f4\u200b\uff0c\u200b\u5979\u200b\u5b9e\u5730\u200b\u8d70\u8bbf\u200b\u4e2d\u56fd\u200b', '\u200b\u8bcd\u200b\u5927\u610f\u200b\uff0c\u200b\u8001\u5e08\u200b\u5c24\u200b\u65af\u62c9\u200b\u00b7\u200b\u7a46\u7f55\u9ed8\u5fb7\u200b\u8428\u5c14\u200b\u00b7\u200b\u4faf\u8d5b\u56e0\u200b\u9010\u200b', '\u200b\u591a\u4e2a\u200b\u5730\u533a\u200b\uff0c\u200b\u83b7\u5f97\u200b\u4e86\u200b\u89c2\u5bdf\u200b\u4e2d\u56fd\u200b\u793e\u4f1a\u200b\u53d1\u5c55\u200b\u7684\u200b\u7b2c\u4e00\u200b', '\u200b\u5728\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u4e0d\u4e45\u524d\u200b\u4e3e\u529e\u200b\u7684\u200b\u7b2c\u516d\u5c4a\u200b\u4e2d\u56fd\u200b\u98ce\u7b5d\u200b\u6587\u5316\u8282\u200b\u4e0a\u200b\uff0c\u200b\u5f53\u5730\u200b\u5c0f\u5b66\u751f\u200b\u4f53\u9a8c\u200b\u98ce\u7b5d\u200b\u5236\u4f5c\u200b\u3002', '\u200b\u5b57\u200b\u7ffb\u8bd1\u200b\u548c\u200b\u89e3\u91ca\u200b\u6b4c\u8bcd\u200b\u3002\u200b\u968f\u7740\u200b\u4f34\u594f\u200b\u58f0\u54cd\u200b\u8d77\u200b\uff0c\u200b\u5b66\u751f\u200b\u4eec\u200b', '\u200b\u624b\u200b\u8d44\u6599\u200b\u3002', '\u200b\u4e2d\u56fd\u200b\u9a7b\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5927\u4f7f\u9986\u200b\u4f9b\u56fe\u200b', '\u200b\u8fb9\u5531\u8fb9\u200b\u968f\u7740\u200b\u8282\u62cd\u200b\u6447\u52a8\u200b\u8eab\u4f53\u200b\uff0c\u200b\u73b0\u573a\u200b\u6c14\u6c1b\u200b\u70ed\u70c8\u200b\u3002', '\u200b\u8c08\u8d77\u200b\u5728\u200b\u4e2d\u56fd\u200b\u6c42\u5b66\u200b\u7684\u200b\u7ecf\u5386\u200b\uff0c\u200b\u7ea6\u7ff0\u200b\u5a1c\u200b\u8bb0\u5fc6\u200b\u72b9\u200b', '\u201c\u200b\u8fd9\u662f\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\u521d\u7ea7\u73ed\u200b\uff0c\u200b\u5171\u6709\u200b32\u200b\u4eba\u200b\u3002\u200b\u5b66\u200b', '\u200b\u65b0\u200b\uff1a\u201c\u200b\u4e2d\u56fd\u200b\u7684\u200b\u53d1\u5c55\u200b\u5728\u200b\u5f53\u4eca\u4e16\u754c\u200b\u662f\u200b\u72ec\u4e00\u65e0\u4e8c\u200b\u7684\u200b\u3002', '\u201c\u200b\u4e0d\u7ba1\u200b\u8fdc\u8fd1\u200b\u90fd\u200b\u662f\u200b\u5ba2\u4eba\u200b\uff0c\u200b\u8bf7\u200b\u4e0d\u7528\u200b\u5ba2\u6c14\u200b\uff1b\u200b\u76f8\u7ea6\u200b', '\u200b\u74e6\u200b\u7684\u200b\u5317\u200b\u7ea2\u6d77\u200b\u7701\u200b\u535a\u7269\u9986\u200b\u3002', '\u200b\u751f\u200b\u5927\u90e8\u5206\u200b\u6765\u81ea\u200b\u9996\u90fd\u200b\u963f\u65af\u9a6c\u62c9\u200b\u7684\u200b\u4e2d\u5c0f\u5b66\u200b\uff0c\u200b\u5e74\u9f84\u200b', '\u200b\u6cbf\u7740\u200b\u4e2d\u56fd\u200b\u7279\u8272\u200b\u793e\u4f1a\u4e3b\u4e49\u200b\u9053\u8def\u200b\u575a\u5b9a\u200b\u524d\u884c\u200b\uff0c\u200b\u4e2d\u56fd\u200b', '\u200b\u597d\u200b\u4e86\u200b\u5728\u200b\u4e00\u8d77\u200b\uff0c\u200b\u6211\u4eec\u200b\u6b22\u8fce\u200b\u4f60\u200b\u2026\u2026\u201d\u200b\u5728\u200b\u4e00\u573a\u200b\u4e2d\u5384\u9752\u200b', '\u200b\u535a\u7269\u9986\u200b\u4e8c\u5c42\u200b\u9648\u5217\u200b\u7740\u200b\u4e00\u4e2a\u200b\u53d1\u6398\u200b\u81ea\u963f\u675c\u5229\u200b', '\u200b\u6700\u5c0f\u200b\u7684\u200b\u4ec5\u200b\u6709\u200b6\u200b\u5c81\u200b\u3002\u201d\u200b\u5c24\u200b\u65af\u62c9\u200b\u544a\u8bc9\u200b\u8bb0\u8005\u200b\u3002', '\u200b\u521b\u9020\u200b\u4e86\u200b\u53d1\u5c55\u200b\u5947\u8ff9\u200b\uff0c\u200b\u8fd9\u200b\u4e00\u5207\u200b\u90fd\u200b\u79bb\u4e0d\u5f00\u200b\u4e2d\u56fd\u5171\u4ea7\u515a\u200b', '\u200b\u5e74\u200b\u8054\u8c0a\u200b\u6d3b\u52a8\u200b\u4e0a\u200b\uff0c\u200b\u56db\u5ddd\u200b\u8def\u6865\u200b\u4e2d\u65b9\u200b\u5458\u5de5\u200b\u540c\u200b\u5f53\u5730\u200b\u5927\u200b', '\u200b\u65af\u200b\u53e4\u57ce\u200b\u7684\u200b\u4e2d\u56fd\u200b\u53e4\u4ee3\u200b\u9676\u5236\u200b\u9152\u5668\u200b\uff0c\u200b\u7f50\u200b\u8eab\u4e0a\u200b\u5199\u200b\u7740\u200b', '\u200b\u5c24\u200b\u65af\u62c9\u200b\u4eca\u5e74\u200b23\u200b\u5c81\u200b\uff0c\u200b\u662f\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u4e00\u6240\u200b\u516c\u7acb\u200b', '\u200b\u7684\u200b\u9886\u5bfc\u200b\u3002\u200b\u4e2d\u56fd\u200b\u7684\u200b\u53d1\u5c55\u200b\u7ecf\u9a8c\u200b\u503c\u5f97\u200b\u8bb8\u591a\u200b\u56fd\u5bb6\u200b\u5b66\u4e60\u200b', '\u200b\u5b66\u751f\u200b\u5408\u5531\u200b\u300a\u200b\u5317\u4eac\u200b\u6b22\u8fce\u200b\u4f60\u200b\u300b\u3002\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u6280\u672f\u200b\u5b66\u200b', '\u201c\u200b\u4e07\u200b\u201d\u201c\u200b\u548c\u200b\u201d\u201c\u200b\u7985\u200b\u201d\u201c\u200b\u5c71\u200b\u201d\u200b\u7b49\u200b\u6c49\u5b57\u200b\u3002\u201c\u200b\u8fd9\u4ef6\u200b\u6587\u7269\u200b\u8bc1\u200b', '\u200b\u5b66\u6821\u200b\u7684\u200b\u827a\u672f\u200b\u8001\u5e08\u200b\u3002\u200b\u5979\u200b12\u200b\u5c81\u200b\u5f00\u59cb\u200b\u5728\u200b\u5384\u200b\u7279\u5b54\u200b\u9662\u5b66\u200b', '\u200b\u501f\u9274\u200b\u3002\u201d', '\u200b\u9662\u200b\u8ba1\u7b97\u673a\u79d1\u5b66\u200b\u4e0e\u200b\u5de5\u7a0b\u200b\u4e13\u4e1a\u200b\u5b66\u751f\u200b\u9c81\u592b\u5854\u200b\u00b7\u200b\u8c22\u62c9\u200b', '\u200b\u660e\u200b\uff0c\u200b\u5f88\u65e9\u4ee5\u524d\u200b\u6211\u4eec\u200b\u5c31\u200b\u901a\u8fc7\u200b\u6d77\u4e0a\u200b\u4e1d\u7ef8\u4e4b\u8def\u200b\u8fdb\u884c\u200b', '\u200b\u4e60\u200b\u4e2d\u6587\u200b\uff0c\u200b\u5728\u200b2017\u200b\u5e74\u200b\u7b2c\u5341\u5c4a\u200b\u201c\u200b\u6c49\u8bed\u200b\u6865\u200b\"\u200b\u4e16\u754c\u200b\u4e2d\u5b66\u751f\u200b', '\u200b\u6b63\u5728\u200b\u897f\u5357\u200b\u5927\u5b66\u200b\u5b66\u4e60\u200b\u7684\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u535a\u58eb\u751f\u200b', '\u200b\u662f\u200b\u5176\u4e2d\u200b\u4e00\u540d\u200b\u6f14\u5531\u8005\u200b\uff0c\u200b\u5979\u200b\u5f88\u65e9\u200b\u4fbf\u200b\u5728\u200b\u5b54\u9662\u200b\u5b66\u4e60\u200b\u4e2d\u200b', '\u200b\u8d38\u6613\u5f80\u6765\u200b\u4e0e\u200b\u6587\u5316\u4ea4\u6d41\u200b\u3002\u200b\u8fd9\u200b\u4e5f\u200b\u662f\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b', '\u200b\u4e2d\u6587\u200b\u6bd4\u8d5b\u200b\u4e2d\u200b\u83b7\u5f97\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u8d5b\u533a\u200b\u7b2c\u4e00\u540d\u200b\uff0c\u200b\u5e76\u200b\u548c\u200b', '\u200b\u7a46\u5362\u76d6\u200b\u5854\u200b\u00b7\u200b\u6cfd\u7a46\u200b\u4f0a\u5bf9\u200b\u4e2d\u56fd\u200b\u6000\u6709\u200b\u6df1\u539a\u611f\u60c5\u200b\u30028', '\u200b\u6587\u200b\uff0c\u200b\u4e00\u76f4\u200b\u5728\u200b\u4e3a\u200b\u53bb\u200b\u4e2d\u56fd\u200b\u7559\u5b66\u200b\u4f5c\u200b\u51c6\u5907\u200b\u3002\u201c\u200b\u8fd9\u53e5\u200b\u6b4c\u8bcd\u200b', '\u200b\u4e0e\u200b\u4e2d\u56fd\u200b\u53cb\u597d\u200b\u4ea4\u5f80\u200b\u5386\u53f2\u200b\u7684\u200b\u6709\u529b\u200b\u8bc1\u660e\u200b\u3002\u201d\u200b\u5317\u200b\u7ea2\u6d77\u200b', '\u200b\u540c\u4f34\u200b\u4ee3\u8868\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u524d\u5f80\u200b\u4e2d\u56fd\u200b\u53c2\u52a0\u200b\u51b3\u8d5b\u200b\uff0c\u200b\u83b7\u5f97\u200b', '\u200b\u5e74\u524d\u200b\uff0c\u200b\u5728\u200b\u5317\u4eac\u5e08\u8303\u5927\u5b66\u200b\u83b7\u5f97\u200b\u7855\u58eb\u5b66\u4f4d\u200b\u540e\u200b\uff0c\u200b\u7a46\u5362\u200b', '\u200b\u662f\u200b\u6211\u4eec\u200b\u4e24\u56fd\u4eba\u6c11\u200b\u53cb\u8c0a\u200b\u7684\u200b\u751f\u52a8\u200b\u5199\u7167\u200b\u3002\u200b\u65e0\u8bba\u662f\u200b\u6295\u200b', '\u200b\u7701\u200b\u535a\u7269\u9986\u200b\u7814\u7a76\u200b\u4e0e\u200b\u6587\u732e\u200b\u90e8\u200b\u8d1f\u8d23\u4eba\u200b\u4f0a\u8428\u200b\u4e9a\u65af\u200b\u00b7\u200b\u7279\u200b', '\u200b\u56e2\u4f53\u200b\u4f18\u80dc\u5956\u200b\u30022022\u200b\u5e74\u200b\u8d77\u200b\uff0c\u200b\u5c24\u200b\u65af\u62c9\u200b\u5f00\u59cb\u200b\u5728\u200b\u5384\u200b\u7279\u5b54\u200b', '\u200b\u76d6\u5854\u200b\u5728\u200b\u793e\u4ea4\u200b\u5a92\u4f53\u200b\u4e0a\u200b\u5199\u4e0b\u200b\u8fd9\u6837\u200b\u4e00\u6bb5\u8bdd\u200b\uff1a\u201c\u200b\u8fd9\u200b\u662f\u200b\u6211\u200b', '\u200b\u8eab\u4e8e\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u57fa\u7840\u8bbe\u65bd\u200b\u5efa\u8bbe\u200b\u7684\u200b\u4e2d\u4f01\u200b\u5458\u5de5\u200b\uff0c', '\u200b\u65af\u6cd5\u5179\u5409\u8bf4\u200b\u3002', '\u200b\u9662\u200b\u517c\u804c\u200b\u6559\u6388\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\uff0c\u200b\u6bcf\u200b\u5468\u672b\u200b\u4e24\u4e2a\u200b\u8bfe\u65f6\u200b\u3002\u201c\u200b\u4e2d\u56fd\u200b', '\u200b\u4eba\u751f\u200b\u7684\u200b\u91cd\u8981\u200b\u4e00\u6b65\u200b\uff0c\u200b\u81ea\u6b64\u200b\u6211\u200b\u62e5\u6709\u200b\u4e86\u200b\u4e00\u53cc\u200b\u575a\u56fa\u200b\u7684\u200b', '\u200b\u8fd8\u662f\u200b\u5728\u200b\u4e2d\u56fd\u200b\u7559\u5b66\u200b\u7684\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5b66\u5b50\u200b\uff0c\u200b\u4e24\u200b\u56fd\u4eba\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u56fd\u5bb6\u535a\u7269\u9986\u200b\u8003\u53e4\u5b66\u200b\u548c\u200b\u4eba\u7c7b\u5b66\u200b', '\u200b\u6587\u5316\u200b\u535a\u5927\u7cbe\u6df1\u200b\uff0c\u200b\u6211\u200b\u5e0c\u671b\u200b\u6211\u200b\u7684\u200b\u5b66\u751f\u200b\u4eec\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u4e2d\u200b', '\u200b\u978b\u5b50\u200b\uff0c\u200b\u8d4b\u4e88\u200b\u6211\u200b\u7a7f\u8d8a\u200b\u8346\u68d8\u200b\u7684\u200b\u529b\u91cf\u200b\u3002\u201d', '\u200b\u6c11\u200b\u643a\u624b\u200b\u52aa\u529b\u200b\uff0c\u200b\u5fc5\u5c06\u200b\u63a8\u52a8\u200b\u4e24\u56fd\u5173\u7cfb\u200b\u4e0d\u65ad\u200b\u5411\u524d\u200b\u53d1\u200b', '\u200b\u7814\u7a76\u5458\u200b\u83f2\u5c14\u200b\u8499\u200b\u00b7\u200b\u7279\u200b\u97e6\u5c14\u5fb7\u200b\u5341\u5206\u200b\u559c\u7231\u200b\u4e2d\u56fd\u200b\u6587\u200b', '\u200b\u6587\u200b\u6b4c\u66f2\u200b\u66f4\u597d\u200b\u5730\u200b\u7406\u89e3\u200b\u4e2d\u56fd\u200b\u6587\u5316\u200b\u3002\u201d\u200b\u5979\u200b\u8bf4\u200b\u3002', '\u200b\u7a46\u5362\u76d6\u200b\u5854\u200b\u5bc6\u5207\u200b\u5173\u6ce8\u200b\u4e2d\u56fd\u200b\u5728\u200b\u7ecf\u6d4e\u200b\u3001\u200b\u79d1\u6280\u200b\u3001\u200b\u6559\u200b', '\u200b\u5c55\u200b\u3002\u201d\u200b\u9c81\u592b\u5854\u200b\u8bf4\u200b\u3002', '\u200b\u5316\u200b\u3002\u200b\u4ed6\u200b\u8868\u793a\u200b\uff1a\u201c\u200b\u5b66\u4e60\u200b\u5f7c\u6b64\u200b\u7684\u200b\u8bed\u8a00\u200b\u548c\u200b\u6587\u5316\u200b\uff0c\u200b\u5c06\u200b\u5e2e\u200b', '\u201c\u200b\u59d0\u59d0\u200b\uff0c\u200b\u4f60\u200b\u60f3\u200b\u53bb\u200b\u4e2d\u56fd\u200b\u5417\u200b?\u201d\u201c\u200b\u975e\u5e38\u200b\u60f3\u200b\uff01\u200b\u6211\u200b\u60f3\u200b', '\u200b\u80b2\u200b\u7b49\u200b\u9886\u57df\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0c\u201c\u200b\u4e2d\u56fd\u200b\u5728\u200b\u79d1\u7814\u200b\u7b49\u200b\u65b9\u9762\u200b\u7684\u200b\u5b9e\u529b\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u9ad8\u7b49\u6559\u80b2\u200b\u59d4\u5458\u4f1a\u200b\u4e3b\u4efb\u200b\u52a9\u7406\u200b\u8428\u200b', '\u200b\u52a9\u5384\u4e2d\u200b\u4e24\u56fd\u4eba\u6c11\u200b\u66f4\u597d\u200b\u5730\u200b\u7406\u89e3\u200b\u5f7c\u6b64\u200b\uff0c\u200b\u52a9\u529b\u200b\u53cc\u65b9\u200b', '\u200b\u53bb\u200b\u770b\u200b\u6545\u5bab\u200b\u3001\u200b\u722c\u200b\u957f\u57ce\u200b\u3002\u201d\u200b\u5c24\u200b\u65af\u62c9\u200b\u7684\u200b\u5b66\u751f\u200b\u4e2d\u6709\u200b\u4e00\u5bf9\u200b', '\u200b\u4e0e\u65e5\u4ff1\u589e\u200b\u3002\u200b\u5728\u200b\u4e2d\u56fd\u200b\u5b66\u4e60\u200b\u7684\u200b\u7ecf\u5386\u200b\u8ba9\u200b\u6211\u200b\u770b\u5230\u200b\u66f4\u5e7f\u200b', '\u200b\u9a6c\u745e\u200b\u8868\u793a\u200b\uff1a\u201c\u200b\u6bcf\u5e74\u200b\u6211\u4eec\u200b\u90fd\u200b\u4f1a\u200b\u7ec4\u7ec7\u200b\u5b66\u751f\u200b\u5230\u200b\u4e2d\u56fd\u200b\u8bbf\u200b', '\u200b\u4ea4\u5f80\u200b\uff0c\u200b\u642d\u5efa\u200b\u53cb\u8c0a\u200b\u6865\u6881\u200b\u3002\u201d', '\u200b\u80fd\u6b4c\u5584\u821e\u200b\u7684\u200b\u59d0\u59b9\u200b\uff0c\u200b\u59d0\u59d0\u200b\u9732\u5a05\u200b\u4eca\u5e74\u200b15\u200b\u5c81\u200b\uff0c\u200b\u59b9\u59b9\u200b', '\u200b\u9614\u200b\u7684\u200b\u4e16\u754c\u200b\uff0c\u200b\u4ece\u4e2d\u200b\u53d7\u76ca\u532a\u6d45\u200b\u3002\u201d', '\u200b\u95ee\u200b\u5b66\u4e60\u200b\uff0c\u200b\u81ea\u524d\u200b\u6709\u200b\u8d85\u8fc7\u200b5000\u200b\u540d\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5b66\u751f\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u56fd\u5bb6\u535a\u7269\u9986\u200b\u9986\u957f\u200b\u5854\u5409\u200b\u4e01\u200b\u00b7\u200b\u52aa\u200b', '\u200b\u8389\u5a05\u200b14\u200b\u5c81\u200b\uff0c\u200b\u4e24\u4eba\u200b\u90fd\u200b\u5df2\u200b\u5728\u200b\u5384\u200b\u7279\u5b54\u9662\u200b\u5b66\u4e60\u200b\u591a\u5e74\u200b\uff0c', '23\u200b\u5c81\u200b\u7684\u200b\u8389\u8fea\u4e9a\u200b\u00b7\u200b\u57c3\u200b\u65af\u8482\u6cd5\u200b\u8bfa\u65af\u200b\u5df2\u200b\u5728\u200b\u5384\u7279\u200b', '\u200b\u5728\u200b\u4e2d\u56fd\u200b\u7559\u5b66\u200b\u3002\u200b\u5b66\u4e60\u200b\u4e2d\u56fd\u200b\u7684\u200b\u6559\u80b2\u200b\u7ecf\u9a8c\u200b\uff0c\u200b\u6709\u52a9\u4e8e\u200b', '\u200b\u91cd\u200b\u8fbe\u59c6\u200b\u00b7\u200b\u4f18\u7d20\u798f\u200b\u66fe\u591a\u6b21\u200b\u8bbf\u95ee\u200b\u4e2d\u56fd\u200b\uff0c\u200b\u5bf9\u200b\u4e2d\u534e\u6587\u660e\u200b', '\u200b\u4e2d\u6587\u200b\u8bf4\u200b\u5f97\u200b\u683c\u5916\u200b\u6d41\u5229\u200b\u3002', '\u200b\u5b54\u9662\u200b\u5b66\u4e60\u200b3\u200b\u5e74\u200b\uff0c\u200b\u5728\u200b\u4e2d\u56fd\u200b\u4e66\u6cd5\u200b\u3001\u200b\u4e2d\u56fd\u753b\u200b\u7b49\u200b\u65b9\u9762\u200b\u8868\u200b', '\u200b\u63d0\u5347\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u7684\u200b\u6559\u80b2\u200b\u6c34\u5e73\u200b\u3002\u201d', '\u200b\u7684\u200b\u4f20\u627f\u200b\u4e0e\u200b\u521b\u65b0\u200b\u3001\u200b\u73b0\u4ee3\u5316\u200b\u535a\u7269\u9986\u200b\u7684\u200b\u5efa\u8bbe\u200b\u4e0e\u200b\u53d1\u5c55\u200b', '\u200b\u9732\u5a05\u200b\u5bf9\u200b\u8bb0\u8005\u200b\u8bf4\u200b\uff1a\u201c\u200b\u8fd9\u4e9b\u5e74\u6765\u200b\uff0c\u200b\u6000\u7740\u200b\u5bf9\u200b\u4e2d\u6587\u200b', '\u200b\u73b0\u5e72\u5206\u200b\u4f18\u79c0\u200b\uff0c\u200b\u5728\u200b2024\u200b\u5e74\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u8d5b\u533a\u200b\u7684\u200b', '\u200b\u5370\u8c61\u200b\u6df1\u523b\u200b\u3002\u201c\u200b\u4e2d\u56fd\u200b\u535a\u7269\u9986\u200b\u4e0d\u4ec5\u200b\u6709\u200b\u8bb8\u591a\u200b\u4fdd\u5b58\u200b\u5b8c\u597d\u200b', '\u201c\u200b\u5171\u540c\u200b\u5411\u200b\u4e16\u754c\u200b\u5c55\u793a\u200b\u975e\u200b', '\u200b\u548c\u200b\u4e2d\u56fd\u200b\u6587\u5316\u200b\u7684\u200b\u70ed\u7231\u200b\uff0c\u200b\u6211\u4eec\u200b\u59d0\u59b9\u4fe9\u200b\u59cb\u7ec8\u200b\u76f8\u4e92\u200b\u9f13\u200b', '\u201c\u200b\u6c49\u8bed\u200b\u6865\u200b\u201d\u200b\u6bd4\u8d5b\u200b\u4e2d\u200b\u83b7\u5f97\u200b\u4e00\u7b49\u5956\u200b\u3002\u200b\u8389\u8fea\u4e9a\u200b\u8bf4\u200b\uff1a\u201c\u200b\u5b66\u200b', '\u200b\u7684\u200b\u6587\u7269\u200b\uff0c\u200b\u8fd8\u200b\u5145\u5206\u8fd0\u7528\u200b\u5148\u8fdb\u200b\u79d1\u6280\u200b\u624b\u6bb5\u200b\u8fdb\u884c\u200b\u5c55\u793a\u200b\uff0c', '\u200b\u52b1\u200b\uff0c\u200b\u4e00\u8d77\u200b\u5b66\u4e60\u200b\u3002\u200b\u6211\u4eec\u200b\u7684\u200b\u4e2d\u6587\u200b\u4e00\u5929\u200b\u6bd4\u200b\u4e00\u5929\u200b\u597d\u200b\uff0c\u200b\u8fd8\u200b', '\u200b\u4e60\u200b\u4e2d\u56fd\u200b\u4e66\u6cd5\u200b\u8ba9\u200b\u6211\u200b\u7684\u200b\u5185\u5fc3\u200b\u53d8\u5f97\u200b\u5b89\u5b81\u200b\u548c\u200b\u7eaf\u7cb9\u200b\u3002\u200b\u6211\u200b', '\u200b\u6d32\u200b\u548c\u200b\u4e9a\u6d32\u200b\u7684\u200b\u707f\u70c2\u200b\u6587\u660e\u200b\u201d', '\u200b\u5e2e\u52a9\u200b\u4eba\u4eec\u200b\u66f4\u597d\u200b\u7406\u89e3\u200b\u4e2d\u534e\u6587\u660e\u200b\u3002\u201d\u200b\u5854\u5409\u200b\u4e01\u8bf4\u200b\uff0c\u201c\u200b\u5371\u200b', '\u200b\u5b66\u4f1a\u200b\u4e86\u200b\u4e2d\u6587\u200b\u6b4c\u200b\u548c\u200b\u4e2d\u56fd\u200b\u821e\u200b\u3002\u200b\u6211\u4eec\u200b\u4e00\u5b9a\u200b\u8981\u200b\u5230\u200b\u4e2d\u56fd\u200b', '\u200b\u4e5f\u200b\u559c\u6b22\u200b\u4e2d\u56fd\u200b\u7684\u200b\u670d\u9970\u200b\uff0c\u200b\u5e0c\u671b\u200b\u672a\u6765\u200b\u80fd\u200b\u53bb\u200b\u4e2d\u56fd\u200b\u5b66\u4e60\u200b\uff0c', '\u200b\u7acb\u200b\u7279\u91cc\u4e9a\u200b\u4e0e\u200b\u4e2d\u56fd\u200b\u90fd\u200b\u62e5\u6709\u200b\u60a0\u4e45\u200b\u7684\u200b\u6587\u660e\u200b\uff0c\u200b\u59cb\u7ec8\u200b\u76f8\u200b', '\u200b\u53bb\u200b\u3002\u200b\u5b66\u597d\u200b\u4e2d\u6587\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u672a\u6765\u200b\u4e0d\u662f\u200b\u68a6\u200b\uff01\u201d', '\u200b\u628a\u200b\u4e2d\u56fd\u200b\u4e0d\u540c\u200b\u6c11\u65cf\u200b\u5143\u7d20\u200b\u878d\u5165\u200b\u670d\u88c5\u8bbe\u8ba1\u200b\u4e2d\u200b\uff0c\u200b\u521b\u4f5c\u200b', '\u200b\u4ece\u200b\u963f\u65af\u9a6c\u62c9\u200b\u51fa\u53d1\u200b\uff0c\u200b\u6cbf\u7740\u200b\u873f\u8712\u200b\u66f2\u6298\u200b\u7684\u200b\u76d8\u5c71\u200b', '\u200b\u4e92\u200b\u7406\u89e3\u200b\u3001\u200b\u76f8\u4e92\u5c0a\u91cd\u200b\u3002\u200b\u6211\u200b\u5e0c\u671b\u200b\u672a\u6765\u200b\u4e0e\u200b\u4e2d\u56fd\u200b\u540c\u884c\u200b', '\u200b\u636e\u200b\u5384\u200b\u7279\u5b54\u9662\u200b\u4e2d\u65b9\u200b\u9662\u957f\u200b\u9ec4\u9e23\u98de\u200b\u4ecb\u7ecd\u200b\uff0c\u200b\u8fd9\u6240\u200b', '\u200b\u51fa\u200b\u66f4\u200b\u591a\u200b\u7cbe\u7f8e\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u4e5f\u200b\u628a\u200b\u5384\u7279\u200b\u6587\u5316\u200b\u5206\u4eab\u200b\u7ed9\u200b\u66f4\u200b\u591a\u200b', '\u200b\u516c\u8def\u200b\u4e00\u8def\u200b\u5411\u4e1c\u200b\u5bfb\u627e\u200b\u4e1d\u8def\u200b\u5370\u8ff9\u200b\u3002\u200b\u9a71\u8f66\u200b\u4e24\u4e2a\u200b\u5c0f\u200b', '\u200b\u52a0\u5f3a\u200b\u5408\u4f5c\u200b\uff0c\u200b\u5171\u540c\u200b\u5411\u200b\u4e16\u754c\u200b\u5c55\u793a\u200b\u975e\u6d32\u200b\u548c\u200b\u4e9a\u6d32\u200b\u7684\u200b\u707f\u200b', '\u200b\u5b54\u9662\u200b\u6210\u7acb\u200b\u4e8e\u200b2013\u200b\u5e74\u200b3\u200b\u6708\u200b\uff0c\u200b\u7531\u200b\u8d35\u5dde\u200b\u8d22\u7ecf\u5927\u5b66\u200b\u548c\u200b', '\u200b\u7684\u200b\u4e2d\u56fd\u200b\u670b\u53cb\u200b\u3002\u201d', '\u200b\u65f6\u200b\uff0c\u200b\u8bb0\u8005\u200b\u6765\u5230\u200b\u4f4d\u4e8e\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u6e2f\u53e3\u57ce\u5e02\u200b\u9a6c\u8428\u200b', '\u200b\u70c2\u200b\u6587\u660e\u200b\u3002\u201d'], 'rec_scores': array([0.99113536, ..., 0.95110035]), 'rec_polys': array([[[ 129,   42],\n        ...,\n        [ 129,  140]],\n\n    ...,\n\n    [[1156, 1330],\n        ...,\n        [1156, 1351]]], dtype=int16), 'rec_boxes': array([[ 129, ...,  140],\n    ...,\n    [1156, ..., 1351]], dtype=int16)}}}\n</code></pre>"},{"location":"en/community/code_and_doc.html","title":"Appendix","text":"<p>This appendix contains python, document specifications and Pull Request process.</p>"},{"location":"en/community/code_and_doc.html#appendix-1python-code-specification","title":"Appendix 1\uff1aPython Code Specification","text":"<p>The Python code of PaddleOCR follows PEP8 Specification, some of the key concerns include the following</p> <ul> <li> <p>Space</p> <ul> <li>Spaces should be added after commas, semicolons, colons, not before them</li> </ul> <pre><code># true:\nprint(x, y)\n\n# false:\nprint(x , y)\n</code></pre> <ul> <li>When specifying a keyword parameter or default parameter value in a function, do not use spaces on both sides of it</li> </ul> <pre><code># true:\ndef complex(real, imag=0.0)\n# false:\ndef complex(real, imag = 0.0)\n</code></pre> </li> <li> <p>comment</p> <ul> <li>Inline comments: inline comments are indicated by the<code>#</code>sign. Two spaces should be left between code and<code>#</code>, and one space should be left between<code>#</code>and comments, for example</li> </ul> <pre><code>x = x + 1  # Compensate for border\n</code></pre> <ul> <li> <p>Functions and methods: The definition of each function should include the following:</p> <ul> <li>Function description: Utility, input and output of function</li> <li>Args: Name and description of each parameter</li> <li>Returns: The meaning and type of the return value</li> </ul> <pre><code>def fetch_bigtable_rows(big_table, keys, other_silly_variable=None):\n    \"\"\"Fetches rows from a Bigtable.\n\n    Retrieves rows pertaining to the given keys from the Table instance\n    represented by big_table.  Silly things may happen if\n    other_silly_variable is not None.\n\n    Args:\n        big_table: An open Bigtable Table instance.\n        keys: A sequence of strings representing the key of each table row\n            to fetch.\n        other_silly_variable: Another optional variable, that has a much\n            longer name than the other args, and which does nothing.\n\n    Returns:\n        A dict mapping keys to the corresponding table row data\n        fetched. Each row is represented as a tuple of strings. For\n        example:\n\n        {'Serak': ('Rigel VII', 'Preparer'),\n        'Zim': ('Irk', 'Invader'),\n        'Lrrr': ('Omicron Persei 8', 'Emperor')}\n\n        If a key from the keys argument is missing from the dictionary,\n        then that row was not found in the table.\n    \"\"\"\n    pass\n</code></pre> </li> </ul> </li> </ul>"},{"location":"en/community/code_and_doc.html#appendix-2-document-specification","title":"Appendix 2: Document Specification","text":""},{"location":"en/community/code_and_doc.html#21-overall-description","title":"2.1 Overall Description","text":"<ul> <li> <p>Document Location: If you add new features to your original Markdown file, please Do not re-create a new file. If you don't know where to add it, you can first PR the code and then ask the official in commit.</p> </li> <li> <p>New Markdown Document Name: Describe the content of the document in English, typically a combination of lowercase letters and underscores, such as <code>add_New_Algorithm.md</code></p> </li> <li> <p>New Markdown Document Format: Catalog - Body - FAQ</p> </li> </ul> <p>The directory generation method can use this site Automatically extract directories after copying MD contents, and then add `</p> <ul> <li>English and Chinese: Any changes or additions to the document need to be made in both Chinese and English documents.</li> </ul>"},{"location":"en/community/code_and_doc.html#22-format-specification","title":"2.2 Format Specification","text":"<ul> <li> <p>Title format: The document title format follows the format of: Arabic decimal point combination-space-title (for example, <code>2.1 XXXX</code>, <code>2.XXXX</code>)</p> </li> <li> <p>Code block: Displays code in code block format that needs to be run, describing the meaning of command parameters before the code block. for example:</p> </li> </ul> <p>Pipeline of detection + direction Classify + recognition: Vertical text can be recognized after set direction classifier parameters<code>--use_angle_cls true</code>.</p> <pre><code>paddleocr --image_dir ./imgs/11.jpg --use_angle_cls true\n</code></pre> <ul> <li> <p>Variable References: If code variables or command parameters are referenced in line, they need to be represented in line code, for example, above <code>--use_angle_cls true</code> with one space in front and one space in back</p> </li> <li> <p>Uniform naming: e.g. PP-OCRv2, PP-OCR mobile, <code>paddleocr</code> whl package, PPOCRLabel, Paddle Lite, etc.</p> </li> <li> <p>Supplementary notes: Supplementary notes by reference format <code>&gt;</code>.</p> </li> <li> <p>Picture: If a picture is added to the description document, specify the naming of the picture (describing its content) and add the picture under <code>doc/</code>.</p> </li> <li> <p>Title: Capitalize the first letter of each word in the title.</p> </li> </ul>"},{"location":"en/community/code_and_doc.html#appendix-3-pull-request-description","title":"Appendix 3: Pull Request Description","text":""},{"location":"en/community/code_and_doc.html#31-paddleocr-branch-description","title":"3.1 PaddleOCR Branch Description","text":"<p>PaddleOCR will maintain two branches in the future, one for each:</p> <ul> <li>release/x.x family branch: stable release version branch, also the default branch. PaddleOCR releases a new release branch based on feature updates and adapts to the release version of Paddle. As versions iterate, more and more release/x.x family branches are maintained by default with the latest version of the release branch.</li> <li>dygraph branch: For the development branch, adapts the dygraph version of the Paddle dynamic graph to primarily develop new functionality. If you need to redevelop, choose the dygraph branch. To ensure that the dygraph branch pulls out the release/x.x branch when needed, the code for the dygraph branch can only use the valid API in the latest release branch of Paddle. That is, if a new API has been developed in the Paddle dygraph branch but has not yet appeared in the release branch code, do not use it in Paddle OCR. In addition, performance optimization, parameter tuning, policy updates that do not involve API can be developed normally.</li> </ul> <p>The historical branch of PaddleOCR will no longer be maintained in the future. These branches will continue to be maintained, considering that some of you may still be using them:</p> <p>Develop branch: This branch was used for the development and testing of static diagrams and is currently compatible with version &gt;=1.7. If you have special needs, you can also use this branch to accommodate older versions of Paddle, but you won't update your code until you fix the bug.</p> <p>PaddleOCR welcomes you to actively contribute code to repo. Here are some basic processes for contributing code.</p>"},{"location":"en/community/code_and_doc.html#32-paddleocr-code-submission-process-and-specification","title":"3.2 PaddleOCR Code Submission Process And Specification","text":"<p>If you are familiar with Git use, you can jump directly to Some Conventions For Submitting Code in 3.2.10</p>"},{"location":"en/community/code_and_doc.html#321-create-your-remote-repo","title":"3.2.1 Create Your <code>Remote Repo</code>","text":"<p>In PaddleOCR GitHub Home Click the <code>Fork</code> button in the upper left corner to create a <code>remote repo</code>in your personal directory, such as <code>https://github.com/ {your_name}/PaddleOCR</code>.</p> <p></p> <p>Clone <code>Remote repo</code></p> <pre><code># pull code of develop branch\ngit clone https://github.com/{your_name}/PaddleOCR.git -b dygraph\ncd PaddleOCR\n</code></pre> <p>Clone failures are mostly due to network reasons, try again later or configure the proxy</p>"},{"location":"en/community/code_and_doc.html#322-login-and-connect-using-token","title":"3.2.2 Login And Connect Using Token","text":"<p>Start by viewing the information for the current <code>remote repo</code>.</p> <pre><code>git remote -v\n# origin    https://github.com/{your_name}/PaddleOCR.git (fetch)\n# origin    https://github.com/{your_name}/PaddleOCR.git (push)\n</code></pre> <p>Only the information of the clone <code>remote repo</code>, i.e. the PaddleOCR under your username, is available. Due to the change in Github's login method, you need to reconfigure the <code>remote repo</code> address by means of a Token. The token is generated as follows:</p> <ol> <li> <p>Find Personal Access Tokens: Click on your avatar in the upper right corner of the Github page and choose Settings --&gt; Developer settings --&gt; Personal access tokens,</p> </li> <li> <p>Click Generate new token: Fill in the token name in Note, such as 'paddle'. In Select scopes, select repo (required), admin:repo_hook, delete_repo, etc. You can check them according to your needs. Then click Generate token to generate the token, and finally copy the generated token.</p> </li> </ol> <p>Delete the original origin configuration</p> <pre><code>git remote rm origin\n</code></pre> <p>Change the remote branch to <code>https://oauth2:{token}@github.com/{your_name}/PaddleOCR.git</code>. For example, if the token value is 12345 and your user name is PPOCR, run the following command</p> <pre><code>git remote add origin https://oauth2:12345@github.com/PPOCR/PaddleOCR.git\n</code></pre> <p>This establishes a connection to our own <code>remote repo</code>. Next we create a remote host of the original PaddleOCR repo, named upstream.</p> <pre><code>git remote add upstream https://github.com/PaddlePaddle/PaddleOCR.git\n</code></pre> <p>Use <code>git remote -v</code> to view current <code>remote warehouse</code> information, output as follows, found to include two origin and two upstream of <code>remote repo</code> .</p> <pre><code>origin    https://github.com/{your_name}/PaddleOCR.git (fetch)\norigin    https://github.com/{your_name}/PaddleOCR.git (push)\nupstream    https://github.com/PaddlePaddle/PaddleOCR.git (fetch)\nupstream    https://github.com/PaddlePaddle/PaddleOCR.git (push)\n</code></pre> <p>This is mainly to keep the local repository up to date when subsequent pull request (PR) submissions are made.</p>"},{"location":"en/community/code_and_doc.html#323-create-local-branch","title":"3.2.3 Create Local Branch","text":"<p>First get the latest code of upstream, then create a new_branch branch based on the dygraph of the upstream repo (upstream).</p> <pre><code>git fetch upstream\ngit checkout -b new_branch upstream/dygraph\n</code></pre> <p>If for a newly forked PaddleOCR project, the user's remote repo (origin) has the same branch updates as the upstream repository (upstream), you can also create a new local branch based on the default branch of the origin repo or a specified branch with the following command</p> <pre><code># Create new_branch branch on user remote repo (origin) based on develop branch\ngit checkout -b new_branch origin/develop\n# Create new_branch branch based on upstream remote repo develop branch\n# If you need to create a new branch from upstream,\n# you need to first use git fetch upstream to get upstream code\ngit checkout -b new_branch upstream/develop\n</code></pre> <p>The final switch to the new branch is displayed with the following output information.</p> <p>Branch new_branch set up to track remote branch develop from upstream. Switched to a new branch 'new_branch'</p> <p>After switching branches, file changes can be made on this branch</p>"},{"location":"en/community/code_and_doc.html#324-use-pre-commit-hook","title":"3.2.4 Use Pre-Commit Hook","text":"<p>Paddle developers use the pre-commit tool to manage Git pre-submit hooks. It helps us format the source code (C++, Python) and automatically check for basic things (such as having only one EOL per file, not adding large files to Git) before committing it.</p> <p>The pre-commit test is part of the unit test in Travis-CI. PR that does not satisfy the hook cannot be submitted to PaddleOCR. Install it first and run it in the current directory\uff1a</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <ol> <li> <p>Paddle uses clang-format to adjust the C/C++ source code format. Make sure the <code>clang-format</code> version is above 3.8.</p> </li> <li> <p>Yapf installed through pip install pre-commit is slightly different from conda install-c conda-forge pre-commit, and PaddleOCR developers use <code>pip install pre-commit</code>.</p> </li> </ol>"},{"location":"en/community/code_and_doc.html#325-modify-and-submit-code","title":"3.2.5 Modify And Submit Code","text":"<p>If you make some changes on <code>README.Md</code> on PaddleOCR, you can view the changed file through <code>git status</code>, and then add the changed file using <code>git add</code>\u3002</p> <pre><code>git status # View change files\ngit add README.md\npre-commit\n</code></pre> <p>Repeat these steps until the pre-comit format check does not error. As shown below.</p> <p></p> <p>Use the following command to complete the submission.</p> <pre><code>git commit -m \"your commit info\"\n</code></pre>"},{"location":"en/community/code_and_doc.html#326-keep-local-repo-up-to-date","title":"3.2.6 Keep Local Repo Up To Date","text":"<p>Get the latest code for upstream and update the current branch. Here the upstream comes from section 2.2, <code>Connecting to a remote repo</code>.</p> <pre><code>git fetch upstream\n# If you want to commit to another branch, you need to pull code from another branch of upstream, here is develop\ngit pull upstream develop\n</code></pre>"},{"location":"en/community/code_and_doc.html#327-push-to-remote-repo","title":"3.2.7 Push To Remote Repo","text":"<pre><code>git push origin new_branch\n</code></pre>"},{"location":"en/community/code_and_doc.html#327-submit-pull-request","title":"3.2.7 Submit Pull Request","text":"<p>Click the new pull request to select the local branch and the target branch, as shown in the following figure. In the description of PR, fill in the functions completed by the PR. Next, wait for review, and if you need to modify something, update the corresponding branch in origin with the steps above.</p> <p></p>"},{"location":"en/community/code_and_doc.html#328-sign-cla-agreement-and-pass-unit-tests","title":"3.2.8 Sign CLA Agreement And Pass Unit Tests","text":"<p>Signing the CLA When submitting a Pull Request to PaddlePaddle for the first time, you need to sign a CLA (Contributor License Agreement) agreement to ensure that your code can be incorporated as follows:</p> <ol> <li> <p>Please check the Check section in PR, find the license/cla, and click on the right detail to enter the CLA website</p> </li> <li> <p>Click Sign in with GitHub to agree on the CLA website and when clicked, it will jump back to your Pull Request page</p> </li> </ol>"},{"location":"en/community/code_and_doc.html#329-delete-branch","title":"3.2.9 Delete Branch","text":"<ul> <li>Remove remote branch</li> </ul> <p>After PR is merged into the main repo, we can delete the branch of the remote repofrom the PR page.   You can also use <code>git push origin:branch name</code> to delete remote branches, such as:</p> <pre><code>git push origin :new_branch\n</code></pre> <ul> <li>Delete local branch</li> </ul> <pre><code># Switch to the development branch, otherwise the current branch cannot be deleted\ngit checkout develop\n\n# Delete new_ Branch Branch\ngit branch -D new_branch\n</code></pre>"},{"location":"en/community/code_and_doc.html#3210-some-conventions-for-submitting-code","title":"3.2.10 Some Conventions For Submitting Code","text":"<p>In order for official maintainers to better focus on the code itself when reviewing it, please follow the following conventions each time you submit your code:</p> <p>1\uff09Please ensure that the unit tests in Travis-CI pass smoothly. If not, indicate that there is a problem with the submitted code, and the official maintainer generally does not review it.</p> <p>2\uff09Before submitting a Pull Request.</p> <ul> <li>Note the number of commits.</li> </ul> <p>Reason: If you only modify one file and submit more than a dozen commits, each commit will only make a few modifications, which can be very confusing to the reviewer. The reviewer needs to look at each commit individually to see what changes have been made, and does not exclude the fact that changes between commits overlap each other.</p> <p>Suggestion: Keep as few commits as possible each time you submit, and supplement your last commit with git commit --amend. For multiple commits that have been Push to a remote warehouse, you can refer to squash commits after push.</p> <ul> <li>Note the name of each commit: it should reflect the content of the current commit, not be too arbitrary.</li> </ul> <p>3\uff09 If you have solved a problem, add in the first comment box of the Pull Request:fix #issue_number\uff0cThis will automatically close the corresponding Issue when the Pull Request is merged. Key words include:close, closes, closed, fix, fixes, fixed, resolve, resolves, resolved,please choose the right vocabulary. Detailed reference Closing issues via commit messages.</p> <p>In addition, in response to the reviewer's comments, you are requested to abide by the following conventions:</p> <p>1\uff09 Each review comment from an official maintainer would like a response, which would better enhance the contribution of the open source community.</p> <ul> <li>If you agree to the review opinion and modify it accordingly, give a simple Done.</li> <li>If you disagree with the review, please give your own reasons for refuting.</li> </ul> <p>2\uff09If there are many reviews:</p> <ul> <li>Please give an overview of the changes.</li> <li>Please reply with `start a review', not directly. The reason is that each reply sends an e-mail message, which can cause a mail disaster.</li> </ul>"},{"location":"en/community/community_contribution.html","title":"\u793e\u533a\u200b\u8d21\u732e","text":"<p>\u200b\u611f\u8c22\u200b\u5927\u5bb6\u200b\u957f\u4e45\u4ee5\u6765\u200b\u5bf9\u200bPaddleOCR\u200b\u7684\u200b\u652f\u6301\u200b\u548c\u200b\u5173\u6ce8\u200b\uff0c\u200b\u4e0e\u200b\u5e7f\u5927\u200b\u5f00\u53d1\u8005\u200b\u5171\u540c\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u4e13\u4e1a\u200b\u3001\u200b\u548c\u8c10\u200b\u3001\u200b\u76f8\u4e92\u200b\u5e2e\u52a9\u200b\u7684\u200b\u5f00\u6e90\u200b\u793e\u533a\u200b\u662f\u200bPaddleOCR\u200b\u7684\u200b\u76ee\u6807\u200b\u3002\u200b\u672c\u200b\u6587\u6863\u200b\u5c55\u793a\u200b\u4e86\u200b\u5df2\u6709\u200b\u7684\u200b\u793e\u533a\u200b\u8d21\u732e\u200b\u3001\u200b\u5bf9\u4e8e\u200b\u5404\u7c7b\u200b\u8d21\u732e\u200b\u8bf4\u660e\u200b\u3001\u200b\u65b0\u200b\u7684\u200b\u673a\u4f1a\u200b\u4e0e\u200b\u6d41\u7a0b\u200b\uff0c\u200b\u5e0c\u671b\u200b\u8d21\u732e\u200b\u6d41\u7a0b\u200b\u66f4\u52a0\u200b\u9ad8\u6548\u200b\u3001\u200b\u8def\u5f84\u200b\u66f4\u52a0\u200b\u6e05\u6670\u200b\u3002</p> <p>PaddleOCR\u200b\u5e0c\u671b\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200bAI\u200b\u7684\u200b\u529b\u91cf\u200b\u52a9\u529b\u200b\u4efb\u4f55\u200b\u4e00\u4f4d\u200b\u6709\u200b\u68a6\u60f3\u200b\u7684\u200b\u5f00\u53d1\u8005\u200b\u5b9e\u73b0\u200b\u81ea\u5df1\u200b\u7684\u200b\u60f3\u6cd5\u200b\uff0c\u200b\u4eab\u53d7\u200b\u521b\u9020\u200b\u4ef7\u503c\u200b\u5e26\u6765\u200b\u7684\u200b\u6109\u60a6\u200b\u3002</p> <p> </p>"},{"location":"en/community/community_contribution.html#1","title":"1. \u200b\u793e\u533a\u200b\u8d21\u732e","text":""},{"location":"en/community/community_contribution.html#11-paddleocr","title":"1.1 \u200b\u57fa\u4e8e\u200bPaddleOCR\u200b\u7684\u200b\u793e\u533a\u200b\u9879\u76ee","text":"\u7c7b\u522b\u200b \u200b\u9879\u76ee\u200b \u200b\u63cf\u8ff0\u200b \u200b\u5f00\u53d1\u8005\u200b \u200b\u901a\u7528\u200b\u5de5\u5177\u200b FastOCRLabel \u200b\u5b8c\u6574\u200b\u7684\u200bC#\u200b\u7248\u672c\u200b\u6807\u6ce8\u200bGUI \u200b\u5305\u5efa\u5f3a\u200b \u200b\u901a\u7528\u200b\u5de5\u5177\u200b DangoOCR\u200b\u79bb\u7ebf\u200b\u7248\u200b \u200b\u901a\u7528\u578b\u200b\u684c\u9762\u200b\u7ea7\u200b\u5373\u65f6\u200b\u7ffb\u8bd1\u200bGUI PantsuDango \u200b\u901a\u7528\u200b\u5de5\u5177\u200b scr2txt \u200b\u622a\u5c4f\u200b\u8f6c\u200b\u6587\u5b57\u200bGUI lstwzd \u200b\u901a\u7528\u200b\u5de5\u5177\u200b ocr_sdk OCR java SDK\u200b\u5de5\u5177\u7bb1\u200b Calvin \u200b\u901a\u7528\u200b\u5de5\u5177\u200b iocr IOCR \u200b\u81ea\u5b9a\u4e49\u200b\u6a21\u677f\u200b\u8bc6\u522b\u200b(\u200b\u652f\u6301\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b) Calvin \u200b\u901a\u7528\u200b\u5de5\u5177\u200b Lmdb Dataset Format Conversion Tool \u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b\u4e2d\u200blmdb\u200b\u6570\u636e\u683c\u5f0f\u200b\u8f6c\u6362\u200b\u5de5\u5177\u200b OneYearIsEnough \u200b\u901a\u7528\u200b\u5de5\u5177\u200b \u200b\u7528\u200bpaddleocr\u200b\u6253\u9020\u200b\u4e00\u6b3e\u200b\u201c\u200b\u76d7\u5e55\u200b\u7b14\u8bb0\u200b\u201d \u200b\u7528\u200bPaddleOCR\u200b\u8bb0\u7b14\u8bb0\u200b kjf4096 \u200b\u5782\u7c7b\u200b\u5de5\u5177\u200b AI Studio\u200b\u9879\u76ee\u200b \u200b\u82f1\u6587\u200b\u89c6\u9891\u200b\u81ea\u52a8\u200b\u751f\u6210\u200b\u5b57\u5e55\u200b \u200b\u53f6\u6708\u200b\u6c34\u72d0\u200b \u200b\u5782\u7c7b\u200b\u5de5\u5177\u200b id_card_ocr \u200b\u8eab\u4efd\u8bc1\u200b\u590d\u5370\u4ef6\u200b\u8bc6\u522b\u200b baseli \u200b\u5782\u7c7b\u200b\u5de5\u5177\u200b Paddle_Table_Image_Reader \u200b\u80fd\u770b\u61c2\u200b\u8868\u683c\u200b\u56fe\u7247\u200b\u7684\u200b\u6570\u636e\u200b\u52a9\u624b\u200b thunder95 \u200b\u5782\u7c7b\u200b\u5de5\u5177\u200b AI Studio\u200b\u9879\u76ee\u200b OCR\u200b\u6d41\u7a0b\u200b\u4e2d\u200b\u5bf9\u200b\u624b\u5199\u4f53\u200b\u8fdb\u884c\u200b\u8fc7\u6ee4\u200b daassh \u200b\u5782\u7c7b\u200b\u573a\u666f\u200b\u8c03\u4f18\u200b AI Studio\u200b\u9879\u76ee\u200b \u200b\u7535\u8868\u200b\u8bfb\u6570\u200b\u548c\u200b\u7f16\u53f7\u200b\u8bc6\u522b\u200b \u200b\u6df1\u6e0a\u200b\u4e0a\u200b\u7684\u200b\u5751\u200b \u200b\u5782\u7c7b\u200b\u573a\u666f\u200b\u8c03\u4f18\u200b AI Studio\u200b\u9879\u76ee\u200b LCD\u200b\u6db2\u6676\u200b\u5b57\u7b26\u200b\u68c0\u6d4b\u200b Dream\u200b\u62d2\u6770\u200b \u200b\u524d\u540e\u200b\u5904\u7406\u200b paddleOCRCorrectOutputs \u200b\u83b7\u53d6\u200bOCR\u200b\u8bc6\u522b\u200b\u7ed3\u679c\u200b\u7684\u200bkey-value yuranusduke \u200b\u524d\u200b\u5904\u7406\u200b optlab OCR\u200b\u524d\u200b\u5904\u7406\u200b\u5de5\u5177\u7bb1\u200b\uff0c\u200b\u57fa\u4e8e\u200bQt\u200b\u548c\u200bLeptonica\u3002 GreatV \u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b PaddleOCRSharp PaddleOCR\u200b\u7684\u200b.NET\u200b\u5c01\u88c5\u200b\u4e0e\u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b\u3002 raoyutian \u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b PaddleSharp PaddleOCR\u200b\u7684\u200b.NET\u200b\u5c01\u88c5\u200b\u4e0e\u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b\uff0c\u200b\u652f\u6301\u200b\u8de8\u5e73\u53f0\u200b\u3001GPU sdcb \u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b PaddleOCR-Streamlit-Demo \u200b\u4f7f\u7528\u200bStreamlit\u200b\u90e8\u7f72\u200bPaddleOCR Lovely-Pig \u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b PaddleOCR-PyWebIO-Demo \u200b\u4f7f\u7528\u200bPyWebIO\u200b\u90e8\u7f72\u200bPaddleOCR Lovely-Pig \u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b PaddleOCR-Paddlejs-Vue-Demo \u200b\u4f7f\u7528\u200bPaddle.js\u200b\u548c\u200bVue\u200b\u90e8\u7f72\u200bPaddleOCR Lovely-Pig \u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b PaddleOCR-Paddlejs-React-Demo \u200b\u4f7f\u7528\u200bPaddle.js\u200b\u548c\u200bReact\u200b\u90e8\u7f72\u200bPaddleOCR Lovely-Pig \u200b\u5b66\u672f\u524d\u6cbf\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u4e0e\u200b\u63a8\u7406\u200b AI Studio\u200b\u9879\u76ee\u200b StarNet-MobileNetV3\u200b\u7b97\u6cd5\u200b\u2013\u200b\u4e2d\u6587\u200b\u8bad\u7ec3\u200b xiaoyangyang2 \u200b\u5b66\u672f\u524d\u6cbf\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u4e0e\u200b\u63a8\u7406\u200b ABINet-paddle ABINet\u200b\u7b97\u6cd5\u200b\u524d\u5411\u200b\u8fd0\u7b97\u200b\u7684\u200bpaddle\u200b\u5b9e\u73b0\u200b\u4ee5\u53ca\u200b\u6a21\u578b\u200b\u5404\u200b\u90e8\u5206\u200b\u7684\u200b\u5b9e\u73b0\u200b\u7ec6\u8282\u200b\u5206\u6790\u200b Huntersdeng"},{"location":"en/community/community_contribution.html#12-paddleocr","title":"1.2 \u200b\u4e3a\u200bPaddleOCR\u200b\u65b0\u589e\u200b\u529f\u80fd","text":"<ul> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b authorfu \u200b\u8d21\u732e\u200bAndroid(#340)\u200b\u548c\u200bxiadeye \u200b\u8d21\u732e\u200bIOS\u200b\u7684\u200bdemo\u200b\u4ee3\u7801\u200b(#325)</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b tangmq \u200b\u7ed9\u200bPaddleOCR\u200b\u589e\u52a0\u200bDocker\u200b\u5316\u200b\u90e8\u7f72\u200b\u670d\u52a1\u200b\uff0c\u200b\u652f\u6301\u200b\u5feb\u901f\u200b\u53d1\u5e03\u200b\u53ef\u200b\u8c03\u7528\u200b\u7684\u200bRestful API\u200b\u670d\u52a1\u200b(#507)\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b lijinhan \u200b\u7ed9\u200bPaddleOCR\u200b\u589e\u52a0\u200bjava SpringBoot \u200b\u8c03\u7528\u200bOCR Hubserving\u200b\u63a5\u53e3\u200b\u5b8c\u6210\u200b\u5bf9\u200bOCR\u200b\u670d\u52a1\u5316\u200b\u90e8\u7f72\u200b\u7684\u200b\u4f7f\u7528\u200b(#1027)\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b Evezerest\uff0c ninetailskim\uff0c edencfc\uff0c BeyondYourself\uff0c 1084667371 \u200b\u8d21\u732e\u200b\u4e86\u200bPPOCRLabel \u200b\u7684\u200b\u5b8c\u6574\u200b\u4ee3\u7801\u200b\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b bupt906 \u200b\u8d21\u732e\u200bMicroNet\u200b\u7ed3\u6784\u200b\u4ee3\u7801\u200b(#5251)\u200b\u548c\u200b\u8d21\u732e\u200bOneCycle\u200b\u5b66\u4e60\u200b\u7387\u200b\u7b56\u7565\u200b\u4ee3\u7801\u200b(#5252)</li> </ul>"},{"location":"en/community/community_contribution.html#13","title":"1.3 \u200b\u4ee3\u7801\u200b\u4fee\u590d","text":"<ul> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b zhangxin(Blog) \u200b\u8d21\u732e\u200b\u65b0\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u65b9\u5f0f\u200b\u3001\u200b\u6dfb\u52a0\u200b.gitgnore\u3001\u200b\u5904\u7406\u200b\u624b\u52a8\u200b\u8bbe\u7f6e\u200bPYTHONPATH\u200b\u73af\u5883\u53d8\u91cf\u200b\u7684\u200b\u95ee\u9898\u200b(#210)\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b lyl120117 \u200b\u8d21\u732e\u200b\u6253\u5370\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u7684\u200b\u4ee3\u7801\u200b(#304)\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b BeyondYourself \u200b\u7ed9\u200bPaddleOCR\u200b\u63d0\u4e86\u200b\u5f88\u591a\u200b\u975e\u5e38\u200b\u68d2\u200b\u7684\u200b\u5efa\u8bae\u200b\uff0c\u200b\u5e76\u200b\u7b80\u5316\u200b\u4e86\u200bPaddleOCR\u200b\u7684\u200b\u90e8\u5206\u200b\u4ee3\u7801\u200b\u98ce\u683c\u200b(so many commits)\u3002</li> </ul>"},{"location":"en/community/community_contribution.html#14","title":"1.4 \u200b\u6587\u6863\u200b\u4f18\u5316\u200b\u4e0e\u200b\u7ffb\u8bd1","text":"<ul> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b RangeKing\uff0cHustBestCat\uff0cv3fc\uff0c1084667371 \u200b\u8d21\u732e\u200b\u7ffb\u8bd1\u200b\u300a\u200b\u52a8\u624b\u200b\u5b66\u200bOCR\u300bnotebook\u200b\u7535\u5b50\u4e66\u200b\u82f1\u6587\u7248\u200b\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b thunderstudying\uff0cRangeKing\uff0clivingbody\uff0c WZMIAOMIAO\uff0chaigang1975 \u200b\u8865\u5145\u200b\u591a\u4e2a\u200b\u82f1\u6587\u200bmarkdown\u200b\u6587\u6863\u200b\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b fanruinet \u200b\u6da6\u8272\u200b\u548c\u200b\u4fee\u590d\u200b35\u200b\u7bc7\u200b\u82f1\u6587\u200b\u6587\u6863\u200b(#5205)\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b Khanh Tran \u200b\u548c\u200b Karl Horky \u200b\u8d21\u732e\u200b\u4fee\u6539\u200b\u82f1\u6587\u200b\u6587\u6863\u200b\u3002</li> </ul>"},{"location":"en/community/community_contribution.html#15","title":"1.5 \u200b\u591a\u200b\u8bed\u8a00\u200b\u8bed\u6599","text":"<ul> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b xiangyubo \u200b\u8d21\u732e\u200b\u624b\u5199\u200b\u4e2d\u6587\u200bOCR\u200b\u6570\u636e\u200b\u96c6\u200b(#321)\u3002</li> <li>\u200b\u975e\u5e38\u611f\u8c22\u200b Mejans \u200b\u7ed9\u200bPaddleOCR\u200b\u589e\u52a0\u200b\u65b0\u200b\u8bed\u8a00\u200b\u5965\u514b\u200b\u897f\u5766\u8bed\u200bOccitan\u200b\u7684\u200b\u5b57\u5178\u200b\u548c\u200b\u8bed\u6599\u200b(#954)\u3002</li> </ul>"},{"location":"en/community/community_contribution.html#2","title":"2. \u200b\u8d21\u732e\u200b\u8bf4\u660e","text":""},{"location":"en/community/community_contribution.html#21","title":"2.1 \u200b\u65b0\u589e\u200b\u529f\u80fd\u200b\u7c7b","text":"<p>PaddleOCR\u200b\u975e\u5e38\u200b\u6b22\u8fce\u200b\u793e\u533a\u200b\u8d21\u732e\u200b\u4ee5\u200bPaddleOCR\u200b\u4e3a\u200b\u6838\u5fc3\u200b\u7684\u200b\u5404\u79cd\u200b\u670d\u52a1\u200b\u3001\u200b\u90e8\u7f72\u200b\u5b9e\u4f8b\u200b\u4e0e\u200b\u8f6f\u4ef6\u5e94\u7528\u200b\uff0c\u200b\u7ecf\u8fc7\u200b\u8ba4\u8bc1\u200b\u7684\u200b\u793e\u533a\u200b\u8d21\u732e\u200b\u4f1a\u200b\u88ab\u200b\u6dfb\u52a0\u200b\u5728\u200b\u4e0a\u8ff0\u200b\u793e\u533a\u200b\u8d21\u732e\u200b\u8868\u4e2d\u200b\uff0c\u200b\u4e3a\u200b\u5e7f\u5927\u200b\u5f00\u53d1\u8005\u200b\u589e\u52a0\u200b\u66dd\u5149\u200b\uff0c\u200b\u4e5f\u200b\u662f\u200bPaddleOCR\u200b\u7684\u200b\u8363\u8000\u200b\uff0c\u200b\u5176\u4e2d\u200b\uff1a</p> <ul> <li> <p>\u200b\u9879\u76ee\u200b\u5f62\u5f0f\u200b\uff1a\u200b\u5b98\u65b9\u200b\u793e\u533a\u200b\u8ba4\u8bc1\u200b\u7684\u200b\u9879\u76ee\u200b\u4ee3\u7801\u200b\u5e94\u6709\u200b\u826f\u597d\u200b\u7684\u200b\u89c4\u8303\u200b\u548c\u200b\u7ed3\u6784\u200b\uff0c\u200b\u540c\u65f6\u200b\uff0c\u200b\u8fd8\u5e94\u200b\u914d\u5907\u200b\u4e00\u4e2a\u200b\u8be6\u7ec6\u200b\u7684\u200bREADME.md\uff0c\u200b\u8bf4\u660e\u200b\u9879\u76ee\u200b\u7684\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b\u3002\u200b\u901a\u8fc7\u200b\u5728\u200brequirements.txt\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u589e\u52a0\u200b\u4e00\u884c\u200b <code>paddleocr</code> \u200b\u53ef\u4ee5\u200b\u81ea\u52a8\u200b\u6536\u5f55\u200b\u5230\u200bPaddleOCR\u200b\u7684\u200busedby\u200b\u4e2d\u200b\u3002</p> </li> <li> <p>\u200b\u5408\u5165\u200b\u65b9\u5f0f\u200b\uff1a\u200b\u5982\u679c\u200b\u662f\u200b\u5bf9\u200bPaddleOCR\u200b\u73b0\u6709\u200b\u5de5\u5177\u200b\u7684\u200b\u66f4\u65b0\u200b\u5347\u7ea7\u200b\uff0c\u200b\u5219\u200b\u4f1a\u5408\u200b\u5165\u4e3b\u200brepo\u3002\u200b\u5982\u679c\u200b\u4e3a\u200bPaddleOCR\u200b\u62d3\u5c55\u200b\u4e86\u200b\u65b0\u200b\u529f\u80fd\u200b\uff0c\u200b\u8bf7\u200b\u5148\u200b\u4e0e\u200b\u5b98\u65b9\u200b\u4eba\u5458\u200b\u8054\u7cfb\u200b\uff0c\u200b\u786e\u8ba4\u200b\u9879\u76ee\u200b\u662f\u5426\u200b\u5408\u200b\u5165\u4e3b\u200brepo\uff0c\u200b\u5373\u4f7f\u200b\u65b0\u200b\u529f\u80fd\u200b\u672a\u5408\u200b\u5165\u4e3b\u200brepo\uff0c\u200b\u6211\u4eec\u200b\u540c\u6837\u200b\u4e5f\u200b\u4f1a\u200b\u4ee5\u200b\u793e\u533a\u200b\u8d21\u732e\u200b\u7684\u200b\u65b9\u5f0f\u200b\u4e3a\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u200b\u9879\u76ee\u200b\u589e\u52a0\u200b\u66dd\u5149\u200b\u3002</p> </li> </ul>"},{"location":"en/community/community_contribution.html#22","title":"2.2 \u200b\u4ee3\u7801\u4f18\u5316","text":"<p>\u200b\u5982\u679c\u200b\u60a8\u200b\u5728\u200b\u4f7f\u7528\u200bPaddleOCR\u200b\u65f6\u200b\u9047\u5230\u200b\u4e86\u200b\u4ee3\u7801\u200bbug\u3001\u200b\u529f\u80fd\u200b\u4e0d\u200b\u7b26\u5408\u200b\u9884\u671f\u200b\u7b49\u200b\u95ee\u9898\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4e3a\u200bPaddleOCR\u200b\u8d21\u732e\u200b\u60a8\u200b\u7684\u200b\u4fee\u6539\u200b\uff0c\u200b\u5176\u4e2d\u200b\uff1a</p> <ul> <li> <p>Python\u200b\u4ee3\u7801\u200b\u89c4\u8303\u200b\u53ef\u200b\u53c2\u8003\u200b\u9644\u5f55\u200b1\uff1aPython\u200b\u4ee3\u7801\u200b\u89c4\u8303\u200b\u3002</p> </li> <li> <p>\u200b\u63d0\u4ea4\u200b\u4ee3\u7801\u200b\u524d\u200b\u8bf7\u200b\u518d\u4e09\u200b\u786e\u8ba4\u200b\u4e0d\u4f1a\u200b\u5f15\u5165\u200b\u65b0\u200b\u7684\u200bbug\uff0c\u200b\u5e76\u200b\u5728\u200bPR\u200b\u4e2d\u200b\u63cf\u8ff0\u200b\u4f18\u5316\u200b\u70b9\u200b\u3002\u200b\u5982\u679c\u200b\u8be5\u200bPR\u200b\u89e3\u51b3\u200b\u4e86\u200b\u67d0\u4e2a\u200bissue\uff0c\u200b\u8bf7\u200b\u5728\u200bPR\u200b\u4e2d\u200b\u8fde\u63a5\u200b\u5230\u200b\u8be5\u200bissue\u3002\u200b\u6240\u6709\u200b\u7684\u200bPR\u200b\u90fd\u200b\u5e94\u8be5\u200b\u9075\u5b88\u200b\u9644\u5f55\u200b3\u200b\u4e2d\u200b\u7684\u200b3.2.10 \u200b\u63d0\u4ea4\u200b\u4ee3\u7801\u200b\u7684\u200b\u4e00\u4e9b\u200b\u7ea6\u5b9a\u200b\u3002</p> </li> <li> <p>\u200b\u8bf7\u200b\u5728\u200b\u63d0\u4ea4\u200b\u4e4b\u524d\u200b\u53c2\u8003\u200b\u4e0b\u65b9\u200b\u7684\u200b\u9644\u5f55\u200b3\uff1aPull Request\u200b\u8bf4\u660e\u200b\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u5bf9\u200bgit\u200b\u7684\u200b\u63d0\u4ea4\u200b\u6d41\u7a0b\u200b\u4e0d\u200b\u719f\u6089\u200b\uff0c\u200b\u540c\u6837\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u9644\u5f55\u200b3\u200b\u7684\u200b3.2\u200b\u8282\u200b\u3002</p> </li> </ul>"},{"location":"en/community/community_contribution.html#23","title":"2.3 \u200b\u6587\u6863\u200b\u4f18\u5316","text":"<p>\u200b\u5982\u679c\u200b\u60a8\u200b\u5728\u200b\u4f7f\u7528\u200bPaddleOCR\u200b\u65f6\u200b\u9047\u5230\u200b\u4e86\u200b\u6587\u6863\u200b\u8868\u8ff0\u200b\u4e0d\u200b\u6e05\u695a\u200b\u3001\u200b\u63cf\u8ff0\u200b\u7f3a\u5931\u200b\u3001\u200b\u94fe\u63a5\u200b\u5931\u6548\u200b\u7b49\u200b\u95ee\u9898\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4e3a\u200bPaddleOCR\u200b\u8d21\u732e\u200b\u60a8\u200b\u7684\u200b\u4fee\u6539\u200b\u3002\u200b\u6587\u6863\u200b\u4e66\u5199\u200b\u89c4\u8303\u200b\u8bf7\u200b\u53c2\u8003\u200b\u9644\u5f55\u200b2\uff1a\u200b\u6587\u6863\u200b\u89c4\u8303\u200b\u3002</p>"},{"location":"en/community/community_contribution.html#3","title":"3. \u200b\u66f4\u200b\u591a\u200b\u8d21\u732e\u200b\u673a\u4f1a","text":"<p>\u200b\u6211\u4eec\u200b\u975e\u5e38\u200b\u9f13\u52b1\u200b\u5f00\u53d1\u8005\u200b\u4f7f\u7528\u200bPaddleOCR\u200b\u5b9e\u73b0\u200b\u81ea\u5df1\u200b\u7684\u200b\u60f3\u6cd5\u200b\uff0c\u200b\u540c\u65f6\u200b\u6211\u4eec\u200b\u4e5f\u200b\u5217\u51fa\u200b\u4e00\u4e9b\u200b\u7ecf\u8fc7\u200b\u5206\u6790\u200b\u540e\u200b\u8ba4\u4e3a\u200b\u6709\u200b\u4ef7\u503c\u200b\u7684\u200b\u62d3\u5c55\u200b\u65b9\u5411\u200b\uff0c\u200b\u6574\u4f53\u200b\u6536\u96c6\u200b\u5728\u200b\u793e\u533a\u200b\u9879\u76ee\u200b\u5e38\u89c4\u8d5b\u200b\u4e2d\u200b\u3002</p>"},{"location":"en/community/community_contribution.html#4","title":"4. \u200b\u8054\u7cfb\u200b\u6211\u4eec","text":"<p>\u200b\u6211\u4eec\u200b\u975e\u5e38\u200b\u6b22\u8fce\u200b\u5e7f\u5927\u200b\u5f00\u53d1\u8005\u200b\u5728\u200b\u6709\u200b\u610f\u5411\u200b\u4e3a\u200bPaddleOCR\u200b\u8d21\u732e\u200b\u4ee3\u7801\u200b\u3001\u200b\u6587\u6863\u200b\u3001\u200b\u8bed\u6599\u200b\u7b49\u200b\u5185\u5bb9\u200b\u524d\u200b\u4e0e\u200b\u6211\u4eec\u200b\u8054\u7cfb\u200b\uff0c\u200b\u8fd9\u6837\u200b\u53ef\u4ee5\u200b\u5927\u5927\u964d\u4f4e\u200bPR\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u7684\u200b\u6c9f\u901a\u200b\u6210\u672c\u200b\u3002\u200b\u540c\u65f6\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u89c9\u5f97\u200b\u67d0\u4e9b\u200b\u60f3\u6cd5\u200b\u4e2a\u4eba\u200b\u96be\u4ee5\u5b9e\u73b0\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200bSIG\u200b\u7684\u200b\u5f62\u5f0f\u200b\u5b9a\u5411\u200b\u4e3a\u200b\u9879\u76ee\u200b\u62db\u52df\u200b\u5fd7\u540c\u9053\u5408\u200b\u7684\u200b\u5f00\u53d1\u8005\u200b\u4e00\u8d77\u200b\u5171\u5efa\u200b\u3002\u200b\u901a\u8fc7\u200bSIG\u200b\u6e20\u9053\u200b\u8d21\u732e\u200b\u7684\u200b\u9879\u76ee\u200b\u5c06\u4f1a\u200b\u83b7\u5f97\u200b\u6df1\u5c42\u6b21\u200b\u7684\u200b\u7814\u53d1\u200b\u652f\u6301\u200b\u4e0e\u200b\u8fd0\u8425\u200b\u8d44\u6e90\u200b\uff08\u200b\u5982\u200b\u516c\u4f17\u200b\u53f7\u200b\u5ba3\u4f20\u200b\u3001\u200b\u76f4\u64ad\u200b\u8bfe\u200b\u7b49\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u63a8\u8350\u200b\u7684\u200b\u8d21\u732e\u200b\u6d41\u7a0b\u200b\u662f\u200b\uff1a</p> <ul> <li>\u200b\u901a\u8fc7\u200b\u5728\u200bgithub issue\u200b\u7684\u200b\u9898\u76ee\u200b\u4e2d\u200b\u589e\u52a0\u200b  <code>\u3010third-party\u3011</code> \u200b\u6807\u8bb0\u200b\uff0c\u200b\u8bf4\u660e\u200b\u9047\u5230\u200b\u7684\u200b\u95ee\u9898\u200b\uff08\u200b\u4ee5\u53ca\u200b\u89e3\u51b3\u200b\u7684\u200b\u601d\u8def\u200b\uff09\u200b\u6216\u200b\u60f3\u200b\u62d3\u5c55\u200b\u7684\u200b\u529f\u80fd\u200b\uff0c\u200b\u7b49\u5f85\u200b\u503c\u73ed\u4eba\u5458\u200b\u56de\u590d\u200b\u3002\u200b\u4f8b\u5982\u200b <code>\u3010third-party\u3011\u200b\u4e3a\u200bPaddleOCR\u200b\u8d21\u732e\u200bIOS\u200b\u793a\u4f8b\u200b</code></li> <li>\u200b\u4e0e\u200b\u6211\u4eec\u200b\u6c9f\u901a\u200b\u786e\u8ba4\u200b\u6280\u672f\u200b\u65b9\u6848\u200b\u6216\u200bbug\u3001\u200b\u4f18\u5316\u200b\u70b9\u200b\u51c6\u786e\u65e0\u8bef\u200b\u540e\u200b\u8fdb\u884c\u200b\u529f\u80fd\u200b\u65b0\u589e\u200b\u6216\u200b\u76f8\u5e94\u200b\u7684\u200b\u4fee\u6539\u200b\uff0c\u200b\u4ee3\u7801\u200b\u4e0e\u200b\u6587\u6863\u200b\u9075\u5faa\u200b\u76f8\u5173\u200b\u89c4\u8303\u200b\u3002</li> <li>PR\u200b\u94fe\u63a5\u200b\u5230\u200b\u4e0a\u8ff0\u200bissue\uff0c\u200b\u7b49\u5f85\u200breview\u3002</li> </ul>"},{"location":"en/community/community_contribution.html#5","title":"5. \u200b\u81f4\u8c22\u200b\u4e0e\u200b\u540e\u7eed","text":"<ul> <li>\u200b\u5408\u5165\u200b\u4ee3\u7801\u200b\u4e4b\u540e\u200b\u4f1a\u200b\u5728\u200b\u672c\u200b\u6587\u6863\u200b\u7b2c\u4e00\u8282\u200b\u4e2d\u200b\u66f4\u65b0\u200b\u4fe1\u606f\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u94fe\u63a5\u200b\u4e3a\u200bgithub\u200b\u540d\u5b57\u200b\u53ca\u200b\u4e3b\u9875\u200b\uff0c\u200b\u5982\u679c\u200b\u6709\u200b\u9700\u8981\u200b\u66f4\u6362\u200b\u4e3b\u9875\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u8054\u7cfb\u200b\u6211\u4eec\u200b\u3002</li> <li>\u200b\u65b0\u589e\u200b\u91cd\u8981\u200b\u529f\u80fd\u200b\u7c7b\u200b\uff0c\u200b\u4f1a\u200b\u5728\u200b\u7528\u6237\u7fa4\u200b\u5e7f\u800c\u544a\u4e4b\u200b\uff0c\u200b\u4eab\u53d7\u200b\u5f00\u6e90\u200b\u793e\u533a\u200b\u8363\u8a89\u200b\u65f6\u523b\u200b\u3002</li> <li>\u200b\u5982\u679c\u200b\u60a8\u200b\u6709\u200b\u57fa\u4e8e\u200bPaddleOCR\u200b\u7684\u200b\u9879\u76ee\u200b\uff0c\u200b\u4f46\u200b\u672a\u200b\u51fa\u73b0\u200b\u5728\u200b\u4e0a\u8ff0\u200b\u5217\u8868\u200b\u4e2d\u200b\uff0c\u200b\u8bf7\u200b\u6309\u7167\u200b <code>4. \u200b\u8054\u7cfb\u200b\u6211\u4eec\u200b</code> \u200b\u7684\u200b\u6b65\u9aa4\u200b\u4e0e\u200b\u6211\u4eec\u200b\u8054\u7cfb\u200b\u3002</li> </ul>"},{"location":"en/data_anno_synth/data_annotation.html","title":"DATA ANNOTATION TOOLS","text":"<p>There are the commonly used data annotation tools, which will be continuously updated. Welcome to contribute tools~</p>"},{"location":"en/data_anno_synth/data_annotation.html#1-labelimg","title":"1. labelImg","text":"<ul> <li>Tool description: Rectangular label</li> <li>Tool address:  https://github.com/tzutalin/labelImg</li> <li> <p>Sketch diagram:</p> <p></p> </li> </ul>"},{"location":"en/data_anno_synth/data_annotation.html#2-rolabelimg","title":"2. roLabelImg","text":"<ul> <li>Tool description: Label tool rewritten based on labelImg, supporting rotating rectangular label</li> <li>Tool address:   https://github.com/cgvict/roLabelImg</li> <li> <p>Sketch diagram:</p> <p></p> </li> </ul>"},{"location":"en/data_anno_synth/data_annotation.html#3-labelme","title":"3. labelme","text":"<ul> <li>Tool description: Support four points, polygons, circles and other labels</li> <li>Tool address:   https://github.com/wkentaro/labelme</li> <li> <p>Sketch diagram:</p> <p></p> </li> </ul>"},{"location":"en/data_anno_synth/data_synthesis.html","title":"DATA SYNTHESIS TOOLS","text":"<p>In addition to open source data, users can also use synthesis tools to synthesize data. There are the commonly used data synthesis tools, which will be continuously updated. Welcome to contribute tools~</p> <ul> <li>Text_renderer</li> <li>SynthText</li> <li>SynthText_Chinese_version</li> <li>TextRecognitionDataGenerator</li> <li>SynthText3D</li> <li>UnrealText</li> <li>SynthTIGER</li> </ul>"},{"location":"en/data_anno_synth/overview.html","title":"Overview","text":"<ul> <li>Semi-automatic Annotation Tool: PPOCRLabel: https://github.com/PFCCLab/PPOCRLabel/blob/main/README_ch.md</li> <li>Data Synthesis Tool: Style-Text: https://github.com/PFCCLab/StyleText/blob/main/README_ch.md</li> </ul>"},{"location":"en/datasets/datasets.html","title":"General Chinese and English OCR dataset","text":"<p>This is a collection of commonly used Chinese datasets, which is being updated continuously. You are welcome to contribute to this list\uff5e</p> <p>In addition to opensource data, users can also use synthesis tools to synthesize data themselves. Current available synthesis tools include text_renderer, SynthText, TextRecognitionDataGenerator, etc.</p>"},{"location":"en/datasets/datasets.html#1-icdar2019-lsvt","title":"1. ICDAR2019-LSVT","text":"<ul> <li>Data sources\uff1ahttps://ai.baidu.com/broad/introduction?dataset=lsvt</li> <li> <p>Introduction\uff1a A total of 45w Chinese street view images, including 5w (2w test + 3w training) fully labeled data (text coordinates + text content), 40w weakly labeled data (text content only), as shown in the following figure:     </p> <p>(a) Fully labeled data</p> <p></p> <p>(b) Weakly labeled data - Download link\uff1ahttps://ai.baidu.com/broad/download?dataset=lsvt</p> </li> </ul>"},{"location":"en/datasets/datasets.html#2-icdar2017-rctw-17","title":"2. ICDAR2017-RCTW-17","text":"<ul> <li>Data sources\uff1ahttps://rctw.vlrlab.net/</li> <li>Introduction\uff1aIt contains 12000 + images, most of them are collected in the wild through mobile camera. Some are screenshots. These images show a variety of scenes, including street views, posters, menus, indoor scenes and screenshots of mobile applications.     </li> <li>Download link\uff1ahttps://rctw.vlrlab.net/dataset/</li> </ul>"},{"location":"en/datasets/datasets.html#3-chinese-street-view-text-recognition","title":"3. Chinese Street View Text Recognition","text":"<ul> <li>Data sources\uff1ahttps://aistudio.baidu.com/aistudio/competition/detail/8</li> <li> <p>Introduction\uff1aA total of 290000 pictures are included, of which 210000 are used as training sets (with labels) and 80000 are used as test sets (without labels). The dataset is collected from the Chinese street view, and is formed by by cutting out the text line area (such as shop signs, landmarks, etc.) in the street view picture. All the images are preprocessed: by using affine transform, the text area is proportionally mapped to a picture with a height of 48 pixels, as shown in the figure:</p> <p></p> <p>(a) Label: \u200b\u9b45\u6d3e\u200b\u96c6\u6210\u200b\u540a\u9876\u200b</p> <p> (b) Label: \u200b\u6bcd\u5a74\u200b\u7528\u54c1\u200b\u8fde\u9501\u200b - Download link https://aistudio.baidu.com/aistudio/datasetdetail/8429</p> </li> </ul>"},{"location":"en/datasets/datasets.html#4-chinese-document-text-recognition","title":"4. Chinese Document Text Recognition","text":"<ul> <li>Data sources\uff1ahttps://github.com/YCG09/chinese_ocr</li> <li>Introduction\uff1a<ul> <li>A total of 3.64 million pictures are divided into training set and validation set according to 99:1.</li> <li>Using Chinese corpus (news + classical Chinese), the data is randomly generated through changes in font, size, grayscale, blur, perspective, stretching, etc.</li> <li>5990 characters including Chinese characters, English letters, numbers and punctuation\uff08Characters set: https://github.com/YCG09/chinese_ocr/blob/master/train/char_std_5990.txt \uff09</li> <li>Each sample is fixed with 10 characters, and the characters are randomly intercepted from the sentences in the corpus</li> <li> <p>Image resolution is 280x32</p> <p></p> <p> - Download link\uff1ahttps://pan.baidu.com/s/1QkI7kjah8SPHwOQ40rS1Pw (Password: lu7m)</p> </li> </ul> </li> </ul>"},{"location":"en/datasets/datasets.html#5icdar2019-art","title":"5\u3001ICDAR2019-ArT","text":"<ul> <li>Data source\uff1ahttps://ai.baidu.com/broad/introduction?dataset=art</li> <li>Introduction\uff1aIt includes 10166 images, 5603 in training sets and 4563 in test sets. It is composed of three parts: total text, scut-ctw1500 and Baidu curved scene text, including text with various shapes such as horizontal, multi-directional and curved.     </li> <li>Download link\uff1ahttps://ai.baidu.com/broad/download?dataset=art</li> </ul>"},{"location":"en/datasets/datasets.html#6-electronic-seal-dataset","title":"6. Electronic seal dataset","text":"<ul> <li>Data source: https://aistudio.baidu.com/aistudio/datasetdetail/154271/0</li> <li>Data introduction: Contains 10,000 images in total, 8,000 images in the training set, and 2,000 images in the test set. The dataset is synthesized by a program and does not involve privacy security. It is mainly used for the training and detection of seal curved text. Contributed by developer jingsongliujing</li> <li>Download address: https://aistudio.baidu.com/aistudio/datasetdetail/154271/0</li> </ul>"},{"location":"en/datasets/datasets.html#references","title":"References","text":"<p>ICDAR 2019-LSVT Challenge</p> <pre><code>@article{sun2019icdar,\n  title={ICDAR 2019 Competition on Large-scale Street View Text with Partial Labeling--RRC-LSVT},\n  author={Sun, Yipeng and Ni, Zihan and Chng, Chee-Kheng and Liu, Yuliang and Luo, Canjie and Ng, Chun Chet and Han, Junyu and Ding, Errui and Liu, Jingtuo and Karatzas, Dimosthenis and others},\n  journal={arXiv preprint arXiv:1909.07741},\n  year={2019}\n}\n</code></pre> <p>ICDAR 2019-ArT Challenge</p> <pre><code>@article{chng2019icdar2019,\n  title={ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text (RRC-ArT)},\n  author={Chng, Chee-Kheng and Liu, Yuliang and Sun, Yipeng and Ng, Chun Chet and Luo, Canjie and Ni, Zihan and Fang, ChuanMing and Zhang, Shuaitao and Han, Junyu and Ding, Errui and others},\n  journal={arXiv preprint arXiv:1909.07145},\n  year={2019}\n}\n</code></pre>"},{"location":"en/datasets/handwritten_datasets.html","title":"Handwritten OCR dataset","text":"<p>Here we have sorted out the commonly used handwritten OCR dataset datasets, which are being updated continuously. We welcome you to contribute datasets ~</p> <ul> <li>Institute of automation, Chinese Academy of Sciences - handwritten Chinese dataset</li> <li>NIST handwritten single character dataset - English</li> </ul>"},{"location":"en/datasets/handwritten_datasets.html#institute-of-automation-chinese-academy-of-sciences-handwritten-chinese-dataset","title":"Institute of automation, Chinese Academy of Sciences - handwritten Chinese dataset","text":"<ul> <li>Data source: http://www.nlpr.ia.ac.cn/databases/handwriting/Download.html</li> <li> <p>Data introduction:</p> <ul> <li>It includes online and offline handwritten data,<code>HWDB1.0~1.2</code> has totally 3895135 handwritten single character samples, which belong to 7356 categories (7185 Chinese characters and 171 English letters, numbers and symbols);<code>HWDB2.0~2.2</code> has totally 5091 pages of images, which are divided into 52230 text lines and 1349414 words. All text and text samples are stored as grayscale images. Some sample words are shown below.</li> </ul> <p></p> </li> <li> <p>Download address:http://www.nlpr.ia.ac.cn/databases/handwriting/Download.html</p> </li> <li>\u200b\u4f7f\u7528\u200b\u5efa\u8bae\u200b:Data for single character, white background, can form a large number of text lines for training. White background can be processed into transparent state, which is convenient to add various backgrounds. For the case of semantic needs, it is suggested to extract single character from real corpus to form text lines.</li> </ul>"},{"location":"en/datasets/handwritten_datasets.html#nist-handwritten-single-character-dataset-englishnist-handprinted-forms-and-characters-database","title":"NIST handwritten single character dataset - English(NIST Handprinted Forms and Characters Database)","text":"<ul> <li>Data source: https://www.nist.gov/srd/nist-special-database-19</li> <li> <p>Data introduction: NIST19 dataset is suitable for handwritten document and character recognition model training. It is extracted from the handwritten sample form of 3600 authors and contains 810000 character images in total. Nine of them are shown below.</p> <p></p> </li> <li> <p>Download address: https://www.nist.gov/srd/nist-special-database-19</p> </li> </ul>"},{"location":"en/datasets/kie_datasets.html","title":"Key Information Extraction Dataset","text":""},{"location":"en/datasets/kie_datasets.html#key-information-extraction-dataset","title":"Key Information Extraction dataset","text":"<p>Here are the common datasets key information extraction, which are being updated continuously. Welcome to contribute datasets.</p>"},{"location":"en/datasets/kie_datasets.html#1-funsd-dataset","title":"1. FUNSD dataset","text":"<ul> <li>Data source: https://guillaumejaume.github.io/FUNSD/</li> <li> <p>Data Introduction: The FUNSD dataset is a dataset for form comprehension. It contains 199 real, fully annotated scanned images, including market reports, advertisements, and academic reports, etc., and is divided into 149 training set and 50 test set. The FUNSD dataset is suitable for many types of DocVQA tasks, such as field-level entity classification, field-level entity connection, etc. Part of the image and the annotation box visualization are shown below:</p> <p></p> <p></p> <p>In the figure, the orange area represents <code>header</code>, the light blue area represents <code>question</code>, the green area represents <code>answer</code>, and the pink area represents <code>other</code>.</p> </li> <li> <p>Download address: https://guillaumejaume.github.io/FUNSD/download/</p> </li> </ul>"},{"location":"en/datasets/kie_datasets.html#2-xfund-dataset","title":"2. XFUND dataset","text":"<ul> <li>Data source: https://github.com/doc-analysis/XFUND</li> <li> <p>Data introduction: XFUND is a multilingual form comprehension dataset, which contains form data in 7 different languages, and all are manually annotated in the form of key-value pairs. The data for each language contains 199 form data, which are divided into 149 training sets and 50 test sets. Part of the image and the annotation box visualization are shown below.</p> <p></p> <p></p> </li> <li> <p>Download address: https://github.com/doc-analysis/XFUND/releases/tag/v1.0</p> </li> </ul>"},{"location":"en/datasets/kie_datasets.html#3-wildreceipt-dataset","title":"3. wildreceipt dataset","text":"<ul> <li>Data source: https://arxiv.org/abs/2103.14470</li> <li> <p>Data introduction: wildreceipt is an English receipt dataset, which contains 26 different categories. There are 1267 training images and 472 evaluation images, in which 50,000 textlines and boxes are annotated. Part of the image and the annotation box visualization are shown below.</p> <p></p> <p></p> </li> </ul> <p>Note\uff1a Boxes with category <code>Ignore</code> or <code>Others</code> are not visualized here.</p> <ul> <li>Download address\uff1a<ul> <li>Official dataset: link</li> <li>Dataset converted for PaddleOCR training process: link</li> </ul> </li> </ul>"},{"location":"en/datasets/layout_datasets.html","title":"Layout Analysis Dataset","text":""},{"location":"en/datasets/layout_datasets.html#layout-analysis-dataset","title":"Layout Analysis Dataset","text":"<p>Here are the common datasets of layout analysis, which are being updated continuously. Welcome to contribute datasets.</p> <p>Most of the layout analysis datasets are object detection datasets. In addition to open source datasets, you can also label or synthesize datasets using tools such as labelme and so on.</p>"},{"location":"en/datasets/layout_datasets.html#1-publaynet-dataset","title":"1. PubLayNet dataset","text":"<ul> <li>Data source: https://github.com/ibm-aur-nlp/PubLayNet</li> <li> <p>Data introduction: The PubLayNet dataset contains 350000 training images and 11000 validation images. There are 5 categories in total, namely: <code>text, title, list, table, figure</code>. Some images and their annotations as shown below.</p> <p></p> <p></p> </li> <li> <p>Download address: https://developer.ibm.com/exchanges/data/all/publaynet/</p> </li> <li>Note: When using this dataset, you need to follow CDLA-Permissive license.</li> </ul>"},{"location":"en/datasets/layout_datasets.html#2cdla-dataset","title":"2\u3001CDLA dataset","text":"<ul> <li>Data source: https://github.com/buptlihang/CDLA</li> <li> <p>Data introduction: CDLA dataset contains 5000 training images and 1000 validation images with 10 categories, which are <code>Text, Title, Figure, Figure caption, Table, Table caption, Header, Footer, Reference, Equation</code>. Some images and their annotations as shown below.</p> <p></p> <p></p> </li> <li> <p>Download address: https://github.com/buptlihang/CDLA</p> </li> <li>Note: When you train detection model on CDLA dataset using PaddleDetection, you need to remove the label <code>__ignore__</code> and <code>_background_</code>.</li> </ul>"},{"location":"en/datasets/layout_datasets.html#3tablebank-dataset","title":"3\u3001TableBank dataset","text":"<ul> <li>Data source: https://doc-analysis.github.io/tablebank-page/index.html</li> <li> <p>Data introduction: TableBank dataset contains 2 types of document: Latex (187199 training images, 7265 validation images and 5719 testing images) and Word (73383 training images 2735 validation images and 2281 testing images). Some images and their annotations as shown below.</p> <p></p> <p></p> </li> <li> <p>Data source: https://doc-analysis.github.io/tablebank-page/index.html</p> </li> <li>Note: When using this dataset, you need to follow Apache-2.0 license.</li> </ul>"},{"location":"en/datasets/ocr_datasets.html","title":"OCR datasets","text":"<p>Here is a list of public datasets commonly used in OCR, which are being continuously updated. Welcome to contribute datasets!</p>"},{"location":"en/datasets/ocr_datasets.html#1-text-detection","title":"1. Text detection","text":""},{"location":"en/datasets/ocr_datasets.html#11-paddleocr-text-detection-format-annotation","title":"1.1 PaddleOCR text detection format annotation","text":"<p>The annotation file formats supported by the PaddleOCR text detection algorithm are as follows, separated by \"\\t\":</p> <pre><code>\"Image file name             Image annotation information encoded by json.dumps\"\nch4_test_images/img_61.jpg    [{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre> <p>The image annotation after json.dumps() encoding is a list containing multiple dictionaries.</p> <p>The <code>points</code> in the dictionary represent the coordinates (x, y) of the four points of the text box, arranged clockwise from the point at the upper left corner.</p> <p><code>transcription</code> represents the text of the current text box. When its content is \"###\" it means that the text box is invalid and will be skipped during training.</p> <p>If you want to train PaddleOCR on other datasets, please build the annotation file according to the above format.</p>"},{"location":"en/datasets/ocr_datasets.html#12-public-dataset","title":"1.2 Public dataset","text":"dataset Image download link PaddleOCR format annotation download link ICDAR 2015 https://rrc.cvc.uab.es/?ch=4&amp;com=downloads train / test ctw1500 https://paddleocr.bj.bcebos.com/dataset/ctw1500.zip Included in the downloaded image zip total text https://paddleocr.bj.bcebos.com/dataset/total_text.tar Included in the downloaded image zip"},{"location":"en/datasets/ocr_datasets.html#121-icdar-2015","title":"1.2.1 ICDAR 2015","text":"<p>The icdar2015 dataset contains train set which has 1000 images obtained with wearable cameras and test set which has 500 images obtained with wearable cameras. The icdar2015 dataset can be downloaded from the link in the table above. Registration is required for downloading.</p> <p>After registering and logging in, download the part marked in the red box in the figure below. And, the content downloaded by <code>Training Set Images</code> should be saved as the folder <code>icdar_c4_train_imgs</code>, and the content downloaded by <code>Test Set Images</code> is saved as the folder <code>ch4_test_images</code></p> <p></p> <p>Decompress the downloaded dataset to the working directory, assuming it is decompressed under PaddleOCR/train_data/. Then download the PaddleOCR format annotation file from the table above.</p> <p>PaddleOCR also provides a data format conversion script, which can convert the official website label to the PaddleOCR format. The data conversion tool is in <code>ppocr/utils/gen_label.py</code>, here is the training set as an example:</p> <pre><code># Convert the label file downloaded from the official website to train_icdar2015_label.txt\npython gen_label.py --mode=\"det\" --root_path=\"/path/to/icdar_c4_train_imgs/\"  \\\n                    --input_path=\"/path/to/ch4_training_localization_transcription_gt\" \\\n                    --output_label=\"/path/to/train_icdar2015_label.txt\"\n</code></pre> <p>After decompressing the data set and downloading the annotation file, PaddleOCR/train_data/ has two folders and two files, which are:</p> <pre><code>/PaddleOCR/train_data/icdar2015/text_localization/\n  \u2514\u2500 icdar_c4_train_imgs/         Training data of icdar dataset\n  \u2514\u2500 ch4_test_images/             Testing data of icdar dataset\n  \u2514\u2500 train_icdar2015_label.txt    Training annotation of icdar dataset\n  \u2514\u2500 test_icdar2015_label.txt     Test annotation of icdar dataset\n</code></pre>"},{"location":"en/datasets/ocr_datasets.html#2-text-recognition","title":"2. Text recognition","text":""},{"location":"en/datasets/ocr_datasets.html#21-paddleocr-text-recognition-format-annotation","title":"2.1 PaddleOCR text recognition format annotation","text":"<p>The text recognition algorithm in PaddleOCR supports two data formats:</p> <ul> <li><code>lmdb</code> is used to train data sets stored in lmdb format, use lmdb_dataset.py to load;</li> <li><code>common dataset</code> is used to train data sets stored in text files, use simple_dataset.py to load.</li> </ul> <p>If you want to use your own data for training, please refer to the following to organize your data.</p>"},{"location":"en/datasets/ocr_datasets.html#training-set","title":"Training set","text":"<p>It is recommended to put the training images in the same folder, and use a txt file (rec_gt_train.txt) to store the image path and label. The contents of the txt file are as follows:</p> <ul> <li>Note: by default, the image path and image label are split with \\t, if you use other methods to split, it will cause training error</li> </ul> <pre><code>\" Image file name           Image annotation \"\n\ntrain_data/rec/train/word_001.jpg   \u200b\u7b80\u5355\u200b\u53ef\u200b\u4f9d\u8d56\u200b\ntrain_data/rec/train/word_002.jpg   \u200b\u7528\u200b\u79d1\u6280\u200b\u8ba9\u200b\u590d\u6742\u200b\u7684\u200b\u4e16\u754c\u200b\u66f4\u200b\u7b80\u5355\u200b\n...\n</code></pre> <p>The final training set should have the following file structure:</p> <pre><code>|-train_data\n  |-rec\n    |- rec_gt_train.txt\n    |- train\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"en/datasets/ocr_datasets.html#test-set","title":"Test set","text":"<p>Similar to the training set, the test set also needs to be provided a folder containing all images (test) and a rec_gt_test.txt. The structure of the test set is as follows:</p> <pre><code>|-train_data\n  |-rec\n    |-ic15_data\n        |- rec_gt_test.txt\n        |- test\n            |- word_001.jpg\n            |- word_002.jpg\n            |- word_003.jpg\n            | ...\n</code></pre>"},{"location":"en/datasets/ocr_datasets.html#22-public-dataset","title":"2.2 Public dataset","text":"dataset Image download link PaddleOCR format annotation download link en benchmark(MJ, SJ, IIIT, SVT, IC03, IC13, IC15, SVTP, and CUTE.) DTRB LMDB format, which can be loaded directly with lmdb_dataset.py ICDAR 2015 http://rrc.cvc.uab.es/?ch=4&amp;com=downloads train/ test Multilingual datasets Baidu network disk Extraction code: frgi  google drive Included in the downloaded image zip"},{"location":"en/datasets/ocr_datasets.html#21-icdar-2015","title":"2.1 ICDAR 2015","text":"<p>The ICDAR 2015 dataset can be downloaded from the link in the table above for quick validation. The lmdb format dataset required by en benchmark can also be downloaded from the table above.</p> <p>Then download the PaddleOCR format annotation file from the table above.</p> <p>PaddleOCR also provides a data format conversion script, which can convert the ICDAR official website label to the data format supported by PaddleOCR. The data conversion tool is in <code>ppocr/utils/gen_label.py</code>, here is the training set as an example:</p> <pre><code># Convert the label file downloaded from the official website to rec_gt_label.txt\npython gen_label.py --mode=\"rec\" --input_path=\"{path/of/origin/label}\" --output_label=\"rec_gt_label.txt\"\n</code></pre> <p>The data format is as follows, (a) is the original picture, (b) is the Ground Truth text file corresponding to each picture:</p> <p></p>"},{"location":"en/datasets/ocr_datasets.html#3-data-storage-path","title":"3. Data storage path","text":"<p>The default storage path for PaddleOCR training data is <code>PaddleOCR/train_data</code>, if you already have a dataset on your disk, just create a soft link to the dataset directory:</p> <pre><code># linux and mac os\nln -sf &lt;path/to/dataset&gt; &lt;path/to/paddle_ocr&gt;/train_data/dataset\n# windows\nmklink /d &lt;path/to/paddle_ocr&gt;/train_data/dataset &lt;path/to/dataset&gt;\n</code></pre>"},{"location":"en/datasets/table_datasets.html","title":"Table Recognition Datasets","text":"<p>Here are the commonly used table recognition datasets, which are being updated continuously. Welcome to contribute datasets~</p>"},{"location":"en/datasets/table_datasets.html#dataset-summary","title":"Dataset Summary","text":"dataset Image download link PPOCR format annotation download link PubTabNet https://github.com/ibm-aur-nlp/PubTabNet jsonl format, which can be loaded directly with pubtab_dataset.py TAL Table Recognition Competition Dataset https://ai.100tal.com/dataset jsonl format, which can be loaded directly with pubtab_dataset.py WTW Chinese scene table dataset https://github.com/wangwen-whu/WTW-Dataset Conversion is required to load with pubtab_dataset.py"},{"location":"en/datasets/table_datasets.html#1-pubtabnet","title":"1. PubTabNet","text":"<ul> <li> <p>Data Introduction\uff1aThe training set of the PubTabNet dataset contains 500,000 images and the validation set contains 9000 images. Part of the image visualization is shown below.</p> <p></p> <p></p> </li> <li> <p>illustrate\uff1aWhen using this dataset, the CDLA-Permissive protocol is required.</p> </li> </ul>"},{"location":"en/datasets/table_datasets.html#2-tal-table-recognition-competition-dataset","title":"2. TAL Table Recognition Competition Dataset","text":"<ul> <li> <p>Data Introduction\uff1aThe training set of the TAL table recognition competition dataset contains 16,000 images. The validation set does not give trainable annotations.</p> <p></p> <p></p> </li> </ul>"},{"location":"en/datasets/table_datasets.html#3-wtw-chinese-scene-table-dataset","title":"3. WTW Chinese scene table dataset","text":"<ul> <li> <p>Data Introduction\uff1aThe WTW Chinese scene table dataset consists of two parts: table detection and table data. The dataset contains images of two scenes, scanned and photographed.</p> <p></p> </li> </ul>"},{"location":"en/datasets/vertical_and_multilingual_datasets.html","title":"Vertical multi-language OCR dataset","text":"<p>Here we have sorted out the commonly used vertical multi-language OCR dataset datasets, which are being updated continuously. We welcome you to contribute datasets \uff5e</p> <ul> <li>Chinese urban license plate dataset</li> <li>Bank credit card dataset</li> <li>Captcha dataset-Captcha</li> <li>multi-language dataset</li> </ul>"},{"location":"en/datasets/vertical_and_multilingual_datasets.html#chinese-urban-license-plate-dataset","title":"Chinese urban license plate dataset","text":"<ul> <li> <p>Data source\uff1aCCPD</p> </li> <li> <p>Data introduction: It contains more than 250000 vehicle license plate images and vehicle license plate detection and recognition information labeling. It contains the following license plate image information in different scenes.</p> <ul> <li>CCPD-Base: General license plate picture</li> <li>CCPD-DB: The brightness of license plate area is bright, dark or uneven</li> <li>CCPD-FN: The license plate is farther or closer to the camera location</li> <li>CCPD-Rotate: License plate includes rotation (horizontal 20~50 degrees, vertical-10~10 degrees)</li> <li>CCPD-Tilt: License plate includes rotation (horizontal 15~45 degrees, vertical 15~45 degrees)</li> <li>CCPD-Blur: The license plate contains blurring due to camera lens jitter</li> <li>CCPD-Weather: The license plate is photographed on rainy, snowy or foggy days</li> <li>CCPD-Challenge: So far, some of the most challenging images in license plate detection and recognition tasks</li> <li>CCPD-NP: Pictures of new cars without license plates.</li> </ul> <p></p> </li> <li> <p>Download address</p> <ul> <li>Baidu cloud download address (extracted code is hm0U): https://pan.baidu.com/s/1i5AOjAbtkwb17Zy-NQGqkw</li> <li>Google drive download address:https://drive.google.com/file/d/1rdEsCUcIUaYOVRkx5IMTRNA7PcGMmSgc/view</li> </ul> </li> </ul>"},{"location":"en/datasets/vertical_and_multilingual_datasets.html#bank-credit-card-dataset","title":"Bank credit card dataset","text":"<ul> <li> <p>Data source: source</p> </li> <li> <p>Data introduction: There are three types of training data</p> <ul> <li>1.Sample card data of China Merchants Bank: including card image data and annotation data, a total of 618 pictures</li> <li>2.Single character data: including pictures and annotation data, 37 pictures in total.</li> <li> <p>3.There are only other bank cards, no more detailed information, a total of 50 pictures.</p> </li> <li> <p>The demo image is shown as follows. The annotation information is stored in excel, and the demo image below is marked as</p> <ul> <li>Top 8 card number: 62257583</li> <li>Card type: card of our bank</li> <li>End of validity: 07/41</li> <li>Chinese phonetic alphabet of card users: MICHAEL</li> </ul> <p></p> </li> </ul> </li> <li> <p>Download address: cmb2017-2.zip</p> </li> </ul>"},{"location":"en/datasets/vertical_and_multilingual_datasets.html#captcha-dataset-captcha","title":"Captcha dataset-Captcha","text":"<ul> <li>Data source: captcha</li> <li> <p>Data introduction: This is a toolkit for data synthesis. You can output captcha images according to the input text. Use the toolkit to generate several demo images as follows.</p> <p></p> </li> <li> <p>Download address: The dataset is generated and has no download address.</p> </li> </ul>"},{"location":"en/datasets/vertical_and_multilingual_datasets.html#multi-language-datasetmulti-lingual-scene-text-detection-and-recognition","title":"multi-language dataset(Multi-lingual scene text detection and recognition)","text":"<ul> <li>Data source: source</li> <li>Data introduction: Multi language detection dataset MLT contains both language recognition and detection tasks.<ul> <li>In the detection task, the training set contains 10000 images in 10 languages, and each language contains 1000 training images. The test set contains 10000 images.</li> <li>In the recognition task, the training set contains 111998 samples.</li> </ul> </li> <li>Download address: The training set is large and can be downloaded in two parts. It can only be downloaded after registering on the website: source</li> </ul>"},{"location":"en/index/index.html","title":"Index","text":""},{"location":"en/index/index.html#introduction","title":"Introduction","text":"<p>PaddleOCR aims to create multilingual, awesome, leading, and practical OCR tools that help users train better models and apply them into practice.</p>"},{"location":"en/index/index.html#community","title":"\ud83d\ude80 Community","text":"<p>PaddleOCR is being oversight by a PMC. Issues and PRs will be reviewed on a best-effort basis. For a complete overview of PaddlePaddle community, please visit community.</p> <p>\u26a0\ufe0f Note: The Issues module is only for reporting program \ud83d\udc1e bugs, for the rest of the questions, please move to the Discussions. Please note that if the Issue mentioned is not a bug, it will be moved to the Discussions module.</p>"},{"location":"en/index/index.html#recent-updates","title":"\ud83d\udce3 Recent updates","text":"<ul> <li> <p>\ud83d\udd25\ud83d\udd252025.3.7 release PaddleOCR v2.10, including:</p> </li> <li> <p>12 new self-developed single models:</p> <ul> <li>Layout Detection series with 3 models: PP-DocLayout-L, PP-DocLayout-M, PP-DocLayout-S, supporting prediction of 23 common layout categories. High-quality layout detection for various document types such as papers, reports, exams, books, magazines, contracts, newspapers in both English and Chinese. mAP@0.5 reaches up to 90.4%, lightweight models can process over 100 pages of document images per second end-to-end.</li> <li>Formula Recognition series with 2 models: PP-FormulaNet-L, PP-FormulaNet-S, supporting 50,000 common LaTeX vocabulary, capable of recognizing complex printed and handwritten formulas. PP-FormulaNet-L has 6 percentage points higher accuracy than models of the same level, and PP-FormulaNet-S is 16 times faster than models with similar accuracy.</li> <li>Table Structure Recognition series with 2 models: SLANeXt_wired, SLANeXt_wireless. A newly developed table structure recognition model, supporting structured prediction for both wired and wireless tables. Compared to SLANet_plus, SLANeXt shows significant improvement in table structure, with 6 percentage points higher accuracy on internal high-difficulty table recognition evaluation sets.</li> <li>Table Classification series with 1 model: PP-LCNet_x1_0_table_cls, an ultra-lightweight classification model for both wired and wireless tables.</li> <li>Table Cell Detection series with 2 models: RT-DETR-L_wired_table_cell_det, RT-DETR-L_wireless_table_cell_det, supporting cell detection in both wired and wireless tables. These can be combined with SLANeXt_wired, SLANeXt_wireless, text detection, and text recognition modules for end-to-end table prediction. (See the newly added Table Recognition v2 pipeline)</li> <li>Text Recognition series with 1 model: PP-OCRv4_server_rec_doc, supports over 15,000 characters, with a broader text recognition range, additionally improving the recognition accuracy of certain texts. The accuracy is more than 3 percentage points higher than PP-OCRv4_server_rec on internal datasets.</li> <li>Text Line Orientation Classification series with 1 model: PP-LCNet_x0_25_textline_ori, an ultra-lightweight text line orientation classification model with only 0.3M storage.</li> </ul> </li> <li> <p>4 high-value multi-model combination solutions:</p> <ul> <li>Document Image Preprocessing Pipeline: Achieve correction of distortion and orientation in document images through the combination of ultra-lightweight models.</li> <li>Layout Parsing v2 Pipeline: Combines multiple self-developed different types of OCR models to optimize complex layout reading order, achieving end-to-end conversion of various complex PDF files to Markdown and JSON files. The conversion effect is better than other open-source solutions in multiple document scenarios. It can provide high-quality data production capabilities for large model training and application.</li> <li>Table Recognition v2 Pipeline: Provides better table recognition capabilities. By combining table classification module, table cell detection module, table structure recognition module, text detection module, text recognition module, etc., it achieves prediction of various styles of tables. Users can customize and finetune any module to improve the effect of vertical tables.</li> <li>PP-ChatOCRv4-doc Pipeline: Based on PP-ChatOCRv3-doc, integrating multi-modal large models, optimizing Prompt and multi-model combination post-processing logic. It effectively addresses common complex document information extraction challenges such as layout analysis, rare characters, multi-page PDFs, tables, and seal recognition, achieving 15 percentage points higher accuracy than PP-ChatOCRv3-doc. The large model upgrades local deployment capabilities, providing a standard OpenAI interface, supporting calls to locally deployed large models like DeepSeek-R1.</li> </ul> </li> <li> <p>\ud83d\udd25 2024.10.18 release PaddleOCR v2.9, including:</p> <ul> <li> <p>PaddleX, an All-in-One development tool based on PaddleOCR's advanced technology, supports low-code full-process development capabilities in the OCR field:</p> <ul> <li> <p>\ud83c\udfa8 Rich Model One-Click Call: Integrates 17 models related to text image intelligent analysis, general OCR, general layout parsing, table recognition, formula recognition, and seal recognition into 6 pipelines, which can be quickly experienced through a simple Python API one-click call. In addition, the same set of APIs also supports a total of 200+ models in image classification, object detection, image segmentation, and time series forecasting, forming 20+ single-function modules, making it convenient for developers to use model combinations.</p> </li> <li> <p>\ud83d\ude80 High Efficiency and Low barrier of entry: Provides two methods based on unified commands and GUI to achieve simple and efficient use, combination, and customization of models. Supports multiple deployment methods such as high-performance inference, service-oriented deployment, and on-device deployment. Additionally, for various mainstream hardware such as NVIDIA GPU, Kunlunxin XPU, Ascend NPU, Cambricon MLU, and Haiguang DCU, models can be developed with seamless switching.</p> </li> </ul> </li> <li> <p>Supports PP-ChatOCRv3-doc, high-precision layout detection model based on RT-DETR and high-efficiency layout area detection model based on PicoDet, high-precision table structure recognition model, text image unwarping model UVDoc, formula recognition model LatexOCR, and document image orientation classification model based on PP-LCNet.</p> </li> </ul> </li> <li> <p>\ud83d\udd252023.8.7 Release PaddleOCRrelease/2.7</p> <ul> <li> <p>Release PP-OCRv4, support mobile version and server version</p> <ul> <li>PP-OCRv4-mobile\uff1aWhen the speed is comparable, the effect of the Chinese scene is improved by 4.5% compared with PP-OCRv3, the English scene is improved by 10%, and the average recognition accuracy of the 80-language multilingual model is increased by more than 8%.</li> <li>PP-OCRv4-server\uff1aRelease the OCR model with the highest accuracy at present, the detection model accuracy increased by 4.9% in the Chinese and English scenes, and the recognition model accuracy increased by 2% refer quickstart quick use by one line command, At the same time, the whole process of model training, reasoning, and high-performance deployment can also be completed with few code in the General OCR Industry Solution in PaddleX.</li> </ul> </li> <li> <p>ReleasePP-ChatOCR, a new scheme for extracting key information of general scenes using PP-OCR model and ERNIE LLM.</p> </li> </ul> </li> <li> <p>\ud83d\udd282022.11 Add implementation of 4 cutting-edge algorithms\uff1aText Detection DRRG,  Text Recognition RFL, Image Super-Resolution Text Telescope\uff0cHandwritten Mathematical Expression Recognition CAN</p> </li> <li> <p>2022.10 release optimized JS version PP-OCRv3 model with 4.3M model size, 8x faster inference time, and a ready-to-use web demo</p> <ul> <li>\ud83d\udca5 Live Playback: Introduction to PP-StructureV2 optimization strategy. Scan the QR code below using WeChat, follow the PaddlePaddle official account and fill out the questionnaire to join the WeChat group, get the live link and 20G OCR learning materials (including PDF2Word application, 10 models in vertical scenarios, etc.)</li> </ul> </li> <li> <p>\ud83d\udd252022.8.24 Release PaddleOCR release/2.6</p> <ul> <li>Release PP-StructureV2\uff0cwith functions and performance fully upgraded, adapted to Chinese scenes, and new support for Layout Recovery and one line command to convert PDF to Word;</li> <li>Layout Analysis optimization: model storage reduced by 95%, while speed increased by 11 times, and the average CPU time-cost is only 41ms;</li> <li>Table Recognition optimization: 3 optimization strategies are designed, and the model accuracy is improved by 6% under comparable time consumption;</li> <li>Key Information Extraction optimization\uff1aa visual-independent model structure is designed, the accuracy of semantic entity recognition is increased by 2.8%, and the accuracy of relation extraction is increased by 9.1%.</li> </ul> </li> <li> <p>\ud83d\udd252022.8 Release OCR scene application collection</p> <ul> <li>Release 9 vertical models such as digital tube, LCD screen, license plate, handwriting recognition model, high-precision SVTR model, etc, covering the main OCR vertical applications in general, manufacturing, finance, and transportation industries.</li> </ul> </li> <li> <p>2022.8 Add implementation of 8 cutting-edge algorithms</p> <ul> <li>Text Detection: FCENet, DB++</li> <li>Text Recognition: ViTSTR, ABINet, VisionLAN, SPIN, RobustScanner</li> <li>Table Recognition: TableMaster</li> <li> <p>2022.5.9 Release PaddleOCR release/2.5</p> </li> <li> <p>Release PP-OCRv3: With comparable speed, the effect of Chinese scene is further improved by 5% compared with PP-OCRv2, the effect of English scene is improved by 11%, and the average recognition accuracy of 80 language multilingual models is improved by more than 5%.</p> </li> <li>Release PPOCRLabelv2: Add the annotation function for table recognition task, key information extraction task and irregular text image.</li> <li>Release interactive e-book \"Dive into OCR\", covers the cutting-edge theory and code practice of OCR full stack technology.</li> <li>more</li> </ul> </li> </ul>"},{"location":"en/index/index.html#features","title":"\ud83c\udf1f Features","text":"<p>PaddleOCR support a variety of cutting-edge algorithms related to OCR, and developed industrial featured models/solution PP-OCR\u3001PP-Structure and PP-ChatOCR on this basis, and get through the whole process of data production, model training, compression, inference and deployment.</p> <p></p> <p>It is recommended to start with the \u201cquick experience\u201d in the document tutorial</p>"},{"location":"en/index/index.html#technical-exchange-and-cooperation","title":"\ud83d\udcd6 Technical exchange and cooperation","text":"<ul> <li> <p>PaddleX provides a one-stop full-process high-efficiency development platform for flying paddle ecological model training, pressure, and push. Its mission is to help AI technology quickly land, and its vision is to make everyone an AI Developer!</p> <ul> <li>PaddleX currently covers areas such as image classification, object detection, image segmentation, 3D, OCR, and time series prediction, and has built-in 36 basic single models, such as RP-DETR, PP-YOLOE, PP-HGNet, PP-LCNet, PP- LiteSeg, etc.; integrated 12 practical industrial solutions, such as PP-OCRv4, PP-ChatOCR, PP-ShiTu, PP-TS, vehicle-mounted road waste detection, identification of prohibited wildlife products, etc.</li> <li>PaddleX provides two AI development modes: \"Toolbox\" and \"Developer\". The toolbox mode can tune key hyperparameters without code, and the developer mode can perform single-model training, push and multi-model serial inference with low code, and supports both cloud and local terminals.</li> <li>PaddleX also supports joint innovation and development, profit sharing! At present, PaddleX is rapidly iterating, and welcomes the participation of individual developers and enterprise developers to create a prosperous AI technology ecosystem!</li> </ul> </li> </ul>"},{"location":"en/index/index.html#guideline-for-new-language-requests","title":"\ud83c\uddfa\ud83c\uddf3 Guideline for New Language Requests","text":"<p>If you want to request a new language support, a PR with 1 following files are needed\uff1a</p> <ul> <li>In folder ppocr/utils/dict, it is necessary to submit the dict text to this path and name it with <code>{language}_dict.txt</code> that contains a list of all characters. Please see the format example from other files in that folder.</li> </ul> <p>If your language has unique elements, please tell me in advance within any way, such as useful links, wikipedia and so on.</p> <p>More details, please refer to Multilingual OCR Development Plan.</p>"},{"location":"en/index/index.html#visualization","title":"Visualization","text":""},{"location":"en/index/index.html#pp-ocrv3","title":"PP-OCRv3","text":""},{"location":"en/index/index.html#pp-ocrv3-chinese-model","title":"PP-OCRv3 Chinese model","text":""},{"location":"en/index/index.html#pp-ocrv3-english-model","title":"PP-OCRv3 English model","text":""},{"location":"en/index/index.html#pp-ocrv3-multilingual-model","title":"PP-OCRv3 Multilingual model","text":""},{"location":"en/index/index.html#pp-structurev2","title":"PP-StructureV2","text":"<ul> <li>layout analysis + table recognition</li> </ul> <ul> <li>SER (Semantic entity recognition)</li> </ul> <ul> <li>RE (Relation Extraction)</li> </ul>"},{"location":"en/index/index.html#license","title":"\ud83d\udcc4 License","text":"<p>This project is released under Apache 2.0 license</p>"},{"location":"en/update/update.html","title":"Recently Update","text":""},{"location":"en/update/update.html#recently-update","title":"Recently Update","text":""},{"location":"en/update/update.html#20250605-release-of-paddleocr-v301-which-includes","title":"\ud83d\udd25\ud83d\udd25 2025.06.05: Release of PaddleOCR v3.0.1, which includes:","text":"<ul> <li>Optimisation of certain models and model configurations:</li> <li>Updated the default model configuration for PP-OCRv5, changing both detection and recognition from mobile to server models. To improve default performance in most scenarios, the parameter <code>limit_side_len</code> in the configuration has been changed from 736 to 64.</li> <li>Added a new text line orientation classification model <code>PP-LCNet_x1_0_textline_ori</code> with an accuracy of 99.42%. The default text line orientation classifier for OCR, PP-StructureV3, and PP-ChatOCRv4 pipelines has been updated to this model.</li> <li> <p>Optimised the text line orientation classification model <code>PP-LCNet_x0_25_textline_ori</code>, improving accuracy by 3.3 percentage points to a current accuracy of 98.85%.</p> </li> <li> <p>Optimisation of issues present in version 3.0.0:</p> </li> <li>Improved CLI usage experience: When using the PaddleOCR CLI without passing any parameters, a usage prompt is now provided.</li> <li>New parameters added: PP-ChatOCRv3 and PP-StructureV3 now support the <code>use_textline_orientation</code> parameter.</li> <li>CPU inference speed optimisation: All pipeline CPU inferences now enable MKL-DNN by default.</li> <li> <p>Support for C++ inference: The detection and recognition concatenation part of PP-OCRv5 now supports C++ inference.</p> </li> <li> <p>Fixes for issues present in version 3.0.0:</p> </li> <li>Fixed an issue where PP-StructureV3 encountered CPU inference errors due to the inability to use MKL-DNN with formula and table recognition models.</li> <li>Fixed an issue where GPU environments encountered the error <code>FatalError: Process abort signal is detected by the operating system</code> during inference.</li> <li>Fixed type hint issues in some Python 3.8 environments.</li> <li>Fixed the issue where the method <code>PPStructureV3.concatenate_markdown_pages</code> was missing.</li> <li>Fixed an issue where specifying both <code>lang</code> and <code>model_name</code> when instantiating <code>paddleocr.PaddleOCR</code> resulted in <code>model_name</code> being ineffective.</li> </ul>"},{"location":"en/update/update.html#20250520-paddleocr-30-official-release-highlights","title":"\ud83d\udd25\ud83d\udd25 2025.05.20: PaddleOCR 3.0 Official Release Highlights","text":"<ul> <li>PP-OCRv5: All-Scene Text Recognition Model </li> <li>Supports five text types and complex handwriting in a single model.  </li> <li> <p>Achieves a 13% accuracy improvement over the previous generation.</p> </li> <li> <p>PP-StructureV3: General Document Parsing Solution </p> </li> <li>Offers high-precision parsing for multi-scene, multi-layout PDFs.  </li> <li> <p>Outperforms numerous open and closed-source solutions in public benchmarks.</p> </li> <li> <p>PP-ChatOCRv4: Intelligent Document Understanding Solution </p> </li> <li>Natively supports ERNIE 4.5 Turbo.  </li> <li> <p>Delivers a 15% accuracy boost over the previous version.</p> </li> <li> <p>Rebuilt Deployment Capabilities with Unified Inference Interface: </p> </li> <li>Integrates PaddleX3.0's core features for a comprehensive upgrade of the inference and deployment modules.  </li> <li>Optimizes the design from version 2.x and unifies the Python API and CLI.  </li> <li> <p>Supports high-performance inference, serving, and on-device deployment scenarios.</p> </li> <li> <p>Optimized Training with PaddlePaddle Framework 3.0: </p> </li> <li>Compatible with the latest features such as the CINN compiler.  </li> <li> <p>Inference model files now use <code>xxx.json</code> instead of <code>xxx.pdmodel</code>.</p> </li> <li> <p>Unified Model Naming: </p> </li> <li> <p>Updated naming conventions for models supported by PaddleOCR 3.0 for consistency and easier maintenance.</p> </li> <li> <p>For more details, check out the Upgrade Notes from 2.x to 3.x.</p> </li> </ul>"},{"location":"en/update/update.html#202537-release-paddleocr-v210-including","title":"\ud83d\udd25\ud83d\udd252025.3.7 release PaddleOCR v2.10, including:","text":"<ul> <li> <p>12 new self-developed single models:</p> <ul> <li>Layout Detection series with 3 models: PP-DocLayout-L, PP-DocLayout-M, PP-DocLayout-S, supporting prediction of 23 common layout categories. High-quality layout detection for various document types such as papers, reports, exams, books, magazines, contracts, newspapers in both English and Chinese. mAP@0.5 reaches up to 90.4%, lightweight models can process over 100 pages of document images per second end-to-end.</li> <li>Formula Recognition series with 2 models: PP-FormulaNet-L, PP-FormulaNet-S, supporting 50,000 common LaTeX vocabulary, capable of recognizing complex printed and handwritten formulas. PP-FormulaNet-L has 6 percentage points higher accuracy than models of the same level, and PP-FormulaNet-S is 16 times faster than models with similar accuracy.</li> <li>Table Structure Recognition series with 2 models: SLANeXt_wired, SLANeXt_wireless. A newly developed table structure recognition model, supporting structured prediction for both wired and wireless tables. Compared to SLANet_plus, SLANeXt shows significant improvement in table structure, with 6 percentage points higher accuracy on internal high-difficulty table recognition evaluation sets.</li> <li>Table Classification series with 1 model: PP-LCNet_x1_0_table_cls, an ultra-lightweight classification model for both wired and wireless tables.</li> <li>Table Cell Detection series with 2 models: RT-DETR-L_wired_table_cell_det, RT-DETR-L_wireless_table_cell_det, supporting cell detection in both wired and wireless tables. These can be combined with SLANeXt_wired, SLANeXt_wireless, text detection, and text recognition modules for end-to-end table prediction. (See the newly added Table Recognition v2 pipeline)</li> <li>Text Recognition series with 1 model: PP-OCRv4_server_rec_doc, supports over 15,000 characters, with a broader text recognition range, additionally improving the recognition accuracy of certain texts. The accuracy is more than 3 percentage points higher than PP-OCRv4_server_rec on internal datasets.</li> <li>Text Line Orientation Classification series with 1 model: PP-LCNet_x0_25_textline_ori, an ultra-lightweight text line orientation classification model with only 0.3M storage.</li> </ul> </li> <li> <p>4 high-value multi-model combination solutions:</p> <ul> <li>Document Image Preprocessing Pipeline: Achieve correction of distortion and orientation in document images through the combination of ultra-lightweight models.</li> <li>Layout Parsing v2 Pipeline: Combines multiple self-developed different types of OCR models to optimize complex layout reading order, achieving end-to-end conversion of various complex PDF files to Markdown and JSON files. The conversion effect is better than other open-source solutions in multiple document scenarios. It can provide high-quality data production capabilities for large model training and application.</li> <li>Table Recognition v2 Pipeline: Provides better table recognition capabilities. By combining table classification module, table cell detection module, table structure recognition module, text detection module, text recognition module, etc., it achieves prediction of various styles of tables. Users can customize and finetune any module to improve the effect of vertical tables.</li> <li>PP-ChatOCRv4-doc Pipeline: Based on PP-ChatOCRv3-doc, integrating multi-modal large models, optimizing Prompt and multi-model combination post-processing logic. It effectively addresses common complex document information extraction challenges such as layout analysis, rare characters, multi-page PDFs, tables, and seal recognition, achieving 15 percentage points higher accuracy than PP-ChatOCRv3-doc. The large model upgrades local deployment capabilities, providing a standard OpenAI interface, supporting calls to locally deployed large models like DeepSeek-R1.</li> </ul> </li> </ul>"},{"location":"en/update/update.html#20241018-release-paddleocr-v29-including","title":"\ud83d\udd25 2024.10.18 release PaddleOCR v2.9, including:","text":"<ul> <li> <p>PaddleX, an All-in-One development tool based on PaddleOCR's advanced technology, supports low-code full-process development capabilities in the OCR field:</p> <ul> <li> <p>\ud83c\udfa8 Rich Model One-Click Call: Integrates 17 models related to text image intelligent analysis, general OCR, general layout parsing, table recognition, formula recognition, and seal recognition into 6 pipelines, which can be quickly experienced through a simple Python API one-click call. In addition, the same set of APIs also supports a total of 200+ models in image classification, object detection, image segmentation, and time series forecasting, forming 20+ single-function modules, making it convenient for developers to use model combinations.</p> </li> <li> <p>\ud83d\ude80 High Efficiency and Low barrier of entry: Provides two methods based on unified commands and GUI to achieve simple and efficient use, combination, and customization of models. Supports multiple deployment methods such as high-performance inference, service-oriented deployment, and on-device deployment. Additionally, for various mainstream hardware such as NVIDIA GPU, Kunlunxin XPU, Ascend NPU, Cambricon MLU, and Haiguang DCU, models can be developed with seamless switching.</p> </li> </ul> </li> <li> <p>Supports PP-ChatOCRv3-doc, high-precision layout detection model based on RT-DETR and high-efficiency layout area detection model based on PicoDet, high-precision table structure recognition model, text image unwarping model UVDoc, formula recognition model LatexOCR, and document image orientation classification model based on PP-LCNet.</p> </li> </ul>"},{"location":"en/update/update.html#202259-release-paddleocr-v25-including","title":"2022.5.9 release PaddleOCR v2.5, including","text":"<ul> <li>PP-OCRv3: With comparable speed, the effect of Chinese scene is further improved by 5% compared with PP-OCRv2, the effect of English scene is improved by 11%, and the average recognition accuracy of 80 language multilingual models is improved by more than 5%.</li> <li>PPOCRLabelv2: Add the annotation function for table recognition task, key information extraction task and irregular text image.</li> <li>Interactive e-book \"Dive into OCR\", covers the cutting-edge theory and code practice of OCR full stack technology.</li> </ul>"},{"location":"en/update/update.html#202257-add-support-for-metric-and-model-logging-during-training-to-weights-biases","title":"2022.5.7 Add support for metric and model logging during training to Weights &amp; Biases","text":""},{"location":"en/update/update.html#20211221-ocr-open-source-online-course-starts-the-lesson-starts-at-830-every-night-and-lasts-for-ten-days-free-registration-httpsaistudiobaiducomaistudiocourseintroduce25207","title":"2021.12.21 OCR open source online course starts. The lesson starts at 8:30 every night and lasts for ten days. Free registration: https://aistudio.baidu.com/aistudio/course/introduce/25207","text":""},{"location":"en/update/update.html#20211221-release-paddleocr-v24-release-1-text-detection-algorithm-psenet-3-text-recognition-algorithms-nrtrseedsar-1-key-information-extraction-algorithm-sdmgr-and-3-docvqa-algorithms-layoutlmlayoutlmv2layoutxlm","title":"2021.12.21 release PaddleOCR v2.4, release 1 text detection algorithm (PSENet), 3 text recognition algorithms (NRTR\u3001SEED\u3001SAR), 1 key information extraction algorithm (SDMGR) and 3 DocVQA algorithms (LayoutLM\u3001LayoutLMv2\uff0cLayoutXLM)","text":""},{"location":"en/update/update.html#202197-release-paddleocr-v23-pp-ocrv2-is-proposed-the-cpu-inference-speed-of-pp-ocrv2-is-220-higher-than-that-of-pp-ocr-server-the-f-score-of-pp-ocrv2-is-7-higher-than-that-of-pp-ocr-mobile","title":"2021.9.7 release PaddleOCR v2.3, PP-OCRv2 is proposed. The CPU inference speed of PP-OCRv2 is 220% higher than that of PP-OCR server. The F-score of PP-OCRv2 is 7% higher than that of PP-OCR mobile","text":""},{"location":"en/update/update.html#202183-released-paddleocr-v22-add-a-new-structured-documents-analysis-toolkit-ie-pp-structure-support-layout-analysis-and-table-recognition-one-key-to-export-chart-images-to-excel-files","title":"2021.8.3 released PaddleOCR v2.2, add a new structured documents analysis toolkit, i.e., PP-Structure, support layout analysis and table recognition (One-key to export chart images to Excel files)","text":""},{"location":"en/update/update.html#202148-release-end-to-end-text-recognition-algorithm-pgnet-which-is-published-in-aaai-2021-find-tutorial-hererelease-multi-language-recognition-models-support-more-than-80-languages-recognition-especially-the-performance-of-english-recognition-model-is-optimized","title":"2021.4.8 release end-to-end text recognition algorithm PGNet which is published in AAAI 2021. Find tutorial here\uff1brelease multi language recognition models, support more than 80 languages recognition; especially, the performance of English recognition model is Optimized","text":""},{"location":"en/update/update.html#2021121-update-more-than-25-multilingual-recognition-models-models-list-includingenglish-chinese-german-french-japanesespanishportuguese-russia-arabic-and-so-on-models-for-more-languages-will-continue-to-be-updated-develop-plan","title":"2021.1.21 update more than 25+ multilingual recognition models models list, including\uff1aEnglish, Chinese, German, French, Japanese\uff0cSpanish\uff0cPortuguese Russia Arabic and so on.  Models for more languages will continue to be updated Develop Plan","text":""},{"location":"en/update/update.html#20201215-update-data-synthesis-tool-ie-style-texteasy-to-synthesize-a-large-number-of-images-which-are-similar-to-the-target-scene-image","title":"2020.12.15 update Data synthesis tool, i.e., Style-Text\uff0ceasy to synthesize a large number of images which are similar to the target scene image","text":""},{"location":"en/update/update.html#20201125-update-a-new-data-annotation-tool-ie-ppocrlabel-which-is-helpful-to-improve-the-labeling-efficiency-moreover-the-labeling-results-can-be-used-in-training-of-the-pp-ocr-system-directly","title":"2020.11.25 Update a new data annotation tool, i.e., PPOCRLabel, which is helpful to improve the labeling efficiency. Moreover, the labeling results can be used in training of the PP-OCR system directly","text":""},{"location":"en/update/update.html#2020922-update-the-pp-ocr-technical-article-httpsarxivorgabs200909941","title":"2020.9.22 Update the PP-OCR technical article, https://arxiv.org/abs/2009.09941","text":""},{"location":"en/update/update.html#2020919-update-the-ultra-lightweight-compressed-ppocr_mobile_slim-series-models-the-overall-model-size-is-35m-suitable-for-mobile-deployment","title":"2020.9.19 Update the ultra lightweight compressed ppocr_mobile_slim series models, the overall model size is 3.5M, suitable for mobile deployment","text":""},{"location":"en/update/update.html#2020917-update-english-recognition-model-and-multilingual-recognition-model-english-chinese-german-french-japanese-and-korean-have-been-supported-models-for-more-languages-will-continue-to-be-updated","title":"2020.9.17 update English recognition model and Multilingual recognition model, <code>English</code>, <code>Chinese</code>, <code>German</code>, <code>French</code>, <code>Japanese</code> and <code>Korean</code> have been supported. Models for more languages will continue to be updated","text":""},{"location":"en/update/update.html#2020824-support-the-use-of-paddleocr-through-whl-package-installationplease-refer-paddleocr-package","title":"2020.8.24 Support the use of PaddleOCR through whl package installation\uff0cplease refer  PaddleOCR Package","text":""},{"location":"en/update/update.html#2020816-release-text-detection-algorithm-sast-and-text-recognition-algorithm-srn","title":"2020.8.16 Release text detection algorithm SAST and text recognition algorithm SRN","text":""},{"location":"en/update/update.html#2020723-release-the-playback-and-ppt-of-live-class-on-bilibili-station-paddleocr-introduction-address","title":"2020.7.23, Release the playback and PPT of live class on BiliBili station, PaddleOCR Introduction, address","text":""},{"location":"en/update/update.html#2020715-add-mobile-app-demo-support-both-ios-and-android-based-on-easyedge-and-paddle-lite","title":"2020.7.15, Add mobile App demo , support both iOS and  Android  (based on easyedge and Paddle Lite)","text":""},{"location":"en/update/update.html#2020715-improve-the-deployment-ability-add-the-c-inference-serving-deployment-in-addition-the-benchmarks-of-the-ultra-lightweight-chinese-ocr-model-are-provided","title":"2020.7.15, Improve the  deployment ability, add the C + +  inference , serving deployment. In addition, the benchmarks of the ultra-lightweight Chinese OCR model are provided","text":""},{"location":"en/update/update.html#2020715-add-several-related-datasets-data-annotation-and-synthesis-tools","title":"2020.7.15, Add several related datasets, data annotation and synthesis tools","text":""},{"location":"en/update/update.html#202079-add-a-new-model-to-support-recognize-the-character-space","title":"2020.7.9 Add a new model to support recognize the  character \"space\"","text":""},{"location":"en/update/update.html#202079-add-the-data-argument-and-learning-rate-decay-strategies-during-training","title":"2020.7.9 Add the data argument and learning rate decay strategies during training","text":""},{"location":"en/update/update.html#202068-add-datasets-and-keep-updating","title":"2020.6.8 Add datasets and keep updating","text":""},{"location":"en/update/update.html#202065-support-exporting-attention-model-to-inference_model","title":"2020.6.5 Support exporting <code>attention</code> model to <code>inference_model</code>","text":""},{"location":"en/update/update.html#202065-support-separate-prediction-and-recognition-output-result-score","title":"2020.6.5 Support separate prediction and recognition, output result score","text":""},{"location":"en/update/update.html#2020530-provide-lightweight-chinese-ocr-online-experience","title":"2020.5.30 Provide Lightweight Chinese OCR online experience","text":""},{"location":"en/update/update.html#2020530-model-prediction-and-training-support-on-windows-system","title":"2020.5.30 Model prediction and training support on Windows system","text":""},{"location":"en/update/update.html#2020530-open-source-general-chinese-ocr-model","title":"2020.5.30 Open source general Chinese OCR model","text":""},{"location":"en/update/update.html#2020514-release-paddleocr-open-class","title":"2020.5.14 Release PaddleOCR Open Class","text":""},{"location":"en/update/update.html#2020514-release-paddleocr-practice-notebook","title":"2020.5.14 Release PaddleOCR Practice Notebook","text":""},{"location":"en/update/update.html#2020514-open-source-86m-lightweight-chinese-ocr-model","title":"2020.5.14 Open source 8.6M lightweight Chinese OCR model","text":""},{"location":"en/update/upgrade_notes.html","title":"PaddleOCR 3.x Upgrade Notes","text":""},{"location":"en/update/upgrade_notes.html#1-why-upgrade-from-paddleocr-2x-to-3x","title":"1. Why Upgrade from PaddleOCR 2.x to 3.x?","text":"<p>Since the release of PaddleOCR 2.0 in February 2021, the community has experienced over four years of rapid growth. The number of GitHub stars, community users and contributors, as well as issues and PRs, have all increased exponentially. With emerging needs such as multilingual recognition and layout analysis, PaddleOCR continued to expand its capabilities in the 2.x series. However, the original lightweight-centric architecture has struggled to accommodate the growing complexity and rising maintenance costs brought by the feature boom.</p> <p>As more module branches and \"bridging\" layers were added to the codebase, issues such as code duplication and inconsistent interfaces became increasingly prominent. Testing became more difficult, and development efficiency was severely constrained. In addition, legacy dependencies became incompatible with newer versions of PaddlePaddle, limiting access to its latest features and slowing down training and inference. Under such circumstances, continuing to patch the existing architecture would only increase technical debt and system fragility.</p> <p>Meanwhile, Transformer-based vision-language models are injecting new momentum into advanced scenarios such as document understanding, image-text summarization, and intelligent proofreading. The community is eager to go beyond traditional OCR recognition and fully harness the powerful contextual understanding and reasoning capabilities of these models. At the same time, lightweight OCR models can still work in tandem with large models\u2014both supporting the input needs of large models in document parsing and achieving complementary strengths to further enhance overall system performance.</p> <p>Moreover, the official release of PaddlePaddle 3.0 in April 2025 brought groundbreaking upgrades in unified training/inference and domestic hardware adaptation. This calls for a significant update to PaddleOCR in both its training and inference components.</p> <p>Given this background, we\u2019ve decided to implement a major, non-backward-compatible upgrade\u2014transitioning from 2.x to 3.x. The new version introduces a modular and plugin-based architecture. While retaining familiar usage patterns for users as much as possible, it integrates large model capabilities, offers richer features, and leverages the latest advancements of PaddlePaddle 3.0. The result is reduced maintenance cost, improved performance, and a solid foundation for future feature expansion.</p>"},{"location":"en/update/upgrade_notes.html#2-key-upgrades-from-paddleocr-2x-to-3x","title":"2. Key Upgrades from PaddleOCR 2.x to 3.x","text":"<p>The 3.x upgrade consists of three major enhancements:</p> <ol> <li>New Model Pipelines: Introduced several new pipelines such as PP-OCRv5, PP-StructureV3, and PP-ChatOCR v4, covering a wide range of base models. These significantly enhance recognition capabilities for various text types, including handwriting, to meet the growing demand for high-precision parsing in complex documents. All models are ready-to-use out of the box, improving development efficiency.</li> <li>Refactored Deployment and Unified Inference Interface: The deployment module in PaddleOCR 3.x is rebuilt using PaddleX\u2019s underlying capabilities, fixing design flaws from 2.x and unifying both Python APIs and CLI interfaces. The deployment now supports three main scenarios: high-performance inference, service-oriented deployment, and on-device deployment.</li> <li>PaddlePaddle 3.0 Compatibility and Optimized Training: The new version is fully compatible with PaddlePaddle 3.0, including features like the CINN compiler. It also introduces a standardized model naming system to streamline future updates and maintenance.</li> </ol> <p>Some legacy features from PaddleOCR 2.x remain partially supported in 3.x. For more information, refer to Legacy Features.</p>"},{"location":"en/update/upgrade_notes.html#3-migrating-inference-code-from-paddleocr-2x-to-3x","title":"3. Migrating Inference Code from PaddleOCR 2.x to 3.x","text":"<p>For OCR tasks, PaddleOCR 3.x still supports a usage pattern similar to 2.x. Here\u2019s an example using the Python API in 2.x:</p> <pre><code>from paddleocr import PaddleOCR\n\nocr = PaddleOCR(lang=\"en\")\nresult = ocr.ocr(\"img.png\")\nfor res in result:\n    for line in res:\n        print(line)\n\n# Visualization\nfrom PIL import Image\nfrom paddleocr import draw_ocr\nresult = result[0]\nimage = Image.open(img_path).convert(\"RGB\")\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path=\"simfang.ttf\")\nim_show = Image.fromarray(im_show)\nim_show.save(\"result.jpg\")\n</code></pre> <p>In PaddleOCR 3.x, this workflow is further simplified:</p> <pre><code>from paddleocr import PaddleOCR\n\n# Basic initialization parameters remain the same\nocr = PaddleOCR(lang=\"en\")\nresult = ocr.ocr(\"img.png\")\n# Or use the new unified interface\n# result = ocr.predict(\"img.png\")\nfor res in result:\n    # Directly print recognition results, no nested loops required\n    res.print()\n\n# Visualization and saving results are simpler\nres.save_to_img(\"result\")\n</code></pre> <p>It\u2019s worth noting that the <code>PPStructure</code> module in PaddleOCR 2.x has been removed in 3.x. We recommend switching to <code>PPStructureV3</code>, which offers richer functionality and better parsing results. Refer to the relevant documentation for usage details.</p> <p>Also, in 2.x, the <code>show_log</code> parameter could be passed when creating a <code>PaddleOCR</code> object to control logging. However, this design affected all <code>PaddleOCR</code> instances due to the use of a shared logger\u2014clearly not the expected behavior. PaddleOCR 3.x introduces a brand-new logging system to address this issue. For more details, see Logging.</p>"},{"location":"en/update/upgrade_notes.html#4-known-issues-in-paddleocr-30","title":"4. Known Issues in PaddleOCR 3.0","text":"<p>PaddleOCR 3.0 is still under active development. Current known limitations include:</p> <ol> <li>Incomplete support for native C++ deployment.</li> <li>High-performance service-oriented deployment is not yet on par with PaddleServing in 2.x.</li> <li>On-Device Deployment currently supports only a subset of key models, with broader support pending.</li> </ol> <p>If you encounter any issues during use, feel free to submit feedback via GitHub issues. We also warmly welcome more community members to contribute to PaddleOCR's future. Thank you for your continued support and interest!</p>"},{"location":"en/version2.x/index.html","title":"Features of the 2.x Branch","text":"<p>Due to the upgrade of the 3.x branch, the wheel package has undergone refactoring, resulting in some models and features no longer being compatible with the older branch. To ensure that users relying on the 2.x branch's features can continue using them, we have retained the code related to the 2.x branch in this directory.</p>"},{"location":"en/version2.x/algorithm/add_new_algorithm.html","title":"Add New Algorithm","text":"<p>PaddleOCR decomposes an algorithm into the following parts, and modularizes each part to make it more convenient to develop new algorithms.</p> <ul> <li>Data loading and processing</li> <li>Network</li> <li>Post-processing</li> <li>Loss</li> <li>Metric</li> <li>Optimizer</li> </ul> <p>The following will introduce each part separately, and introduce how to add the modules required for the new algorithm.</p>"},{"location":"en/version2.x/algorithm/add_new_algorithm.html#data-loading-and-processing","title":"Data loading and processing","text":"<p>Data loading and processing are composed of different modules, which complete the image reading, data augment and label production. This part is under ppocr/data. The explanation of each file and folder are as follows:</p> <pre><code>ppocr/data/\n\u251c\u2500\u2500 imaug             # Scripts for image reading, data augment and label production\n\u2502   \u251c\u2500\u2500 label_ops.py  # Modules that transform the label\n\u2502   \u251c\u2500\u2500 operators.py  # Modules that transform the image\n\u2502   \u251c\u2500\u2500.....\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 lmdb_dataset.py   # The dataset that reads the lmdb\n\u2514\u2500\u2500 simple_dataset.py # Read the dataset saved in the form of `image_path\\tgt`\n</code></pre> <p>PaddleOCR has a large number of built-in image operation related modules. For modules that are not built-in, you can add them through the following steps:</p> <ol> <li>Create a new file under the ppocr/data/imaug folder, such as my_module.py.</li> <li> <p>Add code in the my_module.py file, the sample code is as follows:</p> <pre><code>class MyModule:\n    def __init__(self, *args, **kwargs):\n        # your init code\n        pass\n\n    def __call__(self, data):\n        img = data['image']\n        label = data['label']\n        # your process code\n\n        data['image'] = img\n        data['label'] = label\n        return data\n</code></pre> </li> <li> <p>Import the added module in the ppocr/data/imaug/_init_.py file.</p> </li> </ol> <p>All different modules of data processing are executed by sequence, combined and executed in the form of a list in the config file. Such as:</p> <pre><code># angle class data process\ntransforms:\n  - DecodeImage: # load image\n      img_mode: BGR\n      channel_first: False\n  - MyModule:\n      args1: args1\n      args2: args2\n  - KeepKeys:\n      keep_keys: [ 'image', 'label' ] # dataloader will return list in this order\n</code></pre>"},{"location":"en/version2.x/algorithm/add_new_algorithm.html#network","title":"Network","text":"<p>The network part completes the construction of the network, and PaddleOCR divides the network into four parts, which are under ppocr/modeling. The data entering the network will pass through these four parts in sequence(transforms-&gt;backbones-&gt; necks-&gt;heads).</p> <pre><code>\u251c\u2500\u2500 architectures # Code for building network\n\u251c\u2500\u2500 transforms    # Image Transformation Module\n\u251c\u2500\u2500 backbones     # Feature extraction module\n\u251c\u2500\u2500 necks         # Feature enhancement module\n\u2514\u2500\u2500 heads         # Output module\n</code></pre> <p>PaddleOCR has built-in commonly used modules related to algorithms such as DB, EAST, SAST, CRNN and Attention. For modules that do not have built-in, you can add them through the following steps, the four parts are added in the same steps, take backbones as an example:</p> <ol> <li>Create a new file under the ppocr/modeling/backbones folder, such as my_backbone.py.</li> <li> <p>Add code in the my_backbone.py file, the sample code is as follows:</p> <pre><code>import paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n\nclass MyBackbone(nn.Layer):\n    def __init__(self, *args, **kwargs):\n        super(MyBackbone, self).__init__()\n        # your init code\n        self.conv = nn.xxxx\n\n    def forward(self, inputs):\n        # your network forward\n        y = self.conv(inputs)\n        return y\n</code></pre> </li> <li> <p>Import the added module in the ppocr/modeling/backbones/_init_.py file.</p> </li> </ol> <p>After adding the four-part modules of the network, you only need to configure them in the configuration file to use, such as:</p> <pre><code>Architecture:\nmodel_type: rec\nalgorithm: CRNN\nTransform:\n    name: MyTransform\n    args1: args1\n    args2: args2\nBackbone:\n    name: MyBackbone\n    args1: args1\nNeck:\n    name: MyNeck\n    args1: args1\nHead:\n    name: MyHead\n    args1: args1\n</code></pre>"},{"location":"en/version2.x/algorithm/add_new_algorithm.html#post-processing","title":"Post-processing","text":"<p>Post-processing realizes decoding network output to obtain text box or recognized text. This part is under ppocr/postprocess. PaddleOCR has built-in post-processing modules related to algorithms such as DB, EAST, SAST, CRNN and Attention. For components that are not built-in, they can be added through the following steps:</p> <ol> <li>Create a new file under the ppocr/postprocess folder, such as my_postprocess.py.</li> <li> <p>Add code in the my_postprocess.py file, the sample code is as follows:</p> <pre><code>import paddle\n\n\nclass MyPostProcess:\n    def __init__(self, *args, **kwargs):\n        # your init code\n        pass\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        if isinstance(preds, paddle.Tensor):\n            preds = preds.numpy()\n        # you preds decode code\n        preds = self.decode_preds(preds)\n        if label is None:\n            return preds\n        # you label decode code\n        label = self.decode_label(label)\n        return preds, label\n\n    def decode_preds(self, preds):\n        # you preds decode code\n        pass\n\n    def decode_label(self, preds):\n        # you label decode code\n        pass\n</code></pre> </li> <li> <p>Import the added module in the ppocr/postprocess/_init_.py file.</p> </li> </ol> <p>After the post-processing module is added, you only need to configure it in the configuration file to use, such as:</p> <pre><code>PostProcess:\nname: MyPostProcess\nargs1: args1\nargs2: args2\n</code></pre>"},{"location":"en/version2.x/algorithm/add_new_algorithm.html#loss","title":"Loss","text":"<p>The loss function is used to calculate the distance between the network output and the label. This part is under ppocr/losses. PaddleOCR has built-in loss function modules related to algorithms such as DB, EAST, SAST, CRNN and Attention. For modules that do not have built-in modules, you can add them through the following steps:</p> <ol> <li>Create a new file in the ppocr/losses folder, such as my_loss.py.</li> <li> <p>Add code in the my_loss.py file, the sample code is as follows:</p> <pre><code>import paddle\nfrom paddle import nn\n\n\nclass MyLoss(nn.Layer):\n    def __init__(self, **kwargs):\n        super(MyLoss, self).__init__()\n        # you init code\n        pass\n\n    def __call__(self, predicts, batch):\n        label = batch[1]\n        # your loss code\n        loss = self.loss(input=predicts, label=label)\n        return {'loss': loss}\n</code></pre> </li> <li> <p>Import the added module in the ppocr/losses/_init_.py file.</p> </li> </ol> <p>After the loss function module is added, you only need to configure it in the configuration file to use it, such as:</p> <pre><code>Loss:\n  name: MyLoss\n  args1: args1\n  args2: args2\n</code></pre>"},{"location":"en/version2.x/algorithm/add_new_algorithm.html#metric","title":"Metric","text":"<p>Metric is used to calculate the performance of the network on the current batch. This part is under ppocr/metrics. PaddleOCR has built-in evaluation modules related to algorithms such as detection, classification and recognition. For modules that do not have built-in modules, you can add them through the following steps:</p> <ol> <li>Create a new file under the ppocr/metrics folder, such as my_metric.py.</li> <li> <p>Add code in the my_metric.py file, the sample code is as follows:</p> <pre><code>class MyMetric(object):\n    def __init__(self, main_indicator='acc', **kwargs):\n        # main_indicator is used for select best model\n        self.main_indicator = main_indicator\n        self.reset()\n\n    def __call__(self, preds, batch, *args, **kwargs):\n        # preds is out of postprocess\n        # batch is out of dataloader\n        labels = batch[1]\n        cur_correct_num = 0\n        cur_all_num = 0\n        # you metric code\n        self.correct_num += cur_correct_num\n        self.all_num += cur_all_num\n        return {'acc': cur_correct_num / cur_all_num, }\n\n    def get_metric(self):\n        \"\"\"\n        return metrics {\n                'acc': 0,\n                'norm_edit_dis': 0,\n            }\n        \"\"\"\n        acc = self.correct_num / self.all_num\n        self.reset()\n        return {'acc': acc}\n\n    def reset(self):\n        # reset metric\n        self.correct_num = 0\n        self.all_num = 0\n</code></pre> </li> <li> <p>Import the added module in the ppocr/metrics/_init_.py file.</p> </li> </ol> <p>After the metric module is added, you only need to configure it in the configuration file to use it, such as:</p> <pre><code>Metric:\n  name: MyMetric\n  main_indicator: acc\n</code></pre>"},{"location":"en/version2.x/algorithm/add_new_algorithm.html#optimizer","title":"Optimizer","text":"<p>The optimizer is used to train the network. The optimizer also contains network regularization and learning rate decay modules. This part is under ppocr/optimizer. PaddleOCR has built-in Commonly used optimizer modules such as <code>Momentum</code>, <code>Adam</code> and <code>RMSProp</code>, common regularization modules such as <code>Linear</code>, <code>Cosine</code>, <code>Step</code> and <code>Piecewise</code>, and common learning rate decay modules such as <code>L1Decay</code> and <code>L2Decay</code>. Modules without built-in can be added through the following steps, take <code>optimizer</code> as an example:</p> <ol> <li> <p>Create your own optimizer in the ppocr/optimizer/optimizer.py file, the sample code is as follows:</p> <pre><code>from paddle import optimizer as optim\n\n\nclass MyOptim(object):\n    def __init__(self, learning_rate=0.001, *args, **kwargs):\n        self.learning_rate = learning_rate\n\n    def __call__(self, parameters):\n        # It is recommended to wrap the built-in optimizer of paddle\n        opt = optim.XXX(\n            learning_rate=self.learning_rate,\n            parameters=parameters)\n        return opt\n</code></pre> </li> </ol> <p>After the optimizer module is added, you only need to configure it in the configuration file to use, such as:</p> <pre><code>Optimizer:\n  name: MyOptim\n  args1: args1\n  args2: args2\n  lr:\n    name: Cosine\n    learning_rate: 0.001\n  regularizer:\n    name: 'L2'\n    factor: 0\n</code></pre>"},{"location":"en/version2.x/algorithm/overview.html","title":"Algorithms","text":"<p>This tutorial lists the OCR algorithms supported by PaddleOCR, as well as the models and metrics of each algorithm on English public datasets. It is mainly used for algorithm introduction and algorithm performance comparison. For more models on other datasets including Chinese, please refer to PP-OCRv3 models list.</p> <p>Developers are welcome to contribute more algorithms! Please refer to add new algorithm guideline.</p>"},{"location":"en/version2.x/algorithm/overview.html#1-two-stage-ocr-algorithms","title":"1. Two-stage OCR Algorithms","text":""},{"location":"en/version2.x/algorithm/overview.html#11-text-detection-algorithms","title":"1.1 Text Detection Algorithms","text":"<p>Supported text detection algorithms (Click the link to get the tutorial):</p> <ul> <li> DB &amp;&amp; DB++</li> <li> EAST</li> <li> SAST</li> <li> PSENet</li> <li> FCENet</li> <li> DRRG</li> <li> CT</li> </ul> <p>On the ICDAR2015 dataset, the text detection result is as follows:</p> Model Backbone Precision Recall Hmean Download link EAST ResNet50_vd 88.71% 81.36% 84.88% trained model EAST MobileNetV3 78.20% 79.10% 78.65% trained model DB ResNet50_vd 86.41% 78.72% 82.38% trained model DB MobileNetV3 77.29% 73.08% 75.12% trained model SAST ResNet50_vd 91.39% 83.77% 87.42% trained model PSE ResNet50_vd 85.81% 79.53% 82.55% trained model PSE MobileNetV3 82.20% 70.48% 75.89% trained model DB++ ResNet50 90.89% 82.66% 86.58% pretrained model/trained model <p>On Total-Text dataset, the text detection result is as follows:</p> Model Backbone Precision Recall Hmean Download link SAST ResNet50_vd 89.63% 78.44% 83.66% trained model CT ResNet18_vd 88.68% 81.70% 85.05% trained model <p>On CTW1500 dataset, the text detection result is as follows:</p> Model Backbone Precision Recall Hmean Download link FCE ResNet50_dcn 88.39% 82.18% 85.27% trained model DRRG ResNet50_vd 89.92% 80.91% 85.18% trained model <p>Note\uff1a Additional data, like icdar2013, icdar2017, COCO-Text, ArT, was added to the model training of SAST. Download English public dataset in organized format used by PaddleOCR from:</p> <ul> <li>Baidu Drive (download code: 2bpi).</li> <li>Google Drive</li> </ul>"},{"location":"en/version2.x/algorithm/overview.html#12-text-recognition-algorithms","title":"1.2 Text Recognition Algorithms","text":"<p>Supported text recognition algorithms (Click the link to get the tutorial):</p> <ul> <li> CRNN</li> <li> Rosetta</li> <li> STAR-Net</li> <li> RARE</li> <li> SRN</li> <li> NRTR</li> <li> SAR</li> <li> SEED</li> <li> SVTR</li> <li> ViTSTR</li> <li> ABINet</li> <li> VisionLAN</li> <li> SPIN</li> <li> RobustScanner</li> <li> RFL</li> <li> ParseQ</li> <li> CPPD</li> <li> SATRN</li> </ul> <p>Refer to DTRB, the training and evaluation result of these above text recognition (using MJSynth and SynthText for training, evaluate on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE) is as follow:</p> Model Backbone Avg Accuracy Module combination Download link Rosetta Resnet34_vd 79.11% rec_r34_vd_none_none_ctc trained model Rosetta MobileNetV3 75.80% rec_mv3_none_none_ctc trained model CRNN Resnet34_vd 81.04% rec_r34_vd_none_bilstm_ctc trained model CRNN MobileNetV3 77.95% rec_mv3_none_bilstm_ctc trained model StarNet Resnet34_vd 82.85% rec_r34_vd_tps_bilstm_ctc trained model StarNet MobileNetV3 79.28% rec_mv3_tps_bilstm_ctc trained model RARE Resnet34_vd 83.98% rec_r34_vd_tps_bilstm_att trained model RARE MobileNetV3 81.76% rec_mv3_tps_bilstm_att trained model SRN Resnet50_vd_fpn 86.31% rec_r50fpn_vd_none_srn trained model NRTR NRTR_MTB 84.21% rec_mtb_nrtr trained model SAR Resnet31 87.20% rec_r31_sar trained model SEED Aster_Resnet 85.35% rec_resnet_stn_bilstm_att trained model SVTR SVTR-Tiny 89.25% rec_svtr_tiny_none_ctc_en trained model ViTSTR ViTSTR 79.82% rec_vitstr_none_ce trained model ABINet Resnet45 90.75% rec_r45_abinet trained model VisionLAN Resnet45 90.30% rec_r45_visionlan trained model SPIN ResNet32 90.00% rec_r32_gaspin_bilstm_att trained model RobustScanner ResNet31 87.77% rec_r31_robustscanner trained model RFL ResNetRFL 88.63% rec_resnet_rfl_att trained model ParseQ VIT 91.24% rec_vit_parseq_synth trained model CPPD SVTR-Base 93.8% rec_svtrnet_cppd_base_en trained model SATRN ShallowCNN 88.05% rec_satrn trained model"},{"location":"en/version2.x/algorithm/overview.html#13-text-super-resolution-algorithms","title":"1.3 Text Super-Resolution Algorithms","text":"<p>Supported text super-resolution algorithms (Click the link to get the tutorial):</p> <ul> <li> Text Gestalt</li> <li> Text Telescope</li> </ul> <p>On the TextZoom public dataset, the effect of the algorithm is as follows:</p> Model Backbone PSNR_Avg SSIM_Avg Config Download link Text Gestalt tsrn 19.28 0.6560 configs/sr/sr_tsrn_transformer_strock.yml trained model Text Telescope tbsrn 21.56 0.7411 configs/sr/sr_telescope.yml trained model"},{"location":"en/version2.x/algorithm/overview.html#14-formula-recognition-algorithm","title":"1.4 Formula Recognition Algorithm","text":"<p>Supported formula recognition algorithms (Click the link to get the tutorial):</p> <ul> <li> CAN</li> <li> LaTeX-OCR</li> </ul> <p>On the CROHME handwritten formula dataset, the effect of the algorithm is as follows:</p> Model Backbone Config ExpRate Download link CAN DenseNet rec_d28_can.yml 51.72% trained model"},{"location":"en/version2.x/algorithm/overview.html#2-end-to-end-ocr-algorithms","title":"2. End-to-end OCR Algorithms","text":"<p>Supported end-to-end algorithms (Click the link to get the tutorial):</p> <ul> <li> PGNet</li> </ul>"},{"location":"en/version2.x/algorithm/overview.html#3-table-recognition-algorithms","title":"3. Table Recognition Algorithms","text":"<p>Supported table recognition algorithms (Click the link to get the tutorial):</p> <ul> <li> TableMaster</li> </ul> <p>On the PubTabNet dataset, the algorithm result is as follows:</p> Model Backbone Config Acc Download link TableMaster TableResNetExtra configs/table/table_master.yml 77.47% trained model / inference model"},{"location":"en/version2.x/algorithm/overview.html#4-key-information-extraction-algorithms","title":"4. Key Information Extraction Algorithms","text":"<p>Supported KIE algorithms (Click the link to get the tutorial):</p> <ul> <li> VI-LayoutXLM</li> <li> LayoutLM</li> <li> LayoutLMv2</li> <li> LayoutXLM</li> <li> SDMGR</li> </ul> <p>On wildreceipt dataset, the algorithm result is as follows:</p> Model Backbone Config Hmean Download link SDMGR VGG6 configs/kie/sdmgr/kie_unet_sdmgr.yml 86.70% trained model <p>On XFUND_zh dataset, the algorithm result is as follows:</p> Model Backbone Task Config Hmean Download link VI-LayoutXLM VI-LayoutXLM-base SER ser_vi_layoutxlm_xfund_zh_udml.yml 93.19% trained model LayoutXLM LayoutXLM-base SER ser_layoutxlm_xfund_zh.yml 90.38% trained model LayoutLM LayoutLM-base SER ser_layoutlm_xfund_zh.yml 77.31% trained model LayoutLMv2 LayoutLMv2-base SER ser_layoutlmv2_xfund_zh.yml 85.44% trained model VI-LayoutXLM VI-LayoutXLM-base RE re_vi_layoutxlm_xfund_zh_udml.yml 83.92% trained model LayoutXLM LayoutXLM-base RE re_layoutxlm_xfund_zh.yml 74.83% trained model LayoutLMv2 LayoutLMv2-base RE re_layoutlmv2_xfund_zh.yml 67.77% trained model"},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html","title":"Algorithm e2e pgnet","text":""},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#1-brief-introduction","title":"1. Brief Introduction","text":"<p>OCR algorithms can be divided into two categories: two-stage algorithm and end-to-end algorithm. The two-stage OCR algorithm is generally divided into two parts, text detection and text recognition algorithm. The text detection algorithm locates the box of the text line from the image, and then the recognition algorithm identifies the content of the text box. The end-to-end OCR algorithm combines text detection and recognition in one algorithm. Its basic idea is to design a model with both detection unit and recognition module, share the CNN features of both and train them together. Because one algorithm can complete character recognition, the end-to-end model is smaller and faster.</p>"},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#introduction-of-pgnet-algorithm","title":"Introduction Of PGNet Algorithm","text":"<p>During the recent years, the end-to-end OCR algorithm has been well developed, including MaskTextSpotter series, TextSnake, TextDragon, PGNet series and so on. Among these algorithms, PGNet algorithm has some advantages over the other algorithms.</p> <ul> <li>PGNet loss is designed to guide training, and no character-level annotations is needed.</li> <li>NMS and ROI related operations are not needed. It can accelerate the prediction</li> <li>The reading order prediction module is proposed</li> <li>A graph based modification module (GRM) is proposed to further improve the performance of model recognition</li> <li>Higher accuracy and faster prediction speed</li> </ul> <p>For details of PGNet algorithm, please refer to paper. The schematic diagram of the algorithm is as follows:</p> <p></p> <p>After feature extraction, the input image is sent to four branches: TBO module for text edge offset prediction, TCL module for text center-line prediction, TDO module for text direction offset prediction, and TCC module for text character classification graph prediction. The output of TBO and TCL can get text detection results after post-processing, and TCL, TDO and TCC are responsible for text recognition.</p> <p>The results of detection and recognition are as follows:</p> <p></p> <p></p>"},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#performance","title":"Performance","text":""},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#test-set-total-text","title":"Test set: Total Text","text":""},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#test-environment-nvidia-tesla-v100-sxm2-16gb","title":"Test environment: NVIDIA Tesla V100-SXM2-16GB","text":"PGNetA det_precision det_recall det_f_score e2e_precision e2e_recall e2e_f_score FPS download Paper 85.30 86.80 86.10 - - 61.70 38.20 (size=640) - Ours 87.03 82.48 84.69 61.71 58.43 60.03 48.73 (size=768) download link <p>note:PGNet in PaddleOCR optimizes the prediction speed, and can significantly improve the end-to-end prediction speed within the acceptable range of accuracy reduction</p>"},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#2-environment-configuration","title":"2. Environment Configuration","text":"<p>Please refer to Operation Environment Preparation to configure PaddleOCR operating environment first, refer to Project Clone to clone the project</p>"},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#3-quick-use","title":"3. Quick Use","text":""},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#inference-model-download","title":"Inference model download","text":"<p>This section takes the trained end-to-end model as an example to quickly use the model prediction. First, download the trained end-to-end inference model download address</p> <pre><code>mkdir inference &amp;&amp; cd inference\n# Download the English end-to-end model and unzip it\nwget https://paddleocr.bj.bcebos.com/dygraph_v2.0/pgnet/e2e_server_pgnetA_infer.tar &amp;&amp; tar xf e2e_server_pgnetA_infer.tar\n</code></pre> <ul> <li>In Windows environment, if 'wget' is not installed, the link can be copied to the browser when downloading the model, and decompressed and placed in the corresponding directory</li> </ul> <p>After decompression, there should be the following file structure:</p> <pre><code>\u251c\u2500\u2500 e2e_server_pgnetA_infer\n\u2502   \u251c\u2500\u2500 inference.pdiparams\n\u2502   \u251c\u2500\u2500 inference.pdiparams.info\n\u2502   \u2514\u2500\u2500 inference.pdmodel\n</code></pre>"},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#single-image-or-image-set-prediction","title":"Single image or image set prediction","text":"<pre><code># Prediction single image specified by image_dir\npython3 tools/infer/predict_e2e.py --e2e_algorithm=\"PGNet\" --image_dir=\"./doc/imgs_en/img623.jpg\" --e2e_model_dir=\"./inference/e2e_server_pgnetA_infer/\" --e2e_pgnet_valid_set=\"totaltext\"\n\n# Prediction the collection of images specified by image_dir\npython3 tools/infer/predict_e2e.py --e2e_algorithm=\"PGNet\" --image_dir=\"./doc/imgs_en/\" --e2e_model_dir=\"./inference/e2e_server_pgnetA_infer/\" --e2e_pgnet_valid_set=\"totaltext\"\n\n# If you want to use CPU for prediction, you need to set use_gpu parameter is false\npython3 tools/infer/predict_e2e.py --e2e_algorithm=\"PGNet\" --image_dir=\"./doc/imgs_en/img623.jpg\" --e2e_model_dir=\"./inference/e2e_server_pgnetA_infer/\" --use_gpu=False --e2e_pgnet_valid_set=\"totaltext\"\n</code></pre>"},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#visualization-results","title":"Visualization results","text":"<p>The visualized end-to-end results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'e2e_res'. Examples of results are as follows:</p> <p></p>"},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#4-model-trainingevaluation-and-inference","title":"4. Model Training,Evaluation And Inference","text":"<p>This section takes the totaltext dataset as an example to introduce the training, evaluation and testing of the end-to-end model in PaddleOCR.</p>"},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#data-preparation","title":"Data Preparation","text":"<p>Download and unzip totaltext dataset to PaddleOCR/train_data/, dataset organization structure is as follow:</p> <pre><code>/PaddleOCR/train_data/total_text/train/\n  |- rgb/            # total_text training data of dataset\n      |- img11.png\n      | ...\n  |- train.txt       # total_text training annotation of dataset\n</code></pre> <p>total_text.txt: the format of dimension file is as follows\uff0cthe file name and annotation information are separated by \"\\t\":</p> <pre><code>\" Image file name             Image annotation information encoded by json.dumps\"\nrgb/img11.jpg    [{\"transcription\": \"ASRAMA\", \"points\": [[214.0, 325.0], [235.0, 308.0], [259.0, 296.0], [286.0, 291.0], [313.0, 295.0], [338.0, 305.0], [362.0, 320.0], [349.0, 347.0], [330.0, 337.0], [310.0, 329.0], [290.0, 324.0], [269.0, 328.0], [249.0, 336.0], [231.0, 346.0]]}, {...}]\n</code></pre> <p>The image annotation after json.dumps() encoding is a list containing multiple dictionaries.</p> <p>The <code>points</code> in the dictionary represent the coordinates (x, y) of the four points of the text box, arranged clockwise from the point at the upper left corner.</p> <p><code>transcription</code> represents the text of the current text box. When its content is \"###\" it means that the text box is invalid and will be skipped during training.</p> <p>If you want to train PaddleOCR on other datasets, please build the annotation file according to the above format.</p>"},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#start-training","title":"Start Training","text":"<p>PGNet training is divided into two steps: Step 1: training on the synthetic data to get the pretrain_model, and the accuracy of the model is still low; step 2: loading the pretrain_model and training on the totaltext data set; for fast training, we directly provide the pre training model of step 1download link.</p> <pre><code>cd PaddleOCR/\n\n# download step1 pretrain_models\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/dygraph_v2.0/pgnet/train_step1.tar\n\n# You can get the following file format\n./pretrain_models/train_step1/\n  \u2514\u2500 best_accuracy.pdopt\n  \u2514\u2500 best_accuracy.states\n  \u2514\u2500 best_accuracy.pdparams\n</code></pre> <p>If CPU version installed, please set the parameter <code>use_gpu</code> to <code>false</code> in the configuration.</p> <pre><code># single GPU training\npython3 tools/train.py -c configs/e2e/e2e_r50_vd_pg.yml -o Global.pretrained_model=./pretrain_models/train_step1/best_accuracy Global.load_static_weights=False\n# multi-GPU training\n# Set the GPU ID used by the '--gpus' parameter.\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/e2e/e2e_r50_vd_pg.yml -o Global.pretrained_model=./pretrain_models/train_step1/best_accuracy  Global.load_static_weights=False\n</code></pre> <p>In the above instruction, use <code>-c</code> to select the training to use the <code>configs/e2e/e2e_r50_vd_pg.yml</code> configuration file. For a detailed explanation of the configuration file, please refer to config.</p> <p>You can also use <code>-o</code> to change the training parameters without modifying the yml file. For example, adjust the training learning rate to 0.0001</p> <pre><code>python3 tools/train.py -c configs/e2e/e2e_r50_vd_pg.yml -o Optimizer.base_lr=0.0001\n</code></pre>"},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#load-trained-model-and-continue-training","title":"Load trained model and continue training","text":"<p>If you would like to load trained model and continue the training again, you can specify the parameter <code>Global.checkpoints</code> as the model path to be loaded.</p> <pre><code>python3 tools/train.py -c configs/e2e/e2e_r50_vd_pg.yml -o Global.checkpoints=./your/trained/model\n</code></pre> <p>Note: The priority of <code>Global.checkpoints</code> is higher than that of <code>Global.pretrain_weights</code>, that is, when two parameters are specified at the same time, the model specified by <code>Global.checkpoints</code> will be loaded first. If the model path specified by <code>Global.checkpoints</code> is wrong, the one specified by <code>Global.pretrain_weights</code> will be loaded.</p> <p>PaddleOCR calculates three indicators for evaluating performance of OCR end-to-end task: Precision, Recall, and Hmean.</p> <p>Run the following code to calculate the evaluation indicators. The result will be saved in the test result file specified by <code>save_res_path</code> in the configuration file <code>e2e_r50_vd_pg.yml</code> When evaluating, set post-processing parameters <code>max_side_len=768</code>. If you use different datasets, different models for training. The model parameters during training are saved in the <code>Global.save_model_dir</code> directory by default. When evaluating indicators, you need to set <code>Global.checkpoints</code> to point to the saved parameter file.</p> <pre><code>python3 tools/eval.py -c configs/e2e/e2e_r50_vd_pg.yml  -o Global.checkpoints=\"{path/to/weights}/best_accuracy\"\n</code></pre>"},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#model-test","title":"Model Test","text":"<p>Test the end-to-end result on a single image:</p> <pre><code>python3 tools/infer_e2e.py -c configs/e2e/e2e_r50_vd_pg.yml -o Global.infer_img=\"./doc/imgs_en/img_10.jpg\" Global.pretrained_model=\"./output/e2e_pgnet/best_accuracy\" Global.load_static_weights=false\n</code></pre> <p>Test the end-to-end result on all images in the folder:</p> <pre><code>python3 tools/infer_e2e.py -c configs/e2e/e2e_r50_vd_pg.yml -o Global.infer_img=\"./doc/imgs_en/\" Global.pretrained_model=\"./output/e2e_pgnet/best_accuracy\" Global.load_static_weights=false\n</code></pre>"},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#model-inference","title":"Model inference","text":""},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#1quadrangle-text-detection-model-icdar2015","title":"(1).Quadrangle text detection model (ICDAR2015)","text":"<p>First, convert the model saved in the PGNet end-to-end training process into an inference model. In the first stage of training based on composite dataset, the model of English data set training is taken as an examplemodel download link, you can use the following command to convert:</p> <pre><code>wget https://paddleocr.bj.bcebos.com/dygraph_v2.0/pgnet/en_server_pgnetA.tar &amp;&amp; tar xf en_server_pgnetA.tar\npython3 tools/export_model.py -c configs/e2e/e2e_r50_vd_pg.yml -o Global.pretrained_model=./en_server_pgnetA/best_accuracy Global.load_static_weights=False Global.save_inference_dir=./inference/e2e\n</code></pre> <p>For PGNet quadrangle end-to-end model inference, you need to set the parameter <code>--e2e_algorithm=\"PGNet\"</code> and <code>--e2e_pgnet_valid_set=\"partvgg\"</code>, run the following command:</p> <pre><code>python3 tools/infer/predict_e2e.py --e2e_algorithm=\"PGNet\" --image_dir=\"./doc/imgs_en/img_10.jpg\" --e2e_model_dir=\"./inference/e2e/\" --e2e_pgnet_valid_set=\"partvgg\"\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'e2e_res'. Examples of results are as follows:</p> <p></p>"},{"location":"en/version2.x/algorithm/end_to_end/algorithm_e2e_pgnet.html#2-curved-text-detection-model-total-text","title":"(2). Curved text detection model (Total-Text)","text":"<p>For the curved text example, we use the same model as the quadrilateral For PGNet end-to-end curved text detection model inference, you need to set the parameter <code>--e2e_algorithm=\"PGNet\"</code> and <code>--e2e_pgnet_valid_set=\"totaltext\"</code>, run the following command:</p> <pre><code>python3 tools/infer/predict_e2e.py --e2e_algorithm=\"PGNet\" --image_dir=\"./doc/imgs_en/img623.jpg\" --e2e_model_dir=\"./inference/e2e/\" --e2e_pgnet_valid_set=\"totaltext\"\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'e2e_res'. Examples of results are as follows:</p> <p></p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_can.html","title":"Algorithm rec can","text":""},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_can.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>When Counting Meets HMER: Counting-Aware Network for Handwritten Mathematical Expression Recognition Bohan Li, Ye Yuan, Dingkang Liang, Xiao Liu, Zhilong Ji, Jinfeng Bai, Wenyu Liu, Xiang Bai ECCV, 2022</p> <p>Using CROHME handwrittem mathematical expression recognition datasets for training, and evaluating on its test sets, the algorithm reproduction effect is as follows:</p> Model Backbone config exprate Download link CAN DenseNet rec_d28_can.yml 51.72% trained model"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_can.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_can.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_can.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_d28_can.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_d28_can.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_can.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_d28_can.yml -o Global.pretrained_model=./rec_d28_can_train/best_accuracy.pdparams\n</code></pre> <p>Prediction:</p> <pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_d28_can.yml -o Architecture.Head.attdecoder.is_train=False Global.infer_img='./doc/crohme_demo/hme_00.jpg' Global.pretrained_model=./rec_d28_can_train/best_accuracy.pdparams\n</code></pre>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_can.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_can.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the CAN handwritten mathematical expression recognition training process is converted into an inference model. you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_d28_can.yml -o Global.pretrained_model=./rec_d28_can_train/best_accuracy.pdparams Global.save_inference_dir=./inference/rec_d28_can/ Architecture.Head.attdecoder.is_train=False\n\n# The default output max length of the model is 36. If you need to predict a longer sequence, please specify its output sequence as an appropriate value when exporting the model, as: Architecture.Head.max_ text_ length=72\n</code></pre> <p>For CAN handwritten mathematical expression recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/datasets/crohme_demo/hme_00.jpg\" --rec_algorithm=\"CAN\" --rec_batch_num=1 --rec_model_dir=\"./inference/rec_d28_can/\" --rec_char_dict_path=\"./ppocr/utils/dict/latex_symbol_dict.txt\"\n\n# If you need to predict on a picture with black characters on a white background, please set: -- rec_ image_ inverse=False\n</code></pre>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_can.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_can.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_can.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_can.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_can.html#citation","title":"Citation","text":"<pre><code>@misc{https://doi.org/10.48550/arxiv.2207.11463,\n  doi = {10.48550/ARXIV.2207.11463},\n  url = {https://arxiv.org/abs/2207.11463},\n  author = {Li, Bohan and Yuan, Ye and Liang, Dingkang and Liu, Xiao and Ji, Zhilong and Bai, Jinfeng and Liu, Wenyu and Bai, Xiang},\n  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  title = {When Counting Meets HMER: Counting-Aware Network for Handwritten Mathematical Expression Recognition},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_latex_ocr.html","title":"LaTeX-OCR","text":""},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#1-introduction","title":"1. Introduction","text":"<p>Original Project:</p> <p>https://github.com/lukas-blecher/LaTeX-OCR</p> <p>Using LaTeX-OCR printed mathematical expression recognition datasets for training, and evaluating on its test sets, the algorithm reproduction effect is as follows:</p> Model Backbone config BLEU score normed edit distance ExpRate Download link LaTeX-OCR Hybrid ViT LaTeX_OCR_rec.yaml 0.8821 0.0823 40.01% trained model"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\" to clone the project code.</p> <p>Furthermore, additional dependencies need to be installed: <pre><code>pip install -r docs/algorithm/formula_recognition/requirements.txt\n</code></pre></p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p> <p>Pickle File Generation:</p> <p>Download formulae.zip and math.txt in Google Drive, and then use the following command to generate the pickle file.</p> <pre><code># Create a LaTeX-OCR dataset directory\nmkdir -p train_data/LaTeXOCR\n# Unzip formulae.zip and copy math.txt\nunzip -d train_data/LaTeXOCR path/formulae.zip\ncp path/math.txt train_data/LaTeXOCR\n# Convert the original .txt file to a .pkl file to group images of different scales\n# Training set conversion\npython ppocr/utils/formula_utils/math_txt2pkl.py --image_dir=train_data/LaTeXOCR/train --mathtxt_path=train_data/LaTeXOCR/math.txt --output_dir=train_data/LaTeXOCR/\n# Validation set conversion\npython ppocr/utils/formula_utils/math_txt2pkl.py --image_dir=train_data/LaTeXOCR/val --mathtxt_path=train_data/LaTeXOCR/math.txt --output_dir=train_data/LaTeXOCR/\n# Test set conversion\npython ppocr/utils/formula_utils/math_txt2pkl.py --image_dir=train_data/LaTeXOCR/test --mathtxt_path=train_data/LaTeXOCR/math.txt --output_dir=train_data/LaTeXOCR/\n</code></pre> <p>Training:</p> <p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code>#Single GPU training (Default training method)\npython3 tools/train.py -c configs/rec/LaTeX_OCR_rec.yaml\n\n#Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/LaTeX_OCR_rec.yaml\n</code></pre> <p>Evaluation:</p> <pre><code># GPU evaluation\n# Validation set evaluation\npython3 tools/eval.py -c configs/rec/LaTeX_OCR_rec.yaml -o Global.pretrained_model=./rec_latex_ocr_train/best_accuracy.pdparams\n# Test set evaluation\npython3 tools/eval.py -c configs/rec/LaTeX_OCR_rec.yaml -o Global.pretrained_model=./rec_latex_ocr_train/best_accuracy.pdparams Eval.dataset.data_dir=./train_data/LaTeXOCR/test Eval.dataset.data=./train_data/LaTeXOCR/latexocr_test.pkl\n</code></pre> <p>Prediction:</p> <pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/LaTeX_OCR_rec.yaml  -o  Global.infer_img='./docs/datasets/images/pme_demo/0000013.png' Global.pretrained_model=./rec_latex_ocr_train/best_accuracy.pdparams\n</code></pre>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the LaTeX-OCR printed mathematical expression recognition training process is converted into an inference model. you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/LaTeX_OCR_rec.yaml -o Global.pretrained_model=./rec_latex_ocr_train/best_accuracy.pdparams Global.save_inference_dir=./inference/rec_latex_ocr_infer/ \n\n# The default output max length of the model is 512.\n</code></pre> <p>For LaTeX-OCR printed mathematical expression recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./docs/datasets/images/pme_demo/0000295.png' --rec_algorithm=\"LaTeXOCR\" --rec_batch_num=1 --rec_model_dir=\"./inference/rec_latex_ocr_infer/\"  --rec_char_dict_path=\"./ppocr/utils/dict/latex_ocr_tokenizer.json\"\n</code></pre>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_latex_ocr.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet.html","title":"\u5370\u5237\u200b\u6570\u5b66\u516c\u5f0f\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b-PP-FormulaNet","text":""},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet.html#1","title":"1. \u200b\u7b97\u6cd5\u200b\u7b80\u4ecb","text":"<p><code>PP-FormulaNet</code> \u200b\u662f\u200b\u7531\u200b\u767e\u5ea6\u200b\u98de\u6868\u200b\u89c6\u89c9\u200b\u56e2\u961f\u200b\u5f00\u53d1\u200b\u7684\u200b\u4e00\u6b3e\u200b\u5148\u8fdb\u200b\u7684\u200b\u516c\u5f0f\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u652f\u6301\u200b\u8bc6\u522b\u200b 5 \u200b\u4e07\u4e2a\u200b\u5e38\u89c1\u200b LaTeX \u200b\u6e90\u7801\u200b\u8bcd\u6c47\u200b\u3002\u200b\u5176\u4e2d\u200b\uff0cPP-FormulaNet-S \u200b\u7248\u672c\u200b\u91c7\u7528\u200b PP-HGNetV2-B4 \u200b\u4f5c\u4e3a\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b\uff0c\u200b\u901a\u8fc7\u200b\u5e76\u884c\u200b\u63a9\u7801\u200b\u548c\u200b\u6a21\u578b\u200b\u84b8\u998f\u200b\u7b49\u200b\u6280\u672f\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u5e76\u200b\u4fdd\u6301\u200b\u9ad8\u200b\u8bc6\u522b\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u9002\u7528\u200b\u4e8e\u200b\u7b80\u5355\u200b\u5370\u5237\u200b\u516c\u5f0f\u200b\u548c\u200b\u8de8\u884c\u200b\u7b80\u5355\u200b\u5370\u5237\u200b\u516c\u5f0f\u200b\u7b49\u200b\u573a\u666f\u200b\uff1b\u200b\u800c\u200b PP-FormulaNet-L \u200b\u7248\u672c\u200b\u57fa\u4e8e\u200b Vary_VIT_B \u200b\u5e76\u200b\u7ecf\u8fc7\u200b\u5927\u89c4\u6a21\u200b\u516c\u5f0f\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u6df1\u5165\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5728\u200b\u590d\u6742\u200b\u516c\u5f0f\u200b\u8bc6\u522b\u200b\u65b9\u9762\u200b\u8868\u73b0\u200b\u663e\u8457\u200b\u63d0\u5347\u200b\uff0c\u200b\u9002\u7528\u200b\u4e8e\u200b\u7b80\u5355\u200b\u5370\u5237\u200b\u3001\u200b\u590d\u6742\u200b\u5370\u5237\u200b\u548c\u200b\u624b\u5199\u200b\u516c\u5f0f\u200b\u3002</p> <p>\u200b\u4e0a\u8ff0\u200b\u6a21\u578b\u200b\u5728\u200b\u5bf9\u5e94\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u6a21\u578b\u200b \u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b \u200b\u914d\u7f6e\u6587\u4ef6\u200b En-BLEU\u2191 GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09 \u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b UniMERNet Donut Swin UniMERNet.yaml 85.91 2266.96 \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-FormulaNet-S PPHGNetV2_B4 PP-FormulaNet-S.yaml 87.00 202.25 \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-FormulaNet-L Vary_VIT_B PP-FormulaNet-L.yaml 90.36 1976.52 \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b LaTeX-OCR Hybrid ViT LaTeX_OCR_rec.yaml 74.55 1244.61 \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u82f1\u6587\u200b\u516c\u5f0f\u200b\u8bc4\u4f30\u200b\u96c6\u200b\u5305\u542b\u200bUniMERNet\u200b\u7684\u200b\u7b80\u5355\u200b\u548c\u200b\u590d\u6742\u200b\u516c\u5f0f\u200b\uff0c\u200b\u4ee5\u53ca\u200bPaddleX\u200b\u5185\u90e8\u200b\u81ea\u5efa\u200b\u7684\u200b\u7b80\u5355\u200b\u3001\u200b\u4e2d\u7b49\u200b\u548c\u200b\u590d\u6742\u200b\u516c\u5f0f\u200b\u3002</p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet.html#2","title":"2. \u200b\u73af\u5883\u200b\u914d\u7f6e","text":"<p>\u200b\u8bf7\u200b\u5148\u200b\u53c2\u8003\u200b\u300a\u200b\u8fd0\u884c\u200b\u73af\u5883\u200b\u51c6\u5907\u200b\u300b\u200b\u914d\u7f6e\u200bPaddleOCR\u200b\u8fd0\u884c\u200b\u73af\u5883\u200b\uff0c\u200b\u53c2\u8003\u200b\u300a\u200b\u9879\u76ee\u200b\u514b\u9686\u200b\u300b\u200b\u514b\u9686\u200b\u9879\u76ee\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u6b64\u5916\u200b\uff0c\u200b\u9700\u8981\u200b\u5b89\u88c5\u200b\u989d\u5916\u200b\u7684\u200b\u4f9d\u8d56\u200b\uff1a <pre><code>sudo apt-get update\nsudo apt-get install libmagickwand-dev\npip install -r docs/algorithm/formula_recognition/requirements.txt\n</code></pre></p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet.html#3","title":"3. \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u3001\u200b\u8bc4\u4f30\u200b\u3001\u200b\u9884\u6d4b","text":""},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet.html#31","title":"3.1 \u200b\u51c6\u5907\u200b\u6570\u636e\u200b\u96c6","text":"<pre><code># \u200b\u4e0b\u8f7d\u200b PaddleX \u200b\u5b98\u65b9\u200b\u793a\u4f8b\u200b\u6570\u636e\u200b\u96c6\u200b\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/ocr_rec_latexocr_dataset_example.tar\ntar -xf ocr_rec_latexocr_dataset_example.tar\n</code></pre>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet.html#32","title":"3.2 \u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b","text":"<pre><code># \u200b\u4e0b\u8f7d\u200b PP-FormulaNet-S \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\nwget https://paddleocr.bj.bcebos.com/contribution/rec_ppformulanet_s_train.tar \ntar -xf rec_ppformulanet_s_train.tar\n</code></pre>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet.html#33","title":"3.3 \u200b\u6a21\u578b\u200b\u8bad\u7ec3","text":"<p>\u200b\u8bf7\u200b\u53c2\u8003\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u8bad\u7ec3\u200b\u6559\u7a0b\u200b\u3002PaddleOCR\u200b\u5bf9\u200b\u4ee3\u7801\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6a21\u5757\u5316\u200b\uff0c\u200b\u8bad\u7ec3\u200b <code>PP-FormulaNet-S</code> \u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u65f6\u200b\u9700\u8981\u200b\u66f4\u6362\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e3a\u200b <code>PP-FormulaNet-S</code> \u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u3002</p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet.html#_1","title":"\u542f\u52a8\u200b\u8bad\u7ec3","text":"<p>\u200b\u5177\u4f53\u200b\u5730\u200b\uff0c\u200b\u5728\u200b\u5b8c\u6210\u200b\u6570\u636e\u200b\u51c6\u5907\u200b\u540e\u200b\uff0c\u200b\u4fbf\u200b\u53ef\u4ee5\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a <pre><code>#\u200b\u5355\u5361\u200b\u8bad\u7ec3\u200b (\u200b\u9ed8\u8ba4\u200b\u8bad\u7ec3\u200b\u65b9\u5f0f\u200b)\npython3 tools/train.py -c configs/rec/PP-FormuaNet/PP-FormulaNet-S.yaml \\\n   -o Global.pretrained_model=./rec_ppformulanet_s_train/best_accuracy.pdparams\n#\u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u901a\u8fc7\u200b--gpus\u200b\u53c2\u6570\u200b\u6307\u5b9a\u200b\u5361\u53f7\u200b\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' --ips=127.0.0.1   tools/train.py -c configs/rec/PP-FormuaNet/PP-FormulaNet-S.yaml \\\n        -o Global.pretrained_model=./rec_ppformulanet_s_train/best_accuracy.pdparams\n</code></pre></p> <p>\u200b\u6ce8\u610f\u200b\uff1a</p> <ul> <li>\u200b\u9ed8\u8ba4\u200b\u6bcf\u200b\u8bad\u7ec3\u200b 1\u200b\u4e2a\u200bepoch\uff08179 \u200b\u6b21\u200biteration\uff09\u200b\u8fdb\u884c\u200b1\u200b\u6b21\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u82e5\u200b\u60a8\u200b\u66f4\u6539\u200b\u8bad\u7ec3\u200b\u7684\u200bbatch_size\uff0c\u200b\u6216\u200b\u66f4\u6362\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u8bf7\u200b\u5728\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u4f5c\u51fa\u200b\u5982\u4e0b\u200b\u4fee\u6539\u200b <pre><code>python3  -m paddle.distributed.launch --gpus '0,1,2,3' --ips=127.0.0.1   tools/train.py -c configs/rec/PP-FormuaNet/PP-FormulaNet-S.yaml \\\n  -o Global.eval_batch_step=[0,{length_of_dataset//batch_size//4}] \\\n   Global.pretrained_model=./rec_ppformulanet_s_train/best_accuracy.pdparams\n</code></pre></li> </ul>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet.html#34","title":"3.4 \u200b\u8bc4\u4f30","text":"<p>\u200b\u53ef\u200b\u4e0b\u8f7d\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u7684\u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\uff0c\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff1a</p> <pre><code># \u200b\u6ce8\u610f\u200b\u5c06\u200bpretrained_model\u200b\u7684\u200b\u8def\u5f84\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u672c\u5730\u200b\u8def\u5f84\u200b\u3002\u200b\u82e5\u200b\u4f7f\u7528\u200b\u81ea\u884c\u200b\u8bad\u7ec3\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u6ce8\u610f\u200b\u4fee\u6539\u200b\u8def\u5f84\u200b\u548c\u200b\u6587\u4ef6\u540d\u200b\u4e3a\u200b{path/to/weights}/{model_name}\u3002\n # demo \u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u8bc4\u4f30\u200b\n python3 tools/eval.py -c configs/rec/PP-FormuaNet/PP-FormulaNet-S.yaml -o \\\n Global.pretrained_model=./rec_ppformulanet_s_train/best_accuracy.pdparams\n</code></pre>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet.html#35","title":"3.5 \u200b\u9884\u6d4b","text":"<p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u5355\u5f20\u200b\u56fe\u7247\u200b\u9884\u6d4b\u200b\uff1a <pre><code># \u200b\u6ce8\u610f\u200b\u5c06\u200bpretrained_model\u200b\u7684\u200b\u8def\u5f84\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u672c\u5730\u200b\u8def\u5f84\u200b\u3002\npython3 tools/infer_rec.py -c configs/rec/PP-FormuaNet/PP-FormulaNet-S.yaml \\\n  -o  Global.infer_img='./docs/datasets/images/pme_demo/0000295.png'\\\n   Global.pretrained_model=./rec_ppformulanet_s_train/best_accuracy.pdparams\n# \u200b\u9884\u6d4b\u200b\u6587\u4ef6\u5939\u200b\u4e0b\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\u65f6\u200b\uff0c\u200b\u53ef\u200b\u4fee\u6539\u200binfer_img\u200b\u4e3a\u200b\u6587\u4ef6\u5939\u200b\uff0c\u200b\u5982\u200b Global.infer_img='./doc/datasets/pme_demo/'\u3002\n</code></pre></p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet.html#4-faq","title":"4. FAQ","text":""},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet_en.html","title":"PP-FormulaNet","text":""},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet_en.html#1-introduction","title":"1. Introduction","text":"<p><code>PP-FormulaNet</code> is an advanced formula recognition model developed by Baidu\u2019s PaddlePaddle Vision Team, supporting the recognition of 50,000 common LaTeX source terms. The PP-FormulaNet-S version uses PP-HGNetV2-B4 as its backbone network, leveraging techniques like parallel masking and model distillation to significantly enhance inference speed while maintaining high recognition accuracy, making it suitable for scenarios involving simple printed formulas and cross-line simple printed formulas. On the other hand, the PP-FormulaNet-L version is based on Vary_VIT_B and has undergone extensive training on a large-scale formula dataset, showing significant improvement in recognizing complex formulas, and is applicable to simple printed, complex printed, and handwritten formulas.</p> <p>The accuracy of the above models on the corresponding test sets is as follows:</p> Model Backbone config En-BLEU\u2191 GPU Inference Time (ms) Download link UniMERNet Donut Swin UniMERNet.yaml 85.91 2266.96 trained model PP-FormulaNet-S PPHGNetV2_B4 PP-FormulaNet-S.yaml 87.00 202.25 trained model PP-FormulaNet-L Vary_VIT_B PP-FormulaNet-L.yaml 90.36 1976.52 trained model LaTeX-OCR Hybrid ViT LaTeX_OCR_rec.yaml 74.55 1244.61 trained model <p>The English formula evaluation set here contains both simple and complex formulas from UniMERNet, as well as simple, medium, and complex formulas independently created by PaddleX.</p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet_en.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\" to clone the project code.</p> <p>Furthermore, additional dependencies need to be installed: <pre><code>sudo apt-get update\nsudo apt-get install libmagickwand-dev\npip install -r docs/algorithm/formula_recognition/requirements.txt\n</code></pre></p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet_en.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p> <p>Dataset Preparation:</p> <pre><code># download PaddleX official example dataset\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/ocr_rec_latexocr_dataset_example.tar\ntar -xf ocr_rec_latexocr_dataset_example.tar\n</code></pre> <p>Download the Pre-trained Model:</p> <pre><code># download the PP-FormulaNet-S pretrained model\nwget https://paddleocr.bj.bcebos.com/contribution/rec_ppformulanet_s_train.tar \ntar -xf rec_ppformulanet_s_train.tar\n</code></pre> <p>Training:</p> <p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code>#Single GPU training \npython3 tools/train.py -c configs/rec/PP-FormuaNet/PP-FormulaNet-S.yaml \\\n   -o Global.pretrained_model=./rec_ppformulanet_s_train/best_accuracy.pdparams\n#Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' --ips=127.0.0.1   tools/train.py -c configs/rec/PP-FormuaNet/PP-FormulaNet-S.yaml \\\n        -o Global.pretrained_model=./rec_ppformulanet_s_train/best_accuracy.pdparams\n</code></pre> <p>Evaluation:</p> <pre><code># GPU evaluation\n python3 tools/eval.py -c configs/rec/PP-FormuaNet/PP-FormulaNet-S.yaml -o \\\n Global.pretrained_model=./rec_ppformulanet_s_train/best_accuracy.pdparams\n</code></pre> <p>Prediction:</p> <pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/PP-FormuaNet/PP-FormulaNet-S.yaml \\\n  -o  Global.infer_img='./docs/datasets/images/pme_demo/0000295.png'\\\n   Global.pretrained_model=./rec_ppformulanet_s_train/best_accuracy.pdparams\n</code></pre>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_ppformulanet_en.html#4-faq","title":"4. FAQ","text":""},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet.html","title":"\u901a\u7528\u200b\u6570\u5b66\u516c\u5f0f\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b-UniMERNet","text":""},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet.html#1","title":"1. \u200b\u7b97\u6cd5\u200b\u7b80\u4ecb","text":"<p>\u200b\u539f\u59cb\u200b\u9879\u76ee\u200b\uff1a</p> <p>https://github.com/opendatalab/UniMERNet</p> <p><code>UniMERNet</code>\u200b\u4f7f\u7528\u200b<code>UniMERNet\u200b\u901a\u7528\u200b\u516c\u5f0f\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u96c6\u200b</code>\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5728\u200b\u5bf9\u5e94\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u6a21\u578b\u200b \u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b \u200b\u914d\u7f6e\u6587\u4ef6\u200b SPE-BLEU\u2191 SPE-EditDis\u2193 CPE-BLEU\u2191 CPE-EditDis\u2193 SCE-BLEU\u2191 SCE-EditDis\u2193 HWE-BLEU\u2191 HWE-EditDis\u2193 \u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b UniMERNet Donut Swin UniMERNet.yaml 0.9187 0.0584 0.9252 0.0596 0.6068 0.2297 0.9157 0.0546 \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u5176\u4e2d\u200b\uff0cSPE\u200b\u8868\u793a\u200b\u7b80\u5355\u200b\u516c\u5f0f\u200b\uff0cCPE\u200b\u8868\u793a\u200b\u590d\u6742\u200b\u516c\u5f0f\u200b\uff0cSCE\u200b\u8868\u793a\u200b\u626b\u63cf\u200b\u6355\u6349\u200b\u516c\u5f0f\u200b\uff0cHWE\u200b\u8868\u793a\u200b\u624b\u5199\u200b\u516c\u5f0f\u200b\u3002\u200b\u6bcf\u79cd\u200b\u7c7b\u578b\u200b\u7684\u200b\u516c\u5f0f\u200b\u793a\u4f8b\u200b\u56fe\u200b\u5982\u4e0b\u200b\uff1a </p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet.html#2","title":"2. \u200b\u73af\u5883\u200b\u914d\u7f6e","text":"<p>\u200b\u8bf7\u200b\u5148\u200b\u53c2\u8003\u200b\u300a\u200b\u8fd0\u884c\u200b\u73af\u5883\u200b\u51c6\u5907\u200b\u300b\u200b\u914d\u7f6e\u200bPaddleOCR\u200b\u8fd0\u884c\u200b\u73af\u5883\u200b\uff0c\u200b\u53c2\u8003\u200b\u300a\u200b\u9879\u76ee\u200b\u514b\u9686\u200b\u300b\u200b\u514b\u9686\u200b\u9879\u76ee\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u6b64\u5916\u200b\uff0c\u200b\u9700\u8981\u200b\u5b89\u88c5\u200b\u989d\u5916\u200b\u7684\u200b\u4f9d\u8d56\u200b\uff1a <pre><code>sudo apt-get update\nsudo apt-get install libmagickwand-dev\npip install -r docs/algorithm/formula_recognition/requirements.txt\n</code></pre></p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet.html#3","title":"3. \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u3001\u200b\u8bc4\u4f30\u200b\u3001\u200b\u9884\u6d4b","text":""},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet.html#31","title":"3.1 \u200b\u51c6\u5907\u200b\u6570\u636e\u200b\u96c6","text":"<p>\u200b\u4ece\u200b Hugging Face \u200b\u4e0a\u200b\u4e0b\u8f7d\u200b UniMER-1M.zip \u200b\u548c\u200b UniMER-Test.zip\u3002 \u200b\u4ece\u200b \u200b\u597d\u200b\u672a\u6765\u200b\u5e73\u53f0\u200b \u200b\u4e0b\u8f7d\u200b HME100K \u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u4e4b\u540e\u200b\uff0c \u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u521b\u5efa\u200b\u6570\u636e\u200b\u96c6\u200b\u76ee\u5f55\u200b\uff0c\u200b\u5e76\u200b\u5bf9\u200b\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u8f6c\u6362\u200b\u3002</p> <pre><code># \u200b\u521b\u5efa\u200b UniMERNet \u200b\u6570\u636e\u200b\u96c6\u200b\u76ee\u5f55\u200b\nmkdir -p train_data/UniMERNet\n# \u200b\u89e3\u538b\u200b UniMERNet \u3001 UniMER-Test.zip \u200b\u548c\u200b HME100K.zip\nunzip -d train_data/UniMERNet path/UniMER-1M.zip\nunzip -d train_data/UniMERNet path/UniMER-Test.zip\nunzip -d train_data/UniMERNet/HME100K train_data/UniMERNet/HME100K/train.zip\nunzip -d train_data/UniMERNet/HME100K train_data/UniMERNet/HME100K/test.zip\n# \u200b\u8bad\u7ec3\u200b\u96c6\u200b\u8f6c\u6362\u200b   \npython ppocr/utils/formula_utils/unimernet_data_convert.py \\\n     --image_dir=train_data/UniMERNet \\\n     --datatype=unimernet_train \\\n     --unimernet_txt_path=train_data/UniMERNet/UniMER-1M/train.txt \\\n     --hme100k_txt_path=train_data/UniMERNet/HME100K/train_labels.txt \\\n     --output_path=train_data/UniMERNet/train_unimernet_1M.txt\n# \u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u8f6c\u6362\u200b\n# SPE\npython ppocr/utils/formula_utils/unimernet_data_convert.py \\\n     --image_dir=train_data/UniMERNet/UniMER-Test/spe \\\n     --datatype=unimernet_test \\\n     --unimernet_txt_path=train_data/UniMERNet/UniMER-Test/spe.txt \\\n     --output_path=train_data/UniMERNet/test_unimernet_spe.txt\n# CPE\npython ppocr/utils/formula_utils/unimernet_data_convert.py \\\n     --image_dir=train_data/UniMERNet/UniMER-Test/cpe \\\n     --datatype=unimernet_test \\\n     --unimernet_txt_path=train_data/UniMERNet/UniMER-Test/cpe.txt \\\n     --output_path=train_data/UniMERNet/test_unimernet_cpe.txt\n# SCE\npython ppocr/utils/formula_utils/unimernet_data_convert.py \\\n     --image_dir=train_data/UniMERNet/UniMER-Test/sce \\\n     --datatype=unimernet_test \\\n     --unimernet_txt_path=train_data/UniMERNet/UniMER-Test/sce.txt \\\n     --output_path=train_data/UniMERNet/test_unimernet_sce.txt\n# HWE\npython ppocr/utils/formula_utils/unimernet_data_convert.py \\\n     --image_dir=train_data/UniMERNet/UniMER-Test/hwe \\\n     --datatype=unimernet_test \\\n     --unimernet_txt_path=train_data/UniMERNet/UniMER-Test/hwe.txt \\\n     --output_path=train_data/UniMERNet/test_unimernet_hwe.txt\n</code></pre>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet.html#32","title":"3.2 \u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b","text":"<pre><code># \u200b\u4e0b\u8f7d\u200b Texify \u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/pretrained/texify.pdparams\n</code></pre>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet.html#33","title":"3.3 \u200b\u6a21\u578b\u200b\u8bad\u7ec3","text":"<p>\u200b\u8bf7\u200b\u53c2\u8003\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u8bad\u7ec3\u200b\u6559\u7a0b\u200b\u3002PaddleOCR\u200b\u5bf9\u200b\u4ee3\u7801\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6a21\u5757\u5316\u200b\uff0c\u200b\u8bad\u7ec3\u200b<code>UniMERNet</code>\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u65f6\u200b\u9700\u8981\u200b\u66f4\u6362\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e3a\u200b<code>UniMERNet</code>\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u3002</p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet.html#_1","title":"\u542f\u52a8\u200b\u8bad\u7ec3","text":"<p>\u200b\u5177\u4f53\u200b\u5730\u200b\uff0c\u200b\u5728\u200b\u5b8c\u6210\u200b\u6570\u636e\u200b\u51c6\u5907\u200b\u540e\u200b\uff0c\u200b\u4fbf\u200b\u53ef\u4ee5\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a <pre><code>#\u200b\u5355\u5361\u200b\u8bad\u7ec3\u200b (\u200b\u9ed8\u8ba4\u200b\u8bad\u7ec3\u200b\u65b9\u5f0f\u200b)\npython3 tools/train.py -c configs/rec/UniMERNet.yaml \\\n   -o Global.pretrained_model=./pretrain_models/texify.pdparams\n#\u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u901a\u8fc7\u200b--gpus\u200b\u53c2\u6570\u200b\u6307\u5b9a\u200b\u5361\u53f7\u200b\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' --ips=127.0.0.1   tools/train.py -c configs/rec/UniMERNet.yaml \\\n        -o Global.pretrained_model=./pretrain_models/texify.pdparams\n</code></pre></p> <p>\u200b\u6ce8\u610f\u200b\uff1a</p> <ul> <li>\u200b\u9ed8\u8ba4\u200b\u6bcf\u200b\u8bad\u7ec3\u200b 1\u200b\u4e2a\u200bepoch\uff0837880 \u200b\u6b21\u200biteration\uff09\u200b\u8fdb\u884c\u200b1\u200b\u6b21\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u82e5\u200b\u60a8\u200b\u66f4\u6539\u200b\u8bad\u7ec3\u200b\u7684\u200bbatch_size\uff0c\u200b\u6216\u200b\u66f4\u6362\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u8bf7\u200b\u5728\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u4f5c\u51fa\u200b\u5982\u4e0b\u200b\u4fee\u6539\u200b <pre><code>python3  -m paddle.distributed.launch --gpus '0,1,2,3' --ips=127.0.0.1   tools/train.py -c configs/rec/UniMERNet.yaml \\\n  -o Global.eval_batch_step=[0,{length_of_dataset//batch_size//4}] \\\n   Global.pretrained_model=./pretrain_models/texify.pdparams\n</code></pre></li> </ul>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet.html#34","title":"3.4 \u200b\u8bc4\u4f30","text":"<p>\u200b\u53ef\u200b\u4e0b\u8f7d\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u7684\u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\uff0c\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff1a</p> <pre><code># \u200b\u6ce8\u610f\u200b\u5c06\u200bpretrained_model\u200b\u7684\u200b\u8def\u5f84\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u672c\u5730\u200b\u8def\u5f84\u200b\u3002\u200b\u82e5\u200b\u4f7f\u7528\u200b\u81ea\u884c\u200b\u8bad\u7ec3\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u6ce8\u610f\u200b\u4fee\u6539\u200b\u8def\u5f84\u200b\u548c\u200b\u6587\u4ef6\u540d\u200b\u4e3a\u200b{path/to/weights}/{model_name}\u3002\n # SPE \u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u8bc4\u4f30\u200b\n python3 tools/eval.py -c configs/rec/UniMERNet.yaml -o \\\n  Eval.dataset.data_dir=./train_data/UniMERNet/UniMER-Test/spe \\\n  Eval.dataset.label_file_list=[\"./train_data/UniMERNet/test_unimernet_spe.txt\"] \\\n Global.pretrained_model=./rec_unimernet_train/best_accuracy.pdparams\n # CPE \u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u8bc4\u4f30\u200b\n python3 tools/eval.py -c configs/rec/UniMERNet.yaml -o \\\n  Eval.dataset.data_dir=./train_data/UniMERNet/UniMER-Test/cpe \\\n  Eval.dataset.label_file_list=[\"./train_data/UniMERNet/test_unimernet_cpe.txt\"] \\\n Global.pretrained_model=./rec_unimernet_train/best_accuracy.pdparams\n # SCE \u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u8bc4\u4f30\u200b\n  python3 tools/eval.py -c configs/rec/UniMERNet.yaml -o \\\n  Eval.dataset.data_dir=./train_data/UniMERNet/UniMER-Test/sce \\\n  Eval.dataset.label_file_list=[\"./train_data/UniMERNet/test_unimernet_sce.txt\"] \\\n Global.pretrained_model=./rec_unimernet_train/best_accuracy.pdparams\n # HWE \u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u8bc4\u4f30\u200b\n python3 tools/eval.py -c configs/rec/UniMERNet.yaml -o \\\n  Eval.dataset.data_dir=./train_data/UniMERNet/UniMER-Test/hwe \\\n  Eval.dataset.label_file_list=[\"./train_data/UniMERNet/test_unimernet_hwe.txt\"] \\\n Global.pretrained_model=./rec_unimernet_train/best_accuracy.pdparams\n</code></pre>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet.html#35","title":"3.5 \u200b\u9884\u6d4b","text":"<p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u5355\u5f20\u200b\u56fe\u7247\u200b\u9884\u6d4b\u200b\uff1a <pre><code># \u200b\u6ce8\u610f\u200b\u5c06\u200bpretrained_model\u200b\u7684\u200b\u8def\u5f84\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u672c\u5730\u200b\u8def\u5f84\u200b\u3002\npython3 tools/infer_rec.py -c configs/rec/UniMERNet.yaml \\\n  -o  Global.infer_img='./docs/datasets/images/pme_demo/0000099.png'\\\n   Global.pretrained_model=./rec_unimernet_train/best_accuracy.pdparams\n# \u200b\u9884\u6d4b\u200b\u6587\u4ef6\u5939\u200b\u4e0b\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\u65f6\u200b\uff0c\u200b\u53ef\u200b\u4fee\u6539\u200binfer_img\u200b\u4e3a\u200b\u6587\u4ef6\u5939\u200b\uff0c\u200b\u5982\u200b Global.infer_img='./doc/datasets/pme_demo/'\u3002\n</code></pre></p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet.html#4-faq","title":"4. FAQ","text":"<ol> <li>UniMERNet \u200b\u6570\u636e\u200b\u96c6\u200b\u6765\u81ea\u200b\u4e8e\u200bUniMERNet\u200b\u6e90\u200brepo \u3002</li> </ol>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet_en.html","title":"UniMERNet","text":""},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet_en.html#1-introduction","title":"1. Introduction","text":"<p>Original Project:</p> <p>https://github.com/opendatalab/UniMERNet</p> <p>Using UniMERNet general mathematical expression recognition datasets for training, and evaluating on its test sets, the algorithm reproduction effect is as follows:</p> Model Backbone config SPE-BLEU\u2191 SPE-EditDis\u2193 CPE-BLEU\u2191 CPE-EditDis\u2193 SCE-BLEU\u2191 SCE-EditDis\u2193 HWE-BLEU\u2191 HWE-EditDis\u2193 Download link UniMERNet Donut Swin UniMERNet.yaml 0.9187 0.0584 0.9252 0.0596 0.6068 0.2297 0.9157 0.0546 trained model <p>SPE represents simple formulas, CPE represents complex formulas, SCE represents scanned captured formulas, and HWE represents handwritten formulas. Example images of each type of formula are shown below:</p> <p></p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet_en.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\" to clone the project code.</p> <p>Furthermore, additional dependencies need to be installed: <pre><code>sudo apt-get update\nsudo apt-get install libmagickwand-dev\npip install -r docs/algorithm/formula_recognition/requirements.txt\n</code></pre></p>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet_en.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p> <p>Dataset Preparation:</p> <p>Download UniMER-1M.zip and UniMER-Test.zip from Hugging Face. Download the HME100K dataset from the TAL AI Platform. After that, use the following command to create a dataset directory and convert the dataset.</p> <pre><code># create the UniMERNet dataset directory\nmkdir -p train_data/UniMERNet\n# unzip UniMERNet \u3001 UniMER-Test.zip and HME100K.zip\nunzip -d train_data/UniMERNet path/UniMER-1M.zip\nunzip -d train_data/UniMERNet path/UniMER-Test.zip\nunzip -d train_data/UniMERNet/HME100K train_data/UniMERNet/HME100K/train.zip\nunzip -d train_data/UniMERNet/HME100K train_data/UniMERNet/HME100K/test.zip\n# convert the training set \npython ppocr/utils/formula_utils/unimernet_data_convert.py \\\n     --image_dir=train_data/UniMERNet \\\n     --datatype=unimernet_train \\\n     --unimernet_txt_path=train_data/UniMERNet/UniMER-1M/train.txt \\\n     --hme100k_txt_path=train_data/UniMERNet/HME100K/train_labels.txt \\\n     --output_path=train_data/UniMERNet/train_unimernet_1M.txt\n# convert the test set\n# SPE\npython ppocr/utils/formula_utils/unimernet_data_convert.py \\\n     --image_dir=train_data/UniMERNet/UniMER-Test/spe \\\n     --datatype=unimernet_test \\\n     --unimernet_txt_path=train_data/UniMERNet/UniMER-Test/spe.txt \\\n     --output_path=train_data/UniMERNet/test_unimernet_spe.txt\n# CPE\npython ppocr/utils/formula_utils/unimernet_data_convert.py \\\n     --image_dir=train_data/UniMERNet/UniMER-Test/cpe \\\n     --datatype=unimernet_test \\\n     --unimernet_txt_path=train_data/UniMERNet/UniMER-Test/cpe.txt \\\n     --output_path=train_data/UniMERNet/test_unimernet_cpe.txt\n# SCE\npython ppocr/utils/formula_utils/unimernet_data_convert.py \\\n     --image_dir=train_data/UniMERNet/UniMER-Test/sce \\\n     --datatype=unimernet_test \\\n     --unimernet_txt_path=train_data/UniMERNet/UniMER-Test/sce.txt \\\n     --output_path=train_data/UniMERNet/test_unimernet_sce.txt\n# HWE\npython ppocr/utils/formula_utils/unimernet_data_convert.py \\\n     --image_dir=train_data/UniMERNet/UniMER-Test/hwe \\\n     --datatype=unimernet_test \\\n     --unimernet_txt_path=train_data/UniMERNet/UniMER-Test/hwe.txt \\\n     --output_path=train_data/UniMERNet/test_unimernet_hwe.txt\n</code></pre> <p>Download the Pre-trained Model:</p> <pre><code># download the Texify pre-trained model\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/pretrained/texify.pdparams\n</code></pre> <p>Training:</p> <p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code>#Single GPU training \npython3 tools/train.py -c configs/rec/UniMERNet.yaml \\\n   -o Global.pretrained_model=./pretrain_models/texify.pdparams\n#Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' --ips=127.0.0.1   tools/train.py -c configs/rec/UniMERNet.yaml \\\n        -o Global.pretrained_model=./pretrain_models/texify.pdparams\n</code></pre> <p>Evaluation:</p> <pre><code># GPU evaluation\n # SPE test set evaluation\n python3 tools/eval.py -c configs/rec/UniMERNet.yaml -o \\\n  Eval.dataset.data_dir=./train_data/UniMERNet/UniMER-Test/spe \\\n  Eval.dataset.label_file_list=[\"./train_data/UniMERNet/test_unimernet_spe.txt\"] \\\n Global.pretrained_model=./rec_unimernet_train/best_accuracy.pdparams\n # CPE test set evaluation\n python3 tools/eval.py -c configs/rec/UniMERNet.yaml -o \\\n  Eval.dataset.data_dir=./train_data/UniMERNet/UniMER-Test/cpe \\\n  Eval.dataset.label_file_list=[\"./train_data/UniMERNet/test_unimernet_cpe.txt\"] \\\n Global.pretrained_model=./rec_unimernet_train/best_accuracy.pdparams\n # SCE test set evaluation\n  python3 tools/eval.py -c configs/rec/UniMERNet.yaml -o \\\n  Eval.dataset.data_dir=./train_data/UniMERNet/UniMER-Test/sce \\\n  Eval.dataset.label_file_list=[\"./train_data/UniMERNet/test_unimernet_sce.txt\"] \\\n Global.pretrained_model=./rec_unimernet_train/best_accuracy.pdparams\n # HWE test set evaluation\n python3 tools/eval.py -c configs/rec/UniMERNet.yaml -o \\\n  Eval.dataset.data_dir=./train_data/UniMERNet/UniMER-Test/hwe \\\n  Eval.dataset.label_file_list=[\"./train_data/UniMERNet/test_unimernet_hwe.txt\"] \\\n Global.pretrained_model=./rec_unimernet_train/best_accuracy.pdparams\n</code></pre> <p>Prediction:</p> <pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/UniMERNet.yaml \\\n  -o  Global.infer_img='./docs/datasets/images/pme_demo/0000099.png'\\\n   Global.pretrained_model=./rec_unimernet_train/best_accuracy.pdparams\n</code></pre>"},{"location":"en/version2.x/algorithm/formula_recognition/algorithm_rec_unimernet_en.html#4-faq","title":"4. FAQ","text":""},{"location":"en/version2.x/algorithm/kie/algorithm_kie_layoutxlm.html","title":"KIE Algorithm - LayoutXLM","text":""},{"location":"en/version2.x/algorithm/kie/algorithm_kie_layoutxlm.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding</p> <p>Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei</p> <p>2021</p> <p>On XFUND_zh dataset, the algorithm reproduction Hmean is as follows.</p> Model Backbone Task Cnnfig Hmean Download link LayoutXLM LayoutXLM-base SER ser_layoutxlm_xfund_zh.yml 90.38% trained model/inference model LayoutXLM LayoutXLM-base RE re_layoutxlm_xfund_zh.yml 74.83% trained model/inference model"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_layoutxlm.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_layoutxlm.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to KIE tutorial\u3002PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different models.</p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_layoutxlm.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/kie/algorithm_kie_layoutxlm.html#41-python-inference","title":"4.1 Python Inference","text":""},{"location":"en/version2.x/algorithm/kie/algorithm_kie_layoutxlm.html#ser","title":"SER","text":"<p>First, we need to export the trained model into inference model. Take LayoutXLM model trained on XFUND_zh as an example (trained model download link). Use the following command to export.</p> <pre><code>wget https://paddleocr.bj.bcebos.com/pplayout/ser_LayoutXLM_xfun_zh.tar\ntar -xf ser_LayoutXLM_xfun_zh.tar\npython3 tools/export_model.py -c configs/kie/layoutlm_series/ser_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./ser_LayoutXLM_xfun_zh Global.save_inference_dir=./inference/ser_layoutxlm_infer\n</code></pre> <p>Use the following command to infer using LayoutXLM SER model:</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser.py \\\n  --kie_algorithm=LayoutXLM \\\n  --ser_model_dir=../inference/ser_layoutxlm_infer \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf\n</code></pre> <p>The SER visualization results are saved in the <code>./output</code> directory by default. The results are as follows.</p> <p></p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_layoutxlm.html#re","title":"RE","text":"<p>First, we need to export the trained model into inference model. Take LayoutXLM model trained on XFUND_zh as an example (trained model download link). Use the following command to export.</p> <pre><code>wget https://paddleocr.bj.bcebos.com/pplayout/re_LayoutXLM_xfun_zh.tar\ntar -xf re_LayoutXLM_xfun_zh.tar\npython3 tools/export_model.py -c configs/kie/layoutlm_series/re_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./re_LayoutXLM_xfun_zh Global.save_inference_dir=./inference/re_layoutxlm_infer\n</code></pre> <p>Use the following command to infer using LayoutXLM RE model:</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser_re.py \\\n  --kie_algorithm=LayoutXLM \\\n  --re_model_dir=../inference/re_layoutxlm_infer \\\n  --ser_model_dir=../inference/ser_layoutxlm_infer \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf\n</code></pre> <p>The RE visualization results are saved in the <code>./output</code> directory by default. The results are as follows.</p> <p></p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_layoutxlm.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_layoutxlm.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_layoutxlm.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_layoutxlm.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/kie/algorithm_kie_layoutxlm.html#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-2104-08836,\n  author    = {Yiheng Xu and\n               Tengchao Lv and\n               Lei Cui and\n               Guoxin Wang and\n               Yijuan Lu and\n               Dinei Flor{\\^{e}}ncio and\n               Cha Zhang and\n               Furu Wei},\n  title     = {LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich\n               Document Understanding},\n  journal   = {CoRR},\n  volume    = {abs/2104.08836},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2104.08836},\n  eprinttype = {arXiv},\n  eprint    = {2104.08836},\n  timestamp = {Thu, 14 Oct 2021 09:17:23 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08836.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n@article{DBLP:journals/corr/abs-1912-13318,\n  author    = {Yiheng Xu and\n               Minghao Li and\n               Lei Cui and\n               Shaohan Huang and\n               Furu Wei and\n               Ming Zhou},\n  title     = {LayoutLM: Pre-training of Text and Layout for Document Image Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1912.13318},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1912.13318},\n  eprinttype = {arXiv},\n  eprint    = {1912.13318},\n  timestamp = {Mon, 01 Jun 2020 16:20:46 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-13318.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n@article{DBLP:journals/corr/abs-2012-14740,\n  author    = {Yang Xu and\n               Yiheng Xu and\n               Tengchao Lv and\n               Lei Cui and\n               Furu Wei and\n               Guoxin Wang and\n               Yijuan Lu and\n               Dinei A. F. Flor{\\^{e}}ncio and\n               Cha Zhang and\n               Wanxiang Che and\n               Min Zhang and\n               Lidong Zhou},\n  title     = {LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding},\n  journal   = {CoRR},\n  volume    = {abs/2012.14740},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/2012.14740},\n  eprinttype = {arXiv},\n  eprint    = {2012.14740},\n  timestamp = {Tue, 27 Jul 2021 09:53:52 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-14740.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_sdmgr.html","title":"KIE Algorithm - SDMGR","text":""},{"location":"en/version2.x/algorithm/kie/algorithm_kie_sdmgr.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Spatial Dual-Modality Graph Reasoning for Key Information Extraction</p> <p>Hongbin Sun and Zhanghui Kuang and Xiaoyu Yue and Chenhao Lin and Wayne Zhang</p> <p>2021</p> <p>On wildreceipt dataset, the algorithm reproduction Hmean is as follows.</p> Model Backbone Cnnfig Hmean Download link SDMGR VGG6 configs/kie/sdmgr/kie_unet_sdmgr.yml 86.70% trained model/inference model(coming soon)"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_sdmgr.html#2","title":"2. \u200b\u73af\u5883\u200b\u914d\u7f6e","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_sdmgr.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>SDMGR is a key information extraction algorithm that classifies each detected textline into predefined categories, such as order ID, invoice number, amount, etc.</p> <p>The training and test data are collected in the wildreceipt dataset, use following command to downloaded the dataset.</p> <pre><code>wget https://paddleocr.bj.bcebos.com/ppstructure/dataset/wildreceipt.tar &amp;&amp; tar xf wildreceipt.tar\n</code></pre> <p>Create dataset soft link to <code>PaddleOCR/train_data</code> directory.</p> <pre><code>cd PaddleOCR/ &amp;&amp; mkdir train_data &amp;&amp; cd train_data\nln -s ../../wildreceipt ./\n</code></pre>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_sdmgr.html#31-model-training","title":"3.1 Model training","text":"<p>The config file is <code>configs/kie/sdmgr/kie_unet_sdmgr.yml</code>\uff0c the default dataset path is <code>train_data/wildreceipt</code>.</p> <p>Use the following command to train the model.</p> <pre><code>python3 tools/train.py -c configs/kie/sdmgr/kie_unet_sdmgr.yml -o Global.save_model_dir=./output/kie/\n</code></pre>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_sdmgr.html#32-model-evaluation","title":"3.2 Model evaluation","text":"<p>Use the following command to evaluate the model:</p> <pre><code>python3 tools/eval.py -c configs/kie/sdmgr/kie_unet_sdmgr.yml -o Global.checkpoints=./output/kie/best_accuracy\n</code></pre> <p>An example of output information is shown below.</p> <pre><code>[2022/08/10 05:22:23] ppocr INFO: metric eval ***************\n[2022/08/10 05:22:23] ppocr INFO: hmean:0.8670120239257812\n[2022/08/10 05:22:23] ppocr INFO: fps:10.18816520530961\n</code></pre>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_sdmgr.html#33-model-prediction","title":"3.3 Model prediction","text":"<p>Use the following command to load the model and predict. During the prediction, the text file storing the image path and OCR information needs to be loaded in advance. Use <code>Global.infer_img</code> to assign.</p> <pre><code>python3 tools/infer_kie.py -c configs/kie/kie_unet_sdmgr.yml -o Global.checkpoints=kie_vgg16/best_accuracy  Global.infer_img=./train_data/wildreceipt/1.txt\n</code></pre> <p>The visualization results and texts are saved in the <code>./output/sdmgr_kie/</code> directory by default. The results are as follows.</p> <p></p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_sdmgr.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/kie/algorithm_kie_sdmgr.html#41-python-inference","title":"4.1 Python Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_sdmgr.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_sdmgr.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_sdmgr.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_sdmgr.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/kie/algorithm_kie_sdmgr.html#citation","title":"Citation","text":"<pre><code>@misc{sun2021spatial,\n      title={Spatial Dual-Modality Graph Reasoning for Key Information Extraction},\n      author={Hongbin Sun and Zhanghui Kuang and Xiaoyu Yue and Chenhao Lin and Wayne Zhang},\n      year={2021},\n      eprint={2103.14470},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_vi_layoutxlm.html","title":"KIE Algorithm - VI-LayoutXLM","text":""},{"location":"en/version2.x/algorithm/kie/algorithm_kie_vi_layoutxlm.html#1-introduction","title":"1. Introduction","text":"<p>VI-LayoutXLM is improved based on LayoutXLM. In the process of downstream finetuning, the visual backbone network module is removed, and the model infernce speed is further improved on the basis of almost lossless accuracy.</p> <p>On XFUND_zh dataset, the algorithm reproduction Hmean is as follows.</p> Model Backbone Task Config Hmean Download link VI-LayoutXLM VI-LayoutXLM-base SER ser_vi_layoutxlm_xfund_zh_udml.yml 93.19% trained model/inference model VI-LayoutXLM VI-LayoutXLM-base RE re_vi_layoutxlm_xfund_zh_udml.yml 83.92% trained model/inference model"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_vi_layoutxlm.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_vi_layoutxlm.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to KIE tutorial. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different models.</p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_vi_layoutxlm.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/kie/algorithm_kie_vi_layoutxlm.html#41-python-inference","title":"4.1 Python Inference","text":""},{"location":"en/version2.x/algorithm/kie/algorithm_kie_vi_layoutxlm.html#ser","title":"SER","text":"<p>First, we need to export the trained model into inference model. Take VI-LayoutXLM model trained on XFUND_zh as an example (trained model download link). Use the following command to export.</p> <pre><code>wget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/ser_vi_layoutxlm_xfund_pretrained.tar\ntar -xf ser_vi_layoutxlm_xfund_pretrained.tar\npython3 tools/export_model.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./ser_vi_layoutxlm_xfund_pretrained/best_accuracy Global.save_inference_dir=./inference/ser_vi_layoutxlm_infer\n</code></pre> <p>Use the following command to infer using VI-LayoutXLM SER model.</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser.py \\\n  --kie_algorithm=LayoutXLM \\\n  --ser_model_dir=../inference/ser_vi_layoutxlm_infer \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\"\n</code></pre> <p>The SER visualization results are saved in the <code>./output</code> folder by default. The results are as follows.</p> <p></p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_vi_layoutxlm.html#re","title":"RE","text":"<p>First, we need to export the trained model into inference model. Take VI-LayoutXLM model trained on XFUND_zh as an example (trained model download link). Use the following command to export.</p> <pre><code>wget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/re_vi_layoutxlm_xfund_pretrained.tar\ntar -xf re_vi_layoutxlm_xfund_pretrained.tar\npython3 tools/export_model.py -c configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./re_vi_layoutxlm_xfund_pretrained/best_accuracy Global.save_inference_dir=./inference/re_vi_layoutxlm_infer\n</code></pre> <p>Use the following command to infer using VI-LayoutXLM RE model.</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser_re.py \\\n  --kie_algorithm=LayoutXLM \\\n  --re_model_dir=../inference/re_vi_layoutxlm_infer \\\n  --ser_model_dir=../inference/ser_vi_layoutxlm_infer \\\n  --use_visual_backbone=False \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\"\n</code></pre> <p>The RE visualization results are saved in the <code>./output</code> folder by default. The results are as follows.</p> <p></p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_vi_layoutxlm.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_vi_layoutxlm.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_vi_layoutxlm.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/kie/algorithm_kie_vi_layoutxlm.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/kie/algorithm_kie_vi_layoutxlm.html#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-2104-08836,\n  author    = {Yiheng Xu and\n               Tengchao Lv and\n               Lei Cui and\n               Guoxin Wang and\n               Yijuan Lu and\n               Dinei Flor{\\^{e}}ncio and\n               Cha Zhang and\n               Furu Wei},\n  title     = {LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich\n               Document Understanding},\n  journal   = {CoRR},\n  volume    = {abs/2104.08836},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2104.08836},\n  eprinttype = {arXiv},\n  eprint    = {2104.08836},\n  timestamp = {Thu, 14 Oct 2021 09:17:23 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08836.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n@article{DBLP:journals/corr/abs-1912-13318,\n  author    = {Yiheng Xu and\n               Minghao Li and\n               Lei Cui and\n               Shaohan Huang and\n               Furu Wei and\n               Ming Zhou},\n  title     = {LayoutLM: Pre-training of Text and Layout for Document Image Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1912.13318},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1912.13318},\n  eprinttype = {arXiv},\n  eprint    = {1912.13318},\n  timestamp = {Mon, 01 Jun 2020 16:20:46 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-13318.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n@article{DBLP:journals/corr/abs-2012-14740,\n  author    = {Yang Xu and\n               Yiheng Xu and\n               Tengchao Lv and\n               Lei Cui and\n               Furu Wei and\n               Guoxin Wang and\n               Yijuan Lu and\n               Dinei A. F. Flor{\\^{e}}ncio and\n               Cha Zhang and\n               Wanxiang Che and\n               Min Zhang and\n               Lidong Zhou},\n  title     = {LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding},\n  journal   = {CoRR},\n  volume    = {abs/2012.14740},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/2012.14740},\n  eprinttype = {arXiv},\n  eprint    = {2012.14740},\n  timestamp = {Tue, 27 Jul 2021 09:53:52 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-14740.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_gestalt.html","title":"Text Gestalt","text":""},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_gestalt.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Text Gestalt: Stroke-Aware Scene Text Image Super-Resolution Chen, Jingye and Yu, Haiyang and Ma, Jianqi and Li, Bin and Xue, Xiangyang AAAI, 2022</p> <p>Referring to the FudanOCR data download instructions, the effect of the super-score algorithm on the TextZoom test set is as follows:</p> Model Backbone config Acc Download link Text Gestalt tsrn 19.28 0.6560 configs/sr/sr_tsrn_transformer_strock.yml"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_gestalt.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_gestalt.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_gestalt.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/sr/sr_tsrn_transformer_strock.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/sr/sr_tsrn_transformer_strock.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_gestalt.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/sr/sr_tsrn_transformer_strock.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_gestalt.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\n\npython3 tools/infer_sr.py -c configs/sr/sr_tsrn_transformer_strock.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words_en/word_52.png\n</code></pre> <p>After executing the command, the super-resolution result of the above image is as follows:</p> <p></p>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_gestalt.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_gestalt.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/sr/sr_tsrn_transformer_strock.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.save_inference_dir=./inference/sr_out\n</code></pre> <p>For Text-Gestalt super-resolution model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_sr.py --sr_model_dir=./inference/sr_out --image_dir=doc/imgs_words_en/word_52.png --sr_image_shape=3,32,128\n</code></pre> <p>After executing the command, the super-resolution result of the above image is as follows:</p> <p></p>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_gestalt.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_gestalt.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_gestalt.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_gestalt.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_gestalt.html#citation","title":"Citation","text":"<pre><code>@inproceedings{chen2022text,\n  title={Text gestalt: Stroke-aware scene text image super-resolution},\n  author={Chen, Jingye and Yu, Haiyang and Ma, Jianqi and Li, Bin and Xue, Xiangyang},\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\n  volume={36},\n  number={1},\n  pages={285--293},\n  year={2022}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_telescope.html","title":"Text Gestalt","text":""},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_telescope.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Scene Text Telescope: Text-Focused Scene Image Super-Resolution Chen, Jingye, Bin Li, and Xiangyang Xue CVPR, 2021</p> <p>Referring to the FudanOCR data download instructions, the effect of the super-score algorithm on the TextZoom test set is as follows:</p> Model Backbone config Acc Download link Text Gestalt tsrn 21.56 0.7411 configs/sr/sr_telescope.yml <p>The TextZoom dataset comes from two superfraction data sets, RealSR and SR-RAW, both of which contain LR-HR pairs. TextZoom has 17367 pairs of training data and 4373 pairs of test data.</p>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_telescope.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_telescope.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_telescope.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/sr/sr_telescope.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/sr/sr_telescope.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_telescope.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/sr/sr_telescope.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_telescope.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\n\npython3 tools/infer_sr.py -c configs/sr/sr_telescope.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words_en/word_52.png\n</code></pre> <p>After executing the command, the super-resolution result of the above image is as follows:</p> <p></p>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_telescope.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_telescope.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/sr/sr_telescope.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.save_inference_dir=./inference/sr_out\n</code></pre> <p>For Text-Telescope super-resolution model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_sr.py --sr_model_dir=./inference/sr_out --image_dir=doc/imgs_words_en/word_52.png --sr_image_shape=3,32,128\n</code></pre> <p>After executing the command, the super-resolution result of the above image is as follows:</p> <p></p>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_telescope.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_telescope.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_telescope.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_telescope.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/super_resolution/algorithm_sr_telescope.html#citation","title":"Citation","text":"<pre><code>@INPROCEEDINGS{9578891,\n  author={Chen, Jingye and Li, Bin and Xue, Xiangyang},\n  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  title={Scene Text Telescope: Text-Focused Scene Image Super-Resolution},\n  year={2021},\n  volume={},\n  number={},\n  pages={12021-12030},\n  doi={10.1109/CVPR46437.2021.01185}}\n</code></pre>"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_master.html","title":"Table Recognition Algorithm-TableMASTER","text":""},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_master.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>TableMaster: PINGAN-VCGROUP\u2019S SOLUTION FOR ICDAR 2021 COMPETITION ON SCIENTIFIC LITERATURE PARSING TASK B: TABLE RECOGNITION TO HTML Ye, Jiaquan and Qi, Xianbiao and He, Yelin and Chen, Yihao and Gu, Dengyi and Gao, Peng and Xiao, Rong 2021</p> <p>On the PubTabNet table recognition public data set, the algorithm reproduction acc is as follows:</p> Model Backbone Cnnfig Acc Download link TableMaster TableResNetExtra configs/table/table_master.yml 77.47% trained model/inference model"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_master.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_master.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>The above TableMaster model is trained using the PubTabNet table recognition public dataset. For the download of the dataset, please refer to table_datasets.</p> <p>After the data download is complete, please refer to Text Recognition Training Tutorial for training. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different models.</p>"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_master.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_master.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved in the TableMaster table recognition training process into an inference model. Taking the model based on the TableResNetExtra backbone network and trained on the PubTabNet dataset as example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/table/table_master.yml -o Global.pretrained_model=output/table_master/best_accuracy Global.save_inference_dir=./inference/table_master\n</code></pre> <p>Note:</p> <ul> <li>If you trained the model on your own dataset and adjusted the dictionary file, please pay attention to whether the <code>character_dict_path</code> in the modified configuration file is the correct dictionary file</li> </ul> <p>Execute the following command for model inference:</p> <pre><code>cd ppstructure/\n# When predicting all images in a folder, you can modify image_dir to a folder, such as --image_dir='docs/table'.\npython3 table/predict_structure.py --table_model_dir=../output/table_master/table_structure_tablemaster_infer/ --table_algorithm=TableMaster --table_char_dict_path=../ppocr/utils/dict/table_master_structure_dict.txt --table_max_len=480 --image_dir=docs/table/table.jpg\n</code></pre> <p>After executing the command, the prediction results of the above image (structural information and the coordinates of each cell in the table) are printed to the screen, and the visualization of the cell coordinates is also saved. An example is as follows:</p> <p>result\uff1a</p> <pre><code>[2022/06/16 13:06:54] ppocr INFO: result: ['&lt;html&gt;', '&lt;body&gt;', '&lt;table&gt;', '&lt;thead&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;/thead&gt;', '&lt;tbody&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;/tbody&gt;', '&lt;/table&gt;', '&lt;/body&gt;', '&lt;/html&gt;'], [[72.17591094970703, 10.759100914001465, 60.29658508300781, 16.6805362701416], [161.85562133789062, 10.884308815002441, 14.9495210647583, 16.727018356323242], [277.79876708984375, 29.54340362548828, 31.490320205688477, 18.143272399902344],\n...\n[336.11724853515625, 280.3601989746094, 39.456939697265625, 18.121286392211914]]\n[2022/06/16 13:06:54] ppocr INFO: save vis result to ./output/table.jpg\n[2022/06/16 13:06:54] ppocr INFO: Predict time of docs/table/table.jpg: 17.36806297302246\n</code></pre> <p>Note:</p> <ul> <li>TableMaster is relatively slow during inference, and it is recommended to use GPU for use.</li> </ul>"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_master.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Since the post-processing is not written in CPP, the TableMaster does not support CPP inference.</p>"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_master.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_master.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_master.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_master.html#citation","title":"Citation","text":"<pre><code>@article{ye2021pingan,\n  title={PingAn-VCGroup's Solution for ICDAR 2021 Competition on Scientific Literature Parsing Task B: Table Recognition to HTML},\n  author={Ye, Jiaquan and Qi, Xianbiao and He, Yelin and Chen, Yihao and Gu, Dengyi and Gao, Peng and Xiao, Rong},\n  journal={arXiv preprint arXiv:2105.01848},\n  year={2021}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_slanet.html","title":"\u8868\u683c\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b-SLANet-LCNetV2","text":""},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_slanet.html#1","title":"1. \u200b\u7b97\u6cd5\u200b\u7b80\u4ecb","text":"<p>\u200b\u8be5\u200b\u7b97\u6cd5\u200b\u7531\u200b\u6765\u81ea\u200b\u5317\u4eac\u200b\u4ea4\u901a\u200b\u5927\u5b66\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e0e\u200b\u8ba4\u8bc6\u200b\u8ba1\u7b97\u200b\u7814\u7a76\u200b\u56e2\u961f\u200b\u7684\u200bocr\u200b\u8bc6\u522b\u200b\u961f\u200b\u7814\u53d1\u200b\uff0c\u200b\u5176\u200b\u5728\u200bPaddleOCR\u200b\u7b97\u6cd5\u200b\u6a21\u578b\u200b\u6311\u6218\u8d5b\u200b - \u200b\u8d5b\u9898\u200b\u4e8c\u200b\uff1a\u200b\u901a\u7528\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b\u4e2d\u200b\u6392\u884c\u699c\u200b\u8363\u83b7\u200b\u4e00\u7b49\u5956\u200b\uff0c\u200b\u6392\u884c\u699c\u200b\u7cbe\u5ea6\u200b\u76f8\u6bd4\u200bPP-Structure\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u63d0\u5347\u200b0.8%\uff0c\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u63d0\u5347\u200b3\u200b\u500d\u200b\u3002\u200b\u4f18\u5316\u200b\u601d\u8def\u200b\u5982\u4e0b\u200b\uff1a</p> <ol> <li>\u200b\u6539\u5584\u200b\u63a8\u7406\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u81f3\u200bEOS\u200b\u505c\u6b62\u200b\uff0c\u200b\u901f\u5ea6\u200b\u63d0\u5347\u200b3\u200b\u500d\u200b\uff1b</li> <li>\u200b\u5347\u7ea7\u200bBackbone\u200b\u4e3a\u200bLCNetV2\uff08SSLD\u200b\u7248\u672c\u200b\uff09\uff1b</li> <li>\u200b\u884c\u5217\u200b\u7279\u5f81\u200b\u589e\u5f3a\u200b\u6a21\u5757\u200b\uff1b</li> <li>\u200b\u63d0\u5347\u200b\u5206\u8fa8\u7387\u200b488\u200b\u81f3\u200b512\uff1b</li> <li>\u200b\u4e09\u200b\u9636\u6bb5\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\u3002</li> </ol> <p>\u200b\u5728\u200bPubTabNet\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u516c\u5f00\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\uff0c\u200b\u7b97\u6cd5\u200b\u590d\u73b0\u200b\u6548\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u6a21\u578b\u200b \u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b \u200b\u914d\u7f6e\u6587\u4ef6\u200b acc \u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b SLANet LCNetV2 configs/table/SLANet_lcnetv2.yml 76.67% \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b /\u200b\u63a8\u7406\u6a21\u578b"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_slanet.html#2","title":"2. \u200b\u73af\u5883\u200b\u914d\u7f6e","text":"<p>\u200b\u8bf7\u200b\u5148\u200b\u53c2\u8003\u200b\u300a\u200b\u8fd0\u884c\u200b\u73af\u5883\u200b\u51c6\u5907\u200b\u300b\u200b\u914d\u7f6e\u200bPaddleOCR\u200b\u8fd0\u884c\u200b\u73af\u5883\u200b\uff0c\u200b\u53c2\u8003\u200b\u300a\u200b\u9879\u76ee\u200b\u514b\u9686\u200b\u300b\u200b\u514b\u9686\u200b\u9879\u76ee\u200b\u4ee3\u7801\u200b\u3002</p>"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_slanet.html#3","title":"3. \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u3001\u200b\u8bc4\u4f30\u200b\u3001\u200b\u9884\u6d4b","text":"<p>\u200b\u4e0a\u8ff0\u200bSLANet_LCNetv2\u200b\u6a21\u578b\u200b\u4f7f\u7528\u200bPubTabNet\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u516c\u5f00\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200b\uff0c\u200b\u6570\u636e\u200b\u96c6\u200b\u4e0b\u8f7d\u200b\u53ef\u200b\u53c2\u8003\u200b table_datasets\u3002</p>"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_slanet.html#_1","title":"\u542f\u52a8\u200b\u8bad\u7ec3","text":"<p>\u200b\u6570\u636e\u200b\u4e0b\u8f7d\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u6559\u7a0b\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u3002PaddleOCR\u200b\u5bf9\u200b\u4ee3\u7801\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6a21\u5757\u5316\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\u53ea\u200b\u9700\u8981\u200b\u66f4\u6362\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u5373\u53ef\u200b\u3002</p> <p>\u200b\u8bad\u7ec3\u200b\u547d\u4ee4\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># stage1\npython3 -m paddle.distributed.launch --gpus '0,1,2,3,4,5,6,7'  tools/train.py -c configs/table/SLANet_lcnetv2.yml\n# stage2 \u200b\u52a0\u8f7d\u200bstage1\u200b\u7684\u200bbest model\u200b\u4f5c\u4e3a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u5b66\u4e60\u200b\u7387\u200b\u8c03\u6574\u200b\u4e3a\u200b0.0001;\n# stage3 \u200b\u52a0\u8f7d\u200bstage2\u200b\u7684\u200bbest model\u200b\u4f5c\u4e3a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e0d\u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387\u200b\uff0c\u200b\u5c06\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u6240\u6709\u200b\u7684\u200b488\u200b\u4fee\u6539\u200b\u4e3a\u200b512.\n</code></pre>"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_slanet.html#4","title":"4. \u200b\u63a8\u7406\u200b\u90e8\u7f72","text":""},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_slanet.html#41-python","title":"4.1 Python\u200b\u63a8\u7406","text":"<p>\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200bbest\u200b\u6a21\u578b\u200b\uff0c\u200b\u8f6c\u6362\u6210\u200binference model\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u8f6c\u6362\u200b\uff1a</p> <pre><code># \u200b\u6ce8\u610f\u200b\u5c06\u200bpretrained_model\u200b\u7684\u200b\u8def\u5f84\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u672c\u5730\u200b\u8def\u5f84\u200b\u3002\npython3 tools/export_model.py -c configs/table/SLANet_lcnetv2.yml -o Global.pretrained_model=path/best_accuracy Global.save_inference_dir=./inference/slanet_lcnetv2_infer\n</code></pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u60a8\u200b\u662f\u200b\u5728\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u4e14\u200b\u8c03\u6574\u200b\u4e86\u200b\u5b57\u5178\u200b\u6587\u4ef6\u200b\uff0c\u200b\u8bf7\u200b\u6ce8\u610f\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b<code>character_dict_path</code>\u200b\u662f\u5426\u200b\u4e3a\u200b\u6240\u200b\u6b63\u786e\u200b\u7684\u200b\u5b57\u5178\u200b\u6587\u4ef6\u200b\u3002</p> <p>\u200b\u8f6c\u6362\u200b\u6210\u529f\u200b\u540e\u200b\uff0c\u200b\u5728\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u6709\u200b\u4e09\u4e2a\u200b\u6587\u4ef6\u200b\uff1a</p> <pre><code>./inference/slanet_lcnetv2_infer/\n    \u251c\u2500\u2500 inference.pdiparams         # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\u6587\u4ef6\u200b\n    \u251c\u2500\u2500 inference.pdiparams.info    # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u4fe1\u606f\u200b\uff0c\u200b\u53ef\u200b\u5ffd\u7565\u200b\n    \u2514\u2500\u2500 inference.pdmodel           # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200bprogram\u200b\u6587\u4ef6\u200b\n</code></pre> <p>\u200b\u6267\u884c\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b\uff1a</p> <pre><code>cd ppstructure/\npython table/predict_structure.py --table_model_dir=../inference/slanet_lcnetv2_infer/ --table_char_dict_path=../ppocr/utils/dict/table_structure_dict.txt --image_dir=docs/table/table.jpg --output=../output/table_slanet_lcnetv2 --use_gpu=False --benchmark=True --enable_mkldnn=True --table_max_len=512\n# \u200b\u9884\u6d4b\u200b\u6587\u4ef6\u5939\u200b\u4e0b\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\u65f6\u200b\uff0c\u200b\u53ef\u200b\u4fee\u6539\u200bimage_dir\u200b\u4e3a\u200b\u6587\u4ef6\u5939\u200b\uff0c\u200b\u5982\u200b --image_dir='docs/table'\u3002\n</code></pre> <p>\u200b\u6267\u884c\u547d\u4ee4\u200b\u540e\u200b\uff0c\u200b\u4e0a\u9762\u200b\u56fe\u50cf\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff08\u200b\u7ed3\u6784\u200b\u4fe1\u606f\u200b\u548c\u200b\u8868\u683c\u200b\u4e2d\u200b\u6bcf\u4e2a\u200b\u5355\u5143\u683c\u200b\u7684\u200b\u5750\u6807\u200b\uff09\u200b\u4f1a\u200b\u6253\u5370\u200b\u5230\u200b\u5c4f\u5e55\u200b\u4e0a\u200b\uff0c\u200b\u540c\u65f6\u200b\u4f1a\u200b\u4fdd\u5b58\u200b\u5355\u5143\u683c\u200b\u5750\u6807\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\u3002\u200b\u793a\u4f8b\u200b\u5982\u4e0b\u200b\uff1a \u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>[2022/06/16 13:06:54] ppocr INFO: result: ['&lt;html&gt;', '&lt;body&gt;', '&lt;table&gt;', '&lt;thead&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;/thead&gt;', '&lt;tbody&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;/tbody&gt;', '&lt;/table&gt;', '&lt;/body&gt;', '&lt;/html&gt;'], [[72.17591094970703, 10.759100914001465, 60.29658508300781, 16.6805362701416], [161.85562133789062, 10.884308815002441, 14.9495210647583, 16.727018356323242], [277.79876708984375, 29.54340362548828, 31.490320205688477, 18.143272399902344],\n...\n[336.11724853515625, 280.3601989746094, 39.456939697265625, 18.121286392211914]]\n[2022/06/16 13:06:54] ppocr INFO: save vis result to ./output/table.jpg\n[2022/06/16 13:06:54] ppocr INFO: Predict time of docs/table/table.jpg: 17.36806297302246\n</code></pre>"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_slanet.html#42-c","title":"4.2 C++\u200b\u63a8\u7406\u200b\u90e8\u7f72","text":"<p>\u200b\u7531\u4e8e\u200bC++\u200b\u9884\u5904\u7406\u200b\u540e\u5904\u7406\u200b\u8fd8\u200b\u672a\u200b\u652f\u6301\u200bSLANet</p>"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_slanet.html#43-serving","title":"4.3 Serving\u200b\u670d\u52a1\u5316\u200b\u90e8\u7f72","text":"<p>\u200b\u6682\u200b\u4e0d\u200b\u652f\u6301\u200b</p>"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_slanet.html#44","title":"4.4 \u200b\u66f4\u200b\u591a\u200b\u63a8\u7406\u200b\u90e8\u7f72","text":"<p>\u200b\u6682\u200b\u4e0d\u200b\u652f\u6301\u200b</p>"},{"location":"en/version2.x/algorithm/table_recognition/algorithm_table_slanet.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_ct.html","title":"CT","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_ct.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>CentripetalText: An Efficient Text Instance Representation for Scene Text Detection Tao Sheng, Jie Chen, Zhouhui Lian NeurIPS, 2021</p> <p>On the Total-Text dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download CT ResNet18_vd configs/det/det_r18_vd_ct.yml 88.68% 81.70% 85.05% trained model"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_ct.html#2-environment","title":"2. Environment","text":"<p>Please prepare your environment referring to prepare the environment and clone the repo.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_ct.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>The above CT model is trained using the Total-Text text detection public dataset. For the download of the dataset, please refer to Total-Text-Dataset. PaddleOCR format annotation download link train.txt, test.txt.</p> <p>Please refer to text detection training tutorial. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different detection models.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_ct.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_ct.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved in the CT text detection training process into an inference model. Taking the model based on the Resnet18_vd backbone network and trained on the Total Text English dataset as example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_r18_vd_ct.yml -o Global.pretrained_model=./det_r18_ct_train/best_accuracy  Global.save_inference_dir=./inference/det_ct\n</code></pre> <p>CT text detection model inference, you can execute the following command:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs_en/img623.jpg\" --det_model_dir=\"./inference/det_ct/\" --det_algorithm=\"CT\"\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with <code>det_res</code>. Examples of results are as follows:</p> <p></p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_ct.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_ct.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_ct.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_ct.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_ct.html#citation","title":"Citation","text":"<pre><code>@inproceedings{sheng2021centripetaltext,\n    title={CentripetalText: An Efficient Text Instance Representation for Scene Text Detection},\n    author={Tao Sheng and Jie Chen and Zhouhui Lian},\n    booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},\n    year={2021}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_db.html","title":"DB &amp;&amp; DB++","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_db.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Real-time Scene Text Detection with Differentiable Binarization Liao, Minghui and Wan, Zhaoyi and Yao, Cong and Chen, Kai and Bai, Xiang AAAI, 2020</p> <p>Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion Liao, Minghui and Zou, Zhisheng and Wan, Zhaoyi and Yao, Cong and Bai, Xiang TPAMI, 2022</p> <p>On the ICDAR2015 dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download DB ResNet50_vd configs/det/det_r50_vd_db.yml 86.41% 78.72% 82.38% trained model DB MobileNetV3 configs/det/det_mv3_db.yml 77.29% 73.08% 75.12% trained model DB++ ResNet50 configs/det/det_r50_db++_ic15.yml 90.89% 82.66% 86.58% pretrained model/trained model <p>On the TD_TR dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download DB++ ResNet50 configs/det/det_r50_db++_td_tr.yml 92.92% 86.48% 89.58% pretrained model/trained model"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_db.html#2-environment","title":"2. Environment","text":"<p>Please prepare your environment referring to prepare the environment and clone the repo.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_db.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to text detection training tutorial. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different detection models.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_db.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_db.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved in the DB text detection training process into an inference model. Taking the model based on the Resnet50_vd backbone network and trained on the ICDAR2015 English dataset as example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_r50_vd_db.yml -o Global.pretrained_model=./det_r50_vd_db_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/det_db\n</code></pre> <p>DB text detection model inference, you can execute the following command:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs_en/img_10.jpg\" --det_model_dir=\"./inference/det_db/\"\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with <code>det_res</code>. Examples of results are as follows:</p> <p></p> <p>Note: Since the ICDAR2015 dataset has only 1,000 training images, mainly for English scenes, the above model has very poor detection result on Chinese text images.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_db.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>With the inference model prepared, refer to the cpp infer tutorial for C++ inference.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_db.html#43-serving","title":"4.3 Serving","text":"<p>With the inference model prepared, refer to the pdserving tutorial for service deployment by Paddle Serving.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_db.html#44-more","title":"4.4 More","text":"<p>More deployment schemes supported for DB:</p> <ul> <li>Paddle2ONNX: with the inference model prepared, please refer to the paddle2onnx tutorial.</li> </ul>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_db.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_db.html#citation","title":"Citation","text":"<pre><code>@inproceedings{liao2020real,\n  title={Real-time scene text detection with differentiable binarization},\n  author={Liao, Minghui and Wan, Zhaoyi and Yao, Cong and Chen, Kai and Bai, Xiang},\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\n  volume={34},\n  number={07},\n  pages={11474--11481},\n  year={2020}\n}\n\n@article{liao2022real,\n  title={Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion},\n  author={Liao, Minghui and Zou, Zhisheng and Wan, Zhaoyi and Yao, Cong and Bai, Xiang},\n  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  year={2022},\n  publisher={IEEE}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_drrg.html","title":"DRRG","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_drrg.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection Zhang, Shi-Xue and Zhu, Xiaobin and Hou, Jie-Bo and Liu, Chang and Yang, Chun and Wang, Hongfa and Yin, Xu-Cheng CVPR, 2020</p> <p>On the CTW1500 dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download DRRG ResNet50_vd configs/det/det_r50_drrg_ctw.yml 89.92% 80.91% 85.18% trained model"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_drrg.html#2-environment","title":"2. Environment","text":"<p>Please prepare your environment referring to prepare the environment and clone the repo.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_drrg.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>The above DRRG model is trained using the CTW1500 text detection public dataset. For the download of the dataset, please refer to ocr_datasets.</p> <p>After the data download is complete, please refer to Text Detection Training Tutorial for training. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different detection models.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_drrg.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_drrg.html#41-python-inference","title":"4.1 Python Inference","text":"<p>Since the model needs to be converted to Numpy data for many times in the forward, DRRG dynamic graph to static graph is not supported.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_drrg.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_drrg.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_drrg.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_drrg.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_drrg.html#citation","title":"Citation","text":"<pre><code>@inproceedings{zhang2020deep,\n  title={Deep relational reasoning graph network for arbitrary shape text detection},\n  author={Zhang, Shi-Xue and Zhu, Xiaobin and Hou, Jie-Bo and Liu, Chang and Yang, Chun and Wang, Hongfa and Yin, Xu-Cheng},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={9699--9708},\n  year={2020}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_east.html","title":"EAST","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_east.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>EAST: An Efficient and Accurate Scene Text Detector Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He, Jiajun Liang CVPR, 2017</p> <p>On the ICDAR2015 dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download EAST ResNet50_vd det_r50_vd_east.yml 88.71% 81.36% 84.88% model EAST MobileNetV3 det_mv3_east.yml 78.20% 79.10% 78.65% model"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_east.html#2-environment","title":"2. Environment","text":"<p>Please prepare your environment referring to prepare the environment and clone the repo.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_east.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>The above EAST model is trained using the ICDAR2015 text detection public dataset. For the download of the dataset, please refer to ocr_datasets.</p> <p>After the data download is complete, please refer to Text Detection Training Tutorial for training. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different detection models.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_east.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_east.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved in the EAST text detection training process into an inference model. Taking the model based on the Resnet50_vd backbone network and trained on the ICDAR2015 English dataset as example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_r50_vd_east.yml -o Global.pretrained_model=./det_r50_vd_east_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/det_r50_east/\n</code></pre> <p>For EAST text detection model inference, you need to set the parameter --det_algorithm=\"EAST\", run the following command:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs_en/img_10.jpg\" --det_model_dir=\"./inference/det_r50_east/\" --det_algorithm=\"EAST\"\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with <code>det_res</code>.</p> <p></p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_east.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Since the post-processing is not written in CPP, the EAST text detection model does not support CPP inference.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_east.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_east.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_east.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_east.html#citation","title":"Citation","text":"<pre><code>@inproceedings{zhou2017east,\n  title={East: an efficient and accurate scene text detector},\n  author={Zhou, Xinyu and Yao, Cong and Wen, He and Wang, Yuzhi and Zhou, Shuchang and He, Weiran and Liang, Jiajun},\n  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},\n  pages={5551--5560},\n  year={2017}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_fcenet.html","title":"FCENet","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_fcenet.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Fourier Contour Embedding for Arbitrary-Shaped Text Detection Yiqin Zhu and Jianyong Chen and Lingyu Liang and Zhanghui Kuang and Lianwen Jin and Wayne Zhang CVPR, 2021</p> <p>On the CTW1500 dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download FCE ResNet50_dcn configs/det/det_r50_vd_dcn_fce_ctw.yml 88.39% 82.18% 85.27% trained model"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_fcenet.html#2-environment","title":"2. Environment","text":"<p>Please prepare your environment referring to prepare the environment and clone the repo.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_fcenet.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>The above FCE model is trained using the CTW1500 text detection public dataset. For the download of the dataset, please refer to ocr_datasets.</p> <p>After the data download is complete, please refer to Text Detection Training Tutorial for training. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different detection models.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_fcenet.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_fcenet.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved in the FCE text detection training process into an inference model. Taking the model based on the Resnet50_vd_dcn backbone network and trained on the CTW1500 English dataset as example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_r50_vd_dcn_fce_ctw.yml -o Global.pretrained_model=./det_r50_dcn_fce_ctw_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/det_fce\n</code></pre> <p>FCE text detection model inference, to perform non-curved text detection, you can run the following commands:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs_en/img_10.jpg\" --det_model_dir=\"./inference/det_fce/\" --det_algorithm=\"FCE\" --det_fce_box_type=quad\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'det_res'. Examples of results are as follows:</p> <p></p> <p>If you want to perform curved text detection, you can execute the following command:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs_en/img623.jpg\" --det_model_dir=\"./inference/det_fce/\" --det_algorithm=\"FCE\" --det_fce_box_type=poly\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'det_res'. Examples of results are as follows:</p> <p></p> <p>Note: Since the CTW1500 dataset has only 1,000 training images, mainly for English scenes, the above model has very poor detection result on Chinese or curved text images.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_fcenet.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Since the post-processing is not written in CPP, the FCE text detection model does not support CPP inference.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_fcenet.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_fcenet.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_fcenet.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_fcenet.html#citation","title":"Citation","text":"<pre><code>@InProceedings{zhu2021fourier,\n  title={Fourier Contour Embedding for Arbitrary-Shaped Text Detection},\n  author={Yiqin Zhu and Jianyong Chen and Lingyu Liang and Zhanghui Kuang and Lianwen Jin and Wayne Zhang},\n  year={2021},\n  booktitle = {CVPR}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_psenet.html","title":"PSENet","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_psenet.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Shape robust text detection with progressive scale expansion network Wang, Wenhai and Xie, Enze and Li, Xiang and Hou, Wenbo and Lu, Tong and Yu, Gang and Shao, Shuai CVPR, 2019</p> <p>On the ICDAR2015 dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download PSE ResNet50_vd configs/det/det_r50_vd_pse.yml 85.81% 79.53% 82.55% trained model PSE MobileNetV3 configs/det/det_mv3_pse.yml 82.20% 70.48% 75.89% trained model"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_psenet.html#2-environment","title":"2. Environment","text":"<p>Please prepare your environment referring to prepare the environment and clone the repo.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_psenet.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>The above PSE model is trained using the ICDAR2015 text detection public dataset. For the download of the dataset, please refer to ocr_datasets.</p> <p>After the data download is complete, please refer to Text Detection Training Tutorial for training. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different detection models.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_psenet.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_psenet.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved in the PSE text detection training process into an inference model. Taking the model based on the Resnet50_vd backbone network and trained on the ICDAR2015 English dataset as example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_r50_vd_pse.yml -o Global.pretrained_model=./det_r50_vd_pse_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/det_pse\n</code></pre> <p>PSE text detection model inference, to perform non-curved text detection, you can run the following commands:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs_en/img_10.jpg\" --det_model_dir=\"./inference/det_pse/\" --det_algorithm=\"PSE\" --det_pse_box_type=quad\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'det_res'. Examples of results are as follows:</p> <p></p> <p>If you want to perform curved text detection, you can execute the following command:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs_en/img_10.jpg\" --det_model_dir=\"./inference/det_pse/\" --det_algorithm=\"PSE\" --det_pse_box_type=poly\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'det_res'. Examples of results are as follows:</p> <p></p> <p>Note: Since the ICDAR2015 dataset has only 1,000 training images, mainly for English scenes, the above model has very poor detection result on Chinese or curved text images.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_psenet.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Since the post-processing is not written in CPP, the PSE text detection model does not support CPP inference.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_psenet.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_psenet.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_psenet.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_psenet.html#citation","title":"Citation","text":"<pre><code>@inproceedings{wang2019shape,\n  title={Shape robust text detection with progressive scale expansion network},\n  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Hou, Wenbo and Lu, Tong and Yu, Gang and Shao, Shuai},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={9336--9345},\n  year={2019}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_sast.html","title":"SAST","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_sast.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning Wang, Pengfei and Zhang, Chengquan and Qi, Fei and Huang, Zuming and En, Mengyi and Han, Junyu and Liu, Jingtuo and Ding, Errui and Shi, Guangming ACM MM, 2019</p> <p>On the ICDAR2015 dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download SAST ResNet50_vd configs/det/det_r50_vd_sast_icdar15.yml 91.39% 83.77% 87.42% trained model <p>On the Total-text dataset, the text detection result is as follows:</p> Model Backbone Configuration Precision Recall Hmean Download SAST ResNet50_vd configs/det/det_r50_vd_sast_totaltext.yml 89.63% 78.44% 83.66% trained model"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_sast.html#2-environment","title":"2. Environment","text":"<p>Please prepare your environment referring to prepare the environment and clone the repo.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_sast.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to text detection training tutorial. PaddleOCR has modularized the code structure, so that you only need to replace the configuration file to train different detection models.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_sast.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_sast.html#41-python-inference","title":"4.1 Python Inference","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_sast.html#1-quadrangle-text-detection-model-icdar2015","title":"(1). Quadrangle text detection model (ICDAR2015)","text":"<p>First, convert the model saved in the SAST text detection training process into an inference model. Taking the model based on the Resnet50_vd backbone network and trained on the ICDAR2015 English dataset as an example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_r50_vd_sast_icdar15.yml -o Global.pretrained_model=./det_r50_vd_sast_icdar15_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/det_sast_ic15\n</code></pre> <p>For SAST quadrangle text detection model inference, you need to set the parameter <code>--det_algorithm=\"SAST\"</code>, run the following command:</p> <pre><code>python3 tools/infer/predict_det.py --det_algorithm=\"SAST\" --image_dir=\"./doc/imgs_en/img_10.jpg\" --det_model_dir=\"./inference/det_sast_ic15/\"\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'det_res'. Examples of results are as follows:</p> <p></p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_sast.html#2-curved-text-detection-model-total-text","title":"(2). Curved text detection model (Total-Text)","text":"<p>First, convert the model saved in the SAST text detection training process into an inference model. Taking the model based on the Resnet50_vd backbone network and trained on the Total-Text English dataset as an example (model download link), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_r50_vd_sast_totaltext.yml -o Global.pretrained_model=./det_r50_vd_sast_totaltext_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/det_sast_tt\n</code></pre> <p>For SAST curved text detection model inference, you need to set the parameter <code>--det_algorithm=\"SAST\"</code> and <code>--det_box_type=poly</code>, run the following command:</p> <pre><code>python3 tools/infer/predict_det.py --det_algorithm=\"SAST\" --image_dir=\"./doc/imgs_en/img623.jpg\" --det_model_dir=\"./inference/det_sast_tt/\" --det_box_type='poly'\n</code></pre> <p>The visualized text detection results are saved to the <code>./inference_results</code> folder by default, and the name of the result file is prefixed with 'det_res'. Examples of results are as follows:</p> <p></p> <p>Note: SAST post-processing locality aware NMS has two versions: Python and C++. The speed of C++ version is obviously faster than that of Python version. Due to the compilation version problem of NMS of C++ version, C++ version NMS will be called only in Python 3.5 environment, and python version NMS will be called in other cases.</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_sast.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_sast.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_sast.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_sast.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_detection/algorithm_det_sast.html#citation","title":"Citation","text":"<pre><code>@inproceedings{wang2019single,\n  title={A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning},\n  author={Wang, Pengfei and Zhang, Chengquan and Qi, Fei and Huang, Zuming and En, Mengyi and Han, Junyu and Liu, Jingtuo and Ding, Errui and Shi, Guangming},\n  booktitle={Proceedings of the 27th ACM International Conference on Multimedia},\n  pages={1277--1285},\n  year={2019}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_abinet.html","title":"ABINet","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_abinet.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>ABINet: Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition Shancheng Fang and Hongtao Xie and Yuxin Wang and Zhendong Mao and Yongdong Zhang CVPR, 2021</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link ABINet ResNet45 rec_r45_abinet.yml 90.75% pretrained &amp; trained model"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_abinet.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_abinet.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_abinet.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r45_abinet.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_r45_abinet.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_abinet.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r45_abinet.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_abinet.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r45_abinet.yml -o Global.infer_img='./doc/imgs_words_en/word_10.png' Global.pretrained_model=./rec_r45_abinet_train/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_abinet.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_abinet.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the ABINet text recognition training process is converted into an inference model. ( Model download link) ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r45_abinet.yml -o Global.pretrained_model=./rec_r45_abinet_train/best_accuracy  Global.save_inference_dir=./inference/rec_r45_abinet\n</code></pre> <p>Note:</p> <ul> <li>If you are training the model on your own dataset and have modified the dictionary file, please pay attention to modify the <code>character_dict_path</code> in the configuration file to the modified dictionary file.</li> <li>If you modified the input size during training, please modify the <code>infer_shape</code> corresponding to ABINet in the <code>tools/export_model.py</code> file.</li> </ul> <p>After the conversion is successful, there are three files in the directory:</p> <pre><code>/inference/rec_r45_abinet/\n    \u251c\u2500\u2500 inference.pdiparams\n    \u251c\u2500\u2500 inference.pdiparams.info\n    \u2514\u2500\u2500 inference.pdmodel\n</code></pre> <p>For ABINet text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./doc/imgs_words_en/word_10.png' --rec_model_dir='./inference/rec_r45_abinet/' --rec_algorithm='ABINet' --rec_image_shape='3,32,128' --rec_char_dict_path='./ppocr/utils/ic15_dict.txt'\n</code></pre> <p></p> <p>After executing the command, the prediction result (recognized text and score) of the image above is printed to the screen, an example is as follows: The result is as follows:</p> <pre><code>Predicts of ./doc/imgs_words_en/word_10.png:('pain', 0.9999995231628418)\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_abinet.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_abinet.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_abinet.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_abinet.html#5-faq","title":"5. FAQ","text":"<ol> <li>Note that the MJSynth and SynthText datasets come from ABINet repo.</li> <li>We use the pre-trained model provided by the ABINet authors for finetune training.</li> </ol>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_abinet.html#citation","title":"Citation","text":"<pre><code>@article{Fang2021ABINet,\n  title     = {ABINet: Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition},\n  author    = {Shancheng Fang and Hongtao Xie and Yuxin Wang and Zhendong Mao and Yongdong Zhang},\n  booktitle = {CVPR},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2103.06495},\n  pages     = {7098-7107}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_aster.html","title":"STAR-Net","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_aster.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>STAR-Net: a spatial attention residue network for scene text recognition. Wei Liu, Chaofeng Chen, Kwan-Yee K. Wong, Zhizhong Su and Junyu Han. BMVC, pages 43.1-43.13, 2016</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone ACC config Download link --- --- --- --- --- StarNet Resnet34_vd 84.44% configs/rec/rec_r34_vd_tps_bilstm_ctc.yml \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b StarNet MobileNetV3 81.42% configs/rec/rec_mv3_tps_bilstm_ctc.yml \u200b\u8bad\u7ec3\u200b\u6a21\u578b"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_aster.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_aster.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_aster.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c rec_r34_vd_tps_bilstm_ctc.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_aster.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_aster.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_aster.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_aster.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the STAR-Net text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml -o Global.pretrained_model=./rec_r34_vd_tps_bilstm_ctc_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/rec_starnet\n</code></pre> <p>For STAR-Net text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words_en/word_336.png\" --rec_model_dir=\"./inference/rec_starnet/\" --rec_image_shape=\"3, 32, 100\" --rec_char_dict_path=\"./ppocr/utils/ic15_dict.txt\"\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_aster.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>With the inference model prepared, refer to the cpp infer tutorial for C++ inference.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_aster.html#43-serving","title":"4.3 Serving","text":"<p>With the inference model prepared, refer to the pdserving tutorial for service deployment by Paddle Serving.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_aster.html#44-more","title":"4.4 More","text":"<p>More deployment schemes supported for STAR-Net:</p> <ul> <li>Paddle2ONNX: with the inference model prepared, please refer to the paddle2onnx tutorial.</li> </ul>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_aster.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_aster.html#citation","title":"Citation","text":"<pre><code>@inproceedings{liu2016star,\n  title={STAR-Net: a spatial attention residue network for scene text recognition.},\n  author={Liu, Wei and Chen, Chaofeng and Wong, Kwan-Yee K and Su, Zhizhong and Han, Junyu},\n  booktitle={BMVC},\n  volume={2},\n  pages={7},\n  year={2016}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_cppd.html","title":"CPPD","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_cppd.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Context Perception Parallel Decoder for Scene Text Recognition Yongkun Du and Zhineng Chen and Caiyan Jia and Xiaoting Yin and Chenxia Li and Yuning Du and Yu-Gang Jiang</p> <p>Scene text recognition models based on deep learning typically follow an Encoder-Decoder structure, where the decoder can be categorized into two types: (1) CTC and (2) Attention-based. Currently, most state-of-the-art (SOTA) models use an Attention-based decoder, which can be further divided into AR and PD types. In general, AR decoders achieve higher recognition accuracy than PD, while PD decoders are faster than AR. CPPD, with carefully designed CO and CC modules, achieves a balance between the accuracy of AR and the speed of PD.</p> <p>The accuracy (%) and model files of CPPD on the public dataset of scene text recognition are as follows:\uff1a</p> <ul> <li>English dataset from PARSeq.</li> </ul> Model IC13857 SVT IIIT5k3000 IC151811 SVTP CUTE80 Avg Download CPPD Tiny 97.1 94.4 96.6 86.6 88.5 90.3 92.25 en CPPD Base 98.2 95.5 97.6 87.9 90.0 92.7 93.80 en CPPD Base 48*160 97.5 95.5 97.7 87.7 92.4 93.7 94.10 en <ul> <li>Trained on Synth dataset(MJ+ST), Test on Union14M-L benchmark from U14m.</li> </ul> Model Curve Multi-Oriented Artistic Contextless Salient Multi-word General Avg Download CPPD Tiny 52.4 12.3 48.2 54.4 61.5 53.4 61.4 49.10 Same as the table above. CPPD Base 65.5 18.6 56.0 61.9 71.0 57.5 65.8 56.63 Same as the table above. CPPD Base 48*160 71.9 22.1 60.5 67.9 78.3 63.9 67.1 61.69 Same as the table above. <ul> <li>Trained on Union14M-L training dataset.</li> </ul> Model IC13857 SVT IIIT5k3000 IC151811 SVTP CUTE80 Avg Download CPPD Base 32*128 98.7 98.5 99.4 91.7 96.7 99.7 97.44 en Model Curve Multi-Oriented Artistic Contextless Salient Multi-word General Avg Download CPPD Base 32*128 87.5 70.7 78.2 82.9 85.5 85.4 84.3 82.08 Same as the table above. <ul> <li>Chinese dataset from Chinese Benchmark.</li> </ul> Model Scene Web Document Handwriting Avg Download CPPD Base 74.4 76.1 98.6 55.3 76.10 ch CPPD Base + STN 78.4 79.3 98.9 57.6 78.55 ch"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_cppd.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_cppd.html#dataset-preparation","title":"Dataset Preparation","text":"<p>English dataset download Union14M-Benchmark download Chinese dataset download</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_cppd.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_cppd.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_svtrnet_cppd_base_en.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_svtrnet_cppd_base_en.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_cppd.html#evaluation","title":"Evaluation","text":"<p>You can download the model files and configuration files provided by <code>CPPD</code>: download link, take <code>CPPD-B</code> as an example, using the following command to evaluate:</p> <pre><code># Download the tar archive containing the model files and configuration files of CPPD-B and extract it\nwget https://paddleocr.bj.bcebos.com/CCPD/rec_svtr_cppd_base_en_train.tar &amp;&amp; tar xf rec_svtr_cppd_base_en_train.tar\n# GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c ./rec_svtr_cppd_base_en_train/rec_svtrnet_cppd_base_en.yml -o Global.pretrained_model=./rec_svtr_cppd_base_en_train/best_model\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_cppd.html#prediction","title":"Prediction","text":"<pre><code>python3 tools/infer_rec.py -c ./rec_svtr_cppd_base_en_train/rec_svtrnet_cppd_base_en.yml -o Global.infer_img='./doc/imgs_words_en/word_10.png' Global.pretrained_model=./rec_svtr_cppd_base_en_train/best_model\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_cppd.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_cppd.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the CPPD text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code># export model\n# en\npython3 tools/export_model.py -c configs/rec/rec_svtrnet_cppd_base_en.yml -o Global.pretrained_model=./rec_svtr_cppd_base_en_train/best_model.pdparams Global.save_inference_dir=./rec_svtr_cppd_base_en_infer\n# ch\npython3 tools/export_model.py -c configs/rec/rec_svtrnet_cppd_base_ch.yml -o Global.pretrained_model=./rec_svtr_cppd_base_ch_train/best_model.pdparams Global.save_inference_dir=./rec_svtr_cppd_base_ch_infer\n\n# speed test\n# docker image https://hub.docker.com/r/paddlepaddle/paddle/tags/: sudo docker pull paddlepaddle/paddle:2.4.2-gpu-cuda11.2-cudnn8.2-trt8.0\n# install auto_log: pip install https://paddleocr.bj.bcebos.com/libs/auto_log-1.2.0-py3-none-any.whl\n# en\npython3 tools/infer/predict_rec.py --image_dir='../iiik' --rec_model_dir='./rec_svtr_cppd_base_en_infer/' --rec_algorithm='CPPD' --rec_image_shape='3,32,100' --rec_char_dict_path='./ppocr/utils/ic15_dict.txt' --warmup=True --benchmark=True --rec_batch_num=1 --use_tensorrt=True\n# ch\npython3 tools/infer/predict_rec.py --image_dir='../iiik' --rec_model_dir='./rec_svtr_cppd_base_ch_infer/' --rec_algorithm='CPPDPadding' --rec_image_shape='3,32,256' --warmup=True --benchmark=True --rec_batch_num=1 --use_tensorrt=True\n# stn_ch\npython3 tools/infer/predict_rec.py --image_dir='../iiik' --rec_model_dir='./rec_svtr_cppd_base_stn_ch_infer/' --rec_algorithm='CPPD' --rec_image_shape='3,64,256' --warmup=True --benchmark=True --rec_batch_num=1 --use_tensorrt=True\n</code></pre> <p>Note: If you are training the model on your own dataset and have modified the dictionary file, please pay attention to modify the <code>character_dict_path</code> in the configuration file to the modified dictionary file.</p> <p>After the conversion is successful, there are three files in the directory:</p> <pre><code>/inference/rec_svtr_cppd_base_en_infer/\n    \u251c\u2500\u2500 inference.pdiparams\n    \u251c\u2500\u2500 inference.pdiparams.info\n    \u2514\u2500\u2500 inference.pdmodel\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_cppd.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_cppd.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_cppd.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_cppd.html#citation","title":"Citation","text":"<pre><code>@article{Du2023CPPD,\n  title     = {Context Perception Parallel Decoder for Scene Text Recognition},\n  author    = {Du, Yongkun and Chen, Zhineng and Jia, Caiyan and Yin, Xiaoting and Li, Chenxia and Du, Yuning and Jiang, Yu-Gang},\n  booktitle = {Arxiv},\n  year      = {2023},\n  url       = {https://arxiv.org/abs/2307.12270}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_crnn.html","title":"CRNN","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_crnn.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition</p> <p>Baoguang Shi, Xiang Bai, Cong Yao</p> <p>IEEE, 2015</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone ACC config Download link --- --- --- --- --- CRNN Resnet34_vd 81.04% configs/rec/rec_r34_vd_none_bilstm_ctc.yml \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b CRNN MobileNetV3 77.95% configs/rec/rec_mv3_none_bilstm_ctc.yml \u200b\u8bad\u7ec3\u200b\u6a21\u578b"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_crnn.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_crnn.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_crnn.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r34_vd_none_bilstm_ctc.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_r34_vd_none_bilstm_ctc.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_crnn.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r34_vd_none_bilstm_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_crnn.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r34_vd_none_bilstm_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_crnn.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_crnn.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the CRNN text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r34_vd_none_bilstm_ctc.yml -o Global.pretrained_model=./rec_r34_vd_none_bilstm_ctc_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/rec_crnn\n</code></pre> <p>For CRNN text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words_en/word_336.png\" --rec_model_dir=\"./inference/rec_crnn/\" --rec_image_shape=\"3, 32, 100\" --rec_char_dict_path=\"./ppocr/utils/ic15_dict.txt\"\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_crnn.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>With the inference model prepared, refer to the cpp infer tutorial for C++ inference.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_crnn.html#43-serving","title":"4.3 Serving","text":"<p>With the inference model prepared, refer to the pdserving tutorial for service deployment by Paddle Serving.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_crnn.html#44-more","title":"4.4 More","text":"<p>More deployment schemes supported for CRNN:</p> <ul> <li>Paddle2ONNX: with the inference model prepared, please refer to the paddle2onnx tutorial.</li> </ul>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_crnn.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_crnn.html#citation","title":"Citation","text":"<pre><code>@ARTICLE{7801919,\n  author={Shi, Baoguang and Bai, Xiang and Yao, Cong},\n  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  title={An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition},\n  year={2017},\n  volume={39},\n  number={11},\n  pages={2298-2304},\n  doi={10.1109/TPAMI.2016.2646371}}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_nrtr.html","title":"NRTR","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_nrtr.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>NRTR: A No-Recurrence Sequence-to-Sequence Model For Scene Text Recognition Fenfen Sheng and Zhineng Chen and Bo Xu ICDAR, 2019</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link NRTR MTB rec_mtb_nrtr.yml 84.21% trained model"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_nrtr.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_nrtr.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_nrtr.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_mtb_nrtr.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_mtb_nrtr.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_nrtr.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_mtb_nrtr.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_nrtr.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_mtb_nrtr.yml -o Global.infer_img='./doc/imgs_words_en/word_10.png' Global.pretrained_model=./rec_mtb_nrtr_train/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_nrtr.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_nrtr.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the NRTR text recognition training process is converted into an inference model. ( Model download link) ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_mtb_nrtr.yml -o Global.pretrained_model=./rec_mtb_nrtr_train/best_accuracy  Global.save_inference_dir=./inference/rec_mtb_nrtr\n</code></pre> <p>Note:</p> <ul> <li>If you are training the model on your own dataset and have modified the dictionary file, please pay attention to modify the <code>character_dict_path</code> in the configuration file to the modified dictionary file.</li> <li>If you modified the input size during training, please modify the <code>infer_shape</code> corresponding to NRTR in the <code>tools/export_model.py</code> file.</li> </ul> <p>After the conversion is successful, there are three files in the directory:</p> <pre><code>/inference/rec_mtb_nrtr/\n    \u251c\u2500\u2500 inference.pdiparams\n    \u251c\u2500\u2500 inference.pdiparams.info\n    \u2514\u2500\u2500 inference.pdmodel\n</code></pre> <p>For NRTR text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./doc/imgs_words_en/word_10.png' --rec_model_dir='./inference/rec_mtb_nrtr/' --rec_algorithm='NRTR' --rec_image_shape='1,32,100' --rec_char_dict_path='./ppocr/utils/EN_symbol_dict.txt'\n</code></pre> <p></p> <p>After executing the command, the prediction result (recognized text and score) of the image above is printed to the screen, an example is as follows:</p> <pre><code>Predicts of ./doc/imgs_words_en/word_10.png:('pain', 0.9465042352676392)\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_nrtr.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_nrtr.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_nrtr.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_nrtr.html#5-faq","title":"5. FAQ","text":"<ol> <li>In the <code>NRTR</code> paper, Beam search is used to decode characters, but the speed is slow. Beam search is not used by default here, and greedy search is used to decode characters.</li> </ol>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_nrtr.html#6-release-note","title":"6. Release Note","text":"<ol> <li> <p>The release/2.6 version updates the NRTR code structure. The new version of NRTR can load the model parameters of the old version (release/2.5 and before), and you may use the following code to convert the old version model parameters to the new version model parameters:</p> <p> Click to expand <pre><code>params = paddle.load('path/' + '.pdparams') # the old version parameters\nstate_dict = model.state_dict() # the new version model parameters\nnew_state_dict = {}\n\nfor k1, v1 in state_dict.items():\n\n    k = k1\n    if 'encoder' in k and 'self_attn' in k and 'qkv' in k and 'weight' in k:\n\n        k_para = k[:13] + 'layers.' + k[13:]\n        q = params[k_para.replace('qkv', 'conv1')].transpose((1, 0, 2, 3))\n        k = params[k_para.replace('qkv', 'conv2')].transpose((1, 0, 2, 3))\n        v = params[k_para.replace('qkv', 'conv3')].transpose((1, 0, 2, 3))\n\n        new_state_dict[k1] = np.concatenate([q[:, :, 0, 0], k[:, :, 0, 0], v[:, :, 0, 0]], -1)\n\n    elif 'encoder' in k and 'self_attn' in k and 'qkv' in k and 'bias' in k:\n\n        k_para = k[:13] + 'layers.' + k[13:]\n        q = params[k_para.replace('qkv', 'conv1')]\n        k = params[k_para.replace('qkv', 'conv2')]\n        v = params[k_para.replace('qkv', 'conv3')]\n\n        new_state_dict[k1] = np.concatenate([q, k, v], -1)\n\n    elif 'encoder' in k and 'self_attn' in k and 'out_proj' in k:\n\n        k_para = k[:13] + 'layers.' + k[13:]\n        new_state_dict[k1] = params[k_para]\n\n    elif 'encoder' in k and 'norm3' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        new_state_dict[k1] = params[k_para.replace('norm3', 'norm2')]\n\n    elif 'encoder' in k and 'norm1' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        new_state_dict[k1] = params[k_para]\n\n\n    elif 'decoder' in k and 'self_attn' in k and 'qkv' in k and 'weight' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        q = params[k_para.replace('qkv', 'conv1')].transpose((1, 0, 2, 3))\n        k = params[k_para.replace('qkv', 'conv2')].transpose((1, 0, 2, 3))\n        v = params[k_para.replace('qkv', 'conv3')].transpose((1, 0, 2, 3))\n        new_state_dict[k1] = np.concatenate([q[:, :, 0, 0], k[:, :, 0, 0], v[:, :, 0, 0]], -1)\n\n    elif 'decoder' in k and 'self_attn' in k and 'qkv' in k and 'bias' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        q = params[k_para.replace('qkv', 'conv1')]\n        k = params[k_para.replace('qkv', 'conv2')]\n        v = params[k_para.replace('qkv', 'conv3')]\n        new_state_dict[k1] = np.concatenate([q, k, v], -1)\n\n    elif 'decoder' in k and 'self_attn' in k and 'out_proj' in k:\n\n        k_para = k[:13] + 'layers.' + k[13:]\n        new_state_dict[k1] = params[k_para]\n\n    elif 'decoder' in k and 'cross_attn' in k and 'q' in k and 'weight' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        k_para = k_para.replace('cross_attn', 'multihead_attn')\n        q = params[k_para.replace('q', 'conv1')].transpose((1, 0, 2, 3))\n        new_state_dict[k1] = q[:, :, 0, 0]\n\n    elif 'decoder' in k and 'cross_attn' in k and 'q' in k and 'bias' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        k_para = k_para.replace('cross_attn', 'multihead_attn')\n        q = params[k_para.replace('q', 'conv1')]\n        new_state_dict[k1] = q\n\n    elif 'decoder' in k and 'cross_attn' in k and 'kv' in k and 'weight' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        k_para = k_para.replace('cross_attn', 'multihead_attn')\n        k = params[k_para.replace('kv', 'conv2')].transpose((1, 0, 2, 3))\n        v = params[k_para.replace('kv', 'conv3')].transpose((1, 0, 2, 3))\n        new_state_dict[k1] = np.concatenate([k[:, :, 0, 0], v[:, :, 0, 0]], -1)\n\n    elif 'decoder' in k and 'cross_attn' in k and 'kv' in k and 'bias' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        k_para = k_para.replace('cross_attn', 'multihead_attn')\n        k = params[k_para.replace('kv', 'conv2')]\n        v = params[k_para.replace('kv', 'conv3')]\n        new_state_dict[k1] = np.concatenate([k, v], -1)\n\n    elif 'decoder' in k and 'cross_attn' in k and 'out_proj' in k:\n\n        k_para = k[:13] + 'layers.' + k[13:]\n        k_para = k_para.replace('cross_attn', 'multihead_attn')\n        new_state_dict[k1] = params[k_para]\n    elif 'decoder' in k and 'norm' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        new_state_dict[k1] = params[k_para]\n    elif 'mlp' in k and 'weight' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        k_para = k_para.replace('fc', 'conv')\n        k_para = k_para.replace('mlp.', '')\n        w = params[k_para].transpose((1, 0, 2, 3))\n        new_state_dict[k1] = w[:, :, 0, 0]\n    elif 'mlp' in k and 'bias' in k:\n        k_para = k[:13] + 'layers.' + k[13:]\n        k_para = k_para.replace('fc', 'conv')\n        k_para = k_para.replace('mlp.', '')\n        w = params[k_para]\n        new_state_dict[k1] = w\n\n    else:\n        new_state_dict[k1] = params[k1]\n\n    if list(new_state_dict[k1].shape) != list(v1.shape):\n        print(k1)\n\n\nfor k, v1 in state_dict.items():\n    if k not in new_state_dict.keys():\n        print(1, k)\n    elif list(new_state_dict[k].shape) != list(v1.shape):\n        print(2, k)\n\n\n\nmodel.set_state_dict(new_state_dict)\npaddle.save(model.state_dict(), 'nrtrnew_from_old_params.pdparams')\n</code></pre> <li> <p>The new version has a clean code structure and improved inference speed compared with the old version.</p> </li>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_nrtr.html#citation","title":"Citation","text":"<pre><code>@article{Sheng2019NRTR,\n  title     = {NRTR: A No-Recurrence Sequence-to-Sequence Model For Scene Text Recognition},\n  author    = {Fenfen Sheng and Zhineng Chen and Bo Xu},\n  booktitle = {ICDAR},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1806.00926},\n  pages     = {781-786}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_parseq.html","title":"PasreQ","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_parseq.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Scene Text Recognition with Permuted Autoregressive Sequence Models Darwin Bautista, Rowel Atienza ECCV, 2021</p> <p>Using real datasets (real) and synthetic datasets (synth) for training respectively\uff0cand evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets.</p> <ul> <li>The real datasets include COCO-Text, RCTW17, Uber-Text, ArT, LSVT, MLT19, ReCTS, TextOCR and OpenVINO datasets.</li> <li>The synthesis datasets include MJSynth and SynthText datasets.</li> </ul> <p>the algorithm reproduction effect is as follows:</p> Training Dataset Model Backbone config Acc Download link Synth ParseQ VIT rec_vit_parseq.yml 91.24% train model Real ParseQ VIT rec_vit_parseq.yml 94.74% train model"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_parseq.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_parseq.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_parseq.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_vit_parseq.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_vit_parseq.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_parseq.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_vit_parseq.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_parseq.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_vit_parseq.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_parseq.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_parseq.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the SAR text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_vit_parseq.yml -o Global.pretrained_model=./rec_vit_parseq_real/best_accuracy Global.save_inference_dir=./inference/rec_parseq\n</code></pre> <p>For SAR text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_parseq/\" --rec_image_shape=\"3, 32, 128\" --rec_algorithm=\"ParseQ\" --rec_char_dict_path=\"ppocr/utils/dict/parseq_dict.txt\" --max_text_length=25 --use_space_char=False\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_parseq.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_parseq.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_parseq.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_parseq.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_parseq.html#citation","title":"Citation","text":"<pre><code>@InProceedings{bautista2022parseq,\n  title={Scene Text Recognition with Permuted Autoregressive Sequence Models},\n  author={Bautista, Darwin and Atienza, Rowel},\n  booktitle={European Conference on Computer Vision},\n  pages={178--196},\n  month={10},\n  year={2022},\n  publisher={Springer Nature Switzerland},\n  address={Cham},\n  doi={10.1007/978-3-031-19815-1_11},\n  url={https://doi.org/10.1007/978-3-031-19815-1_11}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rare.html","title":"RARE","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rare.html#1-introduction","title":"1. Introduction","text":"<p>Paper information:</p> <p>Robust Scene Text Recognition with Automatic Rectification Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao, Xiang Bai\u2217 CVPR, 2016</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Models Backbone Networks Configuration Files Avg Accuracy Download Links RARE Resnet34_vd configs/rec/rec_r34_vd_tps_bilstm_att.yml 83.60% training model RARE MobileNetV3 configs/rec/rec_mv3_tps_bilstm_att.yml 82.50% trained model"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rare.html#2-environment","title":"2. Environment","text":"<p>Please refer to Operating Environment Preparation to configure the PaddleOCR operating environment, and refer to Project Cloneto clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rare.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Training Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file. Take the backbone network based on Resnet34_vd as an example:</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rare.html#31-training","title":"3.1 Training","text":"<pre><code>#  Single card training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r34_vd_tps_bilstm_att.yml\n# Multi-card training, specify the card number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/rec/rec_r34_vd_tps_bilstm_att.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rare.html#32-evaluation","title":"3.2 Evaluation","text":"<pre><code># GPU evaluation, Global.pretrained_model is the model to be evaluated\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r34_vd_tps_bilstm_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rare.html#33-prediction","title":"3.3 Prediction","text":"<pre><code>python3 tools/infer_rec.py -c configs/rec/rec_r34_vd_tps_bilstm_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rare.html#4-inference","title":"4. Inference","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rare.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved during the RARE text recognition training process into an inference model. Take the model trained on the MJSynth and SynthText text recognition datasets based on the Resnet34_vd backbone network as an example (Model download address ), which can be converted using the following command:</p> <p>```bash linenums=\"1\" python3 tools/export_model.py -c configs/rec/rec_r34_vd_tps_bilstm_att.yml -o Global.pretrained_model=./rec_r34_vd_tps_bilstm_att_v2.0_train/best_accuracy Global.save_inference_dir=./inference/rec_rare ````</p> <p>RARE text recognition model inference, you can execute the following commands:</p> <p>```bash linenums=\"1\" python3 tools/infer/predict_rec.py --image_dir=\"doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_rare/\" --rec_image_shape=\"3, 32, 100\" --rec_char_dict_path= \"./ppocr/utils/ic15_dict.txt\" ````</p> <p>The inference results are as follows:</p> <p></p> <p><code>text linenums=\"1\" Predicts of doc/imgs_words/en/word_1.png:('joint ', 0.9999969601631165)</code></p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rare.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not currently supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rare.html#43-serving","title":"4.3 Serving","text":"<p>Not currently supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rare.html#44-more","title":"4.4 More","text":"<p>The RARE model also supports the following inference deployment methods:</p> <ul> <li>Paddle2ONNX Inference: After preparing the inference model, refer to the paddle2onnx tutorial.</li> </ul>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rare.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rare.html#citation","title":"Citation","text":"<p><code>bibtex @inproceedings{2016Robust,   title={Robust Scene Text Recognition with Automatic Rectification},   author={ Shi, B. and Wang, X. and Lyu, P. and Cong, Y. and Xiang, B. },   booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   year={2016}, }</code></p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rfl.html","title":"RFL","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rfl.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Reciprocal Feature Learning via Explicit and Implicit Tasks in Scene Text Recognition Hui Jiang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Yi Niu, Wenqi Ren, Fei Wu, and Wenming Tan ICDAR, 2021</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link RFL-CNT ResNetRFL rec_resnet_rfl_visual.yml 93.40% \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b RFL-Att ResNetRFL rec_resnet_rfl_att.yml 88.63% \u200b\u8bad\u7ec3\u200b\u6a21\u578b"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rfl.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rfl.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rfl.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code>#step1:train the CNT branch\n# Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_resnet_rfl_visual.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_resnet_rfl_visual.yml\n\n#step2:joint training of CNT and Att branches\n# Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_resnet_rfl_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_resnet_rfl_att.yml  -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rfl.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_resnet_rfl_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rfl.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_resnet_rfl_att.yml -o Global.infer_img='./doc/imgs_words_en/word_10.png' Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rfl.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rfl.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the RFL text recognition training process is converted into an inference model. ( Model download link) ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_resnet_rfl_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy  Global.save_inference_dir=./inference/rec_resnet_rfl_att\n</code></pre> <p>Note:</p> <ul> <li>If you are training the model on your own dataset and have modified the dictionary file, please pay attention to modify the <code>character_dict_path</code> in the configuration file to the modified dictionary file.</li> <li>If you modified the input size during training, please modify the <code>infer_shape</code> corresponding to NRTR in the <code>tools/export_model.py</code> file.</li> </ul> <p>After the conversion is successful, there are three files in the directory:</p> <pre><code>/inference/rec_resnet_rfl_att/\n    \u251c\u2500\u2500 inference.pdiparams\n    \u251c\u2500\u2500 inference.pdiparams.info\n    \u2514\u2500\u2500 inference.pdmodel\n</code></pre> <p>For RFL text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./doc/imgs_words_en/word_10.png' --rec_model_dir='./inference/rec_resnet_rfl_att/' --rec_algorithm='RFL' --rec_image_shape='1,32,100'\n</code></pre> <p></p> <p>After executing the command, the prediction result (recognized text and score) of the image above is printed to the screen, an example is as follows: The result is as follows:</p> <pre><code>Predicts of ./doc/imgs_words_en/word_10.png:('pain', 0.9999927282333374)\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rfl.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rfl.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rfl.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rfl.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rfl.html#citation","title":"Citation","text":"<pre><code>@article{2021Reciprocal,\n  title     = {Reciprocal Feature Learning via Explicit and Implicit Tasks in Scene Text Recognition},\n  author    = {Jiang, H.  and  Xu, Y.  and  Cheng, Z.  and  Pu, S.  and  Niu, Y.  and  Ren, W.  and  Wu, F.  and  Tan, W. },\n  booktitle = {ICDAR},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.06229}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_robustscanner.html","title":"RobustScanner","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_robustscanner.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>RobustScanner: Dynamically Enhancing Positional Clues for Robust Text Recognition Xiaoyu Yue, Zhanghui Kuang, Chenhao Lin, Hongbin Sun, Wayne Zhang ECCV, 2020</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link RobustScanner ResNet31 rec_r31_robustscanner.yml 87.77% trained model <p>Note:In addition to using the two text recognition datasets MJSynth and SynthText, SynthAdd data (extraction code: 627x), and some real data are used in training, the specific data details can refer to the paper.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_robustscanner.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_robustscanner.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_robustscanner.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r31_robustscanner.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_r31_robustscanner.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_robustscanner.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r31_robustscanner.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_robustscanner.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r31_robustscanner.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_robustscanner.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_robustscanner.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the RobustScanner text recognition training process is converted into an inference model. you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r31_robustscanner.yml -o Global.pretrained_model={path/to/weights}/best_accuracy  Global.save_inference_dir=./inference/rec_r31_robustscanner\n</code></pre> <p>For RobustScanner text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_r31_robustscanner/\" --rec_image_shape=\"3, 48, 48, 160\" --rec_algorithm=\"RobustScanner\" --rec_char_dict_path=\"ppocr/utils/dict90.txt\" --use_space_char=False\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_robustscanner.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_robustscanner.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_robustscanner.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_robustscanner.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_robustscanner.html#citation","title":"Citation","text":"<pre><code>@article{2020RobustScanner,\n  title={RobustScanner: Dynamically Enhancing Positional Clues for Robust Text Recognition},\n  author={Xiaoyu Yue and Zhanghui Kuang and Chenhao Lin and Hongbin Sun and Wayne Zhang},\n  journal={ECCV2020},\n  year={2020},\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rosetta.html","title":"Rosetta","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rosetta.html#1-introduction","title":"1. Introduction","text":"<p>Paper information:</p> <p>Rosetta: Large Scale System for Text Detection and Recognition in Images Borisyuk F , Gordo A , V Sivakumar KDD, 2018</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Models Backbone Networks Configuration Files Avg Accuracy Download Links Rosetta Resnet34_vd configs/rec/rec_r34_vd_none_none_ctc.yml 79.11% training model Rosetta MobileNetV3 configs/rec/rec_mv3_none_none_ctc.yml 75.80% training model"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rosetta.html#2-environment","title":"2. Environment","text":"<p>Please refer to Operating Environment Preparation to configure the PaddleOCR operating environment, and refer to Project Cloneto clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rosetta.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Training Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file. Take the backbone network based on Resnet34_vd as an example:</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rosetta.html#31-training","title":"3.1 Training","text":"<pre><code># Single card training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r34_vd_none_none_ctc.yml\n# Multi-card training, specify the card number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/rec/rec_r34_vd_none_none_ctc.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rosetta.html#32-evaluation","title":"3.2 Evaluation","text":"<pre><code># GPU evaluation, Global.pretrained_model is the model to be evaluated\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r34_vd_none_none_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rosetta.html#33-prediction","title":"3.3 Prediction","text":"<pre><code>python3 tools/infer_rec.py -c configs/rec/rec_r34_vd_none_none_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rosetta.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rosetta.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved during the Rosetta text recognition training process into an inference model. Take the model trained on the MJSynth and SynthText text recognition datasets based on the Resnet34_vd backbone network as an example ( Model download address ), which can be converted using the following command:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r34_vd_none_none_ctc.yml -o Global.pretrained_model=./rec_r34_vd_none_none_ctc_v2.0_train/best_accuracy Global.save_inference_dir=./inference/rec_rosetta\n</code></pre> <p>Rosetta text recognition model inference, you can execute the following commands:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_rosetta/\" --rec_image_shape=\"3, 32, 100\" --rec_char_dict_path= \"./ppocr/utils/ic15_dict.txt\"\n</code></pre> <p>The inference results are as follows:</p> <p></p> <pre><code>Predicts of doc/imgs_words/en/word_1.png:('joint', 0.9999982714653015)\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rosetta.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not currently supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rosetta.html#43-serving","title":"4.3 Serving","text":"<p>Not currently supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rosetta.html#44-more","title":"4.4 More","text":"<p>The Rosetta model also supports the following inference deployment methods:</p> <ul> <li>Paddle2ONNX Inference: After preparing the inference model, refer to the paddle2onnx tutorial.</li> </ul>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rosetta.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_rosetta.html#citation","title":"Citation","text":"<pre><code>@inproceedings{2018Rosetta,\n  title={Rosetta: Large Scale System for Text Detection and Recognition in Images},\n  author={ Borisyuk, Fedor and Gordo, Albert and Sivakumar, Viswanath },\n  booktitle={the 24th ACM SIGKDD International Conference},\n  year={2018},\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_sar.html","title":"SAR","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_sar.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Show, Attend and Read: A Simple and Strong Baseline for Irregular Text Recognition Hui Li, Peng Wang, Chunhua Shen, Guyu Zhang AAAI, 2019</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link SAR ResNet31 rec_r31_sar.yml 87.20% train model <p>Note:In addition to using the two text recognition datasets MJSynth and SynthText, SynthAdd data (extraction code: 627x), and some real data are used in training, the specific data details can refer to the paper.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_sar.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_sar.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_sar.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r31_sar.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_r31_sar.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_sar.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r31_sar.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_sar.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r31_sar.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_sar.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_sar.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the SAR text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r31_sar.yml -o Global.pretrained_model=./rec_r31_sar_train/best_accuracy  Global.save_inference_dir=./inference/rec_sar\n</code></pre> <p>For SAR text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_sar/\" --rec_image_shape=\"3, 48, 48, 160\" --rec_algorithm=\"SAR\" --rec_char_dict_path=\"ppocr/utils/dict90.txt\" --max_text_length=30 --use_space_char=False\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_sar.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_sar.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_sar.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_sar.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_sar.html#citation","title":"Citation","text":"<pre><code>@article{Li2019ShowAA,\n  title={Show, Attend and Read: A Simple and Strong Baseline for Irregular Text Recognition},\n  author={Hui Li and Peng Wang and Chunhua Shen and Guyu Zhang},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1811.00751}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_satrn.html","title":"SATRN","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_satrn.html#1-introduction","title":"1. Introduction","text":"<p>\u200b\u8bba\u6587\u200b\u4fe1\u606f\u200b\uff1a</p> <p>On Recognizing Texts of Arbitrary Shapes with 2D Self-Attention Junyeop Lee, Sungrae Park, Jeonghun Baek, Seong Joon Oh, Seonghyeon Kim, Hwalsuk Lee CVPR, 2020 Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link SATRN ShallowCNN 88.05% configs/rec/rec_satrn.yml \u200b\u8bad\u7ec3\u200b\u6a21\u578b"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_satrn.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_satrn.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_satrn.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_satrn.yml\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_satrn.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_satrn.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_satrn.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_satrn.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_satrn.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_satrn.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_satrn.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the SATRN text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_satrn.yml -o Global.pretrained_model=./rec_satrn_train/best_accuracy  Global.save_inference_dir=./inference/rec_satrn\n</code></pre> <p>For SATRN text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_satrn/\" --rec_image_shape=\"3, 48, 48, 160\" --rec_algorithm=\"SATRN\" --rec_char_dict_path=\"ppocr/utils/dict90.txt\" --max_text_length=30 --use_space_char=False\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_satrn.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_satrn.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_satrn.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_satrn.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_satrn.html#citation","title":"Citation","text":"<pre><code>@article{lee2019recognizing,\n      title={On Recognizing Texts of Arbitrary Shapes with 2D Self-Attention},\n      author={Junyeop Lee and Sungrae Park and Jeonghun Baek and Seong Joon Oh and Seonghyeon Kim and Hwalsuk Lee},\n      year={2019},\n      eprint={1910.04396},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_seed.html","title":"SEED","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_seed.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition</p> <p>Qiao, Zhi and Zhou, Yu and Yang, Dongbao and Zhou, Yucan and Wang, Weiping</p> <p>CVPR, 2020</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone ACC config Download link SEED Aster_Resnet 85.20% configs/rec/rec_resnet_stn_bilstm_att.yml \u200b\u8bad\u7ec3\u200b\u6a21\u578b"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_seed.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_seed.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_seed.html#training","title":"Training","text":"<p>The SEED model needs to additionally load the language model trained by FastText, and install the fasttext dependencies:</p> <pre><code>python3 -m pip install fasttext==0.9.1\n</code></pre> <p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_resnet_stn_bilstm_att.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c rec_resnet_stn_bilstm_att.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_seed.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_resnet_stn_bilstm_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_seed.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_resnet_stn_bilstm_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_seed.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_seed.html#41-python-inference","title":"4.1 Python Inference","text":"<p>Not support</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_seed.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not support</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_seed.html#43-serving","title":"4.3 Serving","text":"<p>Not support</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_seed.html#44-more","title":"4.4 More","text":"<p>Not support</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_seed.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_seed.html#citation","title":"Citation","text":"<pre><code>@inproceedings{qiao2020seed,\n  title={Seed: Semantics enhanced encoder-decoder framework for scene text recognition},\n  author={Qiao, Zhi and Zhou, Yu and Yang, Dongbao and Zhou, Yucan and Wang, Weiping},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={13528--13537},\n  year={2020}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_spin.html","title":"SPIN: Structure-Preserving Inner Offset Network for Scene Text Recognition","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_spin.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>SPIN: Structure-Preserving Inner Offset Network for Scene Text Recognition Chengwei Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Yi Niu, Fei Wu, Futai Zou AAAI, 2020</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets. The algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link SPIN ResNet32 rec_r32_gaspin_bilstm_att.yml 90.00% trained model"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_spin.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_spin.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_spin.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r32_gaspin_bilstm_att.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_r32_gaspin_bilstm_att.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_spin.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r32_gaspin_bilstm_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_spin.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r32_gaspin_bilstm_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_spin.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_spin.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the SPIN text recognition training process is converted into an inference model. you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r32_gaspin_bilstm_att.yml -o Global.pretrained_model={path/to/weights}/best_accuracy  Global.save_inference_dir=./inference/rec_r32_gaspin_bilstm_att\n</code></pre> <p>For SPIN text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_r32_gaspin_bilstm_att/\" --rec_image_shape=\"3, 32, 100\" --rec_algorithm=\"SPIN\" --rec_char_dict_path=\"/ppocr/utils/dict/spin_dict.txt\" --use_space_char=False\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_spin.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_spin.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_spin.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_spin.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_spin.html#citation","title":"Citation","text":"<pre><code>@article{2020SPIN,\n  title={SPIN: Structure-Preserving Inner Offset Network for Scene Text Recognition},\n  author={Chengwei Zhang and Yunlu Xu and Zhanzhan Cheng and Shiliang Pu and Yi Niu and Fei Wu and Futai Zou},\n  journal={AAAI2020},\n  year={2020},\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_srn.html","title":"SRN","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_srn.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Towards Accurate Scene Text Recognition with Semantic Reasoning Networks Deli Yu, Xuan Li, Chengquan Zhang, Junyu Han, Jingtuo Liu, Errui Ding CVPR,2020</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link SRN Resnet50_vd_fpn rec_r50_fpn_srn.yml 86.31% train model"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_srn.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_srn.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_srn.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r50_fpn_srn.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_r50_fpn_srn.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_srn.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r50_fpn_srn.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_srn.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r50_fpn_srn.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_srn.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_srn.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the SRN text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r50_fpn_srn.yml -o Global.pretrained_model=./rec_r50_vd_srn_train/best_accuracy  Global.save_inference_dir=./inference/rec_srn\n</code></pre> <p>For SRN text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./inference/rec_srn/\" --rec_image_shape=\"1,64,256\" --rec_char_type=\"ch\" --rec_algorithm=\"SRN\" --rec_char_dict_path=\"ppocr/utils/ic15_dict.txt\" --use_space_char=False\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_srn.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_srn.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_srn.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_srn.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_srn.html#citation","title":"Citation","text":"<pre><code>@article{Yu2020TowardsAS,\n  title={Towards Accurate Scene Text Recognition With Semantic Reasoning Networks},\n  author={Deli Yu and Xuan Li and Chengquan Zhang and Junyu Han and Jingtuo Liu and Errui Ding},\n  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2020},\n  pages={12110-12119}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_starnet.html","title":"STAR-Net","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_starnet.html#1-introduction","title":"1. Introduction","text":"<p>Paper information:</p> <p>STAR-Net: a spatial attention residue network for scene text recognition. Wei Liu, Chaofeng Chen, Kwan-Yee K. Wong, Zhizhong Su and Junyu Han. BMVC, pages 43.1-43.13, 2016</p> <p>Refer to DTRB text Recognition Training and Evaluation Process . Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Models Backbone Networks Avg Accuracy Configuration Files Download Links StarNet Resnet34_vd 84.44% configs/rec/rec_r34_vd_tps_bilstm_ctc.yml trained model StarNet MobileNetV3 81.42% configs/rec/rec_mv3_tps_bilstm_ctc.yml trained model"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_starnet.html#2-environment","title":"2. Environment","text":"<p>Please refer to Operating Environment Preparation to configure the PaddleOCR operating environment, and refer to Project Cloneto clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_starnet.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Training Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file. Take the backbone network based on Resnet34_vd as an example:</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_starnet.html#31-training","title":"3.1 Training","text":"<p>After the data preparation is complete, the training can be started. The training command is as follows:</p> <pre><code>#  Single card training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml # Multi-card training, specify the card number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c rec_r34_vd_tps_bilstm_ctc.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_starnet.html#32-evaluation","title":"3.2 Evaluation","text":"<pre><code># GPU evaluation, Global.pretrained_model is the model to be evaluated\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_starnet.html#33-prediction","title":"3.3 Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_starnet.html#4-inference","title":"4. Inference","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_starnet.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, convert the model saved during the STAR-Net text recognition training process into an inference model. Take the model trained on the MJSynth and SynthText text recognition datasets based on the Resnet34_vd backbone network as an example Model download address , which can be converted using the following command:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r34_vd_tps_bilstm_ctc.yml -o Global.pretrained_model=./rec_r34_vd_tps_bilstm_ctc_v2.0_train/best_accuracy  Global.save_inference_dir=./inference/rec_starnet\n</code></pre> <p>STAR-Net text recognition model inference, you can execute the following commands:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words_en/word_336.png\" --rec_model_dir=\"./inference/rec_starnet/\" --rec_image_shape=\"3, 32, 100\" --rec_char_dict_path=\"./ppocr/utils/ic15_dict.txt\"\n</code></pre> <p></p> <p>The inference results are as follows:</p> <pre><code>Predicts of ./doc/imgs_words_en/word_336.png:('super', 0.9999073)\n</code></pre> <p>Attention Since the above model refers to the DTRB text recognition training and evaluation process, it is different from the ultra-lightweight Chinese recognition model training in two aspects:</p> <ul> <li> <p>The image resolutions used during training are different. The image resolutions used for training the above models are [3, 32, 100], while for Chinese model training, in order to ensure the recognition effect of long texts, the image resolutions used during training are [ 3, 32, 320]. The default shape parameter of the predictive inference program is the image resolution used for training Chinese, i.e. [3, 32, 320]. Therefore, when inferring the above English model here, it is necessary to set the shape of the recognized image through the parameter rec_image_shape.</p> </li> <li> <p>Character list, the experiment in the DTRB paper is only for 26 lowercase English letters and 10 numbers, a total of 36 characters. All uppercase and lowercase characters are converted to lowercase characters, and characters not listed above are ignored and considered spaces. Therefore, there is no input character dictionary here, but a dictionary is generated by the following command. Therefore, the parameter rec_char_dict_path needs to be set during inference, which is specified as an English dictionary \"./ppocr/utils/ic15_dict.txt\".</p> </li> </ul> <pre><code>self.character_str = \"0123456789abcdefghijklmnopqrstuvwxyz\"\ndict_character = list(self.character_str)\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_starnet.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>After preparing the inference model, refer to the cpp infer tutorial to operate.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_starnet.html#43-serving","title":"4.3 Serving","text":"<p>After preparing the inference model, refer to the pdserving tutorial for Serving deployment, including two modes: Python Serving and C++ Serving.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_starnet.html#44-more","title":"4.4 More","text":"<p>The STAR-Net model also supports the following inference deployment methods:</p> <ul> <li>Paddle2ONNX Inference: After preparing the inference model, refer to the paddle2onnx tutorial.</li> </ul>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_starnet.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_starnet.html#citation","title":"Citation","text":"<pre><code>@inproceedings{liu2016star,\n  title={STAR-Net: a spatial attention residue network for scene text recognition.},\n  author={Liu, Wei and Chen, Chaofeng and Wong, Kwan-Yee K and Su, Zhizhong and Han, Junyu},\n  booktitle={BMVC},\n  volume={2},\n  pages={7},\n  year={2016}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtr.html","title":"SVTR","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtr.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>SVTR: Scene Text Recognition with a Single Visual Model Yongkun Du and Zhineng Chen and Caiyan Jia Xiaoting Yin and Tianlun Zheng and Chenxia Li and Yuning Du and Yu-Gang Jiang IJCAI, 2022</p> <p>The accuracy (%) and model files of SVTR on the public dataset of scene text recognition are as follows:</p> <ul> <li>Chinese dataset from Chinese Benchmark , and the Chinese training evaluation strategy of SVTR follows the paper.</li> </ul> Model IC13857 SVT IIIT5k3000 IC151811 SVTP CUTE80 Avg_6 IC152077 IC131015 IC03867 IC03860 Avg_10 Chinesescene_test Download link SVTR Tiny 96.85 91.34 94.53 83.99 85.43 89.24 90.87 80.55 95.37 95.27 95.70 90.13 67.90 English  / Chinese SVTR Small 95.92 93.04 95.03 84.70 87.91 92.01 91.63 82.72 94.88 96.08 96.28 91.02 69.00 English / Chinese SVTR Base 97.08 91.50 96.03 85.20 89.92 91.67 92.33 83.73 95.66 95.62 95.81 91.61 71.40 English  /                                              - SVTR Large 97.20 91.65 96.30 86.58 88.37 95.14 92.82 84.54 96.35 96.54 96.74 92.24 72.10 English / Chinese"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtr.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtr.html#dataset-preparation","title":"Dataset Preparation","text":"<p>English dataset download Chinese dataset download</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtr.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtr.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_svtrnet.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_svtrnet.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtr.html#evaluation","title":"Evaluation","text":"<p>You can download the model files and configuration files provided by <code>SVTR</code>: download link, take <code>SVTR-T</code> as an example, using the following command to evaluate:</p> <pre><code># Download the tar archive containing the model files and configuration files of SVTR-T and extract it\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/rec_svtr_tiny_none_ctc_en_train.tar &amp;&amp; tar xf rec_svtr_tiny_none_ctc_en_train.tar\n# GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c ./rec_svtr_tiny_none_ctc_en_train/rec_svtr_tiny_6local_6global_stn_en.yml -o Global.pretrained_model=./rec_svtr_tiny_none_ctc_en_train/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtr.html#prediction","title":"Prediction","text":"<pre><code>python3 tools/infer_rec.py -c ./rec_svtr_tiny_none_ctc_en_train/rec_svtr_tiny_6local_6global_stn_en.yml -o Global.infer_img='./doc/imgs_words_en/word_10.png' Global.pretrained_model=./rec_svtr_tiny_none_ctc_en_train/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtr.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtr.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the SVTR text recognition training process is converted into an inference model. ( Model download link ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_svtrnet.yml -o Global.pretrained_model=./rec_svtr_tiny_none_ctc_en_train/best_accuracy  Global.save_inference_dir=./inference/rec_svtr_tiny_stn_en\n</code></pre> <p>Note: If you are training the model on your own dataset and have modified the dictionary file, please pay attention to modify the <code>character_dict_path</code> in the configuration file to the modified dictionary file.</p> <p>After the conversion is successful, there are three files in the directory:</p> <pre><code>/inference/rec_svtr_tiny_stn_en/\n    \u251c\u2500\u2500 inference.pdiparams\n    \u251c\u2500\u2500 inference.pdiparams.info\n    \u2514\u2500\u2500 inference.pdmodel\n</code></pre> <p>For SVTR text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./doc/imgs_words_en/word_10.png' --rec_model_dir='./inference/rec_svtr_tiny_stn_en/' --rec_algorithm='SVTR' --rec_image_shape='3,64,256' --rec_char_dict_path='./ppocr/utils/ic15_dict.txt'\n</code></pre> <p></p> <p>After executing the command, the prediction result (recognized text and score) of the image above is printed to the screen, an example is as follows:</p> <pre><code>Predicts of ./doc/imgs_words_en/word_10.png:('pain', 0.9999998807907104)\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtr.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtr.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtr.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtr.html#5-faq","title":"5. FAQ","text":"<ul> <li> <ol> <li>Speed situation on CPU and GPU</li> </ol> </li> <li>Since most of the operators used by <code>SVTR</code> are matrix multiplication, in the GPU environment, the speed has an advantage, but in the environment where mkldnn is enabled on the CPU, <code>SVTR</code> has no advantage over the optimized convolutional network.</li> <li> <ol> <li>SVTR model convert to ONNX failed</li> </ol> </li> <li>Ensure <code>paddle2onnx</code> and <code>onnxruntime</code> versions are up to date, refer to SVTR model to onnx step-by-step example for the convert onnx command. 1271214273).</li> <li> <ol> <li>SVTR model convert to ONNX is successful but the inference result is incorrect</li> </ol> </li> <li>The possible reason is that the model parameter <code>out_char_num</code> is not set correctly, it should be set to W//4, W//8 or W//12, please refer to Section 3.3.3 of SVTR, a high-precision Chinese scene text recognition model projectdetail/5073182?contributionType=1).</li> <li> <ol> <li>Optimization of long text recognition</li> </ol> </li> <li>Refer to Section 3.3 of SVTR, a high-precision Chinese scene text recognition model.</li> <li> <ol> <li>Notes on the reproduction of the paper results</li> </ol> </li> <li>Dataset using provided by ABINet.</li> <li>By default, 4 cards of GPUs are used for training, the default Batchsize of a single card is 512, and the total Batchsize is 2048, corresponding to a learning rate of 0.0005. When modifying the Batchsize or changing the number of GPU cards, the learning rate should be modified in equal proportion.</li> <li> <ol> <li>Exploration Directions for further optimization</li> </ol> </li> <li>Learning rate adjustment: adjusting to twice the default to keep Batchsize unchanged; or reducing Batchsize to 1/2 the default to keep the learning rate unchanged.</li> <li>Data augmentation strategies: optionally <code>RecConAug</code> and <code>RecAug</code>.</li> <li>If STN is not used, <code>Local</code> of <code>mixer</code> can be replaced by <code>Conv</code> and <code>local_mixer</code> can all be modified to <code>[5, 5]</code>.</li> <li>Grid search for optimal <code>embed_dim</code>, <code>depth</code>, <code>num_heads</code> configurations.</li> <li>Use the <code>Post-Normalization strategy</code>, which is to modify the model configuration <code>prenorm</code> to <code>True</code>.</li> </ul>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtr.html#citation","title":"Citation","text":"<pre><code>@article{Du2022SVTR,\n  title     = {SVTR: Scene Text Recognition with a Single Visual Model},\n  author    = {Du, Yongkun and Chen, Zhineng and Jia, Caiyan and Yin, Xiaoting and Zheng, Tianlun and Li, Chenxia and Du, Yuning and Jiang, Yu-Gang},\n  booktitle = {IJCAI},\n  year      = {2022},\n  url       = {https://arxiv.org/abs/2205.00159}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtrv2.html","title":"\u573a\u666f\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200b-SVTRv2","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtrv2.html#1","title":"1. \u200b\u7b97\u6cd5\u200b\u7b80\u4ecb","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtrv2.html#svtrv2","title":"SVTRv2\u200b\u7b97\u6cd5\u200b\u7b80\u4ecb","text":"<p>\ud83d\udd25 \u200b\u8be5\u200b\u7b97\u6cd5\u200b\u7531\u200b\u6765\u81ea\u200b\u590d\u65e6\u5927\u5b66\u200b\u89c6\u89c9\u200b\u4e0e\u200b\u5b66\u4e60\u200b\u5b9e\u9a8c\u5ba4\u200b(FVL)\u200b\u7684\u200bOpenOCR\u200b\u56e2\u961f\u200b\u7814\u53d1\u200b\uff0c\u200b\u5176\u200b\u5728\u200bPaddleOCR\u200b\u7b97\u6cd5\u200b\u6a21\u578b\u200b\u6311\u6218\u8d5b\u200b - \u200b\u8d5b\u9898\u200b\u4e00\u200b\uff1aOCR\u200b\u7aef\u5230\u200b\u7aef\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b\u4e2d\u200b\u8363\u83b7\u200b\u4e00\u7b49\u5956\u200b\uff0cB\u200b\u699c\u7aef\u200b\u5230\u200b\u7aef\u200b\u8bc6\u522b\u200b\u7cbe\u5ea6\u200b\u76f8\u6bd4\u200bPP-OCRv4\u200b\u63d0\u5347\u200b2.5%\uff0c\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u6301\u5e73\u200b\u3002\u200b\u4e3b\u8981\u200b\u601d\u8def\u200b\uff1a1\u3001\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200bBackbone\u200b\u5347\u7ea7\u200b\u4e3a\u200bRepSVTR\uff1b2\u3001\u200b\u8bc6\u522b\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u5347\u7ea7\u200b\u4e3a\u200bSVTRv2\uff0c\u200b\u53ef\u200b\u8bc6\u522b\u200b\u957f\u200b\u6587\u672c\u200b\u3002</p> \u200b\u6a21\u578b\u200b \u200b\u914d\u7f6e\u6587\u4ef6\u200b \u200b\u7aef\u5230\u200b\u7aef\u200b \u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b PP-OCRv4 A\u200b\u699c\u200b 62.77%  B\u200b\u699c\u200b 62.51% Model List SVTRv2(Rec Sever) configs/rec/SVTRv2/ch_SVTRv2_rec.yml A\u200b\u699c\u200b 68.81% (\u200b\u4f7f\u7528\u200bPP-OCRv4\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b) \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b / \u200b\u63a8\u7406\u6a21\u578b\u200b RepSVTR(Mobile) \u200b\u8bc6\u522b\u200b \u200b\u8bc6\u522b\u200b\u84b8\u998f\u200b \u200b\u68c0\u6d4b\u200b B\u200b\u699c\u200b 65.07% \u200b\u8bc6\u522b\u200b: \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b / \u200b\u63a8\u7406\u6a21\u578b\u200b  \u200b\u8bc6\u522b\u200b\u84b8\u998f\u200b: \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b / \u200b\u63a8\u7406\u6a21\u578b\u200b  \u200b\u68c0\u6d4b\u200b: \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b / \u200b\u63a8\u7406\u6a21\u578b\u200b <p>\ud83d\ude80 \u200b\u5feb\u901f\u200b\u4f7f\u7528\u200b\uff1a\u200b\u53c2\u8003\u200bPP-OCR\u200b\u63a8\u7406\u200b\u8bf4\u660e\u200b\u6587\u6863\u200b\uff0c\u200b\u5c06\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u66ff\u6362\u200b\u4e3a\u200b\u4e0a\u8868\u200b\u4e2d\u200b\u5bf9\u5e94\u200b\u7684\u200bRepSVTR\u200b\u6216\u200bSVTRv2\u200b\u63a8\u7406\u6a21\u578b\u200b\u5373\u53ef\u200b\u4f7f\u7528\u200b\u3002</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtrv2.html#2","title":"2. \u200b\u73af\u5883\u200b\u914d\u7f6e","text":"<p>\u200b\u8bf7\u200b\u5148\u200b\u53c2\u8003\u200b\u300a\u200b\u8fd0\u884c\u200b\u73af\u5883\u200b\u51c6\u5907\u200b\u300b\u200b\u914d\u7f6e\u200bPaddleOCR\u200b\u8fd0\u884c\u200b\u73af\u5883\u200b\uff0c\u200b\u53c2\u8003\u200b\u300a\u200b\u9879\u76ee\u200b\u514b\u9686\u200b\u300b\u200b\u514b\u9686\u200b\u9879\u76ee\u200b\u4ee3\u7801\u200b\u3002</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtrv2.html#3","title":"3. \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u3001\u200b\u8bc4\u4f30\u200b\u3001\u200b\u9884\u6d4b","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtrv2.html#31","title":"3.1 \u200b\u6a21\u578b\u200b\u8bad\u7ec3","text":"<p>\u200b\u8bad\u7ec3\u200b\u547d\u4ee4\u200b\uff1a</p> <pre><code>#\u200b\u5355\u5361\u200b\u8bad\u7ec3\u200b\uff08\u200b\u8bad\u7ec3\u200b\u5468\u671f\u957f\u200b\uff0c\u200b\u4e0d\u200b\u5efa\u8bae\u200b\uff09\npython3 tools/train.py -c configs/rec/SVTRv2/ch_RepSVTR_rec_gtc.yml\n\n# \u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u901a\u8fc7\u200b--gpus\u200b\u53c2\u6570\u200b\u6307\u5b9a\u200b\u5361\u53f7\u200b\n# Rec \u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\npython -m paddle.distributed.launch --gpus '0,1,2,3,4,5,6,7'  tools/train.py -c configs/rec/SVTRv2/ch_RepSVTR_rec_gtc.yml\n# Rec \u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\npython -m paddle.distributed.launch --gpus '0,1,2,3,4,5,6,7'  tools/train.py -c configs/rec/SVTRv2/ch_SVTRv2_rec_gtc.yml\n# Rec \u200b\u84b8\u998f\u200b\u8bad\u7ec3\u200b\npython -m paddle.distributed.launch --gpus '0,1,2,3,4,5,6,7'  tools/train.py -c configs/rec/SVTRv2/ch_SVTRv2_rec_gtc_distill.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtrv2.html#32","title":"3.2 \u200b\u8bc4\u4f30","text":"<pre><code># \u200b\u6ce8\u610f\u200b\u5c06\u200bpretrained_model\u200b\u7684\u200b\u8def\u5f84\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u672c\u5730\u200b\u8def\u5f84\u200b\u3002\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/SVTRv2/ch_RepSVTR_rec_gtc.yml -o Global.pretrained_model=output/ch_RepSVTR_rec_gtc/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtrv2.html#33","title":"3.3 \u200b\u9884\u6d4b","text":"<p>\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u5355\u5f20\u200b\u56fe\u7247\u200b\u9884\u6d4b\u200b\uff1a</p> <pre><code># \u200b\u6ce8\u610f\u200b\u5c06\u200bpretrained_model\u200b\u7684\u200b\u8def\u5f84\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u672c\u5730\u200b\u8def\u5f84\u200b\u3002\npython3 tools/infer_rec.py -c tools/eval.py -c configs/rec/SVTRv2/ch_RepSVTR_rec_gtc.yml -o Global.pretrained_model=output/ch_RepSVTR_rec_gtc/best_accuracy Global.infer_img='./doc/imgs_words_en/word_10.png'\n# \u200b\u9884\u6d4b\u200b\u6587\u4ef6\u5939\u200b\u4e0b\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\u65f6\u200b\uff0c\u200b\u53ef\u200b\u4fee\u6539\u200binfer_img\u200b\u4e3a\u200b\u6587\u4ef6\u5939\u200b\uff0c\u200b\u5982\u200b Global.infer_img='./doc/imgs_words_en/'\u3002\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtrv2.html#4","title":"4. \u200b\u63a8\u7406\u200b\u90e8\u7f72","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtrv2.html#41-python","title":"4.1 Python\u200b\u63a8\u7406","text":"<p>\u200b\u9996\u5148\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200bbest\u200b\u6a21\u578b\u200b\uff0c\u200b\u8f6c\u6362\u6210\u200binference model\uff0c\u200b\u4ee5\u200bRepSVTR\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u8f6c\u6362\u200b\uff1a</p> <pre><code># \u200b\u6ce8\u610f\u200b\u5c06\u200bpretrained_model\u200b\u7684\u200b\u8def\u5f84\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u672c\u5730\u200b\u8def\u5f84\u200b\u3002\npython3 tools/export_model.py -c configs/rec/SVTRv2/ch_RepSVTR_rec_gtc.yml -o Global.pretrained_model=output/ch_RepSVTR_rec_gtc/best_accuracy Global.save_inference_dir=./inference/rec_repsvtr_infer\n</code></pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u60a8\u200b\u662f\u200b\u5728\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u4e14\u200b\u8c03\u6574\u200b\u4e86\u200b\u5b57\u5178\u200b\u6587\u4ef6\u200b\uff0c\u200b\u8bf7\u200b\u6ce8\u610f\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b<code>character_dict_path</code>\u200b\u662f\u5426\u200b\u4e3a\u200b\u6240\u200b\u6b63\u786e\u200b\u7684\u200b\u5b57\u5178\u200b\u6587\u4ef6\u200b\u3002</p> <p>\u200b\u8f6c\u6362\u200b\u6210\u529f\u200b\u540e\u200b\uff0c\u200b\u5728\u200b\u76ee\u5f55\u200b\u4e0b\u200b\u6709\u200b\u4e09\u4e2a\u200b\u6587\u4ef6\u200b\uff1a</p> <pre><code>./inference/rec_repsvtr_infer/\n    \u251c\u2500\u2500 inference.pdiparams         # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\u6587\u4ef6\u200b\n    \u251c\u2500\u2500 inference.pdiparams.info    # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u4fe1\u606f\u200b\uff0c\u200b\u53ef\u200b\u5ffd\u7565\u200b\n    \u2514\u2500\u2500 inference.pdmodel           # \u200b\u8bc6\u522b\u200binference\u200b\u6a21\u578b\u200b\u7684\u200bprogram\u200b\u6587\u4ef6\u200b\n</code></pre> <p>\u200b\u6267\u884c\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b\uff1a</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./doc/imgs_words_en/word_10.png' --rec_model_dir='./inference/rec_repsvtr_infer/'\n# \u200b\u9884\u6d4b\u200b\u6587\u4ef6\u5939\u200b\u4e0b\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\u65f6\u200b\uff0c\u200b\u53ef\u200b\u4fee\u6539\u200bimage_dir\u200b\u4e3a\u200b\u6587\u4ef6\u5939\u200b\uff0c\u200b\u5982\u200b --image_dir='./doc/imgs_words_en/'\u3002\n</code></pre> <p></p> <p>\u200b\u6267\u884c\u547d\u4ee4\u200b\u540e\u200b\uff0c\u200b\u4e0a\u9762\u200b\u56fe\u50cf\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff08\u200b\u8bc6\u522b\u200b\u7684\u200b\u6587\u672c\u200b\u548c\u200b\u5f97\u5206\u200b\uff09\u200b\u4f1a\u200b\u6253\u5370\u200b\u5230\u200b\u5c4f\u5e55\u200b\u4e0a\u200b\uff0c\u200b\u793a\u4f8b\u200b\u5982\u4e0b\u200b\uff1a \u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>Predicts of ./doc/imgs_words_en/word_10.png:('pain', 0.9999998807907104)\n</code></pre> <p>\u200b\u6ce8\u610f\u200b\uff1a</p> <ul> <li>\u200b\u5982\u679c\u200b\u60a8\u200b\u8c03\u6574\u200b\u4e86\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u7684\u200b\u8f93\u5165\u200b\u5206\u8fa8\u7387\u200b\uff0c\u200b\u9700\u8981\u200b\u901a\u8fc7\u200b\u53c2\u6570\u200b<code>rec_image_shape</code>\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u60a8\u200b\u9700\u8981\u200b\u7684\u200b\u8bc6\u522b\u200b\u56fe\u50cf\u200b\u5f62\u72b6\u200b\u3002</li> <li>\u200b\u5728\u200b\u63a8\u7406\u200b\u65f6\u200b\u9700\u8981\u200b\u8bbe\u7f6e\u200b\u53c2\u6570\u200b<code>rec_char_dict_path</code>\u200b\u6307\u5b9a\u200b\u5b57\u5178\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u4fee\u6539\u200b\u4e86\u200b\u5b57\u5178\u200b\uff0c\u200b\u8bf7\u200b\u4fee\u6539\u200b\u8be5\u200b\u53c2\u6570\u200b\u4e3a\u200b\u60a8\u200b\u7684\u200b\u5b57\u5178\u200b\u6587\u4ef6\u200b\u3002</li> <li>\u200b\u5982\u679c\u200b\u60a8\u200b\u4fee\u6539\u200b\u4e86\u200b\u9884\u5904\u7406\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u9700\u200b\u4fee\u6539\u200b<code>tools/infer/predict_rec.py</code>\u200b\u4e2d\u200bSVTR\u200b\u7684\u200b\u9884\u5904\u7406\u200b\u4e3a\u200b\u60a8\u200b\u7684\u200b\u9884\u5904\u7406\u200b\u65b9\u6cd5\u200b\u3002</li> </ul>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtrv2.html#42-c","title":"4.2 C++\u200b\u63a8\u7406\u200b\u90e8\u7f72","text":"<p>\u200b\u51c6\u5907\u200b\u597d\u200b\u63a8\u7406\u6a21\u578b\u200b\u540e\u200b\uff0c\u200b\u53c2\u8003\u200bcpp infer\u200b\u6559\u7a0b\u200b\u8fdb\u884c\u200b\u64cd\u4f5c\u200b\u5373\u53ef\u200b\u3002</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtrv2.html#43-serving","title":"4.3 Serving\u200b\u670d\u52a1\u5316\u200b\u90e8\u7f72","text":"<p>\u200b\u6682\u200b\u4e0d\u200b\u652f\u6301\u200b</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtrv2.html#44","title":"4.4 \u200b\u66f4\u200b\u591a\u200b\u63a8\u7406\u200b\u90e8\u7f72","text":"<ul> <li>Paddle2ONNX\u200b\u63a8\u7406\u200b\uff1a\u200b\u51c6\u5907\u200b\u597d\u200b\u63a8\u7406\u6a21\u578b\u200b\u540e\u200b\uff0c\u200b\u53c2\u8003\u200bpaddle2onnx\u200b\u6559\u7a0b\u200b\u64cd\u4f5c\u200b\u3002</li> </ul>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtrv2.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_svtrv2.html#_1","title":"\u5f15\u7528","text":"<pre><code>@article{Du2022SVTR,\n  title     = {SVTR: Scene Text Recognition with a Single Visual Model},\n  author    = {Du, Yongkun and Chen, Zhineng and Jia, Caiyan and Yin, Xiaoting and Zheng, Tianlun and Li, Chenxia and Du, Yuning and Jiang, Yu-Gang},\n  booktitle = {IJCAI},\n  year      = {2022},\n  url       = {https://arxiv.org/abs/2205.00159}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_visionlan.html","title":"VisionLAN","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_visionlan.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network Yuxin Wang, Hongtao Xie, Shancheng Fang, Jing Wang, Shenggao Zhu, Yongdong Zhang ICCV, 2021</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link VisionLAN ResNet45 rec_r45_visionlan.yml 90.30% model link"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_visionlan.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_visionlan.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_visionlan.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_r45_visionlan.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_r45_visionlan.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_visionlan.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 tools/eval.py -c configs/rec/rec_r45_visionlan.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_visionlan.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_r45_visionlan.yml -o Global.infer_img='./doc/imgs_words/en/word_2.png' Global.pretrained_model=./rec_r45_visionlan_train/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_visionlan.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_visionlan.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the VisionLAN text recognition training process is converted into an inference model. ( Model download link) ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_r45_visionlan.yml -o Global.pretrained_model=./rec_r45_visionlan_train/best_accuracy Global.save_inference_dir=./inference/rec_r45_visionlan/\n</code></pre> <p>Note:</p> <ul> <li>If you are training the model on your own dataset and have modified the dictionary file, please pay attention to modify the <code>character_dict_path</code> in the configuration file to the modified dictionary file.</li> <li>If you modified the input size during training, please modify the <code>infer_shape</code> corresponding to VisionLAN in the <code>tools/export_model.py</code> file.</li> </ul> <p>After the conversion is successful, there are three files in the directory:</p> <pre><code>./inference/rec_r45_visionlan/\n    \u251c\u2500\u2500 inference.pdiparams\n    \u251c\u2500\u2500 inference.pdiparams.info\n    \u2514\u2500\u2500 inference.pdmodel\n</code></pre> <p>For VisionLAN text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./doc/imgs_words/en/word_2.png' --rec_model_dir='./inference/rec_r45_visionlan/' --rec_algorithm='VisionLAN' --rec_image_shape='3,64,256' --rec_char_dict_path='./ppocr/utils/ic15_dict.txt' --use_space_char=False\n</code></pre> <p></p> <p>After executing the command, the prediction result (recognized text and score) of the image above is printed to the screen, an example is as follows: The result is as follows:</p> <pre><code>Predicts of ./doc/imgs_words/en/word_2.png:('yourself', 0.9999493)\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_visionlan.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_visionlan.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_visionlan.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_visionlan.html#5-faq","title":"5. FAQ","text":"<ol> <li>Note that the MJSynth and SynthText datasets come from VisionLAN repo.</li> <li>We use the pre-trained model provided by the VisionLAN authors for finetune training. The dictionary for the pre-trained model is 'ppocr/utils/ic15_dict.txt'.</li> </ol>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_visionlan.html#citation","title":"Citation","text":"<pre><code>@inproceedings{wang2021two,\n  title={From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network},\n  author={Wang, Yuxin and Xie, Hongtao and Fang, Shancheng and Wang, Jing and Zhu, Shenggao and Zhang, Yongdong},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={14194--14203},\n  year={2021}\n}\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_vitstr.html","title":"ViTSTR","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_vitstr.html#1-introduction","title":"1. Introduction","text":"<p>Paper:</p> <p>Vision Transformer for Fast and Efficient Scene Text Recognition Rowel Atienza ICDAR, 2021</p> <p>Using MJSynth and SynthText two text recognition datasets for training, and evaluating on IIIT, SVT, IC03, IC13, IC15, SVTP, CUTE datasets, the algorithm reproduction effect is as follows:</p> Model Backbone config Acc Download link ViTSTR ViTSTR rec_vitstr_none_ce.yml 79.82% trained model"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_vitstr.html#2-environment","title":"2. Environment","text":"<p>Please refer to \"Environment Preparation\" to configure the PaddleOCR environment, and refer to \"Project Clone\"to clone the project code.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_vitstr.html#3-model-training-evaluation-prediction","title":"3. Model Training / Evaluation / Prediction","text":"<p>Please refer to Text Recognition Tutorial. PaddleOCR modularizes the code, and training different recognition models only requires changing the configuration file.</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_vitstr.html#training","title":"Training","text":"<p>Specifically, after the data preparation is completed, the training can be started. The training command is as follows:</p> <pre><code># Single GPU training (long training period, not recommended)\npython3 tools/train.py -c configs/rec/rec_vitstr_none_ce.yml\n\n# Multi GPU training, specify the gpu number through the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/rec_vitstr_none_ce.yml\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_vitstr.html#evaluation","title":"Evaluation","text":"<pre><code># GPU evaluation\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/rec_vitstr_none_ce.yml -o Global.pretrained_model={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_vitstr.html#prediction","title":"Prediction","text":"<pre><code># The configuration file used for prediction must match the training\npython3 tools/infer_rec.py -c configs/rec/rec_vitstr_none_ce.yml -o Global.infer_img='./doc/imgs_words_en/word_10.png' Global.pretrained_model=./rec_vitstr_none_ce_train/best_accuracy\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_vitstr.html#4-inference-and-deployment","title":"4. Inference and Deployment","text":""},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_vitstr.html#41-python-inference","title":"4.1 Python Inference","text":"<p>First, the model saved during the ViTSTR text recognition training process is converted into an inference model. ( Model download link) ), you can use the following command to convert:</p> <pre><code>python3 tools/export_model.py -c configs/rec/rec_vitstr_none_ce.yml -o Global.pretrained_model=./rec_vitstr_none_ce_train/best_accuracy  Global.save_inference_dir=./inference/rec_vitstr\n</code></pre> <p>Note:</p> <ul> <li>If you are training the model on your own dataset and have modified the dictionary file, please pay attention to modify the <code>character_dict_path</code> in the configuration file to the modified dictionary file.</li> <li>If you modified the input size during training, please modify the <code>infer_shape</code> corresponding to ViTSTR in the <code>tools/export_model.py</code> file.</li> </ul> <p>After the conversion is successful, there are three files in the directory:</p> <pre><code>/inference/rec_vitstr/\n    \u251c\u2500\u2500 inference.pdiparams\n    \u251c\u2500\u2500 inference.pdiparams.info\n    \u2514\u2500\u2500 inference.pdmodel\n</code></pre> <p>For ViTSTR text recognition model inference, the following commands can be executed:</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir='./doc/imgs_words_en/word_10.png' --rec_model_dir='./inference/rec_vitstr/' --rec_algorithm='ViTSTR' --rec_image_shape='1,224,224' --rec_char_dict_path='./ppocr/utils/EN_symbol_dict.txt'\n</code></pre> <p></p> <p>After executing the command, the prediction result (recognized text and score) of the image above is printed to the screen, an example is as follows: The result is as follows:</p> <pre><code>Predicts of ./doc/imgs_words_en/word_10.png:('pain', 0.9998350143432617)\n</code></pre>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_vitstr.html#42-c-inference","title":"4.2 C++ Inference","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_vitstr.html#43-serving","title":"4.3 Serving","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_vitstr.html#44-more","title":"4.4 More","text":"<p>Not supported</p>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_vitstr.html#5-faq","title":"5. FAQ","text":"<ol> <li>In the <code>ViTSTR</code> paper, using pre-trained weights on ImageNet1k for initial training, we did not use pre-trained weights in training, and the final accuracy did not change or even improved.</li> </ol>"},{"location":"en/version2.x/algorithm/text_recognition/algorithm_rec_vitstr.html#citation","title":"Citation","text":"<pre><code>@article{Atienza2021ViTSTR,\n  title     = {Vision Transformer for Fast and Efficient Scene Text Recognition},\n  author    = {Rowel Atienza},\n  booktitle = {ICDAR},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.08582}\n}\n</code></pre>"},{"location":"en/version2.x/legacy/index.html","title":"Legacy Features","text":""},{"location":"en/version2.x/legacy/index.html#overview","title":"Overview","text":"<p>This section introduces the features and models related to the PaddleOCR 2.x branch. Due to the upgrades in the 3.x branch, some models and features are no longer compatible with the older branch. Therefore, users who need to use or refer to the features of the older branch can refer to this part of the documentation.</p>"},{"location":"en/version2.x/legacy/index.html#models-supported-by-paddleocr-2x-branch","title":"Models Supported by PaddleOCR 2.x Branch:","text":"<ul> <li>Model List</li> </ul>"},{"location":"en/version2.x/legacy/index.html#features-supported-by-paddleocr-2x-branch","title":"Features Supported by PaddleOCR 2.x Branch:","text":"<ul> <li>Inference with Python Prediction Engine</li> <li>Inference with C++ Prediction Engine</li> <li>Compilation Guide for Visual Studio 2019 Community CMake</li> <li>Service-Oriented Deployment</li> <li>Android Deployment</li> <li>Jetson Deployment</li> <li>Edge Device Deployment</li> <li>Web Frontend Deployment</li> <li>Paddle2ONNX Model Conversion and Prediction</li> <li>PaddlePaddle Cloud Deployment Tool</li> <li>Benchmark</li> </ul>"},{"location":"en/version2.x/legacy/android_demo.html","title":"Android Demo","text":""},{"location":"en/version2.x/legacy/android_demo.html#1-introduction","title":"1. Introduction","text":"<p>This document describes the Android Demo for PaddleOCR. The demo currently supports text detection, text direction classification, and text recognition models. It is developed using PaddleLite v2.10.</p>"},{"location":"en/version2.x/legacy/android_demo.html#2-recent-updates","title":"2. Recent Updates","text":"<ul> <li>February 27, 2022</li> <li>Updated the prediction library to PaddleLite v2.10.</li> <li>Added support for six operation modes:<ul> <li>Detection + Classification + Recognition</li> <li>Detection + Recognition</li> <li>Classification + Recognition</li> <li>Detection</li> <li>Recognition</li> <li>Classification</li> </ul> </li> </ul>"},{"location":"en/version2.x/legacy/android_demo.html#3-quick-start","title":"3. Quick Start","text":""},{"location":"en/version2.x/legacy/android_demo.html#31-environment-setup","title":"3.1 Environment Setup","text":"<ol> <li> <p>Install Android Studio:    Download and install Android Studio on your development machine. For detailed instructions, please refer to the Android Studio official website.</p> </li> <li> <p>Prepare Your Android Device:    Ensure that you have an Android phone and enable USB debugging. To do so, navigate to: Settings \u2192 Developer Options \u2192 Enable Developer Options and USB Debugging.</p> </li> </ol> <p>Note: If your Android Studio installation does not have the NDK configured, please follow the Install and Configure NDK and CMake guide. You may choose the latest NDK version or use the same version as the PaddleLite prediction library.</p>"},{"location":"en/version2.x/legacy/android_demo.html#32-importing-the-project","title":"3.2 Importing the Project","text":"<ol> <li>In Android Studio, select File \u2192 New \u2192 Import Project\u2026 and follow the wizard to import the project.</li> <li>Upon successful import, your project interface should resemble the following: </li> </ol>"},{"location":"en/version2.x/legacy/android_demo.html#33-running-the-demo","title":"3.3 Running the Demo","text":"<ol> <li>Connect your Android device to your computer.</li> <li>Click the Run button in the Android Studio toolbar.  </li> <li>Your device may prompt you with \"Allow installation of software from USB.\" Click Allow.</li> <li>Once the installation is complete, the app icon will appear on the last page of your home screen.</li> <li>Tap the app icon to launch the application. The home page will display as shown below: </li> </ol> <p>On the home screen, you will find the following elements:</p> <ul> <li>Run Model: Executes the model(s) using the selected operation mode.</li> <li>Take Photo for Recognition: Activates the device camera to capture an image. After taking a photo, tap Run Model to perform recognition.</li> <li>Select Image: Opens the photo gallery to select an image. After selection, tap Run Model to perform recognition.</li> <li>Clear Drawing: Clears any drawn text boxes from the currently displayed image.</li> <li>Dropdown List: Allows you to choose one of the six supported operation modes. The default mode is Detection + Classification + Recognition.</li> <li>Menu Button: Opens the settings interface for configurations related to the models and built-in images.</li> </ul> <p>When you tap Run Model, the demo executes the corresponding model(s) in your selected mode. For example, in Detection + Classification + Recognition mode, the output appears as follows:</p> <p></p> <p>The status display area shows the current model status (e.g., <code>run model succeeded</code>), indicating that the model ran successfully. The recognition results are formatted as follows:</p> <pre><code>Serial Number: Det: (x1,y1)(x2,y2)(x3,y3)(x4,y4) Rec: Recognized Text, Confidence Score Cls: Classification Label, Classification Score\n</code></pre>"},{"location":"en/version2.x/legacy/android_demo.html#34-operation-modes","title":"3.4 Operation Modes","text":"<p>The PaddleOCR demo supports six operation modes, shown in the illustration below:</p> <p></p> <p>The expected output for each mode is as follows:</p> Detection + Classification + Recognition Detection + Recognition Classification + Recognition Detection Recognition Classification"},{"location":"en/version2.x/legacy/android_demo.html#35-settings","title":"3.5 Settings","text":"<p>The settings interface allows you to customize the following parameters:</p> <ol> <li>General Settings</li> <li>Enable Custom Settings: Check this box to allow modifications.</li> <li>Model Path: The file path to the model (use the default value unless necessary).</li> <li>Label Path: The dictionary file for the text recognition model.</li> <li> <p>Image Path: The filename of the built-in image used for recognition.</p> </li> <li> <p>Model Runtime Settings: (Changes here trigger an automatic model reload when returning to the main interface.)</p> </li> <li>CPU Thread Number: The number of CPU cores allocated for model inference.</li> <li> <p>CPU Power Mode: Defines the model\u2019s execution strategy, corresponding to different core configurations.</p> </li> <li> <p>Input Settings</p> </li> <li> <p>det long size: The target value for the longer side of the image during DB model preprocessing. If the image's long side exceeds this value, it will be resized proportionally; if it is shorter, no resizing is applied.</p> </li> <li> <p>Output Settings</p> </li> <li>Score Threshold: The minimum score threshold for detection boxes in DB model post-processing. Boxes scoring below this threshold are filtered out and not displayed.</li> </ol>"},{"location":"en/version2.x/legacy/android_demo.html#4-further-support","title":"4. Further Support","text":"<p>For additional support and updates:</p> <ol> <li>Refer to the Paddle-Lite Demo for OCR for real-time recognition and prediction library updates.</li> <li>Visit the PaddleLite repository for more development support and community discussions.</li> </ol>"},{"location":"en/version2.x/legacy/benchmark.html","title":"Benchmark","text":"<p>This document gives the performance of the series models for Chinese and English recognition.</p>"},{"location":"en/version2.x/legacy/benchmark.html#test-data","title":"Test Data","text":"<p>We collected 300 images for different real application scenarios to evaluate the overall OCR system, including contract samples, license plates, nameplates, train tickets, test sheets, forms, certificates, street view images, business cards, digital meter, etc. The following figure shows some images of the test set.</p> <p></p>"},{"location":"en/version2.x/legacy/benchmark.html#measurement","title":"Measurement","text":"<p>Explanation:</p> <ul> <li> <p>The long size of the input for the text detector is 960.</p> </li> <li> <p>The evaluation time-consuming stage is the complete stage from image input to result output, including image pre-processing and post-processing.</p> </li> <li> <p><code>Intel Xeon 6148</code> is the server-side CPU model. Intel MKL-DNN is used in the test to accelerate the CPU prediction speed.</p> </li> <li> <p><code>Snapdragon 855</code> is a mobile processing platform model.</p> </li> </ul> <p>Compares the model size and F-score:</p> Model Name Model Size  of the  Whole System\\(M\\) Model Size of the Text  Detector\\(M\\) Model Size  of the Direction  Classifier\\(M\\) Model Sizeof the Text  Recognizer \\(M\\) F-score PP-OCRv2 11.6 3.0 0.9 8.6 0.5224 PP-OCR mobile 8.1 2.6 0.9 4.6 0.503 PP-OCR server 155.1 47.2 0.9 107 0.570 <p>Compares the time-consuming on CPU and T4 GPU (ms):</p> Model Name CPU T4 GPU PP-OCRv2 330 111 PP-OCR mobile 356 116 PP-OCR server 1056 200"},{"location":"en/version2.x/legacy/cpp_infer.html","title":"Server-side C++ Inference","text":"<p>This chapter introduces the C++ deployment steps of the PaddleOCR model. C++ is better than Python in terms of performance. Therefore, in CPU and GPU deployment scenarios, C++ deployment is mostly used. This section will introduce how to configure the C++ environment and deploy PaddleOCR in Linux (CPU\\GPU) environment. For Windows deployment please refer to Windows compilation guidelines.</p>"},{"location":"en/version2.x/legacy/cpp_infer.html#1-prepare-the-environment","title":"1. Prepare the Environment","text":""},{"location":"en/version2.x/legacy/cpp_infer.html#11-environment","title":"1.1 Environment","text":"<ul> <li>Linux, docker is recommended.</li> <li>Windows.</li> </ul>"},{"location":"en/version2.x/legacy/cpp_infer.html#12-compile-opencv","title":"1.2 Compile OpenCV","text":"<ul> <li>First of all, you need to download the source code compiled package in the Linux environment from the OpenCV official website. Taking OpenCV 3.4.7 as an example, the download command is as follows.</li> </ul> <pre><code>cd deploy/cpp_infer\nwget https://paddleocr.bj.bcebos.com/libs/opencv/opencv-3.4.7.tar.gz\ntar -xf opencv-3.4.7.tar.gz\n</code></pre> <p>Finally, you will see the folder of <code>opencv-3.4.7/</code> in the current directory.</p> <ul> <li>Compile OpenCV, the OpenCV source path (<code>root_path</code>) and installation path (<code>install_path</code>) should be set by yourself. Enter the OpenCV source code path and compile it in the following way.</li> </ul> <pre><code>root_path=your_opencv_root_path\ninstall_path=${root_path}/opencv3\n\nrm -rf build\nmkdir build\ncd build\n\ncmake .. \\\n    -DCMAKE_INSTALL_PREFIX=${install_path} \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DBUILD_SHARED_LIBS=OFF \\\n    -DWITH_IPP=OFF \\\n    -DBUILD_IPP_IW=OFF \\\n    -DWITH_LAPACK=OFF \\\n    -DWITH_EIGEN=OFF \\\n    -DCMAKE_INSTALL_LIBDIR=lib64 \\\n    -DWITH_ZLIB=ON \\\n    -DBUILD_ZLIB=ON \\\n    -DWITH_JPEG=ON \\\n    -DBUILD_JPEG=ON \\\n    -DWITH_PNG=ON \\\n    -DBUILD_PNG=ON \\\n    -DWITH_TIFF=ON \\\n    -DBUILD_TIFF=ON\n\nmake -j\nmake install\n</code></pre> <p>In the above commands, <code>root_path</code> is the downloaded OpenCV source code path, and <code>install_path</code> is the installation path of OpenCV. After <code>make install</code> is completed, the OpenCV header file and library file will be generated in this folder for later OCR source code compilation.</p> <p>The final file structure under the OpenCV installation path is as follows.</p> <pre><code>opencv3/\n|-- bin\n|-- include\n|-- lib\n|-- lib64\n|-- share\n</code></pre>"},{"location":"en/version2.x/legacy/cpp_infer.html#13-compile-or-download-or-the-paddle-inference-library","title":"1.3 Compile or Download or the Paddle Inference Library","text":"<ul> <li>There are 2 ways to obtain the Paddle inference library, described in detail below.</li> </ul>"},{"location":"en/version2.x/legacy/cpp_infer.html#131-direct-download-and-installation","title":"1.3.1 Direct download and installation","text":"<p>Paddle inference library official website. You can review and select the appropriate version of the inference library on the official website.</p> <ul> <li>After downloading, use the following command to extract files.</li> </ul> <pre><code>tar -xf paddle_inference.tgz\n</code></pre> <p>Finally you will see the folder of <code>paddle_inference/</code> in the current path.</p>"},{"location":"en/version2.x/legacy/cpp_infer.html#132-compile-the-inference-source-code","title":"1.3.2 Compile the inference source code","text":"<ul> <li> <p>If you want to get the latest Paddle inference library features, you can download the latest code from Paddle GitHub repository and compile the inference library from the source code. It is recommended to download the inference library with paddle version greater than or equal to 2.0.1.</p> </li> <li> <p>You can refer to Paddle inference library to get the Paddle source code from GitHub, and then compile To generate the latest inference library. The method of using git to access the code is as follows.</p> </li> </ul> <pre><code>git clone https://github.com/PaddlePaddle/Paddle.git\ngit checkout develop\n</code></pre> <ul> <li>Enter the Paddle directory and run the following commands to compile the paddle inference library.</li> </ul> <pre><code>rm -rf build\nmkdir build\ncd build\n\ncmake  .. \\\n    -DWITH_CONTRIB=OFF \\\n    -DWITH_MKL=ON \\\n    -DWITH_MKLDNN=ON  \\\n    -DWITH_TESTING=OFF \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DWITH_INFERENCE_API_TEST=OFF \\\n    -DON_INFER=ON \\\n    -DWITH_PYTHON=ON\nmake -j\nmake inference_lib_dist\n</code></pre> <p>For more compilation parameter options, please refer to the document.</p> <ul> <li>After the compilation process, you can see the following files in the folder of <code>build/paddle_inference_install_dir/</code>.</li> </ul> <pre><code>build/paddle_inference_install_dir/\n|-- CMakeCache.txt\n|-- paddle\n|-- third_party\n|-- version.txt\n</code></pre> <p><code>paddle</code> is the Paddle library required for C++ prediction later, and <code>version.txt</code> contains the version information of the current inference library.</p>"},{"location":"en/version2.x/legacy/cpp_infer.html#2-compile-and-run-the-demo","title":"2. Compile and Run the Demo","text":""},{"location":"en/version2.x/legacy/cpp_infer.html#21-export-the-inference-model","title":"2.1 Export the inference model","text":"<ul> <li>You can refer to Model inference and export the inference model. After the model is exported, assuming it is placed in the <code>inference</code> directory, the directory structure is as follows.</li> </ul> <pre><code>inference/\n|-- det_db\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- rec_rcnn\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- cls\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- table\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- layout\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n</code></pre>"},{"location":"en/version2.x/legacy/cpp_infer.html#22-compile-paddleocr-c-inference-demo","title":"2.2 Compile PaddleOCR C++ inference demo","text":"<ul> <li>The compilation commands are as follows. The addresses of Paddle C++ inference library, opencv and other Dependencies need to be replaced with the actual addresses on your own machines.</li> </ul> <pre><code>sh tools/build.sh\n</code></pre> <p>Specifically, you should modify the paths in <code>tools/build.sh</code>. The related content is as follows.</p> <pre><code>OPENCV_DIR=your_opencv_dir\nLIB_DIR=your_paddle_inference_dir\nCUDA_LIB_DIR=your_cuda_lib_dir\nCUDNN_LIB_DIR=your_cudnn_lib_dir\n</code></pre> <p><code>OPENCV_DIR</code> is the OpenCV installation path; <code>LIB_DIR</code> is the download (<code>paddle_inference</code> folder) or the generated Paddle inference library path (<code>build/paddle_inference_install_dir</code> folder); <code>CUDA_LIB_DIR</code> is the CUDA library file path, in docker; it is <code>/usr/local/cuda/lib64</code>; <code>CUDNN_LIB_DIR</code> is the cuDNN library file path, in docker it is <code>/usr/lib/x86_64-linux-gnu/</code>.</p> <ul> <li>After the compilation is completed, an executable file named <code>ppocr</code> will be generated in the <code>build</code> folder.</li> </ul>"},{"location":"en/version2.x/legacy/cpp_infer.html#23-run-the-demo","title":"2.3 Run the demo","text":"<p>Execute the built executable file:</p> <pre><code>./build/ppocr [--param1] [--param2] [...]\n</code></pre> <p>Note:ppocr uses the <code>PP-OCRv3</code> model by default, and the input shape used by the recognition model is <code>3, 48, 320</code>, if you want to use the old version model, you should add the parameter <code>--rec_img_h=32</code>.</p> <p>Specifically,</p>"},{"location":"en/version2.x/legacy/cpp_infer.html#1-detclsrec","title":"1. det+cls+rec","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --cls_model_dir=inference/cls \\\n    --image_dir=../../doc/imgs/12.jpg \\\n    --use_angle_cls=true \\\n    --det=true \\\n    --rec=true \\\n    --cls=true \\\n</code></pre>"},{"location":"en/version2.x/legacy/cpp_infer.html#2-detrec","title":"2. det+rec","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --image_dir=../../doc/imgs/12.jpg \\\n    --use_angle_cls=false \\\n    --det=true \\\n    --rec=true \\\n    --cls=false \\\n</code></pre>"},{"location":"en/version2.x/legacy/cpp_infer.html#3-det","title":"3. det","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --image_dir=../../doc/imgs/12.jpg \\\n    --det=true \\\n    --rec=false\n</code></pre>"},{"location":"en/version2.x/legacy/cpp_infer.html#4-clsrec","title":"4. cls+rec","text":"<pre><code>./build/ppocr --rec_model_dir=inference/rec_rcnn \\\n    --cls_model_dir=inference/cls \\\n    --image_dir=../../doc/imgs_words/ch/word_1.jpg \\\n    --use_angle_cls=true \\\n    --det=false \\\n    --rec=true \\\n    --cls=true \\\n</code></pre>"},{"location":"en/version2.x/legacy/cpp_infer.html#5-rec","title":"5. rec","text":"<pre><code>./build/ppocr --rec_model_dir=inference/rec_rcnn \\\n    --image_dir=../../doc/imgs_words/ch/word_1.jpg \\\n    --use_angle_cls=false \\\n    --det=false \\\n    --rec=true \\\n    --cls=false \\\n</code></pre>"},{"location":"en/version2.x/legacy/cpp_infer.html#6-cls","title":"6. cls","text":"<pre><code>./build/ppocr --cls_model_dir=inference/cls \\\n    --cls_model_dir=inference/cls \\\n    --image_dir=../../doc/imgs_words/ch/word_1.jpg \\\n    --use_angle_cls=true \\\n    --det=false \\\n    --rec=false \\\n    --cls=true \\\n</code></pre>"},{"location":"en/version2.x/legacy/cpp_infer.html#7-layouttable","title":"7. layout+table","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --table_model_dir=inference/table \\\n    --image_dir=../../ppstructure/docs/table/table.jpg \\\n    --layout_model_dir=inference/layout \\\n    --type=structure \\\n    --table=true \\\n    --layout=true\n</code></pre>"},{"location":"en/version2.x/legacy/cpp_infer.html#8-layout","title":"8. layout","text":"<pre><code>./build/ppocr --layout_model_dir=inference/layout \\\n    --image_dir=../../ppstructure/docs/table/1.png \\\n    --type=structure \\\n    --table=false \\\n    --layout=true \\\n    --det=false \\\n    --rec=false\n</code></pre>"},{"location":"en/version2.x/legacy/cpp_infer.html#9-table","title":"9. table","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --table_model_dir=inference/table \\\n    --image_dir=../../ppstructure/docs/table/table.jpg \\\n    --type=structure \\\n    --table=true\n</code></pre> <p>More parameters are as follows,</p> <p>Common parameters</p> parameter data type default meaning use_gpu bool false Whether to use GPU gpu_id int 0 GPU id when use_gpu is true gpu_mem int 4000 GPU memory requested cpu_math_library_num_threads int 10 Number of threads when using CPU inference. When machine cores is enough, the large the value, the faster the inference speed enable_mkldnn bool true Whether to use mkdlnn library output str ./output Path where visualization results are saved <p>forward</p> parameter data type default meaning det bool true Whether to perform text detection in the forward direction rec bool true Whether to perform text recognition in the forward direction cls bool false Whether to perform text direction classification in the forward direction <p>Detection related parameters</p> parameter data type default meaning det_model_dir string - Address of detection inference model max_side_len int 960 Limit the maximum image height and width to 960 det_db_thresh float 0.3 Used to filter the binarized image of DB prediction, setting 0.-0.3 has no obvious effect on the result det_db_box_thresh float 0.5 DB post-processing filter box threshold, if there is a missing box detected, it can be reduced as appropriate det_db_unclip_ratio float 1.6 Indicates the compactness of the text box, the smaller the value, the closer the text box to the text det_db_score_mode string slow slow: use polygon box to calculate bbox score, fast: use rectangle box to calculate. Use rectangular box to calculate faster, and polygonal box more accurate for curved text area. visualize bool true Whether to visualize the results\uff0cwhen it is set as true, the prediction results will be saved in the folder specified by the <code>output</code> field on an image with the same name as the input image. <p>Classifier related parameters</p> parameter data type default meaning use_angle_cls bool false Whether to use the direction classifier cls_model_dir string - Address of direction classifier inference model cls_thresh float 0.9 Score threshold of the  direction classifier cls_batch_num int 1 batch size of classifier <p>Recognition related parameters</p> parameter data type default meaning rec_model_dir string - Address of recognition inference model rec_char_dict_path string ../../ppocr/utils/ppocr_keys_v1.txt dictionary file rec_batch_num int 6 batch size of recognition rec_img_h int 48 image height of recognition rec_img_w int 320 image width of recognition <p>Layout related parameters</p> parameter data type default meaning layout_model_dir string - Address of layout inference model layout_dict_path string ../../ppocr/utils/dict/layout_dict/layout_publaynet_dict.txt dictionary file layout_score_threshold float 0.5 Threshold of score. layout_nms_threshold float 0.5 Threshold of nms. <p>Table recognition related parameters</p> parameter data type default meaning table_model_dir string - Address of table recognition inference model table_char_dict_path string ../../ppocr/utils/dict/table_structure_dict.txt dictionary file table_max_len int 488 The size of the long side of the input image of the table recognition model, the final input image size of the network is\uff08table_max_len\uff0ctable_max_len\uff09 merge_no_span_structure bool true Whether to merge  and  to &lt;/td <p>Multi-language inference is also supported in PaddleOCR, you can refer to recognition tutorial for more supported languages and models in PaddleOCR. Specifically, if you want to infer using multi-language models, you just need to modify values of <code>rec_char_dict_path</code> and <code>rec_model_dir</code>.</p> <p>The detection results will be shown on the screen, which is as follows.</p> <pre><code>predict img: ../../doc/imgs/12.jpg\n../../doc/imgs/12.jpg\n0       det boxes: [[74,553],[427,542],[428,571],[75,582]] rec text: \u200b\u6253\u200b\u6d66\u8def\u200b252935\u200b\u53f7\u200b rec score: 0.947724\n1       det boxes: [[23,507],[513,488],[515,529],[24,548]] rec text: \u200b\u7eff\u6d32\u200b\u4ed5\u683c\u7ef4\u200b\u82b1\u56ed\u200b\u516c\u5bd3\u200b rec score: 0.993728\n2       det boxes: [[187,456],[399,448],[400,480],[188,488]] rec text: \u200b\u6253\u200b\u6d66\u8def\u200b15\u200b\u53f7\u200b rec score: 0.964994\n3       det boxes: [[42,413],[483,391],[484,428],[43,450]] rec text: \u200b\u4e0a\u6d77\u200b\u65af\u683c\u5a01\u200b\u94c2\u200b\u5c14\u200b\u5927\u9152\u5e97\u200b rec score: 0.980086\nThe detection visualized image saved in ./output//12.jpg\n</code></pre> <ul> <li>layout+table</li> </ul> <pre><code>predict img: ../../ppstructure/docs/table/1.png\n0       type: text, region: [12,729,410,848], score: 0.781044, res: count of ocr result is : 7\n********** print ocr result **********\n0       det boxes: [[4,1],[79,1],[79,12],[4,12]] rec text: CTW1500. rec score: 0.769472\n...\n6       det boxes: [[4,99],[391,99],[391,112],[4,112]] rec text: sate-of-the-artmethods[12.34.36l.ourapproachachieves rec score: 0.90414\n********** end print ocr result **********\n1       type: text, region: [69,342,342,359], score: 0.703666, res: count of ocr result is : 1\n********** print ocr result **********\n0       det boxes: [[8,2],[269,2],[269,13],[8,13]] rec text: Table6.Experimentalresults on CTW-1500 rec score: 0.890454\n********** end print ocr result **********\n2       type: text, region: [70,316,706,332], score: 0.659738, res: count of ocr result is : 2\n********** print ocr result **********\n0       det boxes: [[373,2],[630,2],[630,11],[373,11]] rec text: oroposals.andthegreencontoursarefinal rec score: 0.919729\n1       det boxes: [[8,3],[357,3],[357,11],[8,11]] rec text: Visualexperimentalresultshebluecontoursareboundar rec score: 0.915963\n********** end print ocr result **********\n3       type: text, region: [489,342,789,359], score: 0.630538, res: count of ocr result is : 1\n********** print ocr result **********\n0       det boxes: [[8,2],[294,2],[294,14],[8,14]] rec text: Table7.Experimentalresults onMSRA-TD500 rec score: 0.942251\n********** end print ocr result **********\n4       type: text, region: [444,751,841,848], score: 0.607345, res: count of ocr result is : 5\n********** print ocr result **********\n0       det boxes: [[19,3],[389,3],[389,17],[19,17]] rec text: Inthispaper,weproposeanovel adaptivebound rec score: 0.941031\n1       det boxes: [[4,22],[390,22],[390,36],[4,36]] rec text: aryproposalnetworkforarbitraryshapetextdetection rec score: 0.960172\n2       det boxes: [[4,42],[392,42],[392,56],[4,56]] rec text: whichadoptanboundaryproposalmodeltogeneratecoarse rec score: 0.934647\n3       det boxes: [[4,61],[389,61],[389,75],[4,75]] rec text: ooundaryproposals,andthenadoptanadaptiveboundary rec score: 0.946296\n4       det boxes: [[5,80],[387,80],[387,93],[5,93]] rec text: leformationmodelcombinedwithGCNandRNNtoper rec score: 0.952401\n********** end print ocr result **********\n5       type: title, region: [444,705,564,724], score: 0.785429, res: count of ocr result is : 1\n********** print ocr result **********\n0       det boxes: [[6,2],[113,2],[113,14],[6,14]] rec text: 5.Conclusion rec score: 0.856903\n********** end print ocr result **********\n6       type: table, region: [14,360,402,711], score: 0.963643, res: &lt;html&gt;&lt;body&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;td&gt;Methods&lt;/td&gt;&lt;td&gt;Ext&lt;/td&gt;&lt;td&gt;R&lt;/td&gt;&lt;td&gt;P&lt;/td&gt;&lt;td&gt;F&lt;/td&gt;&lt;td&gt;FPS&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;TextSnake [18]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;85.3&lt;/td&gt;&lt;td&gt;67.9&lt;/td&gt;&lt;td&gt;75.6&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CSE [17]&lt;/td&gt;&lt;td&gt;MiLT&lt;/td&gt;&lt;td&gt;76.1&lt;/td&gt;&lt;td&gt;78.7&lt;/td&gt;&lt;td&gt;77.4&lt;/td&gt;&lt;td&gt;0.38&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;LOMO[40]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;76.5&lt;/td&gt;&lt;td&gt;85.7&lt;/td&gt;&lt;td&gt;80.8&lt;/td&gt;&lt;td&gt;4.4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ATRR[35]&lt;/td&gt;&lt;td&gt;Sy-&lt;/td&gt;&lt;td&gt;80.2&lt;/td&gt;&lt;td&gt;80.1&lt;/td&gt;&lt;td&gt;80.1&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SegLink++ [28]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;79.8&lt;/td&gt;&lt;td&gt;82.8&lt;/td&gt;&lt;td&gt;81.3&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextField [37]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;79.8&lt;/td&gt;&lt;td&gt;83.0&lt;/td&gt;&lt;td&gt;81.4&lt;/td&gt;&lt;td&gt;6.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MSR[38]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;79.0&lt;/td&gt;&lt;td&gt;84.1&lt;/td&gt;&lt;td&gt;81.5&lt;/td&gt;&lt;td&gt;4.3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PSENet-1s [33]&lt;/td&gt;&lt;td&gt;MLT&lt;/td&gt;&lt;td&gt;79.7&lt;/td&gt;&lt;td&gt;84.8&lt;/td&gt;&lt;td&gt;82.2&lt;/td&gt;&lt;td&gt;3.9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DB [12]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;80.2&lt;/td&gt;&lt;td&gt;86.9&lt;/td&gt;&lt;td&gt;83.4&lt;/td&gt;&lt;td&gt;22.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CRAFT [2]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;81.1&lt;/td&gt;&lt;td&gt;86.0&lt;/td&gt;&lt;td&gt;83.5&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextDragon [5]&lt;/td&gt;&lt;td&gt;MLT+&lt;/td&gt;&lt;td&gt;82.8&lt;/td&gt;&lt;td&gt;84.5&lt;/td&gt;&lt;td&gt;83.6&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PAN [34]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;81.2&lt;/td&gt;&lt;td&gt;86.4&lt;/td&gt;&lt;td&gt;83.7&lt;/td&gt;&lt;td&gt;39.8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ContourNet [36]&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;84.1&lt;/td&gt;&lt;td&gt;83.7&lt;/td&gt;&lt;td&gt;83.9&lt;/td&gt;&lt;td&gt;4.5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DRRG [41]&lt;/td&gt;&lt;td&gt;MLT&lt;/td&gt;&lt;td&gt;83.02&lt;/td&gt;&lt;td&gt;85.93&lt;/td&gt;&lt;td&gt;84.45&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextPerception[23]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;81.9&lt;/td&gt;&lt;td&gt;87.5&lt;/td&gt;&lt;td&gt;84.6&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours&lt;/td&gt;&lt;td&gt; Syn&lt;/td&gt;&lt;td&gt;80.57&lt;/td&gt;&lt;td&gt;87.66&lt;/td&gt;&lt;td&gt;83.97&lt;/td&gt;&lt;td&gt;12.08&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;81.45&lt;/td&gt;&lt;td&gt;87.81&lt;/td&gt;&lt;td&gt;84.51&lt;/td&gt;&lt;td&gt;12.15&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours&lt;/td&gt;&lt;td&gt;MLT&lt;/td&gt;&lt;td&gt;83.60&lt;/td&gt;&lt;td&gt;86.45&lt;/td&gt;&lt;td&gt;85.00&lt;/td&gt;&lt;td&gt;12.21&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;\nThe table visualized image saved in ./output//6_1.png\n7       type: table, region: [462,359,820,657], score: 0.953917, res: &lt;html&gt;&lt;body&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;td&gt;Methods&lt;/td&gt;&lt;td&gt;R&lt;/td&gt;&lt;td&gt;P&lt;/td&gt;&lt;td&gt;F&lt;/td&gt;&lt;td&gt;FPS&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;SegLink [26]&lt;/td&gt;&lt;td&gt;70.0&lt;/td&gt;&lt;td&gt;86.0&lt;/td&gt;&lt;td&gt;77.0&lt;/td&gt;&lt;td&gt;8.9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PixelLink [4]&lt;/td&gt;&lt;td&gt;73.2&lt;/td&gt;&lt;td&gt;83.0&lt;/td&gt;&lt;td&gt;77.8&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextSnake [18]&lt;/td&gt;&lt;td&gt;73.9&lt;/td&gt;&lt;td&gt;83.2&lt;/td&gt;&lt;td&gt;78.3&lt;/td&gt;&lt;td&gt;1.1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextField [37]&lt;/td&gt;&lt;td&gt;75.9&lt;/td&gt;&lt;td&gt;87.4&lt;/td&gt;&lt;td&gt;81.3&lt;/td&gt;&lt;td&gt;5.2 &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MSR[38]&lt;/td&gt;&lt;td&gt;76.7&lt;/td&gt;&lt;td&gt;87.4&lt;/td&gt;&lt;td&gt;81.7&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;FTSN[3]&lt;/td&gt;&lt;td&gt;77.1&lt;/td&gt;&lt;td&gt;87.6&lt;/td&gt;&lt;td&gt;82.0&lt;/td&gt;&lt;td&gt;:&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;LSE[30]&lt;/td&gt;&lt;td&gt;81.7&lt;/td&gt;&lt;td&gt;84.2&lt;/td&gt;&lt;td&gt;82.9&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CRAFT [2]&lt;/td&gt;&lt;td&gt;78.2&lt;/td&gt;&lt;td&gt;88.2&lt;/td&gt;&lt;td&gt;82.9&lt;/td&gt;&lt;td&gt;8.6&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MCN [16]&lt;/td&gt;&lt;td&gt;79&lt;/td&gt;&lt;td&gt;88&lt;/td&gt;&lt;td&gt;83&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ATRR[35]&lt;/td&gt;&lt;td&gt;82.1&lt;/td&gt;&lt;td&gt;85.2&lt;/td&gt;&lt;td&gt;83.6&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PAN [34]&lt;/td&gt;&lt;td&gt;83.8&lt;/td&gt;&lt;td&gt;84.4&lt;/td&gt;&lt;td&gt;84.1&lt;/td&gt;&lt;td&gt;30.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DB[12]&lt;/td&gt;&lt;td&gt;79.2&lt;/td&gt;&lt;td&gt;91.5&lt;/td&gt;&lt;td&gt;84.9&lt;/td&gt;&lt;td&gt;32.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DRRG [41]&lt;/td&gt;&lt;td&gt;82.30&lt;/td&gt;&lt;td&gt;88.05&lt;/td&gt;&lt;td&gt;85.08&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours (SynText)&lt;/td&gt;&lt;td&gt;80.68&lt;/td&gt;&lt;td&gt;85.40&lt;/td&gt;&lt;td&gt;82.97&lt;/td&gt;&lt;td&gt;12.68&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours (MLT-17)&lt;/td&gt;&lt;td&gt;84.54&lt;/td&gt;&lt;td&gt;86.62&lt;/td&gt;&lt;td&gt;85.57&lt;/td&gt;&lt;td&gt;12.31&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;\nThe table visualized image saved in ./output//7_1.png\n8       type: figure, region: [14,3,836,310], score: 0.969443, res: count of ocr result is : 26\n********** print ocr result **********\n0       det boxes: [[506,14],[539,15],[539,22],[506,21]] rec text: E rec score: 0.318073\n...\n25      det boxes: [[680,290],[759,288],[759,303],[680,305]] rec text: (d) CTW1500 rec score: 0.95911\n********** end print ocr result **********\n</code></pre>"},{"location":"en/version2.x/legacy/cpp_infer.html#3-faq","title":"3. FAQ","text":"<ol> <li>Encountered the error <code>unable to access 'https://github.com/LDOUBLEV/AutoLog.git/': gnutls_handshake() failed: The TLS connection was non-properly terminated.</code>, change the github address in <code>deploy/cpp_infer/external-cmake/auto-log.cmake</code> to the https://gitee.com/Double_V/AutoLog address.</li> </ol>"},{"location":"en/version2.x/legacy/lite.html","title":"Mobile deployment based on Paddle-Lite","text":"<p>This tutorial will introduce how to use Paddle-Lite to deploy PaddleOCR ultra-lightweight Chinese and English detection models on mobile phones.</p> <p>Paddle-Lite is a lightweight inference engine for PaddlePaddle. It provides efficient inference capabilities for mobile phones and IoT, and extensively integrates cross-platform hardware to provide lightweight deployment solutions for end-side deployment issues.</p>"},{"location":"en/version2.x/legacy/lite.html#1-preparation","title":"1. Preparation","text":""},{"location":"en/version2.x/legacy/lite.html#preparation-environment","title":"Preparation environment","text":"<ul> <li>Computer (for Compiling Paddle Lite)</li> <li>Mobile phone (arm7 or arm8)</li> </ul>"},{"location":"en/version2.x/legacy/lite.html#11-prepare-the-cross-compilation-environment","title":"1.1 Prepare the cross-compilation environment","text":"<p>The cross-compilation environment is used to compile C++ demos of Paddle Lite and PaddleOCR. Supports multiple development environments.</p> <p>For the compilation process of different development environments, please refer to the corresponding documents.</p> <ol> <li>Docker</li> <li>Linux</li> <li>MAC OS</li> </ol>"},{"location":"en/version2.x/legacy/lite.html#12-prepare-paddle-lite-library","title":"1.2 Prepare Paddle-Lite library","text":"<p>There are two ways to obtain the Paddle-Lite library:</p> <ol> <li> <p>[Recommended] Download directly, the download link of the Paddle-Lite library is as follows:</p> Platform Paddle-Lite library download link Android arm7 / arm8 IOS arm7 / arm8 <p>Note: 1. The above Paddle-Lite library is compiled from the Paddle-Lite 2.10 branch. For more information about Paddle-Lite 2.10, please refer to link.</p> <p>Note: It is recommended to use paddlelite&gt;=2.10 version of the prediction library, other prediction library versions download link</p> </li> <li> <p>Compile Paddle-Lite to get the prediction library. The compilation method of Paddle-Lite is as follows:</p> </li> </ol> <pre><code>git clone https://github.com/PaddlePaddle/Paddle-Lite.git\ncd Paddle-Lite\n# Switch to Paddle-Lite release/v2.10 stable branch\ngit checkout release/v2.10\n./lite/tools/build_android.sh  --arch=armv8  --with_cv=ON --with_extra=ON\n</code></pre> <p>Note: When compiling Paddle-Lite to obtain the Paddle-Lite library, you need to turn on the two options <code>--with_cv=ON --with_extra=ON</code>, <code>--arch</code> means the <code>arm</code> version, here is designated as armv8,</p> <p>More compilation commands refer to the introduction link \u3002</p> <p>After directly downloading the Paddle-Lite library and decompressing it, you can get the <code>inference_lite_lib.android.armv8/</code> folder, and the Paddle-Lite library obtained by compiling Paddle-Lite is located <code>Paddle-Lite/build.lite.android.armv8.gcc/inference_lite_lib.android.armv8/</code> folder.</p> <p>The structure of the prediction library is as follows:</p> <pre><code>inference_lite_lib.android.armv8/\n|-- cxx                                        C++ prebuild library\n|   |-- include                                C++\n|   |   |-- paddle_api.h\n|   |   |-- paddle_image_preprocess.h\n|   |   |-- paddle_lite_factory_helper.h\n|   |   |-- paddle_place.h\n|   |   |-- paddle_use_kernels.h\n|   |   |-- paddle_use_ops.h\n|   |   `-- paddle_use_passes.h\n|   `-- lib                                           C++ library\n|       |-- libpaddle_api_light_bundled.a             C++ static library\n|       `-- libpaddle_light_api_shared.so             C++ dynamic library\n|-- java                                     Java library\n|   |-- jar\n|   |   `-- PaddlePredictor.jar\n|   |-- so\n|   |   `-- libpaddle_lite_jni.so\n|   `-- src\n|-- demo                                     C++ and Java demo\n|   |-- cxx                                  C++ demo\n|   `-- java                                 Java demo\n</code></pre>"},{"location":"en/version2.x/legacy/lite.html#2-run","title":"2 Run","text":""},{"location":"en/version2.x/legacy/lite.html#21-inference-model-optimization","title":"2.1 Inference Model Optimization","text":"<p>Paddle Lite provides a variety of strategies to automatically optimize the original training model, including quantization, sub-graph fusion, hybrid scheduling, Kernel optimization and so on. In order to make the optimization process more convenient and easy to use, Paddle Lite provide opt tools to automatically complete the optimization steps and output a lightweight, optimal executable model.</p> <p>If you have prepared the model file ending in .nb, you can skip this step.</p> <p>The following table also provides a series of models that can be deployed on mobile phones to recognize Chinese. You can directly download the optimized model.</p> Version Introduction Model size Detection model Text Direction model Recognition model Paddle-Lite branch PP-OCRv3 extra-lightweight chinese OCR optimized model 16.2M download link download link download link v2.10 PP-OCRv3(slim) extra-lightweight chinese OCR optimized model 5.9M download link download link download link v2.10 <p>If you directly use the model in the above table for deployment, you can skip the following steps and directly read Section 2.2.</p> <p>If the model to be deployed is not in the above table, you need to follow the steps below to obtain the optimized model.</p> <p>Step 1: Refer to document to install paddlelite, which is used to convert paddle inference model to paddlelite required for running nb model</p> <pre><code>pip install paddlelite==2.10 # The paddlelite version should be the same as the prediction library version\n</code></pre> <p>After installation, the following commands can view the help information</p> <pre><code>paddle_lite_opt\n</code></pre> <p>Introduction to paddle_lite_opt parameters:</p> Options Description --model_dir The path of the PaddlePaddle model to be optimized (non-combined form) --model_file The network structure file path of the PaddlePaddle model (combined form) to be optimized --param_file The weight file path of the PaddlePaddle model (combined form) to be optimized --optimize_out_type Output model type, currently supports two types: protobuf and naive_buffer, among which naive_buffer is a more lightweight serialization/deserialization implementation. If you need to perform model prediction on the mobile side, please set this option to naive_buffer. The default is protobuf --optimize_out The output path of the optimized model --valid_targets The executable backend of the model, the default is arm. Currently it supports x86, arm, opencl, npu, xpu, multiple backends can be specified at the same time (separated by spaces), and Model Optimize Tool will automatically select the best method. If you need to support Huawei NPU (DaVinci architecture NPU equipped with Kirin 810/990 Soc), it should be set to npu, arm --record_tailoring_info When using the function of cutting library files according to the model, set this option to true to record the kernel and OP information contained in the optimized model. The default is false <p><code>--model_dir</code> is suitable for the non-combined mode of the model to be optimized, and the inference model of PaddleOCR is the combined mode, that is, the model structure and model parameters are stored in a single file.</p> <p>Step 2: Use paddle_lite_opt to convert the inference model to the mobile model format.</p> <p>The following takes the ultra-lightweight Chinese model of PaddleOCR as an example to introduce the use of the compiled opt file to complete the conversion of the inference model to the Paddle-Lite optimized model</p> <pre><code># \u3010[Recommendation] Download the Chinese and English inference model of PP-OCRv3\nwget  https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_slim_infer.tar &amp;&amp; tar xf  ch_PP-OCRv3_det_slim_infer.tar\nwget  https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_slim_infer.tar &amp;&amp; tar xf  ch_PP-OCRv2_rec_slim_quant_infer.tar\nwget  https://paddleocr.bj.bcebos.com/dygraph_v2.0/slim/ch_ppocr_mobile_v2.0_cls_slim_infer.tar &amp;&amp; tar xf  ch_ppocr_mobile_v2.0_cls_slim_infer.tar\n# Convert detection model\npaddle_lite_opt --model_file=./ch_PP-OCRv3_det_slim_infer/inference.pdmodel  --param_file=./ch_PP-OCRv3_det_slim_infer/inference.pdiparams  --optimize_out=./ch_PP-OCRv3_det_slim_opt --valid_targets=arm  --optimize_out_type=naive_buffer\n# Convert recognition model\npaddle_lite_opt --model_file=./ch_PP-OCRv3_rec_slim_infer/inference.pdmodel  --param_file=./ch_PP-OCRv3_rec_slim_infer/inference.pdiparams  --optimize_out=./ch_PP-OCRv3_rec_slim_opt --valid_targets=arm  --optimize_out_type=naive_buffer\n# Convert angle classifier model\npaddle_lite_opt --model_file=./ch_ppocr_mobile_v2.0_cls_slim_infer/inference.pdmodel  --param_file=./ch_ppocr_mobile_v2.0_cls_slim_infer/inference.pdiparams  --optimize_out=./ch_ppocr_mobile_v2.0_cls_slim_opt --valid_targets=arm  --optimize_out_type=naive_buffer\n</code></pre> <p>After the conversion is successful, there will be more files ending with <code>.nb</code> in the inference model directory, which is the successfully converted model file.</p>"},{"location":"en/version2.x/legacy/lite.html#22-run-optimized-model-on-phone","title":"2.2 Run optimized model on Phone","text":"<p>Some preparatory work is required first.</p> <ol> <li> <p>Prepare an Android phone with arm8. If the compiled prediction library and opt file are armv7, you need an arm7 phone and modify ARM_ABI = arm7 in the Makefile.</p> </li> <li> <p>Make sure the phone is connected to the computer, open the USB debugging option of the phone, and select the file transfer mode.</p> </li> <li> <p>Install the adb tool on the computer.</p> <p>3.1. Install ADB for MAC:</p> <pre><code>brew cask install android-platform-tools\n</code></pre> <p>3.2. Install ADB for Linux</p> <pre><code>sudo apt update\nsudo apt install -y wget adb\n</code></pre> <p>3.3. Install ADB for windows</p> <p>To install on win, you need to go to Google's Android platform to download the adb package for installation:link</p> <p>Verify whether adb is installed successfully</p> <pre><code>adb devices\n</code></pre> <p>If there is device output, it means the installation is successful\u3002</p> <pre><code>List of devices attached\n744be294    device\n</code></pre> </li> <li> <p>Prepare optimized models, prediction library files, test images and dictionary files used.</p> </li> </ol> <pre><code>git clone https://github.com/PaddlePaddle/PaddleOCR.git\ncd PaddleOCR/deploy/lite/\n# run prepare.sh\nsh prepare.sh /{lite prediction library path}/inference_lite_lib.android.armv8\n\n#\ncd /{lite prediction library path}/inference_lite_lib.android.armv8/\ncd demo/cxx/ocr/\n# copy paddle-lite C++ .so file to debug/ directory\ncp ../../../cxx/lib/libpaddle_light_api_shared.so ./debug/\n\ncd inference_lite_lib.android.armv8/demo/cxx/ocr/\ncp ../../../cxx/lib/libpaddle_light_api_shared.so ./debug/\n</code></pre> <p>Prepare the test image, taking PaddleOCR/doc/imgs/11.jpg as an example, copy the image file to the demo/cxx/ocr/debug/ folder. Prepare the model files optimized by the lite opt tool, ch_PP-OCRv3_det_slim_opt.nb , ch_PP-OCRv3_rec_slim_opt.nb , and place them under the demo/cxx/ocr/debug/ folder.</p> <p>The structure of the OCR demo is as follows after the above command is executed:</p> <pre><code>demo/cxx/ocr/\n|-- debug/\n|   |--ch_PP-OCRv3_det_slim_opt.nb           Detection model\n|   |--ch_PP-OCRv3_rec_slim_opt.nb           Recognition model\n|   |--ch_ppocr_mobile_v2.0_cls_slim_opt.nb           Text direction classification model\n|   |--11.jpg                           Image for OCR\n|   |--ppocr_keys_v1.txt                Dictionary file\n|   |--libpaddle_light_api_shared.so    C++ .so file\n|   |--config.txt                       Config file\n|-- config.txt                  Config file\n|-- cls_process.cc              Pre-processing and post-processing files for the angle classifier\n|-- cls_process.h\n|-- crnn_process.cc             Pre-processing and post-processing files for the CRNN model\n|-- crnn_process.h\n|-- db_post_process.cc          Pre-processing and post-processing files for the DB model\n|-- db_post_process.h\n|-- Makefile\n|-- ocr_db_crnn.cc              C++ main code\n</code></pre> <p>Note:</p> <ol> <li><code>ppocr_keys_v1.txt</code> is a Chinese dictionary file. If the nb model is used for English recognition or other language recognition, dictionary file should be replaced with a dictionary of the corresponding language. PaddleOCR provides a variety of dictionaries under ppocr/utils/, including:</li> </ol> <pre><code>dict/french_dict.txt     # french\ndict/german_dict.txt     # german\nic15_dict.txt       # english\ndict/japan_dict.txt      # japan\ndict/korean_dict.txt     # korean\nppocr_keys_v1.txt   # chinese\n</code></pre> <ol> <li> <p><code>config.txt</code> of the detector and classifier, as shown below:</p> <pre><code>max_side_len  960         #  Limit the maximum image height and width to  960\ndet_db_thresh  0.3        # Used to filter the binarized image of DB  prediction, setting 0.-0.3 has no obvious effect on the result\ndet_db_box_thresh  0.5    # DDB post-processing filter box threshold, if  there is a missing box detected, it can be reduced as appropriate\ndet_db_unclip_ratio  1.6  # Indicates the compactness of the text box,  the smaller the value, the closer the text box to the text\nuse_direction_classify  0  # Whether to use the direction classifier, 0  means not to use, 1 means to use\nrec_image_height  48      # The height of the input image of the  recognition model, the PP-OCRv3 model needs to be set to 48, and the  PP-OCRv2 model needs to be set to 32\n</code></pre> </li> <li> <p>Run Model on phone</p> </li> </ol> <p>After the above steps are completed, you can use adb to push the file to the phone to run, the steps are as follows:</p> <pre><code># Execute the compilation and get the executable file ocr_db_crnn\n# The first execution of this command will download dependent libraries such as opencv. After the download is complete, you need to execute it again\nmake -j\n# Move the compiled executable file to the debug folder\nmv ocr_db_crnn ./debug/\n# Push the debug folder to the phone\nadb push debug /data/local/tmp/\nadb shell\ncd /data/local/tmp/debug\nexport LD_LIBRARY_PATH=${PWD}:$LD_LIBRARY_PATH\n# The use of ocr_db_crnn is:\n# ./ocr_db_crnn Mode Detection model file Orientation classifier model file Recognition model file  Hardware  Precision  Threads Batchsize  Test image path Dictionary file path\n./ocr_db_crnn system ch_PP-OCRv3_det_slim_opt.nb  ch_PP-OCRv3_rec_slim_opt.nb  ch_ppocr_mobile_v2.0_cls_slim_opt.nb  arm8 INT8 10 1  ./11.jpg  config.txt  ppocr_keys_v1.txt  True\n# precision can be INT8 for quantitative model or FP32 for normal model.\n\n# Only using detection model\n./ocr_db_crnn  det ch_PP-OCRv3_det_slim_opt.nb arm8 INT8 10 1 ./11.jpg  config.txt\n\n# Only using recognition model\n./ocr_db_crnn  rec ch_PP-OCRv3_rec_slim_opt.nb arm8 INT8 10 1 word_1.jpg ppocr_keys_v1.txt config.txt\n</code></pre> <p>If you modify the code, you need to recompile and push to the phone.</p> <p>The outputs are as follows:</p> <p></p>"},{"location":"en/version2.x/legacy/lite.html#faq","title":"FAQ","text":"<p>Q1: What if I want to change the model, do I need to run it again according to the process?</p> <p>A1: If you have performed the above steps, you only need to replace the .nb model file to complete the model replacement.</p> <p>Q2: How to test with another picture?</p> <p>A2: Replace the .jpg test image under ./debug with the image you want to test, and run adb push to push new image to the phone.</p> <p>Q3: How to package it into the mobile APP?</p> <p>A3: This demo aims to provide the core algorithm part that can run OCR on mobile phones. Further, PaddleOCR/deploy/android_demo is an example of encapsulating this demo into a mobile app for reference.</p> <p>Q4: When running the demo, an error is reported <code>Error: This model is not supported, because kernel for 'io_copy' is not supported by Paddle-Lite.</code></p> <p>A4: The problem is that the installed paddlelite version does not match the downloaded prediction library version. Make sure that the paddleliteopt tool matches your prediction library version, and try to switch to the nb model again.</p>"},{"location":"en/version2.x/legacy/model_list_2.x.html","title":"Models Supported by PaddleOCR 2.x and Earlier Versions","text":"<p>Due to differences in the concatenation logic and configurations used during model training and inference, the PP-OCRv4 and PP-OCRv3 series models from the PaddleOCR 2.x branch cannot be used interchangeably with those from the PaddleOCR 3.0 and later branches.</p>"},{"location":"en/version2.x/legacy/model_list_2.x.html#detection-models","title":"Detection Models","text":""},{"location":"en/version2.x/legacy/model_list_2.x.html#chinese-detection-models","title":"Chinese Detection Models","text":"<ul> <li>ch_PP-OCRv4_det</li> <li>ch_PP-OCRv4_server_det</li> <li>ch_PP-OCRv3_det_slim</li> <li>ch_PP-OCRv3_det</li> <li>ch_PP-OCRv2_det_slim</li> <li>ch_PP-OCRv2_det</li> <li>ch_ppocr_mobile_slim_v2.0_det</li> <li>ch_ppocr_mobile_v2.0_det</li> <li>ch_ppocr_server_v2.0_det</li> </ul>"},{"location":"en/version2.x/legacy/model_list_2.x.html#english-detection-models","title":"English Detection Models","text":"<ul> <li>en_PP-OCRv3_det_slim</li> <li>en_PP-OCRv3_det</li> </ul>"},{"location":"en/version2.x/legacy/model_list_2.x.html#multilingual-detection-models","title":"Multilingual Detection Models","text":"<ul> <li>ml_PP-OCRv3_det_slim</li> <li>ml_PP-OCRv3_det</li> </ul>"},{"location":"en/version2.x/legacy/model_list_2.x.html#recognition-models","title":"Recognition Models","text":""},{"location":"en/version2.x/legacy/model_list_2.x.html#chinese-recognition-models","title":"Chinese Recognition Models","text":"<ul> <li>ch_PP-OCRv4_rec</li> <li>ch_PP-OCRv4_server_rec</li> <li>ch_PP-OCRv4_server_rec_doc</li> <li>ch_PP-OCRv3_rec_slim</li> <li>ch_PP-OCRv3_rec</li> <li>ch_PP-OCRv2_rec_slim</li> <li>ch_PP-OCRv2_rec</li> <li>ch_ppocr_mobile_slim_v2.0_rec</li> <li>ch_ppocr_mobile_v2.0_rec</li> <li>ch_ppocr_server_v2.0_rec</li> <li>SVTRv2(Rec Sever)</li> <li>RepSVTR(Mobile)</li> </ul>"},{"location":"en/version2.x/legacy/model_list_2.x.html#english-recognition-models","title":"English Recognition Models","text":"<ul> <li>en_PP-OCRv4_rec</li> <li>en_PP-OCRv3_rec_slim</li> <li>en_PP-OCRv3_rec</li> <li>en_number_mobile_slim_v2.0_rec</li> <li>en_number_mobile_v2.0_rec</li> </ul>"},{"location":"en/version2.x/legacy/model_list_2.x.html#multilingual-recognition-models","title":"Multilingual Recognition Models","text":"<ul> <li>korean_PP-OCRv3_rec</li> <li>japan_PP-OCRv3_rec</li> <li>chinese_cht_PP-OCRv3_rec</li> <li>te_PP-OCRv3_rec</li> <li>ka_PP-OCRv3_rec</li> <li>ta_PP-OCRv3_rec</li> <li>latin_PP-OCRv3_rec</li> <li>arabic_PP-OCRv3_rec</li> <li>cyrillic_PP-OCRv3_rec</li> <li>devanagari_PP-OCRv3_rec</li> </ul>"},{"location":"en/version2.x/legacy/model_list_2.x.html#end-to-end-ocr-models","title":"End-to-End OCR Models","text":"<ul> <li>PGNet</li> </ul>"},{"location":"en/version2.x/legacy/model_list_2.x.html#text-direction-classification-models","title":"Text Direction Classification Models","text":"<ul> <li>ch_ppocr_mobile_slim_v2.0_cls</li> <li>ch_ppocr_mobile_v2.0_cls</li> </ul>"},{"location":"en/version2.x/legacy/model_list_2.x.html#formula-recognition-models","title":"Formula Recognition Models","text":"<ul> <li>CAN</li> <li>UniMERNet</li> <li>LaTeX-OCR</li> <li>PP-FormulaNet-S</li> <li>PP-FormulaNet-L</li> </ul>"},{"location":"en/version2.x/legacy/model_list_2.x.html#table-recognition-models","title":"Table Recognition Models","text":"<ul> <li>TableMaster</li> <li>SLANet</li> <li>SLANeXt_wired</li> <li>SLANeXt_wireless</li> <li>en_ppocr_mobile_v2.0_table_structure</li> <li>en_ppstructure_mobile_v2.0_SLANet</li> <li>ch_ppstructure_mobile_v2.0_SLANet</li> </ul>"},{"location":"en/version2.x/legacy/model_list_2.x.html#table-ocr-models","title":"Table OCR Models","text":"<ul> <li>en_ppocr_mobile_v2.0_table_det</li> <li>en_ppocr_mobile_v2.0_table_rec</li> </ul>"},{"location":"en/version2.x/legacy/model_list_2.x.html#layout-detection-models","title":"Layout Detection Models","text":"<ul> <li>picodet_lcnet_x1_0_fgd_layout</li> <li>ppyolov2_r50vd_dcn_365e_publaynet</li> <li>picodet_lcnet_x1_0_fgd_layout_cdla</li> <li>picodet_lcnet_x1_0_fgd_layout_table</li> <li>ppyolov2_r50vd_dcn_365e_tableBank_word</li> <li>ppyolov2_r50vd_dcn_365e_tableBank_latex</li> </ul>"},{"location":"en/version2.x/legacy/paddle2onnx.html","title":"Paddle2ONNX Model Conversion and Prediction","text":"<p>This chapter introduces how to convert PaddleOCR models to ONNX models and perform predictions based on the ONNXRuntime engine. We also prepared an online AI Studio project for your convenience.</p>"},{"location":"en/version2.x/legacy/paddle2onnx.html#1-environment-setup","title":"1. Environment Setup","text":"<p>You need to prepare the environments for PaddleOCR, Paddle2ONNX model conversion, and ONNXRuntime prediction.</p>"},{"location":"en/version2.x/legacy/paddle2onnx.html#paddleocr","title":"PaddleOCR","text":"<p>Clone the PaddleOCR repository, use the main branch, and install it. Since the PaddleOCR repository is relatively large and cloning via <code>git clone</code> can be slow, this tutorial has already downloaded it.</p> <pre><code>git clone -b main https://github.com/PaddlePaddle/PaddleOCR.git\ncd PaddleOCR &amp;&amp; python3 -m pip install -e .\n</code></pre>"},{"location":"en/version2.x/legacy/paddle2onnx.html#paddle2onnx","title":"Paddle2ONNX","text":"<p>Paddle2ONNX supports converting models in the PaddlePaddle format to the ONNX format. Operators currently stably support exporting ONNX Opset versions 7~19, and some Paddle operators support conversion to lower ONNX Opsets. For more details, please refer to Paddle2ONNX.</p> <ul> <li>Install Paddle2ONNX</li> </ul> <pre><code>python3 -m pip install paddle2onnx\n</code></pre> <ul> <li>Install ONNXRuntime</li> </ul> <pre><code>python3 -m pip install onnxruntime\n</code></pre>"},{"location":"en/version2.x/legacy/paddle2onnx.html#2-model-conversion","title":"2. Model Conversion","text":""},{"location":"en/version2.x/legacy/paddle2onnx.html#get-paddle-static-graph-models","title":"Get Paddle Static Graph Models","text":"<p>There are two ways to obtain Paddle static graph models: download the prediction models provided by PaddleOCR in the model list; or refer to the Model Export Instructions to convert trained weights into inference models.</p> <p>Using the PP-OCR series English detection, recognition, and classification models as examples:</p>"},{"location":"en/version2.x/legacy/paddle2onnx.html#download-provided-inference-models","title":"Download Provided Inference Models","text":"PP-OCRv3PP-OCRv4 <pre><code>wget -nc -P ./inference https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar\ncd ./inference &amp;&amp; tar xf en_PP-OCRv3_det_infer.tar &amp;&amp; cd ..\n\nwget -nc  -P ./inference https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0.0/en_PP-OCRv3_mobile_rec_infer.tar\ncd ./inference &amp;&amp; tar xf en_PP-OCRv3_mobile_rec_infer.tar &amp;&amp; cd ..\n\nwget -nc  -P ./inference https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar\ncd ./inference &amp;&amp; tar xf ch_ppocr_mobile_v2.0_cls_infer.tar &amp;&amp; cd ..\n</code></pre> <pre><code>wget -nc -P ./inference https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar\ncd ./inference &amp;&amp; tar xf en_PP-OCRv3_det_infer.tar &amp;&amp; cd ..\n\nwget -nc  -P ./inference https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0.0/en_PP-OCRv4_mobile_rec_infer.tar\ncd ./inference &amp;&amp; tar xf en_PP-OCRv4_mobile_rec_infer.tar &amp;&amp; cd ..\n\nwget -nc  -P ./inference https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar\ncd ./inference &amp;&amp; tar xf ch_ppocr_mobile_v2.0_cls_infer.tar &amp;&amp; cd ..\n</code></pre>"},{"location":"en/version2.x/legacy/paddle2onnx.html#convert-dynamic-graph-to-static-graph-optional","title":"Convert Dynamic Graph to Static Graph (Optional)","text":"<p>Download dynamic graph models:</p> <pre><code>wget -nc -P pretrained https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-OCRv4_mobile_det_pretrained.pdparams\n\nwget -nc -P pretrained https://paddleocr.bj.bcebos.com/PP-OCRv4/chinese/ch_PP-OCRv4_rec_train.tar\ncd pretrained &amp;&amp; tar xf ch_PP-OCRv4_rec_train.tar &amp;&amp; cd ..\n\nwget -nc -P pretrained https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar\ncd pretrained &amp;&amp; tar xf ch_ppocr_mobile_v2.0_cls_train.tar &amp;&amp; cd ..\n</code></pre> <p>Convert to static graph models:</p> <pre><code>python3 tools/export_model.py -c configs/det/PP-OCRv4/PP-OCRv4_mobile_det.yml \\\n-o Global.pretrained_model=./pretrained/PP-OCRv4_mobile_det_pretrained \\\nGlobal.save_inference_dir=./inference/PP-OCRv4_mobile_det_infer/\n\npython3 tools/export_model.py -c configs/rec/PP-OCRv4/PP-OCRv4_mobile_rec.yml \\\n-o Global.pretrained_model=./pretrained/ch_PP-OCRv4_rec_train/student \\\nGlobal.save_inference_dir=./inference/PP-OCRv4_mobile_rec_infer/\n\npython3 tools/export_model.py -c configs/cls/cls_mv3.yml \\\n-o Global.pretrained_model=./pretrained/ch_ppocr_mobile_v2.0_cls_train/best_accuracy \\\nGlobal.save_inference_dir=./inference/ch_ppocr_mobile_v2.0_cls_infer/\n</code></pre>"},{"location":"en/version2.x/legacy/paddle2onnx.html#model-conversion","title":"Model Conversion","text":"<p>Use Paddle2ONNX to convert Paddle static graph models to the ONNX model format:</p> PP-OCRv3PP-OCRv4 <pre><code>paddle2onnx --model_dir ./inference/en_PP-OCRv3_det_infer \\\n--model_filename inference.pdmodel \\\n--params_filename inference.pdiparams \\\n--save_file ./inference/det_onnx/model.onnx \\\n--opset_version 11 \\\n--enable_onnx_checker True\n\npaddle2onnx --model_dir ./inference/en_PP-OCRv3_mobile_rec_infer \\\n--model_filename inference.pdmodel \\\n--params_filename inference.pdiparams \\\n--save_file ./inference/rec_onnx/model.onnx \\\n--opset_version 11 \\\n--enable_onnx_checker True\n\npaddle2onnx --model_dir ./inference/ch_ppocr_mobile_v2.0_cls_infer \\\n--model_filename inference.pdmodel \\\n--params_filename inference.pdiparams \\\n--save_file ./inference/cls_onnx/model.onnx \\\n--opset_version 11 \\\n--enable_onnx_checker True\n</code></pre> <pre><code>paddle2onnx --model_dir ./inference/en_PP-OCRv3_det_infer \\\n--model_filename inference.pdmodel \\\n--params_filename inference.pdiparams \\\n--save_file ./inference/det_onnx/model.onnx \\\n--opset_version 11 \\\n--enable_onnx_checker True\n\npaddle2onnx --model_dir ./inference/en_PP-OCRv4_mobile_rec_infer \\\n--model_filename inference.pdmodel \\\n--params_filename inference.pdiparams \\\n--save_file ./inference/rec_onnx/model.onnx \\\n--opset_version 11 \\\n--enable_onnx_checker True\n\npaddle2onnx --model_dir ./inference/ch_ppocr_mobile_v2.0_cls_infer \\\n--model_filename inference.pdmodel \\\n--params_filename inference.pdiparams \\\n--save_file ./inference/cls_onnx/model.onnx \\\n--opset_version 11 \\\n--enable_onnx_checker True\n</code></pre> <p>After execution, the ONNX models will be saved respectively under <code>./inference/det_onnx/</code>, <code>./inference/rec_onnx/</code>, and <code>./inference/cls_onnx/</code>.</p> <ul> <li> <p>Note: For OCR models, dynamic shapes must be used during conversion; otherwise, the prediction results may slightly differ from directly using Paddle for prediction. Additionally, the following models currently do not support conversion to ONNX models: NRTR, SAR, RARE, SRN.</p> </li> <li> <p>Note: After Paddle2ONNX version v1.2.3, dynamic shapes are supported by default, i.e., <code>float32[p2o.DynamicDimension.0,3,p2o.DynamicDimension.1,p2o.DynamicDimension.2]</code>. The option <code>--input_shape_dict</code> has been deprecated. If you need to adjust shapes, you can use the following command to adjust the input shape of the Paddle model.</p> </li> </ul> <pre><code>python3 -m paddle2onnx.optimize --input_model inference/det_onnx/model.onnx \\\n  --output_model inference/det_onnx/model.onnx \\\n  --input_shape_dict \"{'x': [-1,3,-1,-1]}\"\n</code></pre> <p>If you have optimization requirements for the exported ONNX model, it is recommended to use OnnxSlim to optimize the model:</p> <pre><code>pip install onnxslim\nonnxslim model.onnx slim.onnx\n</code></pre>"},{"location":"en/version2.x/legacy/paddle2onnx.html#3-inference-and-prediction","title":"3. Inference and Prediction","text":"<p>Using the Chinese OCR model as an example, you can perform prediction using ONNXRuntime by executing the following command:</p> <pre><code>python3 tools/infer/predict_system.py --use_gpu=False --use_onnx=True \\\n--det_model_dir=./inference/det_onnx/model.onnx  \\\n--rec_model_dir=./inference/rec_onnx/model.onnx  \\\n--cls_model_dir=./inference/cls_onnx/model.onnx  \\\n--image_dir=./docs/infer_deploy/images/img_12.jpg \\\n--rec_char_dict_path=ppocr/utils/en_dict.txt\n</code></pre> <p>Taking the English OCR model as an example, you can perform prediction using Paddle Inference by executing the following command:</p> PP-OCRv3PP-OCRv4 <pre><code>python3 tools/infer/predict_system.py --use_gpu=False \\\n--cls_model_dir=./inference/ch_ppocr_mobile_v2.0_cls_infer \\\n--rec_model_dir=./inference/en_PP-OCRv3_mobile_rec_infer \\\n--det_model_dir=./inference/en_PP-OCRv3_det_infer \\\n--image_dir=./docs/infer_deploy/images/img_12.jpg\\\n--rec_char_dict_path=ppocr/utils/en_dict.txt\n</code></pre> <pre><code>python3 tools/infer/predict_system.py --use_gpu=False \\\n--cls_model_dir=./inference/ch_ppocr_mobile_v2.0_cls_infer \\\n--rec_model_dir=./inference/en_PP-OCRv4_mobile_rec_infer \\\n--det_model_dir=./inference/en_PP-OCRv3_det_infer \\\n--image_dir=./docs/infer_deploy/images/img_12.jpg \\\n--rec_char_dict_path=ppocr/utils/en_dict.txt\n</code></pre> <p>After executing the command, the terminal will print out the predicted recognition information, and the visualization results will be saved under <code>./inference_results/</code>.</p> <p>ONNXRuntime Execution Result:</p> <p></p> <p>Paddle Inference Execution Result:</p> <p></p> <p>Using ONNXRuntime for prediction, terminal output:</p> <pre><code>[2022/10/10 12:06:28] ppocr DEBUG: dt_boxes num : 11, elapse : 0.3568880558013916\n[2022/10/10 12:06:31] ppocr DEBUG: rec_res num  : 11, elapse : 2.6445000171661377\n[2022/10/10 12:06:31] ppocr DEBUG: 0  Predict time of doc/imgs_en/img_12.jpg: 3.021s\n[2022/10/10 12:06:31] ppocr DEBUG: ACKNOWLEDGEMENTS, 0.997\n[2022/10/10 12:06:31] ppocr DEBUG: We would like to thank all the designers and, 0.976\n[2022/10/10 12:06:31] ppocr DEBUG: contributors who have been involved in the, 0.979\n[2022/10/10 12:06:31] ppocr DEBUG: production of this book; their contributions, 0.989\n[2022/10/10 12:06:31] ppocr DEBUG: have been indispensable to its creation. We, 0.956\n[2022/10/10 12:06:31] ppocr DEBUG: would also like to express our gratitude to all, 0.991\n[2022/10/10 12:06:31] ppocr DEBUG: the producers for their invaluable opinions, 0.978\n[2022/10/10 12:06:31] ppocr DEBUG: and assistance throughout this project. And to, 0.988\n[2022/10/10 12:06:31] ppocr DEBUG: the many others whose names are not credited, 0.958\n[2022/10/10 12:06:31] ppocr DEBUG: but have made specific input in this book, we, 0.970\n[2022/10/10 12:06:31] ppocr DEBUG: thank you for your continuous support., 0.998\n[2022/10/10 12:06:31] ppocr DEBUG: The visualized image saved in ./inference_results/img_12.jpg\n[2022/10/10 12:06:31] ppocr INFO: The predict total time is 3.2482550144195557\n</code></pre> <p>Using Paddle Inference for prediction, terminal output:</p> <pre><code>[2022/10/10 12:06:28] ppocr DEBUG: dt_boxes num : 11, elapse : 0.3568880558013916\n[2022/10/10 12:06:31] ppocr DEBUG: rec_res num  : 11, elapse : 2.6445000171661377\n[2022/10/10 12:06:31] ppocr DEBUG: 0  Predict time of doc/imgs_en/img_12.jpg: 3.021s\n[2022/10/10 12:06:31] ppocr DEBUG: ACKNOWLEDGEMENTS, 0.997\n[2022/10/10 12:06:31] ppocr DEBUG: We would like to thank all the designers and, 0.976\n[2022/10/10 12:06:31] ppocr DEBUG: contributors who have been involved in the, 0.979\n[2022/10/10 12:06:31] ppocr DEBUG: production of this book; their contributions, 0.989\n[2022/10/10 12:06:31] ppocr DEBUG: have been indispensable to its creation. We, 0.956\n[2022/10/10 12:06:31] ppocr DEBUG: would also like to express our gratitude to all, 0.991\n[2022/10/10 12:06:31] ppocr DEBUG: the producers for their invaluable opinions, 0.978\n[2022/10/10 12:06:31] ppocr DEBUG: and assistance throughout this project. And to, 0.988\n[2022/10/10 12:06:31] ppocr DEBUG: the many others whose names are not credited, 0.958\n[2022/10/10 12:06:31] ppocr DEBUG: but have made specific input in this book, we, 0.970\n[2022/10/10 12:06:31] ppocr DEBUG: thank you for your continuous support., 0.998\n[2022/10/10 12:06:31] ppocr DEBUG: The visualized image saved in ./inference_results/img_12.jpg\n[2022/10/10 12:06:31] ppocr INFO: The predict total time is 3.2482550144195557\n</code></pre>"},{"location":"en/version2.x/legacy/paddle_cloud.html","title":"\u4e91\u4e0a\u200b\u98de\u6868\u200b\u90e8\u7f72\u200b\u5de5\u5177","text":"<p>\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\uff08PaddleCloud\uff09 \u200b\u662f\u200b\u9762\u5411\u200b\u98de\u6868\u200b\u6846\u67b6\u200b\u53ca\u5176\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200b\u7684\u200b\u90e8\u7f72\u200b\u5de5\u5177\u200b\uff0c \u200b\u4e3a\u200b\u7528\u6237\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200bDocker\u200b\u5316\u200b\u90e8\u7f72\u200b\u548c\u200bKubernetes\u200b\u96c6\u7fa4\u200b\u90e8\u7f72\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u6ee1\u8db3\u200b\u4e0d\u540c\u200b\u573a\u666f\u200b\u4e0e\u200b\u73af\u5883\u200b\u7684\u200b\u90e8\u7f72\u200b\u9700\u6c42\u200b\u3002 \u200b\u672c\u200b\u7ae0\u8282\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200bPaddleCloud\u200b\u63d0\u4f9b\u200b\u7684\u200bOCR\u200b\u6807\u51c6\u200b\u955c\u50cf\u200b\u4ee5\u53ca\u200b\u4e91\u200b\u539f\u751f\u200b\u7ec4\u4ef6\u200b\u6765\u200b\u8bad\u7ec3\u200b\u548c\u200b\u90e8\u7f72\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u3002</p>"},{"location":"en/version2.x/legacy/paddle_cloud.html#_2","title":"\u4e91\u4e0a\u200b\u98de\u6868\u200b\u90e8\u7f72\u200b\u5de5\u5177\u200b\u7684\u200b\u4f18\u52bf","text":"<ul> <li>\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200bDocker\u200b\u955c\u50cf\u200b\u5927\u793c\u5305\u200b\u3002</li> </ul> <p>PaddleCloud\u200b\u4e3a\u200b\u7528\u6237\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u98de\u6868\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200bDocker\u200b\u955c\u50cf\u200b\u5927\u793c\u5305\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u955c\u50cf\u200b\u4e2d\u200b\u5305\u542b\u200b\u8fd0\u884c\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200b\u6848\u4f8b\u200b\u7684\u200b\u6240\u6709\u200b\u4f9d\u8d56\u200b\u5e76\u200b\u80fd\u200b\u6301\u7eed\u200b\u66f4\u65b0\u200b\uff0c\u200b\u652f\u6301\u200b\u5f02\u6784\u200b\u786c\u4ef6\u200b\u73af\u5883\u200b\u548c\u200b\u5e38\u89c1\u200bCUDA\u200b\u7248\u672c\u200b\u3001\u200b\u5f00\u7bb1\u200b\u5373\u7528\u200b\u3002</p> <ul> <li>\u200b\u5177\u6709\u200b\u4e30\u5bcc\u200b\u7684\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u7ec4\u4ef6\u200b\u3002</li> </ul> <p>\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u5177\u6709\u200b\u4e30\u5bcc\u200b\u7684\u200b\u4e91\u200b\u539f\u751f\u200b\u529f\u80fd\u200b\u7ec4\u4ef6\u200b\uff0c\u200b\u5305\u62ec\u200b\u6837\u672c\u200b\u6570\u636e\u200b\u7f13\u5b58\u200b\u7ec4\u4ef6\u200b\u3001\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u7ec4\u4ef6\u200b\u3001\u200b\u63a8\u7406\u200b\u670d\u52a1\u200b\u7ec4\u4ef6\u200b\u7b49\u200b\uff0c\u200b\u4f7f\u7528\u200b\u8fd9\u4e9b\u200b\u7ec4\u4ef6\u200b\u7528\u6237\u200b\u53ef\u4ee5\u200b\u5feb\u901f\u200b\u5730\u200b\u5728\u200bKubernetes\u200b\u96c6\u7fa4\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u548c\u200b\u90e8\u7f72\u200b\u5de5\u4f5c\u200b\u3002</p> <ul> <li>\u200b\u529f\u80fd\u5f3a\u5927\u200b\u7684\u200b\u81ea\u8fd0\u7ef4\u200b\u80fd\u529b\u200b\u3002</li> </ul> <p>\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u7ec4\u4ef6\u200b\u57fa\u4e8e\u200bKubernetes\u200b\u7684\u200bOperator\u200b\u673a\u5236\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u529f\u80fd\u5f3a\u5927\u200b\u7684\u200b\u81ea\u8fd0\u7ef4\u200b\u80fd\u529b\u200b\uff0c\u200b\u5982\u200b\u8bad\u7ec3\u200b\u7ec4\u4ef6\u200b\u652f\u6301\u200b\u591a\u79cd\u200b\u67b6\u6784\u200b\u6a21\u5f0f\u200b\u5e76\u200b\u5177\u6709\u200b\u5206\u5e03\u5f0f\u200b\u5bb9\u9519\u200b\u4e0e\u200b\u5f39\u6027\u200b\u8bad\u7ec3\u200b\u7684\u200b\u80fd\u529b\u200b\uff0c\u200b\u63a8\u7406\u200b\u670d\u52a1\u200b\u7ec4\u4ef6\u200b\u652f\u6301\u200b\u81ea\u52a8\u200b\u6269\u7f29\u5bb9\u200b\u4e0e\u200b\u84dd\u7eff\u200b\u53d1\u7248\u200b\u7b49\u200b\u3002</p> <ul> <li>\u200b\u9488\u5bf9\u200b\u98de\u6868\u200b\u6846\u67b6\u200b\u7684\u200b\u5b9a\u5236\u200b\u4f18\u5316\u200b\u3002</li> </ul> <p>\u200b\u9664\u4e86\u200b\u90e8\u7f72\u200b\u4fbf\u6377\u200b\u4e0e\u200b\u81ea\u8fd0\u7ef4\u200b\u7684\u200b\u4f18\u52bf\u200b\uff0cPaddleCloud\u200b\u8fd8\u200b\u9488\u5bf9\u200b\u98de\u6868\u200b\u6846\u67b6\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6b63\u200b\u5bf9\u6027\u200b\u4f18\u5316\u200b\uff0c\u200b\u5982\u200b\u901a\u8fc7\u200b\u7f13\u5b58\u200b\u6837\u672c\u200b\u6570\u636e\u200b\u6765\u200b\u52a0\u901f\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u4f5c\u4e1a\u200b\u3001\u200b\u57fa\u4e8e\u200b\u98de\u6868\u200b\u6846\u67b6\u200b\u548c\u200b\u8c03\u5ea6\u200b\u5668\u200b\u7684\u200b\u534f\u540c\u200b\u8bbe\u8ba1\u200b\u6765\u200b\u4f18\u5316\u200b\u96c6\u7fa4\u200bGPU\u200b\u5229\u7528\u7387\u200b\u7b49\u200b\u3002</p>"},{"location":"en/version2.x/legacy/paddle_cloud.html#1-pp-ocrv3-docker","title":"1. PP-OCRv3 Docker\u200b\u5316\u200b\u90e8\u7f72","text":"<p>PaddleCloud\u200b\u57fa\u4e8e\u200b Tekton \u200b\u4e3a\u200bOCR\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u955c\u50cf\u200b\u6301\u7eed\u200b\u6784\u5efa\u200b\u7684\u200b\u80fd\u529b\u200b\uff0c\u200b\u5e76\u200b\u652f\u6301\u200bCPU\u3001GPU\u200b\u4ee5\u53ca\u200b\u5e38\u89c1\u200bCUDA\u200b\u7248\u672c\u200b\u7684\u200b\u955c\u50cf\u200b\u3002 \u200b\u60a8\u200b\u53ef\u4ee5\u200b\u67e5\u770b\u200b PaddleOCR \u200b\u955c\u50cf\u200b\u4ed3\u5e93\u200b \u200b\u6765\u200b\u83b7\u53d6\u200b\u6240\u6709\u200b\u7684\u200b\u955c\u50cf\u200b\u5217\u8868\u200b\u3002 \u200b\u540c\u65f6\u200b\u6211\u4eec\u200b\u4e5f\u200b\u5c06\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u4e0e\u200b\u63a8\u7406\u200b\u5b9e\u6218\u200b\u6848\u4f8b\u200b\u653e\u7f6e\u200b\u5230\u200b\u4e86\u200bAI Studio\u200b\u5e73\u53f0\u200b\u4e0a\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u70b9\u51fb\u200b PP-OCRv3\u200b\u8bc6\u522b\u200b\u8bad\u63a8\u200b\u4e00\u4f53\u200b\u9879\u76ee\u200b\u5b9e\u6218\u200b \u200b\u5728\u200b\u5e73\u53f0\u200b\u4e0a\u200b\u5feb\u901f\u200b\u4f53\u9a8c\u200b\u3002</p> <p>\u200b\u9002\u7528\u200b\u573a\u666f\u200b\uff1a\u200b\u672c\u5730\u200b\u6d4b\u8bd5\u200b\u5f00\u53d1\u200b\u73af\u5883\u200b\u3001\u200b\u5355\u673a\u200b\u90e8\u7f72\u200b\u73af\u5883\u200b\u3002</p>"},{"location":"en/version2.x/legacy/paddle_cloud.html#11-docker","title":"1.1 \u200b\u5b89\u88c5\u200bDocker","text":"<p>\u200b\u5982\u679c\u200b\u60a8\u200b\u6240\u200b\u4f7f\u7528\u200b\u7684\u200b\u673a\u5668\u200b\u4e0a\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u5b89\u88c5\u200b Docker\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b Docker \u200b\u5b98\u65b9\u200b\u6587\u6863\u200b \u200b\u6765\u200b\u8fdb\u884c\u200b\u5b89\u88c5\u200b\u3002 \u200b\u5982\u679c\u200b\u60a8\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b\u652f\u6301\u200b GPU \u200b\u7248\u672c\u200b\u7684\u200b\u955c\u50cf\u200b\uff0c\u200b\u5219\u200b\u8fd8\u200b\u9700\u200b\u5b89\u88c5\u200b\u597d\u200bNVIDIA\u200b\u76f8\u5173\u9a71\u52a8\u200b\u548c\u200b nvidia-docker \u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a\u200b\u5982\u679c\u200b\u60a8\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200bWindows\u200b\u7cfb\u7edf\u200b\uff0c\u200b\u9700\u8981\u200b\u5f00\u542f\u200b WSL2\uff08Linux\u200b\u5b50\u7cfb\u7edf\u200b\u529f\u80fd\u200b\uff09\u200b\u529f\u80fd\u200b\u3002</p>"},{"location":"en/version2.x/legacy/paddle_cloud.html#12","title":"1.2 \u200b\u542f\u52a8\u200b\u5bb9\u5668","text":"<p>\u200b\u4f7f\u7528\u200bCPU\u200b\u7248\u672c\u200b\u7684\u200bDocker\u200b\u955c\u50cf\u200b</p> <pre><code># \u200b\u8fd9\u662f\u200b\u52a0\u4e0a\u200b\u53c2\u6570\u200b --shm-size=32g \u200b\u662f\u200b\u4e3a\u4e86\u200b\u9632\u6b62\u200b\u5bb9\u5668\u200b\u91cc\u200b\u5185\u5b58\u4e0d\u8db3\u200b\ndocker run --name ppocr -v $PWD:/mnt -p 8888:8888 -it --shm-size=32g paddlecloud/paddleocr:2.5-cpu-efbb0a /bin/bash\n</code></pre> <p>\u200b\u4f7f\u7528\u200bGPU\u200b\u7248\u672c\u200b\u7684\u200bDocker\u200b\u955c\u50cf\u200b</p> <pre><code>docker run --name ppocr --runtime=nvidia -v $PWD:/mnt -p 8888:8888 -it --shm-size=32g paddlecloud/paddleocr:2.5-gpu-cuda10.2-cudnn7-efbb0a /bin/bash\n</code></pre> <p>\u200b\u8fdb\u5165\u200b\u5bb9\u5668\u200b\u5185\u200b\uff0c\u200b\u5219\u200b\u53ef\u200b\u8fdb\u884c\u200b PP-OCRv3 \u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u90e8\u7f72\u200b\u5de5\u4f5c\u200b\u3002</p>"},{"location":"en/version2.x/legacy/paddle_cloud.html#13","title":"1.3 \u200b\u51c6\u5907\u200b\u8bad\u7ec3\u200b\u6570\u636e","text":"<p>\u200b\u672c\u200b\u6559\u7a0b\u200b\u4ee5\u200bHierText\u200b\u6570\u636e\u200b\u96c6\u4e3a\u4f8b\u200b\uff0cHierText\u200b\u662f\u200b\u7b2c\u4e00\u4e2a\u200b\u5177\u6709\u200b\u81ea\u7136\u200b\u573a\u666f\u200b\u548c\u200b\u6587\u6863\u200b\u4e2d\u200b\u6587\u672c\u200b\u5206\u5c42\u200b\u6ce8\u91ca\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u3002 \u200b\u8be5\u200b\u6570\u636e\u200b\u96c6\u200b\u5305\u542b\u200b\u4ece\u200b Open Images \u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u9009\u62e9\u200b\u7684\u200b 11639 \u200b\u5f20\u200b\u56fe\u50cf\u200b\uff0c\u200b\u63d0\u4f9b\u200b\u9ad8\u8d28\u91cf\u200b\u7684\u200b\u5355\u8bcd\u200b (~1.2M)\u3001\u200b\u884c\u200b\u548c\u200b\u6bb5\u843d\u200b\u7ea7\u522b\u200b\u7684\u200b\u6ce8\u91ca\u200b\u3002 \u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5c06\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u4f20\u5230\u200b\u767e\u5ea6\u200b\u4e91\u200b\u5bf9\u8c61\u200b\u5b58\u50a8\u200b\uff08BOS\uff09\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8fd0\u884c\u200b\u5982\u4e0b\u200b\u6307\u4ee4\u200b\uff0c\u200b\u5b8c\u6210\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u4e0b\u8f7d\u200b\u548c\u200b\u89e3\u538b\u200b\u64cd\u4f5c\u200b\uff1a</p> <pre><code># \u200b\u4e0b\u8f7d\u200b\u6570\u636e\u200b\u96c6\u200b\n$ wget -P /mnt https://paddleflow-public.hkg.bcebos.com/ppocr/hiertext1.tar\n\n# \u200b\u89e3\u538b\u200b\u6570\u636e\u200b\u96c6\u200b\n$ tar xf /mnt/hiertext1.tar -C /mnt &amp;&amp; mv /mnt/hiertext1 /mnt/hiertext\n</code></pre> <p>\u200b\u8fd0\u884c\u200b\u4e0a\u8ff0\u200b\u547d\u4ee4\u200b\u540e\u200b\uff0c\u200b\u5728\u200b <code>/mnt</code> \u200b\u76ee\u5f55\u200b\u4e0b\u200b\u5305\u542b\u200b\u4ee5\u4e0b\u200b\u6587\u4ef6\u200b\uff1a</p> <pre><code>/mnt/hiertext\n  \u2514\u2500 train/     HierText\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6570\u636e\u200b\n  \u2514\u2500 validation/     HierText\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6570\u636e\u200b\n  \u2514\u2500 label_hiertext_train.txt  HierText\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u7684\u200b\u884c\u200b\u6807\u6ce8\u200b\n  \u2514\u2500 label_hiertext_val.txt    HierText\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u7684\u200b\u884c\u200b\u6807\u6ce8\u200b\n</code></pre>"},{"location":"en/version2.x/legacy/paddle_cloud.html#14","title":"1.4 \u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6","text":"<p>PP-OCRv3\u200b\u6a21\u578b\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4f4d\u4e8e\u200b<code>/home/PaddleOCR/configs/det/PP-OCRv3/PP-OCRv3_det_cml.yml</code>\uff0c\u200b\u9700\u8981\u200b\u4fee\u6539\u200b\u7684\u200b\u914d\u7f6e\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>\u200b\u4fee\u6539\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u914d\u7f6e\u200b\uff1a</li> </ul> <pre><code>Train:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/icdar2015/text_localization/\n    label_file_list:\n      - ./train_data/icdar2015/text_localization/train_icdar2015_label.txt\n</code></pre> <p>\u200b\u4fee\u6539\u200b\u4e3a\u200b\uff1a</p> <pre><code>Train:\n  dataset:\n    name: SimpleDataSet\n    data_dir: /mnt/\n    label_file_list:\n      - /mnt/hiertext/label_hiertext_train.txt\n</code></pre> <ul> <li>\u200b\u4fee\u6539\u200b\u9a8c\u8bc1\u200b\u6570\u636e\u200b\u914d\u7f6e\u200b\uff1a</li> </ul> <pre><code>Eval:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/icdar2015/text_localization/\n    label_file_list:\n      - ./train_data/icdar2015/text_localization/test_icdar2015_label.txt\n</code></pre> <p>\u200b\u4fee\u6539\u200b\u4e3a\u200b\uff1a</p> <pre><code>Eval:\n  dataset:\n    name: SimpleDataSet\n    data_dir: /mnt/\n    label_file_list:\n      - /mnt/hiertext/label_hiertext_val.txt\n</code></pre>"},{"location":"en/version2.x/legacy/paddle_cloud.html#15","title":"1.5 \u200b\u542f\u52a8\u200b\u8bad\u7ec3","text":"<p>\u200b\u4e0b\u8f7d\u200bPP-OCRv3\u200b\u7684\u200b\u84b8\u998f\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5e76\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5982\u4e0b\u200b</p> <pre><code># \u200b\u4e0b\u8f7d\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5230\u200b/home/PaddleOCR/pre_train\u200b\u6587\u4ef6\u5939\u200b\u4e0b\u200b\n$ mkdir /home/PaddleOCR/pre_train\n\n$ wget -P /home/PaddleOCR/pre_train https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-OCRv3_mobile_det_pretrained.pdparams\n</code></pre> <p>\u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u9ed8\u8ba4\u200b\u4fdd\u5b58\u200b\u5728\u200b<code>output</code>\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u52a0\u8f7d\u200bPP-OCRv3\u200b\u68c0\u6d4b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002</p> <pre><code># \u200b\u8fd9\u91cc\u200b\u4ee5\u200b GPU \u200b\u8bad\u7ec3\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u4f7f\u7528\u200b CPU \u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u7684\u8bdd\u200b\uff0c\u200b\u9700\u8981\u200b\u6307\u5b9a\u200b\u53c2\u6570\u200b Global.use_gpu=false\npython3 tools/train.py -c configs/det/PP-OCRv3/PP-OCRv3_det_cml.yml -o Global.save_model_dir=./output/ Global.pretrained_model=./pre_train/ch_PP-OCRv3_det_distill_train/best_accuracy\n</code></pre> <p>\u200b\u5982\u679c\u200b\u8981\u200b\u4f7f\u7528\u200b\u591a\u200bGPU\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u8bf7\u200b\u4f7f\u7528\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\uff1a</p> <pre><code># \u200b\u542f\u52a8\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u9ed8\u8ba4\u200b\u4fdd\u5b58\u200b\u5728\u200boutput\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c--gpus '0,1,2,3'\u200b\u8868\u793a\u200b\u4f7f\u7528\u200b0\uff0c1\uff0c2\uff0c3\u200b\u53f7\u200bGPU\u200b\u8bad\u7ec3\u200b\npython3 -m paddle.distributed.launch --log_dir=./debug/ --gpus '0,1,2,3' tools/train.py -c configs/det/PP-OCRv3/PP-OCRv3_det_cml.yml -o Global.save_model_dir=./output/ Global.pretrained_model=./pre_train/ch_PP-OCRv3_det_distill_train/best_accuracy\n</code></pre>"},{"location":"en/version2.x/legacy/paddle_cloud.html#16","title":"1.6 \u200b\u6a21\u578b\u200b\u8bc4\u4f30","text":"<p>\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200boutput\u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u5305\u542b\u200b\u4ee5\u4e0b\u200b\u6587\u4ef6\u200b\uff1a</p> <pre><code>best_accuracy.states\nbest_accuracy.pdparams  # \u200b\u9ed8\u8ba4\u200b\u4fdd\u5b58\u200b\u6700\u4f18\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\nbest_accuracy.pdopt     # \u200b\u9ed8\u8ba4\u200b\u4fdd\u5b58\u200b\u6700\u4f18\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u4f18\u5316\u200b\u5668\u200b\u76f8\u5173\u200b\u53c2\u6570\u200b\nlatest.states\nlatest.pdparams  # \u200b\u9ed8\u8ba4\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6700\u65b0\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\nlatest.pdopt     # \u200b\u9ed8\u8ba4\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6700\u65b0\u200b\u6a21\u578b\u200b\u7684\u200b\u4f18\u5316\u200b\u5668\u200b\u76f8\u5173\u200b\u53c2\u6570\u200b\n</code></pre> <p>\u200b\u5176\u4e2d\u200b\uff0cbest_accuracy\u200b\u662f\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6700\u4f18\u200b\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200b\u8be5\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b</p> <pre><code># \u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b\ncd /home/PaddleOCR/\n\npython3 tools/eval.py -c configs/det/PP-OCRv3/PP-OCRv3_det_cml.yml -o Global.checkpoints=./output/best_accuracy\n</code></pre>"},{"location":"en/version2.x/legacy/paddle_cloud.html#2-pp-ocrv3","title":"2. PP-OCRv3\u200b\u4e91\u7aef\u200b\u90e8\u7f72","text":"<p>PaddleCloud\u200b\u57fa\u4e8e\u200bKubernetes\u200b\u7684\u200bOperator\u200b\u673a\u5236\u200b\u4e3a\u200b\u60a8\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u591a\u4e2a\u200b\u529f\u80fd\u5f3a\u5927\u200b\u7684\u200b\u4e91\u200b\u539f\u751f\u200b\u7ec4\u4ef6\u200b\uff0c\u200b\u5982\u200b\u6837\u672c\u200b\u6570\u636e\u200b\u7f13\u5b58\u200b\u7ec4\u4ef6\u200b\u3001\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u7ec4\u4ef6\u200b\u3001 \u200b\u4ee5\u53ca\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b\u670d\u52a1\u200b\u7ec4\u4ef6\u200b\uff0c \u200b\u4f7f\u7528\u200b\u8fd9\u4e9b\u200b\u7ec4\u4ef6\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5feb\u901f\u200b\u5730\u200b\u5728\u200b\u4e91\u4e0a\u200b\u8fdb\u884c\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6a21\u578b\u200b\u670d\u52a1\u5316\u200b\u90e8\u7f72\u200b\u3002\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200bPaddleCloud\u200b\u4e91\u200b\u539f\u751f\u200b\u7ec4\u4ef6\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b\u6587\u6863\u200b PaddleCloud\u200b\u67b6\u6784\u200b\u6982\u89c8\u200b \u3002</p> <p>\u200b\u9002\u7528\u200b\u573a\u666f\u200b\uff1a\u200b\u57fa\u4e8e\u200bKubernetes\u200b\u7684\u200b\u591a\u673a\u200b\u90e8\u7f72\u200b\u73af\u5883\u200b\u3002</p>"},{"location":"en/version2.x/legacy/paddle_cloud.html#21","title":"2.1 \u200b\u5b89\u88c5\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u7ec4\u4ef6","text":""},{"location":"en/version2.x/legacy/paddle_cloud.html#_3","title":"\u73af\u5883\u200b\u8981\u6c42","text":"<ul> <li>Kubernetes v1.16+</li> <li>kubectl</li> <li>Helm</li> </ul> <p>\u200b\u5982\u679c\u200b\u60a8\u200b\u6ca1\u6709\u200bKubernetes\u200b\u73af\u5883\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bMicroK8S\u200b\u5728\u200b\u672c\u5730\u200b\u642d\u5efa\u200b\u73af\u5883\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u8be6\u60c5\u8bf7\u200b\u53c2\u8003\u200b MicroK8S\u200b\u5b98\u65b9\u200b\u6587\u6863\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200bHelm\u200b\u4e00\u952e\u200b\u5b89\u88c5\u200b\u6240\u6709\u200b\u7ec4\u4ef6\u200b\u548c\u200b\u6240\u6709\u200b\u4f9d\u8d56\u200b</p> <pre><code># \u200b\u6dfb\u52a0\u200bPaddleCloud Chart\u200b\u4ed3\u5e93\u200b\n$ helm repo add paddlecloud https://paddleflow-public.hkg.bcebos.com/charts\n$ helm repo update\n\n# \u200b\u5b89\u88c5\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u7ec4\u4ef6\u200b\n$ helm install pdc paddlecloud/paddlecloud --set tags.all-dep=true --namespace paddlecloud --create-namespace\n\n# \u200b\u68c0\u67e5\u200b\u6240\u6709\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u7ec4\u4ef6\u200b\u662f\u5426\u200b\u6210\u529f\u200b\u542f\u52a8\u200b\uff0c\u200b\u547d\u540d\u200b\u7a7a\u95f4\u200b\u4e0b\u200b\u7684\u200b\u6240\u6709\u200bPod\u200b\u90fd\u200b\u4e3a\u200bRuning\u200b\u72b6\u6001\u200b\u5219\u200b\u5b89\u88c5\u200b\u6210\u529f\u200b\u3002\n$ kubectl get pods -n paddlecloud\nNAME                                                 READY   STATUS    RESTARTS   AGE\npdc-hostpath-5b6bd6787d-bxvxg                        1/1     Running   0          10h\njuicefs-csi-node-pkldt                               3/3     Running   0          10h\njuicefs-csi-controller-0                             3/3     Running   0          10h\npdc-paddlecloud-sampleset-767bdf6947-pb6zm           1/1     Running   0          10h\npdc-paddlecloud-paddlejob-7cc8b7bfc6-7gqnh           1/1     Running   0          10h\npdc-minio-7cc967669d-824q5                           1/1     Running   0          10h\npdc-redis-master-0                                   1/1     Running   0          10h\n</code></pre> <p>\u200b\u66f4\u200b\u591a\u200b\u5b89\u88c5\u200b\u53c2\u6570\u200b\u8bf7\u200b\u53c2\u8003\u200bPaddleCloud\u200b\u5b89\u88c5\u200b\u6307\u5357\u200b</p>"},{"location":"en/version2.x/legacy/paddle_cloud.html#22","title":"2.2 \u200b\u4e91\u200b\u539f\u751f\u200b\u7ec4\u4ef6\u200b\u4ecb\u7ecd","text":"<ul> <li>\u200b\u6570\u636e\u200b\u7f13\u5b58\u200b\u7ec4\u4ef6\u200b\u3002 \u200b\u6570\u636e\u200b\u7f13\u5b58\u200b\u7ec4\u4ef6\u200b\u4f7f\u7528\u200bJuiceFS\u200b\u4f5c\u4e3a\u200b\u7f13\u5b58\u200b\u5f15\u64ce\u200b\uff0c\u200b\u80fd\u591f\u200b\u5c06\u200b\u8fdc\u7a0b\u200b\u6837\u672c\u200b\u6570\u636e\u200b\u7f13\u5b58\u200b\u5230\u200b\u8bad\u7ec3\u200b\u96c6\u7fa4\u200b\u672c\u5730\u200b\uff0c\u200b\u5927\u5e45\u200b\u52a0\u901f\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u4f5c\u4e1a\u200b\u3002</li> <li>\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u7ec4\u4ef6\u200b\u3002 \u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u7ec4\u4ef6\u200b\u652f\u6301\u200b\u53c2\u6570\u200b\u670d\u52a1\u5668\u200b\uff08PS\uff09\u200b\u4e0e\u200b\u96c6\u5408\u200b\u901a\u4fe1\u200b\uff08Collective\uff09\u200b\u4e24\u79cd\u200b\u67b6\u6784\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u65b9\u4fbf\u200b\u7528\u6237\u200b\u5728\u200b\u4e91\u4e0a\u200b\u5feb\u901f\u200b\u8fd0\u884c\u200b\u98de\u6868\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u4f5c\u4e1a\u200b\u3002</li> </ul> <p>\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u4e91\u200b\u539f\u751f\u200b\u7ec4\u4ef6\u200b\u6765\u200b\u5728\u200bKubernetes\u200b\u96c6\u7fa4\u200b\u4e2d\u200b\u90e8\u7f72\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u4f5c\u4e1a\u200b\u3002</p>"},{"location":"en/version2.x/legacy/paddle_cloud.html#23-hiertext","title":"2.3 \u200b\u51c6\u5907\u200bhiertext\u200b\u6570\u636e\u200b\u96c6","text":"<p>\u200b\u4f7f\u7528\u200b\u6570\u636e\u200b\u7f13\u5b58\u200b\u7ec4\u4ef6\u200b\u6765\u200b\u51c6\u5907\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u7f16\u5199\u200bSampleSet Yaml\u200b\u6587\u4ef6\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># hiertext.yaml\napiVersion: batch.paddlepaddle.org/v1alpha1\nkind: SampleSet\nmetadata:\n  name: hiertext\n  namespace: paddlecloud\nspec:\n  partitions: 1\n  source:\n    uri: bos://paddleflow-public.hkg.bcebos.com/ppocr/hiertext\n    secretRef:\n      name: none\n  secretRef:\n    name: data-center\n</code></pre> <p>\u200b\u7136\u540e\u200b\u5728\u200b\u547d\u4ee4\u884c\u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200bkubectl\u200b\u6267\u884c\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u3002</p> <pre><code># \u200b\u521b\u5efa\u200bhiertext\u200b\u6570\u636e\u200b\u96c6\u200b\n$ kubectl apply -f hiertext.yaml\nsampleset.batch.paddlepaddle.org/hiertext created\n\n# \u200b\u67e5\u770b\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u72b6\u6001\u200b\n$ kubectl get sampleset hiertext -n paddlecloud\nNAME       TOTAL SIZE   CACHED SIZE   AVAIL SPACE   RUNTIME   PHASE   AGE\nhiertext   3.3 GiB       3.2 GiB      12 GiB        1/1       Ready   11m\n</code></pre>"},{"location":"en/version2.x/legacy/paddle_cloud.html#24-pp-ocrv3","title":"2.4 \u200b\u8bad\u7ec3\u200bPP-OCRv3\u200b\u6a21\u578b","text":"<p>\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u7ec4\u4ef6\u200b\u5728\u200bKubernetes\u200b\u96c6\u7fa4\u200b\u4e0a\u200b\u8bad\u7ec3\u200bPP-OCRv3\u200b\u6a21\u578b\u200b\uff0c\u200b\u7f16\u5199\u200bPaddleJob Yaml\u200b\u6587\u4ef6\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># ppocrv3.yaml\napiVersion: batch.paddlepaddle.org/v1\nkind: PaddleJob\nmetadata:\n  name: ppocrv3\n  namespace: paddlecloud\nspec:\n  cleanPodPolicy: OnCompletion\n  sampleSetRef:\n    name: hiertext\n    namespace: paddlecloud\n    mountPath: /mnt/hiertext\n  worker:\n    replicas: 1\n    template:\n      spec:\n        containers:\n          - name: ppocrv3\n            image: paddlecloud/paddleocr:2.5-gpu-cuda10.2-cudnn7-efbb0a\n            command:\n              - /bin/bash\n            args:\n              - \"-c\"\n              - &gt;\n                mkdir /home/PaddleOCR/pre_train &amp;&amp;\n                wget -P ./pre_train https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar &amp;&amp;\n                tar xf ./pre_train/ch_PP-OCRv3_det_distill_train.tar -C ./pre_train/ &amp;&amp;\n                python tools/train.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml -o\n                Train.dataset.data_dir=/mnt/\n                Train.dataset.label_file_list=[\\\"/mnt/hiertext/label_hiertext_train.txt\\\"]\n                Eval.dataset.data_dir=/mnt/\n                Eval.dataset.label_file_list=[\\\"/mnt/hiertext/label_hiertext_val.txt\\\"]\n                Global.save_model_dir=./output/\n                Global.pretrained_model=./pre_train/ch_PP-OCRv3_det_distill_train/best_accuracy\n            resources:\n              limits:\n                nvidia.com/gpu: 1\n            volumeMounts:  # \u200b\u6dfb\u52a0\u200b shared memory \u200b\u6302\u8f7d\u200b\u4ee5\u200b\u9632\u6b62\u200b\u7f13\u5b58\u200b\u51fa\u9519\u200b\n              - mountPath: /dev/shm\n                name: dshm\n        volumes:\n          - name: dshm\n            emptyDir:\n              medium: Memory\n</code></pre> <p>\u200b\u672c\u200b\u6848\u4f8b\u200b\u91c7\u7528\u200bGPU\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u53ea\u6709\u200bCPU\u200b\u673a\u5668\u200b\uff0c\u200b\u5219\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u955c\u50cf\u200b\u66ff\u6362\u6210\u200bCPU\u200b\u7248\u672c\u200b <code>paddlecloud/paddleocr:2.5-cpu-efbb0a</code>\uff0c\u200b\u5e76\u200b\u5728\u200bargs\u200b\u4e2d\u200b\u52a0\u4e0a\u200b\u53c2\u6570\u200b<code>Global.use_gpu=false</code>\u3002</p> <pre><code># \u200b\u521b\u5efa\u200bPaddleJob\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\n$ kubectl apply -f ppocrv3.yaml\npaddlejob.batch.paddlepaddle.org/ppocrv3 created\n\n# \u200b\u67e5\u770b\u200bPaddleJob\u200b\u72b6\u6001\u200b\n$ kubectl get pods -n paddlecloud -l paddle-res-name=ppocrv3-worker-0\nNAME               READY   STATUS    RESTARTS   AGE\nppocrv3-worker-0   1/1     Running   0          4s\n\n# \u200b\u67e5\u770b\u200b\u8bad\u7ec3\u200b\u65e5\u5fd7\u200b\n$ kubectl logs -f ppocrv3-worker-0 -n paddlecloud\n</code></pre>"},{"location":"en/version2.x/legacy/paddle_cloud.html#_4","title":"\u66f4\u200b\u591a\u200b\u8d44\u6e90","text":"<p>\u200b\u6b22\u8fce\u200b\u5173\u6ce8\u200b\u4e91\u4e0a\u200b\u98de\u6868\u200b\u9879\u76ee\u200bPaddleCloud\uff0c\u200b\u6211\u4eec\u200b\u4e3a\u200b\u60a8\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u98de\u6868\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200b\u6807\u51c6\u200b\u955c\u50cf\u200b\u4ee5\u53ca\u200b\u5168\u6808\u200b\u7684\u200b\u4e91\u200b\u539f\u751f\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200b\u90e8\u7f72\u200b\u7ec4\u4ef6\u200b\uff0c\u200b\u5982\u200b\u60a8\u200b\u6709\u200b\u4efb\u4f55\u200b\u5173\u4e8e\u200b\u98de\u6868\u200b\u6a21\u578b\u200b\u5957\u4ef6\u200b\u7684\u200b\u90e8\u7f72\u200b\u95ee\u9898\u200b\uff0c\u200b\u8bf7\u200b\u8054\u7cfb\u200b\u6211\u4eec\u200b\u3002 \u200b\u5982\u679c\u200b\u4f60\u200b\u53d1\u73b0\u200b\u4efb\u4f55\u200bPaddleCloud\u200b\u5b58\u5728\u200b\u7684\u200b\u95ee\u9898\u200b\u6216\u8005\u200b\u662f\u200b\u5efa\u8bae\u200b, \u200b\u6b22\u8fce\u200b\u901a\u8fc7\u200bGitHub Issues\u200b\u7ed9\u200b\u6211\u4eec\u200b\u63d0\u200bissues\u3002</p>"},{"location":"en/version2.x/legacy/paddle_server.html","title":"Sever Deployment","text":""},{"location":"en/version2.x/legacy/paddle_server.html#ocr-pipeline-webservice","title":"OCR Pipeline WebService","text":"<p>PaddleOCR provides two service deployment methods:</p> <ul> <li>Based on PaddleHub Serving: Code path is \"<code>./deploy/hubserving</code>\". Please refer to the tutorial</li> <li>Based on PaddleServing: Code path is \"<code>./deploy/pdserving</code>\". Please follow this tutorial.</li> </ul>"},{"location":"en/version2.x/legacy/paddle_server.html#service-deployment-based-on-paddleserving","title":"Service deployment based on PaddleServing","text":"<p>This document will introduce how to use the PaddleServing to deploy the PPOCR dynamic graph model as a pipeline online service.</p> <p>Some Key Features of Paddle Serving:</p> <ul> <li>Integrate with Paddle training pipeline seamlessly, most paddle models can be deployed with one line command.</li> <li>Industrial serving features supported, such as models management, online loading, online A/B testing etc.</li> <li>Highly concurrent and efficient communication between clients and servers supported.</li> </ul> <p>PaddleServing supports deployment in multiple languages. In this example, two deployment methods, python pipeline and C++, are provided. The comparison between the two is as follows:</p> Language Speed Secondary development Do you need to compile C++ fast Slightly difficult Single model prediction does not need to be compiled, multi-model concatenation needs to be compiled python general easy single-model/multi-model no compilation required <p>The introduction and tutorial of Paddle Serving service deployment framework reference document.</p>"},{"location":"en/version2.x/legacy/paddle_server.html#environmental-preparation","title":"Environmental preparation","text":"<p>PaddleOCR operating environment and Paddle Serving operating environment are needed.</p> <ol> <li> <p>Please prepare PaddleOCR operating environment reference link.    Download the corresponding paddlepaddle whl package according to the environment, it is recommended to install version 2.2.2.</p> </li> <li> <p>The steps of PaddleServing operating environment prepare are as follows:</p> <pre><code># Install serving which used to start the service\nwget https://paddle-serving.bj.bcebos.com/test-dev/whl/paddle_serving_server_gpu-0.8.3.post102-py3-none-any.whl\npip3 install paddle_serving_server_gpu-0.8.3.post102-py3-none-any.whl\n\n# Install paddle-serving-server for cuda10.1\n# wget https://paddle-serving.bj.bcebos.com/test-dev/whl/paddle_serving_server_gpu-0.8.3.post101-py3-none-any.whl\n# pip3 install paddle_serving_server_gpu-0.8.3.post101-py3-none-any.whl\n\n# Install serving which used to start the service\nwget https://paddle-serving.bj.bcebos.com/test-dev/whl/paddle_serving_client-0.8.3-cp37-none-any.whl\npip3 install paddle_serving_client-0.8.3-cp37-none-any.whl\n\n# Install serving-app\nwget https://paddle-serving.bj.bcebos.com/test-dev/whl/paddle_serving_app-0.8.3-py3-none-any.whl\npip3 install paddle_serving_app-0.8.3-py3-none-any.whl\n</code></pre> </li> </ol> <p>note: If you want to install the latest version of PaddleServing, refer to link.</p>"},{"location":"en/version2.x/legacy/paddle_server.html#model-conversion","title":"Model conversion","text":"<p>When using PaddleServing for service deployment, you need to convert the saved inference model into a serving model that is easy to deploy.</p> <p>Firstly, download the inference model of PPOCR</p> <pre><code># Download and unzip the OCR text detection model\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0.0/PP-OCRv3_mobile_det_infer.tar -O PP-OCRv3_mobile_det_infer.tar &amp;&amp; tar -xf PP-OCRv3_mobile_det_infer.tar\n# Download and unzip the OCR text recognition model\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0.0/PP-OCRv3_mobile_rec_infer.tar -O PP-OCRv3_mobile_rec_infer.tar &amp;&amp;  tar -xf PP-OCRv3_mobile_rec_infer.tar\n</code></pre> <p>Then, you can use installed paddle_serving_client tool to convert inference model to mobile model.</p> <pre><code>#  Detection model conversion\npython3 -m paddle_serving_client.convert --dirname ./PP-OCRv3_mobile_det_infer/ \\\n                                         --model_filename inference.pdmodel          \\\n                                         --params_filename inference.pdiparams       \\\n                                         --serving_server ./ppocr_det_v3_serving/ \\\n                                         --serving_client ./ppocr_det_v3_client/\n\n#  Recognition model conversion\npython3 -m paddle_serving_client.convert --dirname ./PP-OCRv3_mobile_rec_infer/ \\\n                                         --model_filename inference.pdmodel          \\\n                                         --params_filename inference.pdiparams       \\\n                                         --serving_server ./ppocr_rec_v3_serving/  \\\n                                         --serving_client ./ppocr_rec_v3_client/\n</code></pre> <p>After the detection model is converted, there will be additional folders of <code>ppocr_det_v3_serving</code> and <code>ppocr_det_v3_client</code> in the current folder, with the following format:</p> <pre><code>|- ppocr_det_v3_serving/\n  |- __model__\n  |- __params__\n  |- serving_server_conf.prototxt\n  |- serving_server_conf.stream.prototxt\n\n|- ppocr_det_v3_client\n  |- serving_client_conf.prototxt\n  |- serving_client_conf.stream.prototxt\n</code></pre> <p>The recognition model is the same.</p>"},{"location":"en/version2.x/legacy/paddle_server.html#paddle-serving-pipeline-deployment","title":"Paddle Serving pipeline deployment","text":"<ol> <li> <p>Download the PaddleOCR code, if you have already downloaded it, you can skip this step.</p> <pre><code>git clone https://github.com/PaddlePaddle/PaddleOCR\n\n# Enter the working directory\ncd PaddleOCR/deploy/pdserving/\n</code></pre> <p>The pdserver directory contains the code to start the pipeline service and send prediction requests, including:</p> <pre><code>__init__.py\nconfig.yml # Start the service configuration file\nocr_reader.py # OCR model pre-processing and post-processing code implementation\npipeline_http_client.py # Script to send pipeline prediction request\nweb_service.py # Start the script of the pipeline server\n</code></pre> </li> <li> <p>Run the following command to start the service.</p> <pre><code># Start the service and save the running log in log.txt\npython3 web_service.py --config=config.yml &amp;&gt;log.txt &amp;\n</code></pre> <p>After the service is successfully started, a log similar to the following will be printed in log.txt</p> <p></p> </li> <li> <p>Send service request</p> <pre><code>python3 pipeline_http_client.py\n</code></pre> <p>After successfully running, the predicted result of the model will be printed in the cmd window. An example of the result is:</p> <p></p> <p>Adjust the number of concurrency in config.yml to get the largest QPS. Generally, the number of concurrent detection and recognition is 2:1</p> <pre><code>det:\n    concurrency: 8\n    ...\nrec:\n    concurrency: 4\n    ...\n</code></pre> <p>Multiple service requests can be sent at the same time if necessary.</p> <p>The predicted performance data will be automatically written into the <code>PipelineServingLogs/pipeline.tracer</code> file.</p> <p>Tested on 200 real pictures, and limited the detection long side to 960. The average QPS on T4 GPU can reach around 23:</p> <pre><code>2021-05-13 03:42:36,895 ==================== TRACER ======================\n2021-05-13 03:42:36,975 Op(rec):\n2021-05-13 03:42:36,976         in[14.472382882882883 ms]\n2021-05-13 03:42:36,976         prep[9.556855855855856 ms]\n2021-05-13 03:42:36,976         midp[59.921905405405404 ms]\n2021-05-13 03:42:36,976         postp[15.345945945945946 ms]\n2021-05-13 03:42:36,976         out[1.9921216216216215 ms]\n2021-05-13 03:42:36,976         idle[0.16254943864471572]\n2021-05-13 03:42:36,976 Op(det):\n2021-05-13 03:42:36,976         in[315.4468035714286 ms]\n2021-05-13 03:42:36,976         prep[69.5980625 ms]\n2021-05-13 03:42:36,976         midp[18.989535714285715 ms]\n2021-05-13 03:42:36,976         postp[18.857803571428573 ms]\n2021-05-13 03:42:36,977         out[3.1337544642857145 ms]\n2021-05-13 03:42:36,977         idle[0.7477961159203756]\n2021-05-13 03:42:36,977 DAGExecutor:\n2021-05-13 03:42:36,977         Query count[224]\n2021-05-13 03:42:36,977         QPS[22.4 q/s]\n2021-05-13 03:42:36,977         Succ[0.9910714285714286]\n2021-05-13 03:42:36,977         Error req[169, 170]\n2021-05-13 03:42:36,977         Latency:\n2021-05-13 03:42:36,977                 ave[535.1678348214285 ms]\n2021-05-13 03:42:36,977                 .50[172.651 ms]\n2021-05-13 03:42:36,977                 .60[187.904 ms]\n2021-05-13 03:42:36,977                 .70[245.675 ms]\n2021-05-13 03:42:36,977                 .80[526.684 ms]\n2021-05-13 03:42:36,977                 .90[854.596 ms]\n2021-05-13 03:42:36,977                 .95[1722.728 ms]\n2021-05-13 03:42:36,977                 .99[3990.292 ms]\n2021-05-13 03:42:36,978 Channel (server worker num[10]):\n2021-05-13 03:42:36,978         chl0(In: ['@DAGExecutor'], Out: ['det']) size[0/0]\n2021-05-13 03:42:36,979         chl1(In: ['det'], Out: ['rec']) size[6/0]\n2021-05-13 03:42:36,979         chl2(In: ['rec'], Out: ['@DAGExecutor']) size[0/0]\n</code></pre> </li> </ol>"},{"location":"en/version2.x/legacy/paddle_server.html#c-serving","title":"C++ Serving","text":"<p>Service deployment based on python obviously has the advantage of convenient secondary development. However, the real application often needs to pursue better performance. PaddleServing also provides a more performant C++ deployment version.</p> <p>The C++ service deployment is the same as python in the environment setup and data preparation stages, the difference is when the service is started and the client sends requests.</p> <ol> <li>Compile Serving</li> </ol> <p>To improve predictive performance, C++ services also provide multiple model concatenation services. Unlike Python Pipeline services, multiple model concatenation requires the pre - and post-model processing code to be written on the server side, so local recompilation is required to generate serving. Specific may refer to the official document: how to compile Serving</p> <ol> <li> <p>Run the following command to start the service.</p> <pre><code># Start the service and save the running log in log.txt\npython3 -m paddle_serving_server.serve --model ppocr_det_v3_serving ppocr_rec_v3_serving --op GeneralDetectionOp GeneralInferOp --port 8181 &amp;&gt;log.txt &amp;\n</code></pre> <p>After the service is successfully started, a log similar to the following will be printed in log.txt </p> </li> <li> <p>Send service request</p> </li> </ol> <p>Due to the need for pre and post-processing in the C++Server part, in order to speed up the input to the C++Server is only the base64 encoded string of the picture, it needs to be manually modified    Change the feed_type field and shape field in ppocr_det_v3_client/serving_client_conf.prototxt to the following:</p> <pre><code> feed_var {\n name: \"x\"\n alias_name: \"x\"\n is_lod_tensor: false\n feed_type: 20\n shape: 1\n }\n</code></pre> <p>start the client:</p> <pre><code>```bash linenums=\"1\"\npython3 ocr_cpp_client.py ppocr_det_v3_client ppocr_rec_v3_client\n```\n\nAfter successfully running, the predicted result of the model will be printed in the cmd window. An example of the result is:\n\n![](./images/results.png)\n</code></pre>"},{"location":"en/version2.x/legacy/paddle_server.html#windows-users","title":"WINDOWS Users","text":"<p>Windows does not support Pipeline Serving, if we want to launch paddle serving on Windows, we should use Web Service, for more information please refer to Paddle Serving for Windows Users</p> <p>WINDOWS user can only use version 0.5.0 CPU Mode</p> <p>Prepare Stage:</p> <pre><code>pip3 install paddle-serving-server==0.5.0\npip3 install paddle-serving-app==0.3.1\n</code></pre> <ol> <li> <p>Start Server</p> <pre><code>cd win\npython3 ocr_web_server.py gpu(for gpu user)\nor\npython3 ocr_web_server.py cpu(for cpu user)\n</code></pre> </li> <li> <p>Client Send Requests</p> <pre><code>python3 ocr_web_client.py\n</code></pre> </li> </ol>"},{"location":"en/version2.x/legacy/paddle_server.html#faq","title":"FAQ","text":"<p>Q1: No result return after sending the request.</p> <p>A1: Do not set the proxy when starting the service and sending the request. You can close the proxy before starting the service and before sending the request. The command to close the proxy is:</p> <pre><code>unset https_proxy\nunset http_proxy\n</code></pre>"},{"location":"en/version2.x/legacy/python_infer.html","title":"Python Inference for PP-OCR Model Zoo","text":"<p>This article introduces the use of the Python inference engine for the PP-OCR model library. The content is in order of text detection, text recognition, direction classifier and the prediction method of the three in series on the CPU and GPU.</p>"},{"location":"en/version2.x/legacy/python_infer.html#text-detection-model-inference","title":"Text Detection Model Inference","text":"<p>The default configuration is based on the inference setting of the DB text detection model. For lightweight Chinese detection model inference, you can execute the following commands:</p> <pre><code># download DB text detection inference model\nwget  https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0.0/PP-OCRv3_mobile_det_infer.tar\ntar xf PP-OCRv3_mobile_det_infer.tar\n# run inference\npython3 tools/infer/predict_det.py --image_dir=\"./doc/imgs/00018069.jpg\" --det_model_dir=\"./PP-OCRv3_mobile_det_infer/\"\n</code></pre> <p>The visual text detection results are saved to the ./inference_results folder by default, and the name of the result file is prefixed with 'det_res'. Examples of results are as follows:</p> <p></p> <p>You can use the parameters <code>limit_type</code> and <code>det_limit_side_len</code> to limit the size of the input image, The optional parameters of <code>limit_type</code> are [<code>max</code>, <code>min</code>], and <code>det_limit_size_len</code> is a positive integer, generally set to a multiple of 32, such as 960.</p> <p>The default setting of the parameters is <code>limit_type='max', det_limit_side_len=960</code>. Indicates that the longest side of the network input image cannot exceed 960, If this value is exceeded, the image will be resized with the same width ratio to ensure that the longest side is <code>det_limit_side_len</code>. Set as <code>limit_type='min', det_limit_side_len=960</code>, it means that the shortest side of the image is limited to 960.</p> <p>If the resolution of the input picture is relatively large and you want to use a larger resolution prediction, you can set det_limit_side_len to the desired value, such as 1216:</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs/1.jpg\" --det_model_dir=\"./PP-OCRv3_mobile_det_infer/\" --det_limit_type=max --det_limit_side_len=1216\n</code></pre> <p>If you want to use the CPU for prediction, execute the command as follows</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs/1.jpg\" --det_model_dir=\"./PP-OCRv3_mobile_det_infer/\"  --use_gpu=False\n</code></pre>"},{"location":"en/version2.x/legacy/python_infer.html#text-recognition-model-inference","title":"Text Recognition Model Inference","text":""},{"location":"en/version2.x/legacy/python_infer.html#1-lightweight-chinese-recognition-model-inference","title":"1. Lightweight Chinese Recognition Model Inference","text":"<p>Note: The input shape used by the recognition model of <code>PP-OCRv3</code> is <code>3, 48, 320</code>. If you use other recognition models, you need to set the parameter <code>--rec_image_shape</code> according to the model. In addition, the <code>rec_algorithm</code> used by the recognition model of <code>PP-OCRv3</code> is <code>SVTR_LCNet</code> by default. Note the difference from the original <code>SVTR</code>.</p> <p>For lightweight Chinese recognition model inference, you can execute the following commands:</p> <pre><code># download CRNN text recognition inference model\nwget  https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0.0/PP-OCRv4_mobile_rec_infer.tar\ntar xf PP-OCRv3_mobile_rec_infer.tar\n# run inference\npython3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words_en/word_10.png\" --rec_model_dir=\"./PP-OCRv3_mobile_rec_infer/\" --rec_image_shape=3,48,320\n</code></pre> <p></p> <p>After executing the command, the prediction results (recognized text and score) of the above image will be printed on the screen.</p> <pre><code>Predicts of ./doc/imgs_words_en/word_10.png:('PAIN', 0.988671)\n</code></pre>"},{"location":"en/version2.x/legacy/python_infer.html#2-english-recognition-model-inference","title":"2. English Recognition Model Inference","text":"<p>For English recognition model inference, you can execute the following commands,you need to specify the dictionary path used by <code>--rec_char_dict_path</code>:</p> <pre><code># download en model\uff1a\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0.0/en_PP-OCRv3_mobile_rec_infer.tar\ntar xf en_PP-OCRv3_mobile_rec_infer.tar\npython3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/en/word_1.png\" --rec_model_dir=\"./en_PP-OCRv3_mobile_rec_infer/\" --rec_char_dict_path=\"ppocr/utils/en_dict.txt\"\n</code></pre> <p></p> <p>After executing the command, the prediction result of the above figure is:</p> <pre><code>Predicts of ./doc/imgs_words/en/word_1.png: ('JOINT', 0.998160719871521)\n</code></pre>"},{"location":"en/version2.x/legacy/python_infer.html#3-multilingual-model-inference","title":"3. Multilingual Model Inference","text":"<p>If you need to predict other language models, when using inference model prediction, you need to specify the dictionary path used by <code>--rec_char_dict_path</code>. At the same time, in order to get the correct visualization results, You need to specify the visual font path through <code>--vis_font_path</code>. There are small language fonts provided by default under the <code>doc/fonts</code> path, such as Korean recognition:</p> <pre><code>wget wget https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/korean_mobile_v2.0_rec_infer.tar\n\npython3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words/korean/1.jpg\" --rec_model_dir=\"./your inference model\" --rec_char_dict_path=\"ppocr/utils/dict/korean_dict.txt\" --vis_font_path=\"doc/fonts/korean.ttf\"\n</code></pre> <p></p> <p>After executing the command, the prediction result of the above figure is:</p> <pre><code>Predicts of ./doc/imgs_words/korean/1.jpg:('\ubc14\ud0d5\uc73c\ub85c', 0.9948904)\n</code></pre>"},{"location":"en/version2.x/legacy/python_infer.html#angle-classification-model-inference","title":"Angle Classification Model Inference","text":"<p>For angle classification model inference, you can execute the following commands:</p> <pre><code># download text angle class inference model\uff1a\nwget  https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar\ntar xf ch_ppocr_mobile_v2.0_cls_infer.tar\npython3 tools/infer/predict_cls.py --image_dir=\"./doc/imgs_words_en/word_10.png\" --cls_model_dir=\"ch_ppocr_mobile_v2.0_cls_infer\"\n</code></pre> <p></p> <p>After executing the command, the prediction results (classification angle and score) of the above image will be printed on the screen.</p> <pre><code> Predicts of ./doc/imgs_words_en/word_10.png:['0', 0.9999995]\n</code></pre>"},{"location":"en/version2.x/legacy/python_infer.html#text-detection-angle-classification-and-recognition-inference-concatenation","title":"Text Detection Angle Classification and Recognition Inference Concatenation","text":"<p>Note: The input shape used by the recognition model of <code>PP-OCRv3</code> is <code>3, 48, 320</code>. If you use other recognition models, you need to set the parameter <code>--rec_image_shape</code> according to the model. In addition, the <code>rec_algorithm</code> used by the recognition model of <code>PP-OCRv3</code> is <code>SVTR_LCNet</code> by default. Note the difference from the original <code>SVTR</code>.</p> <p>When performing prediction, you need to specify the path of a single image or a folder of images through the parameter <code>image_dir</code>, pdf file is also supported, the parameter <code>det_model_dir</code> specifies the path to detect the inference model, the parameter <code>cls_model_dir</code> specifies the path to angle classification inference model and the parameter <code>rec_model_dir</code> specifies the path to identify the inference model. The parameter <code>use_angle_cls</code> is used to control whether to enable the angle classification model. The parameter <code>use_mp</code> specifies whether to use multi-process to infer <code>total_process_num</code> specifies process number when using multi-process. The parameter . The visualized recognition results are saved to the <code>./inference_results</code> folder by default.</p> <pre><code># use direction classifier\npython3 tools/infer/predict_system.py --image_dir=\"./doc/imgs/00018069.jpg\" --det_model_dir=\"./PP-OCRv3_mobile_det_infer/\" --cls_model_dir=\"./cls/\" --rec_model_dir=\"./PP-OCRv3_mobile_rec_infer/\" --use_angle_cls=true\n# not use use direction classifier\npython3 tools/infer/predict_system.py --image_dir=\"./doc/imgs/00018069.jpg\" --det_model_dir=\"./PP-OCRv3_mobile_det_infer/\" --rec_model_dir=\"./PP-OCRv3_mobile_rec_infer/\" --use_angle_cls=false\n# use multi-process\npython3 tools/infer/predict_system.py --image_dir=\"./doc/imgs/00018069.jpg\" --det_model_dir=\"./PP-OCRv3_mobile_det_infer/\" --rec_model_dir=\"./PP-OCRv3_mobile_rec_infer/\" --use_angle_cls=false --use_mp=True --total_process_num=6\n# use PDF files, you can infer the first few pages by using the `page_num` parameter, the default is 0, which means infer all pages\npython3 tools/infer/predict_system.py --image_dir=\"./xxx.pdf\" --det_model_dir=\"./PP-OCRv3_mobile_det_infer/\" --cls_model_dir=\"./cls/\" --rec_model_dir=\"./PP-OCRv3_mobile_rec_infer/\" --use_angle_cls=true --page_num=2\n</code></pre> <p>After executing the command, the recognition result image is as follows:</p> <p></p> <p>For more configuration and explanation of inference parameters, please refer to\uff1aModel Inference Parameters Explained Tutorial\u3002</p>"},{"location":"en/version2.x/legacy/python_infer.html#tensorrt-inference","title":"TensorRT Inference","text":"<p>Paddle Inference ensembles TensorRT using subgraph mode. For GPU deployment scenarios, TensorRT can optimize some subgraphs, including horizontal and vertical integration of OPs, filter redundant OPs, and automatically select the optimal OP kernels for to speed up inference.</p> <p>You need to do the following 2 steps for inference using TRT.</p> <ul> <li>(1) Collect the dynamic shape information of the model about a specific dataset and store it in a file.</li> <li>(2) Load the dynamic shape information file for TRT inference.</li> </ul> <p>Taking the text detection model as an example. Firstly, you can use the following command to generate a dynamic shape file, which will eventually be named as <code>det_trt_dynamic_shape.txt</code> and stored in the <code>PP-OCRv3_mobile_det_infer</code> folder.</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs/1.jpg\" --det_model_dir=\"./PP-OCRv3_mobile_det_infer/\" --use_tensorrt=True\n</code></pre> <p>The above command is only used to collect dynamic shape information, and TRT is not used during inference.</p> <p>Then, you can use the following command to perform TRT inference.</p> <pre><code>python3 tools/infer/predict_det.py --image_dir=\"./doc/imgs/1.jpg\" --det_model_dir=\"./PP-OCRv3_mobile_det_infer/\" --use_tensorrt=True\n</code></pre> <p>Note:</p> <ul> <li>In the first step, if the dynamic shape information file already exists, it does not need to be collected again. If you want to regenerate the dynamic shape information file, you need to delete the dynamic shape information file in the model folder firstly, and then regenerate it.</li> <li>In general, dynamic shape information file only needs to be generated once. In the actual deployment process, it is recommended that the dynamic shape information file can be generated on offline validation set or test set, and then the file can be directly loaded for online TRT inference.</li> </ul>"},{"location":"en/version2.x/legacy/windows_vs2019_build.html","title":"Visual Studio 2022 Community CMake Compilation Guide","text":"<p>PaddleOCR has been tested on Windows using <code>Visual Studio 2022 Community</code>. Microsoft started supporting direct <code>CMake</code> project management from <code>Visual Studio 2017</code>, but it wasn't fully stable and reliable until <code>2019</code>. If you want to use CMake for project management and compilation, we recommend using <code>Visual Studio 2022</code>.</p> <p>All examples below assume the working directory is <code>D:\\projects\\cpp</code>.</p>"},{"location":"en/version2.x/legacy/windows_vs2019_build.html#1-environment-preparation","title":"1. Environment Preparation","text":""},{"location":"en/version2.x/legacy/windows_vs2019_build.html#11-install-required-dependencies","title":"1.1 Install Required Dependencies","text":"<ul> <li>Visual Studio 2019 or newer</li> <li>CUDA 10.2, cuDNN 7+ (only required for the GPU version of the prediction library). Additionally, the NVIDIA Computing Toolkit must be installed, and the NVIDIA cuDNN library must be downloaded.</li> <li>CMake 3.22+</li> </ul> <p>Ensure that the above dependencies are installed before proceeding. In this tutorial the Community Edition of <code>VS2022</code>\u00a0was used.</p>"},{"location":"en/version2.x/legacy/windows_vs2019_build.html#12-download-paddlepaddle-c-prediction-library-and-opencv","title":"1.2 Download PaddlePaddle C++ Prediction Library and OpenCV","text":""},{"location":"en/version2.x/legacy/windows_vs2019_build.html#121-download-paddlepaddle-c-prediction-library","title":"1.2.1 Download PaddlePaddle C++ Prediction Library","text":"<p>PaddlePaddle C++ prediction libraries offer different precompiled versions for various <code>CPU</code> and <code>CUDA</code> configurations. Download the appropriate version from: C++ Prediction Library Download List</p> <p>After extraction, the <code>D:\\projects\\paddle_inference</code> directory should contain:</p> <pre><code>paddle_inference\n\u251c\u2500\u2500 paddle # Core Paddle library and header files\n|\n\u251c\u2500\u2500 third_party # Third-party dependencies and headers\n|\n\u2514\u2500\u2500 version.txt # Version and compilation information\n</code></pre>"},{"location":"en/version2.x/legacy/windows_vs2019_build.html#122-install-and-configure-opencv","title":"1.2.2 Install and Configure OpenCV","text":"<ol> <li>Download OpenCV for Windows from the official release page.</li> <li>Run the downloaded executable and extract OpenCV to a specified directory, e.g., <code>D:\\projects\\cpp\\opencv</code>.</li> </ol>"},{"location":"en/version2.x/legacy/windows_vs2019_build.html#123-download-paddleocr-code","title":"1.2.3 Download PaddleOCR Code","text":"<pre><code>git clone https://github.com/PaddlePaddle/Paddle.git\ngit checkout develop\n</code></pre>"},{"location":"en/version2.x/legacy/windows_vs2019_build.html#2-running-the-project","title":"2. Running the Project","text":""},{"location":"en/version2.x/legacy/windows_vs2019_build.html#step-1-create-a-visual-studio-project","title":"Step 1: Create a Visual Studio Project","text":"<p>Once CMake is installed, open the <code>cmake-gui</code> application. Specify the source code directory in the first input box and the build output directory in the second input box.</p> <p></p>"},{"location":"en/version2.x/legacy/windows_vs2019_build.html#step-2-run-cmake-configuration","title":"Step 2: Run CMake Configuration","text":"<p>Click the <code>Configure</code> button at the bottom of the interface. The first time you run it, a prompt will appear asking for the Visual Studio configuration. Select your <code>Visual Studio</code> version and set the target platform to <code>x64</code>. Click <code>Finish</code> to start the configuration process.</p> <p></p> <p>The first run will result in errors, which is expected. You now need to configure OpenCV and the prediction library.</p> <ul> <li> <p>For CPU version, configure the following variables:</p> </li> <li> <p><code>OPENCV_DIR</code>: Path to the OpenCV <code>lib</code> folder</p> </li> <li><code>OpenCV_DIR</code>: Same as <code>OPENCV_DIR</code></li> <li> <p><code>PADDLE_LIB</code>: Path to the <code>paddle_inference</code> folder</p> </li> <li> <p>For GPU version, configure additional variables:</p> </li> <li> <p><code>CUDA_LIB</code>: CUDA path, e.g., <code>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\lib\\x64</code></p> </li> <li><code>CUDNN_LIB</code>: Path to extracted CuDNN library, e.g., <code>D:\\CuDNN-8.9.7.29</code></li> <li><code>TENSORRT_DIR</code>: Path to extracted TensorRT, e.g., <code>D:\\TensorRT-8.0.1.6</code></li> <li><code>WITH_GPU</code>: Check this option</li> <li><code>WITH_TENSORRT</code>: Check this option</li> </ul> <p>Example configuration:</p> <p></p> <p>Once configured, click <code>Configure</code> again.</p> <p>Note:</p> <ol> <li>If using <code>openblas</code>, uncheck <code>WITH_MKL</code>.</li> <li>If you encounter the error <code>unable to access 'https://github.com/LDOUBLEV/AutoLog.git/': gnutls_handshake() failed</code>, update <code>deploy/cpp_infer/external-cmake/auto-log.cmake</code> to use <code>https://gitee.com/Double_V/AutoLog</code>.</li> </ol>"},{"location":"en/version2.x/legacy/windows_vs2019_build.html#step-3-generate-visual-studio-project","title":"Step 3: Generate Visual Studio Project","text":"<p>Click <code>Generate</code> to create the <code>.sln</code> file for the Visual Studio project. </p> <p>Click <code>Open Project</code> to launch the project in Visual Studio. The interface should look like this: </p> <p>Before building the solution, perform the following steps:</p> <ol> <li>Change <code>Debug</code> to <code>Release</code> mode.</li> <li>Download dirent.h and copy it to the Visual Studio include directory, e.g., <code>C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\include</code>.</li> </ol> <p>Click <code>Build -&gt; Build Solution</code>. Once completed, the <code>ppocr.exe</code> file should appear in the <code>build/Release/</code> folder.</p> <p>Before running, copy the following files to <code>build/Release/</code>:</p> <ol> <li><code>paddle_inference/paddle/lib/paddle_inference.dll</code></li> <li><code>paddle_inference/paddle/lib/common.dll</code></li> <li><code>paddle_inference/third_party/install/mklml/lib/mklml.dll</code></li> <li><code>paddle_inference/third_party/install/mklml/lib/libiomp5md.dll</code></li> <li><code>paddle_inference/third_party/install/onednn/lib/mkldnn.dll</code></li> <li><code>opencv/build/x64/vc15/bin/opencv_world455.dll</code></li> <li>If using the <code>openblas</code> version, also copy <code>paddle_inference/third_party/install/openblas/lib/openblas.dll</code>.</li> </ol>"},{"location":"en/version2.x/legacy/windows_vs2019_build.html#step-4-run-the-prediction","title":"Step 4: Run the Prediction","text":"<p>The compiled executable is located in the <code>build/Release/</code> directory. Open <code>cmd</code> and navigate to <code>D:\\projects\\cpp\\PaddleOCR\\deploy\\cpp_infer\\</code>:</p> <pre><code>cd /d D:\\projects\\cpp\\PaddleOCR\\deploy\\cpp_infer\n</code></pre> <p>Run the prediction using <code>ppocr.exe</code>. For more usage details, refer to the to the Instructions section of running the demo.</p> <pre><code># Switch terminal encoding to UTF-8\nCHCP 65001\n\n# If using PowerShell, run this command before execution to fix character encoding issues:\n$OutputEncoding = [console]::InputEncoding = [console]::OutputEncoding = New-Object System.Text.UTF8Encoding\n\n# Execute prediction\n.\\build\\Release\\ppocr.exe system --det_model_dir=D:\\projects\\cpp\\ch_PP-OCRv2_det_slim_quant_infer --rec_model_dir=D:\\projects\\cpp\\ch_PP-OCRv2_rec_slim_quant_infer --image_dir=D:\\projects\\cpp\\PaddleOCR\\doc\\imgs\\11.jpg\n</code></pre> <p></p>"},{"location":"en/version2.x/legacy/windows_vs2019_build.html#sample-result","title":"Sample result:","text":""},{"location":"en/version2.x/legacy/windows_vs2019_build.html#faq","title":"FAQ","text":"<ul> <li>Issue: Application fails to start with error <code>(0xc0000142)</code> and <code>cmd</code> output shows <code>You are using Paddle compiled with TensorRT, but TensorRT dynamic library is not found.</code></li> <li>Solution: Copy all <code>.dll</code> files from the <code>TensorRT</code> directory's <code>lib</code> folder into the <code>release</code> directory and try running it again.</li> </ul>"},{"location":"en/version2.x/model/index.html","title":"Index","text":""},{"location":"en/version2.x/model/index.html#pp-ocr","title":"PP-OCR \u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u5217\u8868\u200b\uff08\u200b\u66f4\u65b0\u200b\u4e2d\u200b\uff09","text":"\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u63a8\u8350\u200b\u573a\u666f\u200b \u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b \u200b\u65b9\u5411\u200b\u5206\u7c7b\u5668\u200b \u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b \u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b PP-OCRv4 \u200b\u6a21\u578b\u200b\uff0815.8M\uff09 PP-OCRv4_mobile_det \u200b\u79fb\u52a8\u200b\u7aef\u200b&amp;\u200b\u670d\u52a1\u5668\u7aef\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u4e2d\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b PP-OCRv3 \u200b\u6a21\u578b\u200b\uff0816.2M\uff09 PP-OCRv3_mobile_det \u200b\u79fb\u52a8\u200b\u7aef\u200b&amp;\u200b\u670d\u52a1\u5668\u7aef\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u82f1\u6587\u200b\u8d85\u200b\u8f7b\u91cf\u200b PP-OCRv3 \u200b\u6a21\u578b\u200b\uff0813.4M\uff09 en_PP-OCRv3_xx \u200b\u79fb\u52a8\u200b\u7aef\u200b&amp;\u200b\u670d\u52a1\u5668\u7aef\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <ul> <li>\u200b\u8d85\u200b\u8f7b\u91cf\u200b OCR \u200b\u7cfb\u5217\u200b\u66f4\u200b\u591a\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\uff08\u200b\u5305\u62ec\u200b\u591a\u200b\u8bed\u8a00\u200b\uff09\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200bPP-OCR \u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\uff0c\u200b\u6587\u6863\u200b\u5206\u6790\u200b\u76f8\u5173\u200b\u6a21\u578b\u200b\u53c2\u8003\u200bPP-Structure \u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b</li> </ul>"},{"location":"en/version2.x/model/index.html#paddleocr","title":"PaddleOCR \u200b\u573a\u666f\u200b\u5e94\u7528\u200b\u6a21\u578b","text":"\u884c\u4e1a\u200b \u200b\u7c7b\u522b\u200b \u200b\u4eae\u70b9\u200b \u200b\u6587\u6863\u200b\u8bf4\u660e\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b \u200b\u5236\u9020\u200b \u200b\u6570\u7801\u7ba1\u200b\u8bc6\u522b\u200b \u200b\u6570\u7801\u7ba1\u200b\u6570\u636e\u200b\u5408\u6210\u200b\u3001\u200b\u6f0f\u200b\u8bc6\u522b\u200b\u8c03\u4f18\u200b \u200b\u5149\u200b\u529f\u7387\u200b\u8ba1\u200b\u6570\u7801\u7ba1\u200b\u5b57\u7b26\u8bc6\u522b\u200b \u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b \u200b\u91d1\u878d\u200b \u200b\u901a\u7528\u200b\u8868\u5355\u200b\u8bc6\u522b\u200b \u200b\u591a\u200b\u6a21\u6001\u200b\u901a\u7528\u200b\u8868\u5355\u200b\u7ed3\u6784\u5316\u200b\u63d0\u53d6\u200b \u200b\u591a\u200b\u6a21\u6001\u200b\u8868\u5355\u200b\u8bc6\u522b\u200b \u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b \u200b\u4ea4\u901a\u200b \u200b\u8f66\u724c\u200b\u8bc6\u522b\u200b \u200b\u591a\u89d2\u5ea6\u200b\u56fe\u50cf\u5904\u7406\u200b\u3001\u200b\u8f7b\u91cf\u200b\u6a21\u578b\u200b\u3001\u200b\u7aef\u4fa7\u200b\u90e8\u7f72\u200b \u200b\u8f7b\u91cf\u7ea7\u200b\u8f66\u724c\u200b\u8bc6\u522b\u200b \u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b <ul> <li>\u200b\u66f4\u200b\u591a\u200b\u5236\u9020\u200b\u3001\u200b\u91d1\u878d\u200b\u3001\u200b\u4ea4\u901a\u200b\u884c\u4e1a\u200b\u7684\u200b\u4e3b\u8981\u200b OCR \u200b\u5782\u7c7b\u200b\u5e94\u7528\u200b\u6a21\u578b\u200b\uff08\u200b\u5982\u200b\u7535\u8868\u200b\u3001\u200b\u6db2\u6676\u5c4f\u200b\u3001\u200b\u9ad8\u7cbe\u5ea6\u200b SVTR \u200b\u6a21\u578b\u200b\u7b49\u200b\uff09\uff0c\u200b\u53ef\u200b\u53c2\u8003\u200b\u573a\u666f\u200b\u5e94\u7528\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b</li> </ul>"},{"location":"en/version2.x/model/hardware/install_other_devices.html","title":"\u591a\u200b\u786c\u4ef6\u200b\u5b89\u88c5\u200b\u98de\u6868","text":"<p>\u200b\u672c\u200b\u6587\u6863\u200b\u4e3b\u8981\u200b\u9488\u5bf9\u200b\u6607\u200b\u817e\u200b NPU \u200b\u786c\u4ef6\u5e73\u53f0\u200b\uff0c\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u5b89\u88c5\u200b\u98de\u6868\u200b\u3002</p>"},{"location":"en/version2.x/model/hardware/install_other_devices.html#1-npu","title":"1. \u200b\u6607\u200b\u817e\u200b NPU \u200b\u98de\u6868\u200b\u5b89\u88c5","text":""},{"location":"en/version2.x/model/hardware/install_other_devices.html#11","title":"1.1 \u200b\u73af\u5883\u200b\u51c6\u5907","text":"<p>\u200b\u5f53\u524d\u200b PaddleOCR \u200b\u652f\u6301\u200b\u6607\u200b\u817e\u200b 910B \u200b\u82af\u7247\u200b\uff0c\u200b\u6607\u200b\u817e\u200b\u9a71\u52a8\u200b\u7248\u672c\u200b\u4e3a\u200b 23.0.3\u3002\u200b\u8003\u8651\u200b\u5230\u200b\u73af\u5883\u200b\u5dee\u5f02\u6027\u200b\uff0c\u200b\u6211\u4eec\u200b\u63a8\u8350\u200b\u4f7f\u7528\u200b\u98de\u6868\u200b\u5b98\u65b9\u200b\u63d0\u4f9b\u200b\u7684\u200b\u6807\u51c6\u200b\u955c\u50cf\u200b\u5b8c\u6210\u200b\u73af\u5883\u200b\u51c6\u5907\u200b\u3002</p>"},{"location":"en/version2.x/model/hardware/install_other_devices.html#_2","title":"\u62c9\u53d6\u200b\u955c\u50cf","text":"<p>\u200b\u6b64\u200b\u955c\u50cf\u200b\u4ec5\u4e3a\u200b\u5f00\u53d1\u200b\u73af\u5883\u200b\uff0c\u200b\u955c\u50cf\u200b\u4e2d\u200b\u4e0d\u200b\u5305\u542b\u200b\u9884\u200b\u7f16\u8bd1\u200b\u7684\u200b\u98de\u6868\u200b\u5b89\u88c5\u5305\u200b\uff0c\u200b\u955c\u50cf\u200b\u4e2d\u200b\u5df2\u7ecf\u200b\u9ed8\u8ba4\u200b\u5b89\u88c5\u200b\u4e86\u200b\u6607\u200b\u817e\u200b\u7b97\u5b50\u200b\u5e93\u200b CANN-8.0.RC1\u3002</p> <pre><code># \u200b\u9002\u7528\u200b\u4e8e\u200b X86 \u200b\u67b6\u6784\u200b\uff0c\u200b\u6682\u65f6\u200b\u4e0d\u200b\u63d0\u4f9b\u200b Arch64 \u200b\u67b6\u6784\u200b\u955c\u50cf\u200b\ndocker pull registry.baidubce.com/device/paddle-npu:cann80RC1-ubuntu20-x86_64-gcc84-py39\n</code></pre>"},{"location":"en/version2.x/model/hardware/install_other_devices.html#_3","title":"\u542f\u52a8\u200b\u5bb9\u5668","text":"<p>ASCEND_RT_VISIBLE_DEVICES \u200b\u6307\u5b9a\u200b\u53ef\u89c1\u200b\u7684\u200b NPU \u200b\u5361\u53f7\u200b</p> <pre><code>docker run -it --name paddle-npu-dev -v $(pwd):/work \\\n    --privileged --network=host --shm-size=128G -w=/work \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -e ASCEND_RT_VISIBLE_DEVICES=\"0,1,2,3,4,5,6,7\" \\\n    registry.baidubce.com/device/paddle-npu:cann80RC1-ubuntu20-x86_64-gcc84-py39 /bin/bash\n</code></pre>"},{"location":"en/version2.x/model/hardware/install_other_devices.html#12-paddle","title":"1.2 \u200b\u5b89\u88c5\u200b paddle \u200b\u5305","text":"<p>\u200b\u5f53\u524d\u200b\u63d0\u4f9b\u200b Python3.9 \u200b\u7684\u200b wheel \u200b\u5b89\u88c5\u5305\u200b\u3002\u200b\u5982\u200b\u6709\u200b\u5176\u4ed6\u200b Python \u200b\u7248\u672c\u200b\u9700\u6c42\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u98de\u6868\u200b\u5b98\u65b9\u200b\u6587\u6863\u200b\u81ea\u884c\u200b\u7f16\u8bd1\u200b\u5b89\u88c5\u200b\u3002</p>"},{"location":"en/version2.x/model/hardware/install_other_devices.html#1-python39-wheel","title":"1. \u200b\u4e0b\u8f7d\u5b89\u88c5\u200b Python3.9 \u200b\u7684\u200b wheel \u200b\u5b89\u88c5\u5305","text":"<pre><code># \u200b\u6ce8\u610f\u200b\u9700\u8981\u200b\u5148\u200b\u5b89\u88c5\u200b\u98de\u6868\u200b cpu \u200b\u7248\u672c\u200b\npip install https://paddle-model-ecology.bj.bcebos.com/paddlex/whl/paddle-device/npu/paddlepaddle-0.0.0-cp39-cp39-linux_x86_64.whl\npip install https://paddle-model-ecology.bj.bcebos.com/paddlex/whl/paddle-device/npu/paddle_custom_npu-0.0.0-cp39-cp39-linux_x86_64.whl\n</code></pre>"},{"location":"en/version2.x/model/hardware/install_other_devices.html#2","title":"2. \u200b\u9a8c\u8bc1\u200b\u5b89\u88c5\u5305","text":"<p>\u200b\u5b89\u88c5\u200b\u5b8c\u6210\u200b\u4e4b\u540e\u200b\uff0c\u200b\u8fd0\u884c\u200b\u5982\u4e0b\u200b\u547d\u4ee4\u200b\u3002</p> <pre><code>python -c \"import paddle; paddle.utils.run_check()\"\n</code></pre> <p>\u200b\u9884\u671f\u200b\u5f97\u5230\u200b\u5982\u4e0b\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b</p> <pre><code>Running verify PaddlePaddle program ...\nPaddlePaddle works well on 1 npu.\nPaddlePaddle works well on 8 npus.\nPaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\n</code></pre>"},{"location":"en/version2.x/model/hardware/supported_models.html","title":"PaddleOCR\u200b\u6a21\u578b\u200b\u5217\u8868","text":"<p>\u200b\u591a\u200b\u786c\u4ef6\u200b\u5b89\u88c5\u200b\u65b9\u5f0f\u200b\u8bf7\u200b\u53c2\u8003\u200b\u591a\u200b\u786c\u4ef6\u200b\u5b89\u88c5\u200b\u6587\u6863\u200b</p> \u200b\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u6607\u200b\u817e\u200bNPU PP-OCRv4 \u221a"},{"location":"en/version2.x/ppocr/environment.html","title":"Environment Preparation","text":"<p>Windows and Mac users are recommended to use Anaconda to build a Python environment, and Linux users are recommended to use docker to build a Python environment.</p> <p>Recommended working environment:</p> <ul> <li>PaddlePaddle &gt;= 2.1.2</li> <li>Python 3</li> <li>CUDA 10.1 / CUDA 10.2</li> <li>cuDNN 7.6</li> </ul> <p>If you already have a Python environment installed, you can skip to PaddleOCR Quick Start.</p>"},{"location":"en/version2.x/ppocr/environment.html#1-python-environment-setup","title":"1. Python Environment Setup","text":""},{"location":"en/version2.x/ppocr/environment.html#11-windows","title":"1.1 Windows","text":""},{"location":"en/version2.x/ppocr/environment.html#111-install-anaconda","title":"1.1.1 Install Anaconda","text":"<ul> <li> <p>Note: To use PaddlePaddle you need to install python environment first, here we choose python integrated environment Anaconda toolkit</p> </li> <li> <p>Anaconda is a common python package manager</p> </li> <li> <p>After installing Anaconda, you can install the python environment, as well as numpy and other required toolkit environment.</p> </li> <li> <p>Anaconda download.</p> </li> <li> <p>Address: https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=M&amp;O=D</p> </li> <li> <p>Most Win10 computers are 64-bit operating systems, choose x86_64 version; if the computer is a 32-bit operating system, choose x86.exe</p> <p></p> </li> <li> <p>After the download is complete, double-click the installer to enter the graphical interface</p> </li> <li> <p>The default installation location is C drive, it is recommended to change the installation location to D drive.</p> <p></p> </li> <li> <p>Check Conda to add environment variables and ignore the warning that     </p> </li> </ul>"},{"location":"en/version2.x/ppocr/environment.html#112-opening-the-terminal-and-creating-the-conda-environment","title":"1.1.2 Opening the terminal and creating the Conda environment","text":"<ul> <li>Open Anaconda Prompt terminal: bottom left Windows Start Menu -&gt; Anaconda3 -&gt; Anaconda Prompt start console</li> </ul> <ul> <li>Create a new Conda environment</li> </ul> <pre><code># Enter the following command at the command line to create an environment named paddle_env\n# Here to speed up the download, use the Tsinghua source\nconda create --name paddle_env python=3.8 --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ # This is a one line command\n</code></pre> <p>This command will create an executable environment named paddle_env with python version 3.8, which will take a while depending on the network status</p> <p>The command line will then output a prompt, type y and enter to continue the installation</p> <p></p> <ul> <li>To activate the Conda environment you just created, enter the following command at the command line.</li> </ul> <pre><code># Activate the paddle_env environment\nconda activate paddle_env\n# View the current location of python\nwhere python\n</code></pre> <p></p> <p>The above anaconda environment and python environment are installed</p>"},{"location":"en/version2.x/ppocr/environment.html#12-mac","title":"1.2 Mac","text":""},{"location":"en/version2.x/ppocr/environment.html#121-installing-anaconda","title":"1.2.1 Installing Anaconda","text":"<ul> <li> <p>Note: To use PaddlePaddle you need to install the python environment first, here we choose the python integrated environment Anaconda toolkit</p> </li> <li> <p>Anaconda is a common python package manager</p> </li> <li> <p>After installing Anaconda, you can install the python environment, as well as numpy and other required toolkit environment</p> </li> <li> <p>Anaconda download:.</p> </li> <li> <p>Address: https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=M&amp;O=D</p> </li> </ul> <p></p> <ul> <li> <p>Select <code>Anaconda3-2021.05-MacOSX-x86_64.pkg</code> at the bottom to download</p> </li> <li> <p>After downloading, double click on the .pkg file to enter the graphical interface</p> </li> <li> <p>Just follow the default settings, it will take a while to install</p> </li> <li> <p>It is recommended to install a code editor such as VSCode or PyCharm</p> </li> </ul>"},{"location":"en/version2.x/ppocr/environment.html#122-open-a-terminal-and-create-a-conda-environment","title":"1.2.2 Open a terminal and create a Conda environment","text":"<ul> <li> <p>Open the terminal</p> </li> <li> <p>Press command and spacebar at the same time, type \"terminal\" in the focus search, double click to enter terminal</p> </li> <li> <p>Add Conda to the environment variables</p> </li> <li> <p>Environment variables are added so that the system can recognize the Conda command</p> </li> <li> <p>Open <code>~/.bash_profile</code> in the terminal by typing the following command.</p> <pre><code>vim ~/.bash_profile\n</code></pre> </li> <li> <p>Add Conda as an environment variable in <code>~/.bash_profile</code>.</p> <pre><code># Press i first to enter edit mode\n# In the first line type.\nexport PATH=\"~/opt/anaconda3/bin:$PATH\"\n# If you customized the installation location during installation, change ~/opt/anaconda3/bin to the bin folder in the customized installation directory\n\n# The modified ~/.bash_profile file should look like this (where xxx is the username)\nexport PATH=\"~/opt/anaconda3/bin:$PATH\"\n# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !!! Contents within this block are managed by 'conda init' !!!\n__conda_setup=\"$('/Users/xxx/opt/anaconda3/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n        eval \"$__conda_setup\"\nelse\n        if [ -f \"/Users/xxx/opt/anaconda3/etc/profile.d/conda.sh\" ]; then\n                . \"/Users/xxx/opt/anaconda3/etc/profile.d/conda.sh\"\n        else\n                export PATH=\"/Users/xxx/opt/anaconda3/bin:$PATH\"\n        fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n</code></pre> <ul> <li>When you are done, press <code>esc</code> to exit edit mode, then type <code>:wq!</code> and enter to save and exit</li> </ul> </li> <li> <p>Verify that the Conda command is recognized.</p> <ul> <li>Enter <code>source ~/.bash_profile</code> in the terminal to update the environment variables</li> <li>Enter <code>conda info --envs</code> in the terminal again, if it shows that there is a base environment, then Conda has been added to the environment variables</li> </ul> </li> <li> <p>Create a new Conda environment</p> </li> </ul> <pre><code># Enter the following command at the command line to create an environment called paddle_env\n# Here to speed up the download, use Tsinghua source\nconda create --name paddle_env python=3.8 --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n</code></pre> <ul> <li> <p>This command will create an executable environment named paddle_env with python version 3.8, which will take a while depending on the network status</p> </li> <li> <p>The command line will then output a prompt, type y and enter to continue the installation</p> <p></p> </li> <li> <p>To activate the Conda environment you just created, enter the following command at the command line.</p> </li> </ul> <pre><code># Activate the paddle_env environment\nconda activate paddle_env\n# View the current location of python\nwhere python\n</code></pre> <p></p> <p>The above anaconda environment and python environment are installed</p>"},{"location":"en/version2.x/ppocr/environment.html#13-linux","title":"1.3 Linux","text":"<p>Linux users can choose to run either Anaconda or Docker. If you are familiar with Docker and need to train the PaddleOCR model, it is recommended to use the Docker environment, where the development process of PaddleOCR is run. If you are not familiar with Docker, you can also use Anaconda to run the project.</p>"},{"location":"en/version2.x/ppocr/environment.html#131-anaconda-environment-configuration","title":"1.3.1 Anaconda environment configuration","text":"<ul> <li> <p>Note: To use PaddlePaddle you need to install the python environment first, here we choose the python integrated environment Anaconda toolkit</p> </li> <li> <p>Anaconda is a common python package manager</p> </li> <li> <p>After installing Anaconda, you can install the python environment, as well as numpy and other required toolkit environment</p> </li> <li> <p>Download Anaconda.</p> </li> <li> <p>Download at: https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=M&amp;O=D</p> </li> </ul> <p></p> <ul> <li> <p>Select the appropriate version for your operating system</p> <ul> <li>Type <code>uname -m</code> in the terminal to check the command set used by your system</li> </ul> </li> <li> <p>Download method 1: Download locally, then transfer the installation package to the Linux server</p> </li> <li> <p>Download method 2: Directly use Linux command line to download</p> <pre><code># First install wget\nsudo apt-get install wget # Ubuntu\nsudo yum install wget # CentOS\n</code></pre> <pre><code># Then use wget to download from Tsinghua source\n# If you want to download Anaconda3-2021.05-Linux-x86_64.sh, the download command is as follows\nwget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2021.05-Linux-x86_64.sh\n# If you want to download another version, you need to change the file name after the last 1 / to the version you want to download\n</code></pre> </li> <li> <p>To install Anaconda.</p> </li> <li> <p>Type <code>sh Anaconda3-2021.05-Linux-x86_64.sh</code> at the command line</p> <ul> <li>If you downloaded a different version, replace the file name of the command with the name of the file you downloaded</li> </ul> </li> <li> <p>Just follow the installation instructions</p> <ul> <li>You can exit by typing q when viewing the license</li> </ul> </li> <li> <p>Add conda to the environment variables</p> </li> <li> <p>If you have already added conda to the environment variable path during the installation, you can skip this step</p> </li> <li> <p>Open <code>~/.bashrc</code> in a terminal.</p> <pre><code># Enter the following command in the terminal.\nvim ~/.bashrc\n</code></pre> </li> <li> <p>Add conda as an environment variable in <code>~/.bashrc</code>.</p> <pre><code># Press i first to enter edit mode # In the first line enter.\nexport PATH=\"~/anaconda3/bin:$PATH\"\n# If you customized the installation location during installation, change ~/anaconda3/bin to the bin folder in the customized installation directory\n</code></pre> <pre><code># The modified ~/.bash_profile file should look like this (where xxx is the username)\nexport PATH=\"~/opt/anaconda3/bin:$PATH\"\n# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !!! Contents within this block are managed by 'conda init' !!!\n__conda_setup=\"$('/Users/xxx/opt/anaconda3/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"/Users/xxx/opt/anaconda3/etc/profile.d/conda.sh\" ]; then\n        . \"/Users/xxx/opt/anaconda3/etc/profile.d/conda.sh\"\n    else\n        export PATH=\"/Users/xxx/opt/anaconda3/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n</code></pre> <ul> <li>When you are done, press <code>esc</code> to exit edit mode, then type <code>:wq!</code> and enter to save and exit</li> </ul> </li> <li> <p>Verify that the Conda command is recognized.</p> <ul> <li>Enter <code>source ~/.bash_profile</code> in the terminal to update the environment variables</li> <li>Enter <code>conda info --envs</code> in the terminal again, if it shows that there is a base environment, then Conda has been added to the environment variables</li> </ul> </li> <li> <p>Create a new Conda environment</p> </li> </ul> <pre><code># Enter the following command at the command line to create an environment called paddle_env\n# Here to speed up the download, use Tsinghua source\nconda create --name paddle_env python=3.8 --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n</code></pre> <ul> <li> <p>This command will create an executable environment named paddle_env with python version 3.8, which will take a while depending on the network status</p> </li> <li> <p>The command line will then output a prompt, type y and enter to continue the installation</p> <p></p> </li> <li> <p>To activate the Conda environment you just created, enter the following command at the command line.</p> </li> </ul> <pre><code># Activate the paddle_env environment\nconda activate paddle_env\n</code></pre> <p>The above anaconda environment and python environment are installed</p>"},{"location":"en/version2.x/ppocr/environment.html#132-docker-environment-preparation","title":"1.3.2 Docker environment preparation","text":"<p>The first time you use this docker image, it will be downloaded automatically. Please be patient.</p> <pre><code># Switch to the working directory\ncd /home/Projects\n# You need to create a docker container for the first run, and do not need to run the current command when you run it again\n# Create a docker container named ppocr and map the current directory to the /paddle directory of the container\n\n# If using CPU, use docker instead of nvidia-docker to create docker\nsudo docker run --name ppocr -v $PWD:/paddle --network=host -it  registry.baidubce.com/paddlepaddle/paddle:2.1.3-gpu-cuda10.2-cudnn7  /bin/bash\n\n# If using GPU, use nvidia-docker to create docker\n# docker image registry.baidubce.com/paddlepaddle/paddle:2.1.3-gpu-cuda11.2-cudnn8 is recommended for CUDA11.2 + CUDNN8.\nsudo nvidia-docker run --name ppocr -v $PWD:/paddle --shm-size=64G --network=host -it registry.baidubce.com/paddlepaddle/paddle:2.1.3-gpu-cuda10.2-cudnn7 /bin/bash\n</code></pre> <p>You can also visit DockerHub to get the image that fits your machine.</p> <pre><code># ctrl+P+Q to exit docker, to re-enter docker using the following command:\nsudo docker container exec -it ppocr /bin/bash\n</code></pre>"},{"location":"en/version2.x/ppocr/installation.html","title":"Installation","text":""},{"location":"en/version2.x/ppocr/installation.html#quick-installation","title":"Quick Installation","text":"<p>After testing, PaddleOCR can run on glibc 2.23. You can also test other glibc versions or install glibc 2.23 for the best compatibility.</p> <p>PaddleOCR working environment:</p> <ul> <li>PaddlePaddle &gt; 2.0.0</li> <li>Python 3</li> <li>glibc 2.23</li> </ul> <p>It is recommended to use the docker provided by us to run PaddleOCR. Please refer to the docker tutorial link.</p> <p>If you want to directly run the prediction code on Mac or Windows, you can start from step 2.</p>"},{"location":"en/version2.x/ppocr/installation.html#1-recommended-prepare-a-docker-environment","title":"1. (Recommended) Prepare a docker environment","text":"<p>For the first time you use this docker image, it will be downloaded automatically. Please be patient.</p> <pre><code># Switch to the working directory\ncd /home/Projects\n# You need to create a docker container for the first run, and do not need to run the current command when you run it again\n# Create a docker container named ppocr and map the current directory to the /paddle directory of the container\n\n#If using CPU, use docker instead of nvidia-docker to create docker\nsudo docker run --name ppocr -v $PWD:/paddle --network=host -it  paddlepaddle/paddle:latest-dev-cuda10.1-cudnn7-gcc82  /bin/bash\n</code></pre> <p>With CUDA10, please run the following command to create a container. It is recommended to set a shared memory greater than or equal to 32G through the --shm-size parameter:</p> <pre><code>sudo nvidia-docker run --name ppocr -v $PWD:/paddle --shm-size=64G --network=host -it paddlepaddle/paddle:latest-dev-cuda10.1-cudnn7-gcc82 /bin/bash\n</code></pre> <p>You can also visit DockerHub to get the image that fits your machine.</p> <pre><code># ctrl+P+Q to exit docker, to re-enter docker using the following command:\nsudo docker container exec -it ppocr /bin/bash\n</code></pre>"},{"location":"en/version2.x/ppocr/installation.html#2-install-paddlepaddle-20","title":"2. Install PaddlePaddle 2.0","text":"<pre><code>pip3 install --upgrade pip\n\n# If you have cuda9 or cuda10 installed on your machine, please run the following command to install\npython3 -m pip install paddlepaddle-gpu==2.0.0 -i https://mirror.baidu.com/pypi/simple\n\n# If you only have cpu on your machine, please run the following command to install\npython3 -m pip install paddlepaddle==2.0.0 -i https://mirror.baidu.com/pypi/simple\n</code></pre> <p>For more software version requirements, please refer to the instructions in Installation Document for operation.</p>"},{"location":"en/version2.x/ppocr/installation.html#3-clone-paddleocr-repo","title":"3. Clone PaddleOCR repo","text":"<pre><code># Recommend\ngit clone https://github.com/PaddlePaddle/PaddleOCR\n\n# If you cannot pull successfully due to network problems, you can switch to the mirror hosted on Gitee:\n\ngit clone https://gitee.com/paddlepaddle/PaddleOCR\n\n# Note: The mirror on Gitee may not keep in synchronization with the latest update with the project on GitHub. There might be a delay of 3-5 days. Please try GitHub at first.\n</code></pre>"},{"location":"en/version2.x/ppocr/installation.html#4-install-third-party-libraries","title":"4. Install third-party libraries","text":"<pre><code>cd PaddleOCR\npip3 install -r requirements.txt\n</code></pre> <p>If you getting this error <code>OSError: [WinError 126] The specified module could not be found</code> when you install shapely on windows.</p> <p>Please try to download Shapely whl file from http://www.lfd.uci.edu/~gohlke/pythonlibs/#shapely.</p> <p>Reference: Solve shapely installation on windows</p>"},{"location":"en/version2.x/ppocr/model_list.html","title":"OCR Model List\uff08V3, updated on 2022.4.28\uff09","text":"<p>Note</p> <ol> <li>Compared with model v2, the 3rd version of the detection model has an improvement in accuracy, and the 2.1 version of the recognition model has optimizations in accuracy and speed with CPU.</li> <li>Compared with models 1.1, which are trained with static graph programming paradigm, models 2.0 or higher are the dynamic graph trained version and achieve close performance.</li> <li>All models in this tutorial are from the PaddleOCR series, for more introduction to algorithms and models based on the public dataset, you can refer to algorithm overview tutorial.</li> </ol> <p>The downloadable models provided by PaddleOCR include the <code>inference model</code>, <code>trained model</code>, <code>pre-trained model</code> and <code>nb model</code>. The differences between the models are as follows:</p> model type model format description inference model inference.pdmodel\u3001inference.pdiparams Used for inference based on Paddle inference engine\uff0cdetail trained model, pre-trained model *.pdparams\u3001*.pdopt\u3001*.states The checkpoints model saved in the training process, which stores the parameters of the model, is mostly used for model evaluation and continuous training. nb model *.nb Model optimized by Paddle-Lite, which is suitable for mobile-side deployment scenarios (Paddle-Lite is needed for nb model deployment). <p>The relationship of the above models is as follows.</p> <p></p>"},{"location":"en/version2.x/ppocr/model_list.html#1-text-detection-model","title":"1. Text Detection Model","text":""},{"location":"en/version2.x/ppocr/model_list.html#1-chinese-detection-model","title":"1. Chinese Detection Model","text":"model name description config model size download PP-OCRv4_mobile_det [New] Original lightweight model, supporting Chinese, English, multilingual text detection PP-OCRv4_mobile_det.yml 4.70M inference model / trained model PP-OCRv4_server_det [New] Original high-precision model, supporting Chinese, English, multilingual text detection PP-OCRv4_server_det.yml 110M inference model / trained model PP-OCRv3_mobile_det Original lightweight model, supporting Chinese, English, multilingual text detection PP-OCRv3_mobile_det.yml 1.1M inference model / trained model / nb\u200b\u6a21\u578b\u200b PP-OCRv3_server_det Original high-precision model, supporting Chinese, English, multilingual text detection PP-OCRv3_server_det.yml 3.80M inference model / trained model"},{"location":"en/version2.x/ppocr/model_list.html#12-english-detection-model","title":"1.2 English Detection Model","text":"model name description config model size download en_PP-OCRv3_det_slim [New] Slim quantization with distillation lightweight detection model, supporting English PP-OCRv3_det_cml.yml 1.1M inference model / trained model / nb model en_PP-OCRv3_det [New] Original lightweight detection model, supporting English PP-OCRv3_det_cml.yml 3.8M inference model / trained model <ul> <li>Note: English configuration file is the same as Chinese except for training data, here we only provide one configuration file.</li> </ul>"},{"location":"en/version2.x/ppocr/model_list.html#13-multilingual-detection-model","title":"1.3 Multilingual Detection Model","text":"model name description config model size download ml_PP-OCRv3_det_slim [New] Slim quantization with distillation lightweight detection model, supporting English PP-OCRv3_det_cml.yml 1.1M inference model / trained model / nb model ml_PP-OCRv3_det [New] Original lightweight detection model, supporting English PP-OCRv3_det_cml.yml 3.8M inference model / trained model <ul> <li>Note: English configuration file is the same as Chinese except for training data, here we only provide one configuration file.</li> </ul>"},{"location":"en/version2.x/ppocr/model_list.html#2-text-recognition-model","title":"2. Text Recognition Model","text":""},{"location":"en/version2.x/ppocr/model_list.html#21-chinese-recognition-model","title":"2.1 Chinese Recognition Model","text":"model name description config model size download PP-OCRv3_mobile_rec_slim [New] Slim quantization with distillation lightweight model, supporting Chinese, English text recognition PP-OCRv3_mobile_rec_distillation.yml 4.9M inference model / trained model / nb model PP-OCRv3_mobile_rec [New] Original lightweight model, supporting Chinese, English, multilingual text recognition PP-OCRv3_mobile_rec_distillation.yml 12.4M inference model / trained model ch_PP-OCRv2_rec_slim Slim quantization with distillation lightweight model, supporting Chinese, English text recognition ch_PP-OCRv2_rec.yml 9.0M inference model / trained model ch_PP-OCRv2_rec Original lightweight model, supporting Chinese, English, and multilingual text recognition ch_PP-OCRv2_rec_distillation.yml 8.5M inference model / trained model ch_ppocr_mobile_slim_v2.0_rec Slim pruned and quantized lightweight model, supporting Chinese, English and number recognition rec_chinese_lite_train_v2.0.yml 6.0M inference model / trained model ch_ppocr_mobile_v2.0_rec Original lightweight model, supporting Chinese, English and number recognition rec_chinese_lite_train_v2.0.yml 5.2M inference model / trained model / pre-trained model ch_ppocr_server_v2.0_rec General model, supporting Chinese, English and number recognition rec_chinese_common_train_v2.0.yml 94.8M inference model / trained model / pre-trained model <p>Note: The <code>trained model</code> is fine-tuned on the <code>pre-trained model</code> with real data and synthesized vertical text data, which achieved better performance in the real scene. The <code>pre-trained model</code> is directly trained on the full amount of real data and synthesized data, which is more suitable for fine-tuning your dataset.</p>"},{"location":"en/version2.x/ppocr/model_list.html#22-english-recognition-model","title":"2.2 English Recognition Model","text":"model name description config model size download en_PP-OCRv3_mobile_rec_slim [New] Slim quantization with distillation lightweight model, supporting English, English text recognition en_PP-OCRv3_mobile_rec.yml 3.2M inference model / trained model / nb model en_PP-OCRv3_mobile_rec [New] Original lightweight model, supporting English, English, multilingual text recognition en_PP-OCRv3_mobile_rec.yml 9.6M inference model / trained model en_number_mobile_slim_v2.0_rec Slim pruned and quantized lightweight model, supporting English and number recognition rec_en_number_lite_train.yml 2.7M inference model / trained model en_number_mobile_v2.0_rec Original lightweight model, supporting English and number recognition rec_en_number_lite_train.yml 2.6M inference model / trained model <p>Note: Dictionary file of all English recognition models is <code>ppocr/utils/en_dict.txt</code>.</p>"},{"location":"en/version2.x/ppocr/model_list.html#23-multilingual-recognition-modelupdating","title":"2.3 Multilingual Recognition Model\uff08Updating...\uff09","text":"model name dict file description config model size download korean_PP-OCRv3_mobile_rec ppocr/utils/dict/korean_dict.txt Lightweight model for Korean recognition korean_PP-OCRv3_mobile_rec.yml 11.0M inference model / trained model japan_PP-OCRv3_mobile_rec ppocr/utils/dict/japan_dict.txt Lightweight model for Japanese recognition japan_PP-OCRv3_mobile_rec.yml 11.0M inference model / trained model chinese_cht_PP-OCRv3_mobile_rec ppocr/utils/dict/chinese_cht_dict.txt Lightweight model for chinese cht chinese_cht_PP-OCRv3_mobile_rec.yml 12.0M inference model / trained model te_PP-OCRv3_mobile_rec ppocr/utils/dict/te_dict.txt Lightweight model for Telugu recognition te_PP-OCRv3_mobile_rec.yml 9.6M inference model / trained model ka_PP-OCRv3_mobile_rec ppocr/utils/dict/ka_dict.txt Lightweight model for Kannada recognition ka_PP-OCRv3_mobile_rec.yml 9.9M inference model / trained model ta_PP-OCRv3_mobile_rec ppocr/utils/dict/ta_dict.txt Lightweight model for Tamil recognition ta_PP-OCRv3_mobile_rec.yml 9.6M inference model / trained model latin_PP-OCRv3_mobile_rec ppocr/utils/dict/latin_dict.txt Lightweight model for latin recognition latin_PP-OCRv3_mobile_rec.yml 9.7M inference model / trained model arabic_PP-OCRv3_mobile_rec ppocr/utils/dict/arabic_dict.txt Lightweight model for arabic recognition arabic_PP-OCRv3_mobile_rec.yml 9.6M inference model / trained model cyrillic_PP-OCRv3_mobile_rec ppocr/utils/dict/cyrillic_dict.txt Lightweight model for cyrillic recognition cyrillic_PP-OCRv3_mobile_rec.yml 9.6M inference model / trained model devanagari_PP-OCRv3_mobile_rec ppocr/utils/dict/devanagari_dict.txt Lightweight model for devanagari recognition devanagari_PP-OCRv3_mobile_rec.yml 9.9M inference model / trained model <p>For a complete list of languages \u200b\u200band tutorials, please refer to Multi-language model</p>"},{"location":"en/version2.x/ppocr/model_list.html#3-text-angle-classification-model","title":"3. Text Angle Classification Model","text":"model name description config model size download ch_ppocr_mobile_slim_v2.0_cls Slim quantized model for text angle classification cls_mv3.yml 2.1M inference model / trained model / nb model ch_ppocr_mobile_v2.0_cls Original model for text angle classification cls_mv3.yml 1.38M inference model / trained model"},{"location":"en/version2.x/ppocr/model_list.html#4-paddle-lite-model","title":"4. Paddle-Lite Model","text":"<p>Paddle Lite is an updated version of Paddle-Mobile, an open-open source deep learning framework designed to make it easy to perform inference on mobile, embedded, and IoT devices. It can further optimize the inference model and generate the <code>nb model</code> used for edge devices. It's suggested to optimize the quantization model using Paddle-Lite because the <code>INT8</code> format is used for the model storage and inference.</p> <p>This chapter lists OCR nb models with PP-OCRv2 or earlier versions. You can access the latest nb models from the above tables.</p> Version Introduction Model size Detection model Text Direction model Recognition model Paddle-Lite branch PP-OCRv2 extra-lightweight chinese OCR optimized model 11.0M download link download link download link v2.10 PP-OCRv2(slim) extra-lightweight chinese OCR optimized model 4.6M download link download link download link v2.10 PP-OCRv2 extra-lightweight chinese OCR optimized model 11.0M download link download link download link v2.9 PP-OCRv2(slim) extra-lightweight chinese OCR optimized model 4.9M download link download link download link v2.9 V2.0 ppocr_v2.0 extra-lightweight chinese OCR optimized model 7.8M download link download link download link v2.9 V2.0(slim) ppocr_v2.0 extra-lightweight chinese OCR optimized model 3.3M download link download link download link v2.9"},{"location":"en/version2.x/ppocr/overview.html","title":"PP-OCR","text":""},{"location":"en/version2.x/ppocr/overview.html#1-introduction","title":"1. Introduction","text":"<p>PP-OCR is a self-developed practical ultra-lightweight OCR system, which is slimed and optimized based on the reimplemented academic algorithms, considering the balance between accuracy and speed.</p>"},{"location":"en/version2.x/ppocr/overview.html#pp-ocr_1","title":"PP-OCR","text":"<p>PP-OCR is a two-stage OCR system, in which the text detection algorithm is DB, and the text recognition algorithm is CRNN. Besides, a text direction classifier is added between the detection and recognition modules to deal with text in different directions.</p> <p>PP-OCR pipeline is as follows:</p> <p></p> <p>PP-OCR system is in continuous optimization. At present, PP-OCR and PP-OCRv2 have been released:</p> <p>PP-OCR adopts 19 effective strategies from 8 aspects including backbone network selection and adjustment, prediction head design, data augmentation, learning rate transformation strategy, regularization parameter selection, pre-training model use, and automatic model tailoring and quantization to optimize and slim down the models of each module (as shown in the green box above). The final results are an ultra-lightweight Chinese and English OCR model with an overall size of 3.5M and a 2.8M English digital OCR model. For more details, please refer to PP-OCR technical report.</p>"},{"location":"en/version2.x/ppocr/overview.html#pp-ocrv2","title":"PP-OCRv2","text":"<p>On the basis of PP-OCR, PP-OCRv2 is further optimized in five aspects. The detection model adopts CML(Collaborative Mutual Learning) knowledge distillation strategy and CopyPaste data expansion strategy. The recognition model adopts LCNet lightweight backbone network, U-DML knowledge distillation strategy and enhanced CTC loss function improvement (as shown in the red box above), which further improves the inference speed and prediction effect. For more details, please refer to PP-OCRv2 technical report.</p>"},{"location":"en/version2.x/ppocr/overview.html#pp-ocrv3","title":"PP-OCRv3","text":"<p>PP-OCRv3 upgraded the detection model and recognition model in 9 aspects based on PP-OCRv2:</p> <ul> <li>PP-OCRv3 detector upgrades the CML(Collaborative Mutual Learning) text detection strategy proposed in PP-OCRv2, and further optimizes the effect of teacher model and student model respectively. In the optimization of teacher model, a PAN module with large receptive field named LK-PAN is proposed and the DML distillation strategy is adopted; In the optimization of student model, a FPN module with residual attention mechanism named RSE-FPN is proposed.</li> <li>PP-OCRv3 recognizer is optimized based on text recognition algorithm SVTR. SVTR no longer adopts RNN by introducing transformers structure, which can mine the context information of text line image more effectively, so as to improve the ability of text recognition. PP-OCRv3 adopts lightweight text recognition network SVTR_LCNet, guided training of CTC by attention, data augmentation strategy TextConAug, better pre-trained model by self-supervised TextRotNet, UDML(Unified Deep Mutual Learning), and UIM (Unlabeled Images Mining) to accelerate the model and improve the effect.</li> </ul> <p>PP-OCRv3 pipeline is as follows:</p> <p></p> <p>For more details, please refer to PP-OCRv3 technical report.</p>"},{"location":"en/version2.x/ppocr/overview.html#2-features","title":"2. Features","text":"<ul> <li>Ultra lightweight PP-OCRv3 series models: detection (3.6M) + direction classifier (1.4M) + recognition 12M) = 17.0M</li> <li>Ultra lightweight PP-OCRv2 series models: detection (3.1M) + direction classifier (1.4M) + recognition 8.5M) = 13.0M</li> <li>Ultra lightweight PP-OCR mobile series models: detection (3.0M) + direction classifier (1.4M) + recognition (5.0M) = 9.4M</li> <li>General PP-OCR server series models: detection (47.1M) + direction classifier (1.4M) + recognition (94.9M) = 143.4M</li> <li>Support Chinese, English, and digit recognition, vertical text recognition, and long text recognition</li> <li>Support multi-lingual recognition: about 80 languages like Korean, Japanese, German, French, etc</li> </ul>"},{"location":"en/version2.x/ppocr/overview.html#3-benchmark","title":"3. Benchmark","text":"<p>For the performance comparison between PP-OCR series models, please check the benchmark documentation.</p>"},{"location":"en/version2.x/ppocr/overview.html#4-visualization-more","title":"4. Visualization more","text":""},{"location":"en/version2.x/ppocr/overview.html#pp-ocrv3-chinese-model","title":"PP-OCRv3 Chinese model","text":""},{"location":"en/version2.x/ppocr/overview.html#pp-ocrv3-english-model","title":"PP-OCRv3 English model","text":""},{"location":"en/version2.x/ppocr/overview.html#pp-ocrv3-multilingual-model","title":"PP-OCRv3 Multilingual model","text":""},{"location":"en/version2.x/ppocr/overview.html#5-tutorial","title":"5. Tutorial","text":""},{"location":"en/version2.x/ppocr/overview.html#51-quick-start","title":"5.1 Quick start","text":"<ul> <li>You can also quickly experience the ultra-lightweight OCR : Online Experience</li> <li>Mobile DEMO experience (based on EasyEdge and Paddle-Lite, supports iOS and Android systems): Sign in to the website to obtain the QR code for  installing the App</li> <li>One line of code quick use: Quick Start</li> </ul>"},{"location":"en/version2.x/ppocr/overview.html#52-model-training-compression-deployment","title":"5.2 Model training / compression / deployment","text":"<p>For more tutorials, including model training, model compression, deployment, etc., please refer to tutorials.</p>"},{"location":"en/version2.x/ppocr/overview.html#6-model-zoo","title":"6. Model zoo","text":""},{"location":"en/version2.x/ppocr/overview.html#pp-ocr-series-model-listupdate-on-20220428","title":"PP-OCR Series Model List\uff08Update on 2022.04.28\uff09","text":"Model introduction Model name Recommended scene Detection model Direction classifier Recognition model Chinese and English ultra-lightweight PP-OCRv3 model\uff0816.2M\uff09 ch_PP-OCRv3_xx Mobile &amp; Server inference model / trained model inference model / trained model inference model / trained model English ultra-lightweight PP-OCRv3 model\uff0813.4M\uff09 en_PP-OCRv3_xx Mobile &amp; Server inference model / trained model inference model / trained model inference model / trained model Chinese and English ultra-lightweight PP-OCRv2 model\uff0811.6M\uff09 ch_PP-OCRv2_xx Mobile &amp; Server inference model / trained model inference model / trained model inference model / trained model Chinese and English ultra-lightweight PP-OCR model (9.4M) ch_ppocr_mobile_v2.0_xx Mobile &amp; server inference model / trained model inference model / trained model inference model / trained model Chinese and English general PP-OCR model (143.4M) ch_ppocr_server_v2.0_xx Server inference model / trained model inference model / trained model inference model / trained model <p>For more model downloads (including multiple languages), please refer to PP-OCR series model downloads.</p>"},{"location":"en/version2.x/ppocr/quick_start.html","title":"PaddleOCR Quick Start","text":"<p>Note: This tutorial mainly introduces the usage of PP-OCR series models, please refer to PP-Structure Quick Start for the quick use of document analysis related functions. In addition, the All-in-One development tool PaddleX relies on the advanced technology of PaddleOCR to support low-code full-process development capabilities in the OCR field, significantly reducing development time and complexity. It also integrates the 17 models involved in text image intelligent analysis, OCR, layout parsing, table recognition, formula recognition, and seal text recognition into 6 pipelines, which can be invoked with a simple Python API. For more details, please see Low-Code Full-Process Development.</p>"},{"location":"en/version2.x/ppocr/quick_start.html#1-installation","title":"1. Installation","text":""},{"location":"en/version2.x/ppocr/quick_start.html#11-install-paddlepaddle","title":"1.1 Install PaddlePaddle","text":"<p>If you do not have a Python environment, please refer to Environment Preparation.</p> <ul> <li>If you have CUDA 11 installed on your machine, please run the following command to install</li> </ul> <pre><code>pip install \"paddlepaddle-gpu&lt;=2.6\"\n</code></pre> <ul> <li>If you have no available GPU on your machine, please run the following command to install the CPU version</li> </ul> <pre><code>python -m pip install \"paddlepaddle&lt;=2.6\"\n</code></pre> <p>For more software version requirements, please refer to the instructions in Installation Document for operation.</p>"},{"location":"en/version2.x/ppocr/quick_start.html#12-install-paddleocr-whl-package","title":"1.2 Install PaddleOCR Whl Package","text":"<pre><code>pip install \"paddleocr&gt;=2.0.1\" # Recommend to use version 2.0.1+\n</code></pre> <ul> <li>For windows users: If you getting this error <code>OSError: [WinError 126] The specified module could not be found</code> when you install shapely on windows. Please try to download Shapely whl file here.</li> </ul> <p>Reference: Solve shapely installation on windows</p>"},{"location":"en/version2.x/ppocr/quick_start.html#2-easy-to-use","title":"2. Easy-to-Use","text":""},{"location":"en/version2.x/ppocr/quick_start.html#21-use-by-command-line","title":"2.1 Use by Command Line","text":"<p>PaddleOCR provides a series of test images, click here to download, and then switch to the corresponding directory in the terminal</p> <pre><code>cd /path/to/ppocr_img\n</code></pre> <p>If you do not use the provided test image, you can replace the following <code>--image_dir</code> parameter with the corresponding test image path</p>"},{"location":"en/version2.x/ppocr/quick_start.html#211-chinese-and-english-model","title":"2.1.1 Chinese and English Model","text":"<ul> <li>Detection, direction classification and recognition: set the parameter<code>--use_gpu false</code> to disable the gpu device</li> </ul> <pre><code>paddleocr --image_dir ./imgs_en/img_12.jpg --use_angle_cls true --lang en --use_gpu false\n</code></pre> <p>Output will be a list, each item contains bounding box, text and recognition confidence</p> <pre><code>[[[441.0, 174.0], [1166.0, 176.0], [1165.0, 222.0], [441.0, 221.0]], ('ACKNOWLEDGEMENTS', 0.9971134662628174)]\n[[[403.0, 346.0], [1204.0, 348.0], [1204.0, 384.0], [402.0, 383.0]], ('We would like to thank all the designers and', 0.9761400818824768)]\n[[[403.0, 396.0], [1204.0, 398.0], [1204.0, 434.0], [402.0, 433.0]], ('contributors who have been involved in the', 0.9791957139968872)]\n......\n</code></pre> <p>pdf file is also supported, you can infer the first few pages by using the <code>page_num</code> parameter, the default is 0, which means infer all pages</p> <pre><code>paddleocr --image_dir ./xxx.pdf --use_angle_cls true --use_gpu false --page_num 2\n</code></pre> <ul> <li>Only detection: set <code>--rec</code> to <code>false</code></li> </ul> <pre><code>paddleocr --image_dir ./imgs_en/img_12.jpg --rec false\n</code></pre> <p>Output will be a list, each item only contains bounding box</p> <pre><code>[[397.0, 802.0], [1092.0, 802.0], [1092.0, 841.0], [397.0, 841.0]]\n[[397.0, 750.0], [1211.0, 750.0], [1211.0, 789.0], [397.0, 789.0]]\n[[397.0, 702.0], [1209.0, 698.0], [1209.0, 734.0], [397.0, 738.0]]\n......\n</code></pre> <ul> <li>Only recognition: set <code>--det</code> to <code>false</code></li> </ul> <pre><code>paddleocr --image_dir ./imgs_words_en/word_10.png --det false --lang en\n</code></pre> <p>Output will be a list, each item contains text and recognition confidence</p> <pre><code>['PAIN', 0.9934559464454651]\n</code></pre> <p>Version paddleocr uses the PP-OCRv4 model by default(<code>--ocr_version PP-OCRv4</code>). If you want to use other versions, you can set the parameter <code>--ocr_version</code>, the specific version description is as follows:</p> version name description PP-OCRv4 support Chinese and English detection and recognition, direction classifier, support multilingual recognition PP-OCRv3 support Chinese and English detection and recognition, direction classifier, support multilingual recognition PP-OCRv2 only supports Chinese and English detection and recognition, direction classifier, multilingual model is not updated PP-OCR support Chinese and English detection and recognition, direction classifier, support multilingual recognition <p>If you want to add your own trained model, you can add model links and keys in paddleocr and recompile.</p> <p>More whl package usage can be found in whl package</p>"},{"location":"en/version2.x/ppocr/quick_start.html#212-multi-language-model","title":"2.1.2 Multi-language Model","text":"<p>PaddleOCR currently supports 80 languages, which can be switched by modifying the <code>--lang</code> parameter.</p> <pre><code>paddleocr --image_dir ./doc/imgs_en/254.jpg --lang=en\n</code></pre> <p></p> <p></p> <p>The result is a list, each item contains a text box, text and recognition confidence</p> <pre><code>[[[67.0, 51.0], [327.0, 46.0], [327.0, 74.0], [68.0, 80.0]], ('PHOCAPITAL', 0.9944712519645691)]\n[[[72.0, 92.0], [453.0, 84.0], [454.0, 114.0], [73.0, 122.0]], ('107 State Street', 0.9744491577148438)]\n[[[69.0, 135.0], [501.0, 125.0], [501.0, 156.0], [70.0, 165.0]], ('Montpelier Vermont', 0.9357033967971802)]\n......\n</code></pre> <p>Commonly used multilingual abbreviations include</p> Language Abbreviation Language Abbreviation Language Abbreviation Chinese &amp; English ch French fr Japanese japan English en German german Korean korean Chinese Traditional chinese_cht Italian it Russian ru <p>A list of all languages and their corresponding abbreviations can be found in Multi-Language Model Tutorial</p>"},{"location":"en/version2.x/ppocr/quick_start.html#22-use-by-code","title":"2.2 Use by Code","text":""},{"location":"en/version2.x/ppocr/quick_start.html#221-chinese-english-model-and-multilingual-model","title":"2.2.1 Chinese &amp; English Model and Multilingual Model","text":"<ul> <li>detection, angle classification and recognition:</li> </ul> <pre><code>from paddleocr import PaddleOCR,draw_ocr\n# Paddleocr supports Chinese, English, French, German, Korean and Japanese.\n# You can set the parameter `lang` as `ch`, `en`, `fr`, `german`, `korean`, `japan`\n# to switch the language model in order.\nocr = PaddleOCR(use_angle_cls=True, lang='en') # need to run only once to download and load model into memory\nimg_path = './imgs_en/img_12.jpg'\nresult = ocr.ocr(img_path, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n\n# draw result\nfrom PIL import Image\nresult = result[0]\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='./fonts/simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre> <p>Output will be a list, each item contains bounding box, text and recognition confidence</p> <pre><code>[[[441.0, 174.0], [1166.0, 176.0], [1165.0, 222.0], [441.0, 221.0]], ('ACKNOWLEDGEMENTS', 0.9971134662628174)]\n  [[[403.0, 346.0], [1204.0, 348.0], [1204.0, 384.0], [402.0, 383.0]], ('We would like to thank all the designers and', 0.9761400818824768)]\n  [[[403.0, 396.0], [1204.0, 398.0], [1204.0, 434.0], [402.0, 433.0]], ('contributors who have been involved in the', 0.9791957139968872)]\n  ......\n</code></pre> <p>Visualization of results</p> <p></p> <p>If the input is a PDF file, you can refer to the following code for visualization</p> <pre><code>from paddleocr import PaddleOCR, draw_ocr\n\n# Paddleocr supports Chinese, English, French, German, Korean and Japanese.\n# You can set the parameter `lang` as `ch`, `en`, `fr`, `german`, `korean`, `japan`\n# to switch the language model in order.\nPAGE_NUM = 10 # Set the recognition page number\npdf_path = 'default.pdf'\nocr = PaddleOCR(use_angle_cls=True, lang=\"ch\", page_num=PAGE_NUM)  # need to run only once to download and load model into memory\n# ocr = PaddleOCR(use_angle_cls=True, lang=\"ch\", page_num=PAGE_NUM,use_gpu=0) # To Use GPU,uncomment this line and comment the above one.\nresult = ocr.ocr(pdf_path, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    if res == None: # Skip when empty result detected to avoid TypeError:NoneType\n        print(f\"[DEBUG] Empty page {idx+1} detected, skip it.\")\n        continue\n    for line in res:\n        print(line)\n\n# draw the result\nimport fitz\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimgs = []\nwith fitz.open(pdf_path) as pdf:\n    for pg in range(0, PAGE_NUM):\n        page = pdf[pg]\n        mat = fitz.Matrix(2, 2)\n        pm = page.get_pixmap(matrix=mat, alpha=False)\n        # if width or height &gt; 2000 pixels, don't enlarge the image\n        if pm.width &gt; 2000 or pm.height &gt; 2000:\n            pm = page.get_pixmap(matrix=fitz.Matrix(1, 1), alpha=False)\n        img = Image.frombytes(\"RGB\", [pm.width, pm.height], pm.samples)\n        img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n        imgs.append(img)\nfor idx in range(len(result)):\n    res = result[idx]\n    if res == None:\n        continue\n    image = imgs[idx]\n    boxes = [line[0] for line in res]\n    txts = [line[1][0] for line in res]\n    scores = [line[1][1] for line in res]\n    im_show = draw_ocr(image, boxes, txts, scores, font_path='doc/fonts/simfang.ttf')\n    im_show = Image.fromarray(im_show)\n    im_show.save('result_page_{}.jpg'.format(idx))\n</code></pre> <ul> <li>Detection and Recognition Using Sliding Windows</li> </ul> <p>To perform OCR using sliding windows, the following code snippet can be employed:</p> <pre><code>from paddleocr import PaddleOCR\nfrom PIL import Image, ImageDraw, ImageFont\n\n# Initialize OCR engine\nocr = PaddleOCR(use_angle_cls=True, lang=\"en\")\n\nimg_path = \"./very_large_image.jpg\"\nslice = {'horizontal_stride': 300, 'vertical_stride': 500, 'merge_x_thres': 50, 'merge_y_thres': 35}\nresults = ocr.ocr(img_path, cls=True, slice=slice)\n\n# Load image\nimage = Image.open(img_path).convert(\"RGB\")\ndraw = ImageDraw.Draw(image)\nfont = ImageFont.truetype(\"./doc/fonts/simfang.ttf\", size=20)  # Adjust size as needed\n\n# Process and draw results\nfor res in results:\n    for line in res:\n        box = [tuple(point) for point in line[0]]\n        # Finding the bounding box\n        box = [(min(point[0] for point in box), min(point[1] for point in box)),\n               (max(point[0] for point in box), max(point[1] for point in box))]\n        txt = line[1][0]\n        draw.rectangle(box, outline=\"red\", width=2)  # Draw rectangle\n        draw.text((box[0][0], box[0][1] - 25), txt, fill=\"blue\", font=font)  # Draw text above the box\n\n# Save result\nimage.save(\"result.jpg\")\n</code></pre> <p>This example initializes the PaddleOCR instance with angle classification enabled and sets the language to English. The <code>ocr</code> method is then called with several parameters to customize the detection and recognition process, including the <code>slice</code> parameter for handling image slices.</p> <p>For a more comprehensive understanding of the slicing operation, please refer to the slice operation documentation.</p>"},{"location":"en/version2.x/ppocr/quick_start.html#3-summary","title":"3. Summary","text":"<p>In this section, you have mastered the use of PaddleOCR whl package.</p>"},{"location":"en/version2.x/ppocr/visualization.html","title":"Visualization","text":""},{"location":"en/version2.x/ppocr/visualization.html#pp-ocrv3","title":"PP-OCRv3","text":""},{"location":"en/version2.x/ppocr/visualization.html#pp-ocrv3-chinese-model","title":"PP-OCRv3 Chinese model","text":""},{"location":"en/version2.x/ppocr/visualization.html#pp-ocrv3-english-model","title":"PP-OCRv3 English model","text":""},{"location":"en/version2.x/ppocr/visualization.html#pp-ocrv3-multilingual-model","title":"PP-OCRv3 Multilingual model","text":""},{"location":"en/version2.x/ppocr/visualization.html#pp-ocrv2","title":"PP-OCRv2","text":""},{"location":"en/version2.x/ppocr/visualization.html#ch_ppocr_server_20","title":"ch_ppocr_server_2.0","text":""},{"location":"en/version2.x/ppocr/visualization.html#en_ppocr_mobile_20","title":"en_ppocr_mobile_2.0","text":""},{"location":"en/version2.x/ppocr/visualization.html#multilingual_ppocr_mobile_20","title":"(multilingual)_ppocr_mobile_2.0","text":""},{"location":"en/version2.x/ppocr/blog/PP-OCRv3_introduction.html","title":"PP-OCRv3","text":""},{"location":"en/version2.x/ppocr/blog/PP-OCRv3_introduction.html#1","title":"1. \u200b\u7b80\u4ecb","text":"<p>PP-OCRv3\u200b\u5728\u200bPP-OCRv2\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\u8fdb\u4e00\u6b65\u200b\u5347\u7ea7\u200b\u3002\u200b\u6574\u4f53\u200b\u7684\u200b\u6846\u67b6\u56fe\u200b\u4fdd\u6301\u200b\u4e86\u200b\u4e0e\u200bPP-OCRv2\u200b\u76f8\u540c\u200b\u7684\u200bpipeline\uff0c\u200b\u9488\u5bf9\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4f18\u5316\u200b\u3002\u200b\u5176\u4e2d\u200b\uff0c\u200b\u68c0\u6d4b\u200b\u6a21\u5757\u200b\u4ecd\u200b\u57fa\u4e8e\u200bDB\u200b\u7b97\u6cd5\u200b\u4f18\u5316\u200b\uff0c\u200b\u800c\u200b\u8bc6\u522b\u200b\u6a21\u5757\u200b\u4e0d\u518d\u200b\u91c7\u7528\u200bCRNN\uff0c\u200b\u6362\u6210\u200b\u4e86\u200bIJCAI 2022\u200b\u6700\u65b0\u200b\u6536\u5f55\u200b\u7684\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200bSVTR\uff0c\u200b\u5e76\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u4ea7\u4e1a\u200b\u9002\u914d\u200b\u3002PP-OCRv3\u200b\u7cfb\u7edf\u200b\u6846\u56fe\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff08\u200b\u7c89\u8272\u200b\u6846\u4e2d\u200b\u4e3a\u200bPP-OCRv3\u200b\u65b0\u589e\u200b\u7b56\u7565\u200b\uff09\uff1a</p> <p></p> <p>\u200b\u4ece\u200b\u7b97\u6cd5\u200b\u6539\u8fdb\u200b\u601d\u8def\u200b\u4e0a\u200b\u770b\u200b\uff0c\u200b\u5206\u522b\u200b\u9488\u5bf9\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5171\u200b9\u200b\u4e2a\u200b\u65b9\u9762\u200b\u7684\u200b\u6539\u8fdb\u200b\uff1a</p> <ul> <li> <p>\u200b\u68c0\u6d4b\u200b\u6a21\u5757\u200b\uff1a</p> <ul> <li>LK-PAN\uff1a\u200b\u5927\u200b\u611f\u53d7\u200b\u91ce\u200b\u7684\u200bPAN\u200b\u7ed3\u6784\u200b\uff1b</li> <li>DML\uff1a\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u4e92\u200b\u5b66\u4e60\u7b56\u7565\u200b\uff1b</li> <li>RSE-FPN\uff1a\u200b\u6b8b\u5dee\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\u7684\u200bFPN\u200b\u7ed3\u6784\u200b\uff1b</li> </ul> </li> <li> <p>\u200b\u8bc6\u522b\u200b\u6a21\u5757\u200b\uff1a</p> <ul> <li>SVTR_LCNet\uff1a\u200b\u8f7b\u91cf\u7ea7\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7f51\u7edc\u200b\uff1b</li> <li>GTC\uff1aAttention\u200b\u6307\u5bfc\u200bCTC\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\uff1b</li> <li>TextConAug\uff1a\u200b\u6316\u6398\u200b\u6587\u5b57\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5e7f\u200b\u7b56\u7565\u200b\uff1b</li> <li>TextRotNet\uff1a\u200b\u81ea\u200b\u76d1\u7763\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff1b</li> <li>UDML\uff1a\u200b\u8054\u5408\u200b\u4e92\u200b\u5b66\u4e60\u7b56\u7565\u200b\uff1b</li> <li>UIM\uff1a\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848\u200b\u3002</li> </ul> </li> </ul> <p>\u200b\u4ece\u200b\u6548\u679c\u200b\u4e0a\u200b\u770b\u200b\uff0c\u200b\u901f\u5ea6\u200b\u53ef\u6bd4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u591a\u79cd\u200b\u573a\u666f\u200b\u7cbe\u5ea6\u200b\u5747\u200b\u6709\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\uff1a</p> <ul> <li>\u200b\u4e2d\u6587\u200b\u573a\u666f\u200b\uff0c\u200b\u76f8\u5bf9\u200b\u4e8e\u200bPP-OCRv2\u200b\u4e2d\u6587\u200b\u6a21\u578b\u200b\u63d0\u5347\u200b\u8d85\u200b5%\uff1b</li> <li>\u200b\u82f1\u6587\u200b\u6570\u5b57\u200b\u573a\u666f\u200b\uff0c\u200b\u76f8\u6bd4\u200b\u4e8e\u200bPP-OCRv2\u200b\u82f1\u6587\u200b\u6a21\u578b\u200b\u63d0\u5347\u200b11%\uff1b</li> <li>\u200b\u591a\u200b\u8bed\u8a00\u200b\u573a\u666f\u200b\uff0c\u200b\u4f18\u5316\u200b80+\u200b\u8bed\u79cd\u200b\u8bc6\u522b\u200b\u6548\u679c\u200b\uff0c\u200b\u5e73\u5747\u200b\u51c6\u786e\u7387\u200b\u63d0\u5347\u200b\u8d85\u200b5%\u3002</li> </ul>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv3_introduction.html#2","title":"2. \u200b\u68c0\u6d4b\u200b\u4f18\u5316","text":"<p>PP-OCRv3\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u662f\u200b\u5bf9\u200bPP-OCRv2\u200b\u4e2d\u200b\u7684\u200bCML\uff08Collaborative Mutual Learning) \u200b\u534f\u540c\u200b\u4e92\u200b\u5b66\u4e60\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5347\u7ea7\u200b\u3002\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0cCML\u200b\u7684\u200b\u6838\u5fc3\u601d\u60f3\u200b\u7ed3\u5408\u200b\u4e86\u200b\u2460\u200b\u4f20\u7edf\u200b\u7684\u200bTeacher\u200b\u6307\u5bfc\u200bStudent\u200b\u7684\u200b\u6807\u51c6\u200b\u84b8\u998f\u200b\u4e0e\u200b \u2461Students\u200b\u7f51\u7edc\u200b\u4e4b\u95f4\u200b\u7684\u200bDML\u200b\u4e92\u200b\u5b66\u4e60\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8ba9\u200bStudents\u200b\u7f51\u7edc\u200b\u4e92\u200b\u5b66\u4e60\u200b\u7684\u200b\u540c\u65f6\u200b\uff0cTeacher\u200b\u7f51\u7edc\u200b\u4e88\u4ee5\u200b\u6307\u5bfc\u200b\u3002PP-OCRv3\u200b\u5206\u522b\u200b\u9488\u5bf9\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u548c\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8fdb\u4e00\u6b65\u200b\u6548\u679c\u200b\u4f18\u5316\u200b\u3002\u200b\u5176\u4e2d\u200b\uff0c\u200b\u5728\u200b\u5bf9\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u4f18\u5316\u200b\u65f6\u200b\uff0c\u200b\u63d0\u51fa\u200b\u4e86\u200b\u5927\u200b\u611f\u53d7\u200b\u91ce\u200b\u7684\u200bPAN\u200b\u7ed3\u6784\u200bLK-PAN\u200b\u548c\u200b\u5f15\u5165\u200b\u4e86\u200bDML\uff08Deep Mutual Learning\uff09\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\uff1b\u200b\u5728\u200b\u5bf9\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u4f18\u5316\u200b\u65f6\u200b\uff0c\u200b\u63d0\u51fa\u200b\u4e86\u200b\u6b8b\u5dee\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\u7684\u200bFPN\u200b\u7ed3\u6784\u200bRSE-FPN\u3002</p> <p></p> <p>\u200b\u6d88\u878d\u200b\u5b9e\u9a8c\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u5e8f\u53f7\u200b \u200b\u7b56\u7565\u200b \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b hmean \u200b\u901f\u5ea6\u200b\uff08cpu + mkldnn) baseline teacher PP-OCR server 49.0M 83.20% 171ms teacher1 DB-R50-LK-PAN 124.0M 85.00% 396ms teacher2 DB-R50-LK-PAN-DML 124.0M 86.00% 396ms baseline student PP-OCRv2 3.0M 83.20% 117ms student0 DB-MV3-RSE-FPN 3.6M 84.50% 124ms student1 DB-MV3-CML\uff08teacher2\uff09 3.0M 84.30% 117ms student2 DB-MV3-RSE-FPN-CML\uff08teacher2\uff09 3.60M 85.40% 124ms <p>\u200b\u6d4b\u8bd5\u73af\u5883\u200b\uff1a Intel Gold 6148 CPU\uff0c\u200b\u9884\u6d4b\u200b\u65f6\u200b\u5f00\u542f\u200bMKLDNN\u200b\u52a0\u901f\u200b\u3002</p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv3_introduction.html#1lk-panpan","title":"\uff081\uff09LK-PAN\uff1a\u200b\u5927\u200b\u611f\u53d7\u200b\u91ce\u200b\u7684\u200bPAN\u200b\u7ed3\u6784","text":"<p>LK-PAN (Large Kernel PAN) \u200b\u662f\u200b\u4e00\u4e2a\u200b\u5177\u6709\u200b\u66f4\u5927\u200b\u611f\u53d7\u200b\u91ce\u200b\u7684\u200b\u8f7b\u91cf\u7ea7\u200bPAN\u200b\u7ed3\u6784\u200b\uff0c\u200b\u6838\u5fc3\u200b\u662f\u200b\u5c06\u200bPAN\u200b\u7ed3\u6784\u200b\u7684\u200bpath augmentation\u200b\u4e2d\u200b\u5377\u79ef\u200b\u6838\u4ece\u200b<code>3*3</code>\u200b\u6539\u4e3a\u200b<code>9*9</code>\u3002\u200b\u901a\u8fc7\u200b\u589e\u5927\u200b\u5377\u79ef\u200b\u6838\u200b\uff0c\u200b\u63d0\u5347\u200b\u7279\u5f81\u200b\u56fe\u200b\u6bcf\u4e2a\u200b\u4f4d\u7f6e\u200b\u8986\u76d6\u200b\u7684\u200b\u611f\u53d7\u200b\u91ce\u200b\uff0c\u200b\u66f4\u200b\u5bb9\u6613\u200b\u68c0\u6d4b\u200b\u5927\u5b57\u4f53\u200b\u7684\u200b\u6587\u5b57\u200b\u4ee5\u53ca\u200b\u6781\u7aef\u200b\u957f\u5bbd\u200b\u6bd4\u200b\u7684\u200b\u6587\u5b57\u200b\u3002\u200b\u4f7f\u7528\u200bLK-PAN\u200b\u7ed3\u6784\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u7684\u200bhmean\u200b\u4ece\u200b83.2%\u200b\u63d0\u5347\u200b\u5230\u200b85.0%\u3002</p> <p></p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv3_introduction.html#2dml","title":"\uff082\uff09DML\uff1a\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u4e92\u200b\u5b66\u4e60\u7b56\u7565","text":"<p>DML \uff08Deep Mutual Learning\uff09\u200b\u4e92\u200b\u5b66\u4e60\u200b\u84b8\u998f\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0c\u200b\u901a\u8fc7\u200b\u4e24\u4e2a\u200b\u7ed3\u6784\u200b\u76f8\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\u4e92\u76f8\u5b66\u4e60\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u6709\u6548\u200b\u63d0\u5347\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u3002\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u91c7\u7528\u200bDML\u200b\u7b56\u7565\u200b\uff0chmean\u200b\u4ece\u200b85%\u200b\u63d0\u5347\u200b\u5230\u200b86%\u3002\u200b\u5c06\u200bPP-OCRv2\u200b\u4e2d\u200bCML\u200b\u7684\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u66f4\u65b0\u200b\u4e3a\u200b\u4e0a\u8ff0\u200b\u66f4\u200b\u9ad8\u7cbe\u5ea6\u200b\u7684\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\uff0c\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200bhmean\u200b\u53ef\u4ee5\u200b\u8fdb\u4e00\u6b65\u200b\u4ece\u200b83.2%\u200b\u63d0\u5347\u200b\u5230\u200b84.3%\u3002</p> <p></p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv3_introduction.html#3rse-fpnfpn","title":"\uff083\uff09RSE-FPN\uff1a\u200b\u6b8b\u5dee\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\u7684\u200bFPN\u200b\u7ed3\u6784","text":"<p>RSE-FPN\uff08Residual Squeeze-and-Excitation FPN\uff09\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0c\u200b\u5f15\u5165\u200b\u6b8b\u5dee\u200b\u7ed3\u6784\u200b\u548c\u200b\u901a\u9053\u200b\u6ce8\u610f\u529b\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5c06\u200bFPN\u200b\u4e2d\u200b\u7684\u200b\u5377\u79ef\u200b\u5c42\u200b\u66f4\u6362\u200b\u4e3a\u200b\u901a\u9053\u200b\u6ce8\u610f\u529b\u200b\u7ed3\u6784\u200b\u7684\u200bRSEConv\u200b\u5c42\u200b\uff0c\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u8868\u5f81\u200b\u80fd\u529b\u200b\u3002\u200b\u8003\u8651\u200b\u5230\u200bPP-OCRv2\u200b\u7684\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u4e2d\u200bFPN\u200b\u901a\u9053\u200b\u6570\u200b\u975e\u5e38\u200b\u5c0f\u200b\uff0c\u200b\u4ec5\u4e3a\u200b96\uff0c\u200b\u5982\u679c\u200b\u76f4\u63a5\u200b\u7528\u200bSEblock\u200b\u4ee3\u66ff\u200bFPN\u200b\u4e2d\u200b\u5377\u79ef\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u67d0\u4e9b\u200b\u901a\u9053\u200b\u7684\u200b\u7279\u5f81\u200b\u88ab\u200b\u6291\u5236\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u4f1a\u200b\u4e0b\u964d\u200b\u3002RSEConv\u200b\u5f15\u5165\u200b\u6b8b\u5dee\u200b\u7ed3\u6784\u200b\u4f1a\u200b\u7f13\u89e3\u200b\u4e0a\u8ff0\u200b\u95ee\u9898\u200b\uff0c\u200b\u63d0\u5347\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u6548\u679c\u200b\u3002\u200b\u8fdb\u4e00\u6b65\u200b\u5c06\u200bPP-OCRv2\u200b\u4e2d\u200bCML\u200b\u7684\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200bFPN\u200b\u7ed3\u6784\u200b\u66f4\u65b0\u200b\u4e3a\u200bRSE-FPN\uff0c\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200bhmean\u200b\u53ef\u4ee5\u200b\u8fdb\u4e00\u6b65\u200b\u4ece\u200b84.3%\u200b\u63d0\u5347\u200b\u5230\u200b85.4%\u3002</p> <p></p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv3_introduction.html#3","title":"3. \u200b\u8bc6\u522b\u200b\u4f18\u5316","text":"<p>PP-OCRv3\u200b\u7684\u200b\u8bc6\u522b\u200b\u6a21\u5757\u200b\u662f\u200b\u57fa\u4e8e\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7b97\u6cd5\u200bSVTR\u200b\u4f18\u5316\u200b\u3002SVTR\u200b\u4e0d\u518d\u200b\u91c7\u7528\u200bRNN\u200b\u7ed3\u6784\u200b\uff0c\u200b\u901a\u8fc7\u200b\u5f15\u5165\u200bTransformers\u200b\u7ed3\u6784\u200b\u66f4\u52a0\u200b\u6709\u6548\u200b\u5730\u200b\u6316\u6398\u200b\u6587\u672c\u200b\u884c\u200b\u56fe\u50cf\u200b\u7684\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4ece\u800c\u200b\u63d0\u5347\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u80fd\u529b\u200b\u3002\u200b\u76f4\u63a5\u200b\u5c06\u200bPP-OCRv2\u200b\u7684\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u66ff\u6362\u6210\u200bSVTR_Tiny\uff0c\u200b\u8bc6\u522b\u200b\u51c6\u786e\u7387\u200b\u4ece\u200b74.8%\u200b\u63d0\u5347\u200b\u5230\u200b80.1%\uff08+5.3%\uff09\uff0c\u200b\u4f46\u662f\u200b\u9884\u6d4b\u200b\u901f\u5ea6\u6162\u200b\u4e86\u200b\u5c06\u8fd1\u200b11\u200b\u500d\u200b\uff0cCPU\u200b\u4e0a\u200b\u9884\u6d4b\u200b\u4e00\u6761\u200b\u6587\u672c\u200b\u884c\u200b\uff0c\u200b\u5c06\u8fd1\u200b100ms\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0cPP-OCRv3\u200b\u91c7\u7528\u200b\u5982\u4e0b\u200b6\u200b\u4e2a\u200b\u4f18\u5316\u200b\u7b56\u7565\u200b\u8fdb\u884c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u52a0\u901f\u200b\u3002</p> <p></p> <p>\u200b\u57fa\u4e8e\u200b\u4e0a\u8ff0\u200b\u7b56\u7565\u200b\uff0cPP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u76f8\u6bd4\u200bPP-OCRv2\uff0c\u200b\u5728\u200b\u901f\u5ea6\u200b\u53ef\u6bd4\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b4.6%\u3002 \u200b\u5177\u4f53\u200b\u6d88\u878d\u200b\u5b9e\u9a8c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> ID \u200b\u7b56\u7565\u200b \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u7cbe\u5ea6\u200b \u200b\u9884\u6d4b\u200b\u8017\u65f6\u200b\uff08CPU + MKLDNN) 01 PP-OCRv2 8.0M 74.80% 8.54ms 02 SVTR_Tiny 21.0M 80.10% 97.00ms 03 SVTR_LCNet(h32) 12.0M 71.90% 6.60ms 04 SVTR_LCNet(h48) 12.0M 73.98% 7.60ms 05 + GTC 12.0M 75.80% 7.60ms 06 + TextConAug 12.0M 76.30% 7.60ms 07 + TextRotNet 12.0M 76.90% 7.60ms 08 + UDML 12.0M 78.40% 7.60ms 09 + UIM 12.0M 79.40% 7.60ms <p>\u200b\u6ce8\u200b\uff1a \u200b\u6d4b\u8bd5\u200b\u901f\u5ea6\u200b\u65f6\u200b\uff0c\u200b\u5b9e\u9a8c\u200b01-03\u200b\u8f93\u5165\u200b\u56fe\u7247\u5c3a\u5bf8\u200b\u5747\u200b\u4e3a\u200b(3,32,320)\uff0c04-08\u200b\u8f93\u5165\u200b\u56fe\u7247\u5c3a\u5bf8\u200b\u5747\u200b\u4e3a\u200b(3,48,320)\u3002\u200b\u5728\u200b\u5b9e\u9645\u200b\u9884\u6d4b\u200b\u65f6\u200b\uff0c\u200b\u56fe\u50cf\u200b\u4e3a\u200b\u53d8\u957f\u200b\u8f93\u5165\u200b\uff0c\u200b\u901f\u5ea6\u200b\u4f1a\u200b\u6709\u6240\u200b\u53d8\u5316\u200b\u3002\u200b\u6d4b\u8bd5\u73af\u5883\u200b\uff1a Intel Gold 6148 CPU\uff0c\u200b\u9884\u6d4b\u200b\u65f6\u200b\u5f00\u542f\u200bMKLDNN\u200b\u52a0\u901f\u200b\u3002</p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv3_introduction.html#1svtr_lcnet","title":"\uff081\uff09SVTR_LCNet\uff1a\u200b\u8f7b\u91cf\u7ea7\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7f51\u7edc","text":"<p>SVTR_LCNet\u200b\u662f\u200b\u9488\u5bf9\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200b\uff0c\u200b\u5c06\u200b\u57fa\u4e8e\u200bTransformer\u200b\u7684\u200bSVTR\u200b\u7f51\u7edc\u200b\u548c\u200b\u8f7b\u91cf\u7ea7\u200bCNN\u200b\u7f51\u7edc\u200bPP-LCNet \u200b\u878d\u5408\u200b\u7684\u200b\u4e00\u79cd\u200b\u8f7b\u91cf\u7ea7\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7f51\u7edc\u200b\u3002\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7f51\u7edc\u200b\uff0c\u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b\u4f18\u4e8e\u200bPP-OCRv2\u200b\u7684\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b20%\uff0c\u200b\u4f46\u662f\u200b\u7531\u4e8e\u200b\u6ca1\u6709\u200b\u91c7\u7528\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\uff0c\u200b\u8be5\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\u7565\u5dee\u200b\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u8fdb\u4e00\u6b65\u200b\u5c06\u200b\u8f93\u5165\u200b\u56fe\u7247\u200b\u89c4\u8303\u5316\u200b\u9ad8\u5ea6\u200b\u4ece\u200b32\u200b\u63d0\u5347\u200b\u5230\u200b48\uff0c\u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b\u7a0d\u5fae\u200b\u53d8\u6162\u200b\uff0c\u200b\u4f46\u662f\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\uff0c\u200b\u8bc6\u522b\u200b\u51c6\u786e\u7387\u200b\u8fbe\u5230\u200b73.98%\uff08+2.08%\uff09\uff0c\u200b\u63a5\u8fd1\u200bPP-OCRv2\u200b\u91c7\u7528\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\u7684\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\u3002</p> <p>SVTR_Tiny \u200b\u7f51\u7edc\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p>\u200b\u7531\u4e8e\u200b MKLDNN \u200b\u52a0\u901f\u200b\u5e93\u200b\u652f\u6301\u200b\u7684\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u6709\u9650\u200b\uff0cSVTR \u200b\u5728\u200b CPU+MKLDNN \u200b\u4e0a\u200b\u76f8\u6bd4\u200b PP-OCRv2 \u200b\u6162\u200b\u4e86\u200b10\u200b\u500d\u200b\u3002PP-OCRv3 \u200b\u671f\u671b\u200b\u5728\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u540c\u65f6\u200b\uff0c\u200b\u4e0d\u200b\u5e26\u6765\u200b\u989d\u5916\u200b\u7684\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\u3002\u200b\u901a\u8fc7\u200b\u5206\u6790\u200b\u53d1\u73b0\u200b\uff0cSVTR_Tiny \u200b\u7ed3\u6784\u200b\u7684\u200b\u4e3b\u8981\u200b\u8017\u65f6\u200b\u6a21\u5757\u200b\u4e3a\u200b Mixing Block\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u5bf9\u200b SVTR_Tiny \u200b\u7684\u200b\u7ed3\u6784\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u7cfb\u5217\u200b\u4f18\u5316\u200b\uff08\u200b\u8be6\u7ec6\u200b\u901f\u5ea6\u200b\u6570\u636e\u200b\u8bf7\u200b\u53c2\u8003\u200b\u4e0b\u65b9\u200b\u6d88\u878d\u200b\u5b9e\u9a8c\u200b\u8868\u683c\u200b\uff09:</p> <ol> <li> <p>\u200b\u5c06\u200b SVTR \u200b\u7f51\u7edc\u200b\u524d\u534a\u90e8\u200b\u5206\u200b\u66ff\u6362\u200b\u4e3a\u200b PP-LCNet \u200b\u7684\u200b\u524d\u200b\u4e09\u4e2a\u200bstage\uff0c\u200b\u4fdd\u7559\u200b4\u200b\u4e2a\u200b Global Mixing Block \uff0c\u200b\u7cbe\u5ea6\u200b\u4e3a\u200b76%\uff0c\u200b\u52a0\u901f\u200b69%\uff0c\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p> </li> <li> <p>\u200b\u5c06\u200b4\u200b\u4e2a\u200b Global Mixing Block \u200b\u51cf\u5c0f\u200b\u5230\u200b2\u200b\u4e2a\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u4e3a\u200b72.9%\uff0c\u200b\u52a0\u901f\u200b69%\uff0c\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p> </li> <li> <p>\u200b\u5b9e\u9a8c\u200b\u53d1\u73b0\u200b Global Mixing Block \u200b\u7684\u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b\u4e0e\u200b\u8f93\u5165\u200b\u5176\u200b\u7279\u5f81\u200b\u7684\u200bshape\u200b\u6709\u5173\u200b\uff0c\u200b\u56e0\u6b64\u200b\u540e\u79fb\u200b Global Mixing Block \u200b\u7684\u200b\u4f4d\u7f6e\u200b\u5230\u200b\u6c60\u5316\u5c42\u200b\u4e4b\u540e\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u4e0b\u964d\u200b\u4e3a\u200b71.9%\uff0c\u200b\u901f\u5ea6\u200b\u8d85\u8d8a\u200b\u57fa\u4e8e\u200bCNN\u200b\u7ed3\u6784\u200b\u7684\u200bPP-OCRv2-baseline 22%\uff0c\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p> </li> </ol> <p>\u200b\u5177\u4f53\u200b\u6d88\u878d\u200b\u5b9e\u9a8c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> ID \u200b\u7b56\u7565\u200b \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u7cbe\u5ea6\u200b \u200b\u901f\u5ea6\u200b\uff08CPU + MKLDNN) 01 PP-OCRv2-baseline 8.0M 69.30% 8.54ms 02 SVTR_Tiny 21.0M 80.10% 97.00ms 03 SVTR_LCNet(G4) 9.2M 76.00% 30.00ms 04 SVTR_LCNet(G2) 13.0M 72.98% 9.37ms 05 SVTR_LCNet(h32) 12.0M 71.90% 6.60ms 06 SVTR_LCNet(h48) 12.0M 73.98% 7.60ms <p>\u200b\u6ce8\u200b\uff1a \u200b\u6d4b\u8bd5\u200b\u901f\u5ea6\u200b\u65f6\u200b\uff0c01-05\u200b\u8f93\u5165\u200b\u56fe\u7247\u5c3a\u5bf8\u200b\u5747\u200b\u4e3a\u200b(3,32,320)\uff1b PP-OCRv2-baseline \u200b\u4ee3\u8868\u200b\u6ca1\u6709\u200b\u501f\u52a9\u200b\u84b8\u998f\u200b\u65b9\u6cd5\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200b\u7684\u200b\u6a21\u578b\u200b</p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv3_introduction.html#2gtcattentionctc","title":"\uff082\uff09GTC\uff1aAttention\u200b\u6307\u5bfc\u200bCTC\u200b\u8bad\u7ec3\u200b\u7b56\u7565","text":"<p>GTC\uff08Guided Training of CTC\uff09\uff0c\u200b\u5229\u7528\u200bAttention\u200b\u6a21\u5757\u200bCTC\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u878d\u5408\u200b\u591a\u79cd\u200b\u6587\u672c\u200b\u7279\u5f81\u200b\u7684\u200b\u8868\u8fbe\u200b\uff0c\u200b\u662f\u200b\u4e00\u79cd\u200b\u6709\u6548\u200b\u7684\u200b\u63d0\u5347\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7684\u200b\u7b56\u7565\u200b\u3002\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\uff0c\u200b\u9884\u6d4b\u200b\u65f6\u200b\u5b8c\u5168\u200b\u53bb\u9664\u200b Attention \u200b\u6a21\u5757\u200b\uff0c\u200b\u5728\u200b\u63a8\u7406\u200b\u9636\u6bb5\u200b\u4e0d\u200b\u589e\u52a0\u200b\u4efb\u4f55\u200b\u8017\u65f6\u200b\uff0c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u5230\u200b75.8%\uff08+1.82%\uff09\u3002\u200b\u8bad\u7ec3\u200b\u6d41\u7a0b\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv3_introduction.html#3textconaug","title":"\uff083\uff09TextConAug\uff1a\u200b\u6316\u6398\u200b\u6587\u5b57\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5e7f\u200b\u7b56\u7565","text":"<p>TextConAug\u200b\u662f\u200b\u4e00\u79cd\u200b\u6316\u6398\u200b\u6587\u5b57\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\u7684\u200b\u6570\u636e\u200b\u589e\u5e7f\u200b\u7b56\u7565\u200b\uff0c\u200b\u4e3b\u8981\u200b\u601d\u60f3\u200b\u6765\u6e90\u4e8e\u200b\u8bba\u6587\u200bConCLR\uff0c\u200b\u4f5c\u8005\u200b\u63d0\u51fa\u200bConAug\u200b\u6570\u636e\u200b\u589e\u5e7f\u200b\uff0c\u200b\u5728\u200b\u4e00\u4e2a\u200bbatch\u200b\u5185\u200b\u5bf9\u200b2\u200b\u5f20\u200b\u4e0d\u540c\u200b\u7684\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u8054\u7ed3\u200b\uff0c\u200b\u7ec4\u6210\u200b\u65b0\u200b\u7684\u200b\u56fe\u50cf\u200b\u5e76\u200b\u8fdb\u884c\u200b\u81ea\u200b\u76d1\u7763\u200b\u5bf9\u6bd4\u200b\u5b66\u4e60\u200b\u3002PP-OCRv3\u200b\u5c06\u200b\u6b64\u200b\u65b9\u6cd5\u200b\u5e94\u7528\u200b\u5230\u200b\u6709\u200b\u76d1\u7763\u200b\u7684\u200b\u5b66\u4e60\u200b\u4efb\u52a1\u200b\u4e2d\u200b\uff0c\u200b\u8bbe\u8ba1\u200b\u4e86\u200bTextConAug\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4e30\u5bcc\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e0a\u4e0b\u6587\u200b\u4fe1\u606f\u200b\uff0c\u200b\u63d0\u5347\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u591a\u6837\u6027\u200b\u3002\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\uff0c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u5230\u200b76.3%\uff08+0.5%\uff09\u3002TextConAug\u200b\u793a\u610f\u56fe\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv3_introduction.html#4textrotnet","title":"\uff084\uff09TextRotNet\uff1a\u200b\u81ea\u200b\u76d1\u7763\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b","text":"<p>TextRotNet\u200b\u662f\u200b\u4f7f\u7528\u200b\u5927\u91cf\u200b\u65e0\u200b\u6807\u6ce8\u200b\u7684\u200b\u6587\u672c\u200b\u884c\u200b\u6570\u636e\u200b\uff0c\u200b\u901a\u8fc7\u200b\u81ea\u200b\u76d1\u7763\u200b\u65b9\u5f0f\u200b\u8bad\u7ec3\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u53c2\u8003\u200b\u4e8e\u200b\u8bba\u6587\u200bSTR-Fewer-Labels\u3002\u200b\u8be5\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u521d\u59cb\u5316\u200bSVTR_LCNet\u200b\u7684\u200b\u521d\u59cb\u200b\u6743\u91cd\u200b\uff0c\u200b\u4ece\u800c\u200b\u5e2e\u52a9\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u6536\u655b\u200b\u5230\u200b\u66f4\u4f73\u200b\u4f4d\u7f6e\u200b\u3002\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\uff0c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u5230\u200b76.9%\uff08+0.6%\uff09\u3002TextRotNet\u200b\u8bad\u7ec3\u200b\u6d41\u7a0b\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff1a</p> <p></p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv3_introduction.html#5udml","title":"\uff085\uff09UDML\uff1a\u200b\u8054\u5408\u200b\u4e92\u200b\u5b66\u4e60\u7b56\u7565","text":"<p>UDML\uff08Unified-Deep Mutual Learning\uff09\u200b\u8054\u5408\u200b\u4e92\u200b\u5b66\u4e60\u200b\u662f\u200bPP-OCRv2\u200b\u4e2d\u200b\u5c31\u200b\u91c7\u7528\u200b\u7684\u200b\u5bf9\u4e8e\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u975e\u5e38\u200b\u6709\u6548\u200b\u7684\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\u6548\u679c\u200b\u7684\u200b\u7b56\u7565\u200b\u3002\u200b\u5728\u200bPP-OCRv3\u200b\u4e2d\u200b\uff0c\u200b\u9488\u5bf9\u200b\u4e24\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200bSVTR_LCNet\u200b\u548c\u200bAttention\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5bf9\u200b\u4ed6\u4eec\u200b\u4e4b\u95f4\u200b\u7684\u200bPP-LCNet\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u200b\u3001SVTR\u200b\u6a21\u5757\u200b\u7684\u200b\u8f93\u51fa\u200b\u548c\u200bAttention\u200b\u6a21\u5757\u200b\u7684\u200b\u8f93\u51fa\u200b\u540c\u65f6\u200b\u8fdb\u884c\u200b\u76d1\u7763\u200b\u8bad\u7ec3\u200b\u3002\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\uff0c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u5230\u200b78.4%\uff08+1.5%\uff09\u3002</p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv3_introduction.html#6uim","title":"\uff086\uff09UIM\uff1a\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848","text":"<p>UIM\uff08Unlabeled Images Mining\uff09\u200b\u662f\u200b\u4e00\u79cd\u200b\u975e\u5e38\u7b80\u5355\u200b\u7684\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848\u200b\u3002\u200b\u6838\u5fc3\u601d\u60f3\u200b\u662f\u200b\u5229\u7528\u200b\u9ad8\u7cbe\u5ea6\u200b\u7684\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u5927\u200b\u6a21\u578b\u200b\u5bf9\u200b\u65e0\u200b\u6807\u6ce8\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u83b7\u53d6\u200b\u4f2a\u200b\u6807\u7b7e\u200b\uff0c\u200b\u5e76\u4e14\u200b\u9009\u62e9\u200b\u9884\u6d4b\u200b\u7f6e\u4fe1\u5ea6\u200b\u9ad8\u200b\u7684\u200b\u6837\u672c\u200b\u4f5c\u4e3a\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff0c\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u5c0f\u200b\u6a21\u578b\u200b\u3002\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\uff0c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u5230\u200b79.4%\uff08+1%\uff09\u3002\u200b\u5b9e\u9645\u64cd\u4f5c\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u5168\u91cf\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u9ad8\u7cbe\u5ea6\u200bSVTR-Tiny\u200b\u6a21\u578b\u200b\uff08acc=82.5%\uff09\u200b\u8fdb\u884c\u200b\u6570\u636e\u6316\u6398\u200b\uff0c\u200b\u70b9\u51fb\u200b\u83b7\u53d6\u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b\u548c\u200b\u4f7f\u7528\u200b\u6559\u7a0b\u200b\u3002</p> <p></p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv3_introduction.html#4","title":"4. \u200b\u7aef\u5230\u200b\u7aef\u200b\u8bc4\u4f30","text":"<p>\u200b\u7ecf\u8fc7\u200b\u4ee5\u4e0a\u200b\u4f18\u5316\u200b\uff0c\u200b\u6700\u7ec8\u200bPP-OCRv3\u200b\u5728\u200b\u901f\u5ea6\u200b\u53ef\u6bd4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4e2d\u6587\u200b\u573a\u666f\u200b\u7aef\u5230\u200b\u7aef\u200bHmean\u200b\u6307\u6807\u200b\u76f8\u6bd4\u200b\u4e8e\u200bPP-OCRv2\u200b\u63d0\u5347\u200b5%\uff0c\u200b\u6548\u679c\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\u3002\u200b\u5177\u4f53\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\uff1a</p> Model Hmean Model Size (M) Time Cost (CPU, ms) Time Cost (T4 GPU, ms) PP-OCR mobile 50.30% 8.1 356.00 116.00 PP-OCR server 57.00% 155.1 1056.00 200.00 PP-OCRv2 57.60% 11.6 330.00 111.00 PP-OCRv3 62.90% 15.6 331.00 86.64 <p>\u200b\u6d4b\u8bd5\u73af\u5883\u200b\uff1aCPU\u200b\u578b\u53f7\u200b\u4e3a\u200bIntel Gold 6148\uff0cCPU\u200b\u9884\u6d4b\u200b\u65f6\u200b\u5f00\u542f\u200bMKLDNN\u200b\u52a0\u901f\u200b\u3002</p> <p>\u200b\u9664\u4e86\u200b\u66f4\u65b0\u200b\u4e2d\u6587\u200b\u6a21\u578b\u200b\uff0c\u200b\u672c\u6b21\u200b\u5347\u7ea7\u200b\u4e5f\u200b\u540c\u6b65\u200b\u4f18\u5316\u200b\u4e86\u200b\u82f1\u6587\u200b\u6570\u5b57\u6a21\u578b\u200b\uff0c\u200b\u7aef\u5230\u200b\u7aef\u200b\u6548\u679c\u200b\u63d0\u5347\u200b11%\uff0c\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\uff1a</p> Model Recall Precision Hmean PP-OCR_en 38.99% 45.91% 42.17% PP-OCRv3_en 50.95% 55.53% 53.14% <p>\u200b\u540c\u65f6\u200b\uff0c\u200b\u4e5f\u200b\u5bf9\u200b\u5df2\u200b\u652f\u6301\u200b\u7684\u200b80\u200b\u4f59\u79cd\u200b\u8bed\u8a00\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5347\u7ea7\u200b\u66f4\u65b0\u200b\uff0c\u200b\u5728\u200b\u6709\u200b\u8bc4\u4f30\u200b\u96c6\u200b\u7684\u200b\u56db\u79cd\u200b\u8bed\u7cfb\u200b\u8bc6\u522b\u200b\u51c6\u786e\u7387\u200b\u5e73\u5747\u200b\u63d0\u5347\u200b5%\u200b\u4ee5\u4e0a\u200b\uff0c\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\uff1a</p> Model \u200b\u62c9\u4e01\u8bed\u7cfb\u200b \u200b\u963f\u62c9\u4f2f\u8bed\u200b\u7cfb\u200b \u200b\u65e5\u8bed\u200b \u200b\u97e9\u8bed\u200b PP-OCR_mul 69.60% 40.50% 38.50% 55.40% PP-OCRv3_mul 75.20% 45.37% 45.80% 60.10%"},{"location":"en/version2.x/ppocr/blog/PP-OCRv4_introduction.html","title":"PP-OCRv4","text":""},{"location":"en/version2.x/ppocr/blog/PP-OCRv4_introduction.html#1","title":"1. \u200b\u7b80\u4ecb","text":"<p>PP-OCRv4\u200b\u5728\u200bPP-OCRv3\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\u8fdb\u4e00\u6b65\u200b\u5347\u7ea7\u200b\u3002\u200b\u6574\u4f53\u200b\u7684\u200b\u6846\u67b6\u56fe\u200b\u4fdd\u6301\u200b\u4e86\u200b\u4e0e\u200bPP-OCRv3\u200b\u76f8\u540c\u200b\u7684\u200bpipeline\uff0c\u200b\u9488\u5bf9\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6570\u636e\u200b\u3001\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u3001\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\u7b49\u200b\u591a\u4e2a\u200b\u6a21\u5757\u200b\u7684\u200b\u4f18\u5316\u200b\u3002 PP-OCRv4\u200b\u7cfb\u7edf\u200b\u6846\u56fe\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <p></p> <p>\u200b\u4ece\u200b\u7b97\u6cd5\u200b\u6539\u8fdb\u200b\u601d\u8def\u200b\u4e0a\u200b\u770b\u200b\uff0c\u200b\u5206\u522b\u200b\u9488\u5bf9\u200b\u68c0\u6d4b\u200b\u548c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\uff0c\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5171\u200b10\u200b\u4e2a\u200b\u65b9\u9762\u200b\u7684\u200b\u6539\u8fdb\u200b\uff1a</p> <ul> <li> <p>\u200b\u68c0\u6d4b\u200b\u6a21\u5757\u200b\uff1a</p> <ul> <li>LCNetV3\uff1a\u200b\u7cbe\u5ea6\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b</li> <li>PFHead\uff1a\u200b\u5e76\u884c\u200bhead\u200b\u5206\u652f\u200b\u878d\u5408\u200b\u7ed3\u6784\u200b</li> <li>DSR: \u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u52a8\u6001\u200b\u589e\u52a0\u200bshrink ratio</li> <li>CML\uff1a\u200b\u6dfb\u52a0\u200bStudent\u200b\u548c\u200bTeacher\u200b\u7f51\u7edc\u200b\u8f93\u51fa\u200b\u7684\u200bKL div loss</li> </ul> </li> <li> <p>\u200b\u8bc6\u522b\u200b\u6a21\u5757\u200b\uff1a</p> <ul> <li>SVTR_LCNetV3\uff1a\u200b\u7cbe\u5ea6\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b</li> <li>Lite-Neck\uff1a\u200b\u7cbe\u7b80\u200b\u7684\u200bNeck\u200b\u7ed3\u6784\u200b</li> <li>GTC-NRTR\uff1a\u200b\u7a33\u5b9a\u200b\u7684\u200bAttention\u200b\u6307\u5bfc\u200b\u5206\u652f\u200b</li> <li>Multi-Scale\uff1a\u200b\u591a\u200b\u5c3a\u5ea6\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b</li> <li>DF: \u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848\u200b</li> <li>DKD \uff1aDKD\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b</li> </ul> </li> </ul> <p>\u200b\u4ece\u200b\u6548\u679c\u200b\u4e0a\u200b\u770b\u200b\uff0c\u200b\u901f\u5ea6\u200b\u53ef\u6bd4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u591a\u79cd\u200b\u573a\u666f\u200b\u7cbe\u5ea6\u200b\u5747\u200b\u6709\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\uff1a</p> <ul> <li>\u200b\u4e2d\u6587\u200b\u573a\u666f\u200b\uff0c\u200b\u76f8\u5bf9\u200b\u4e8e\u200bPP-OCRv3\u200b\u4e2d\u6587\u200b\u6a21\u578b\u200b\u63d0\u5347\u200b\u8d85\u200b4%\uff1b</li> <li>\u200b\u82f1\u6587\u200b\u6570\u5b57\u200b\u573a\u666f\u200b\uff0c\u200b\u76f8\u6bd4\u200b\u4e8e\u200bPP-OCRv3\u200b\u82f1\u6587\u200b\u6a21\u578b\u200b\u63d0\u5347\u200b6%\uff1b</li> <li>\u200b\u591a\u200b\u8bed\u8a00\u200b\u573a\u666f\u200b\uff0c\u200b\u4f18\u5316\u200b80\u200b\u4e2a\u200b\u8bed\u79cd\u200b\u8bc6\u522b\u200b\u6548\u679c\u200b\uff0c\u200b\u5e73\u5747\u200b\u51c6\u786e\u7387\u200b\u63d0\u5347\u200b\u8d85\u200b8%\u3002</li> </ul>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv4_introduction.html#2","title":"2. \u200b\u68c0\u6d4b\u200b\u4f18\u5316","text":"<p>PP-OCRv4\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u5728\u200bPP-OCRv3\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\uff0c\u200b\u5728\u200b\u7f51\u7edc\u7ed3\u6784\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\uff0c\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\u4e09\u4e2a\u200b\u65b9\u9762\u200b\u505a\u200b\u4e86\u200b\u4f18\u5316\u200b\u3002\u200b\u9996\u5148\u200b\uff0cPP-OCRv4\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u4f7f\u7528\u200bPP-LCNetV3\u200b\u66ff\u6362\u200bMobileNetv3\uff0c\u200b\u5e76\u200b\u63d0\u51fa\u200b\u5e76\u884c\u200b\u5206\u652f\u200b\u878d\u5408\u200b\u7684\u200bPFhead\u200b\u7ed3\u6784\u200b\uff1b\u200b\u5176\u6b21\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u52a8\u6001\u200b\u8c03\u6574\u200bshrink ratio\u200b\u7684\u200b\u6bd4\u4f8b\u200b\uff1b\u200b\u6700\u540e\u200b\uff0cPP-OCRv4\u200b\u5bf9\u200bCML\u200b\u7684\u200b\u84b8\u998f\u200bloss\u200b\u8fdb\u884c\u200b\u4f18\u5316\u200b\uff0c\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b\u6548\u679c\u200b\u3002</p> <p>\u200b\u6d88\u878d\u200b\u5b9e\u9a8c\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u5e8f\u53f7\u200b \u200b\u7b56\u7565\u200b \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b hmean \u200b\u901f\u5ea6\u200b\uff08cpu + mkldnn) baseline PP-OCRv3 3.4M 78.84% 69ms baseline student PP-OCRv3 student 3.4M 76.22% 69ms 01 +PFHead 3.6M 76.97% 96ms 02 +Dynamic Shrink Ratio 3.6M 78.24% 96ms 03 +PP-LCNetv3 4.8M 79.08% 94ms 03 +CML 4.8M 79.87% 67ms <p>\u200b\u6d4b\u8bd5\u73af\u5883\u200b\uff1a Intel Gold 6148 CPU\uff0c\u200b\u9884\u6d4b\u200b\u5f15\u64ce\u200b\u4f7f\u7528\u200bopenvino\u3002</p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv4_introduction.html#1pfheadhead","title":"\uff081\uff09PFhead\uff1a\u200b\u591a\u200b\u5206\u652f\u200b\u878d\u5408\u200bHead\u200b\u7ed3\u6784","text":"<p>PFhead\u200b\u7ed3\u6784\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0cPFHead\u200b\u5728\u200b\u7ecf\u8fc7\u200b\u7b2c\u4e00\u4e2a\u200b\u8f6c\u7f6e\u200b\u5377\u79ef\u200b\u540e\u200b\uff0c\u200b\u5206\u522b\u200b\u8fdb\u884c\u200b\u4e0a\u200b\u91c7\u6837\u200b\u548c\u200b\u8f6c\u7f6e\u200b\u5377\u79ef\u200b\uff0c\u200b\u4e0a\u200b\u91c7\u6837\u200b\u7684\u200b\u8f93\u51fa\u200b\u901a\u8fc7\u200b3x3\u200b\u5377\u79ef\u200b\u5f97\u5230\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\uff0c\u200b\u7136\u540e\u200b\u548c\u200b\u8f6c\u7f6e\u200b\u5377\u79ef\u200b\u7684\u200b\u5206\u652f\u200b\u7684\u200b\u7ed3\u679c\u200b\u7ea7\u8054\u200b\u5e76\u200b\u7ecf\u8fc7\u200b1x1\u200b\u5377\u79ef\u200b\u5c42\u200b\uff0c\u200b\u6700\u540e\u200b1x1\u200b\u5377\u79ef\u200b\u7684\u200b\u7ed3\u679c\u200b\u548c\u200b\u8f6c\u7f6e\u200b\u5377\u79ef\u200b\u7684\u200b\u7ed3\u679c\u200b\u76f8\u52a0\u200b\u5f97\u5230\u200b\u6700\u540e\u200b\u8f93\u51fa\u200b\u7684\u200b\u6982\u7387\u200b\u56fe\u200b\u3002PP-OCRv4\u200b\u5b66\u751f\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u4f7f\u7528\u200bPFhead\uff0chmean\u200b\u4ece\u200b76.22%\u200b\u589e\u52a0\u200b\u5230\u200b76.97%\u3002</p> <p></p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv4_introduction.html#2dsr","title":"\uff082\uff09DSR: \u200b\u6536\u7f29\u200b\u6bd4\u4f8b\u200b\u52a8\u6001\u200b\u8c03\u6574\u200b\u7b56\u7565","text":"<p>\u200b\u52a8\u6001\u200bshrink ratio(dynamic shrink ratio): \u200b\u5728\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\uff0cshrink ratio\u200b\u7531\u200b\u56fa\u5b9a\u503c\u200b\u8c03\u6574\u200b\u4e3a\u200b\u52a8\u6001\u53d8\u5316\u200b\uff0c\u200b\u968f\u7740\u200b\u8bad\u7ec3\u200bepoch\u200b\u7684\u200b\u589e\u52a0\u200b\uff0cshrink ratio\u200b\u4ece\u200b0.4\u200b\u7ebf\u6027\u200b\u589e\u52a0\u200b\u5230\u200b0.6\u3002\u200b\u8be5\u200b\u7b56\u7565\u200b\u5728\u200bPP-OCRv4\u200b\u5b66\u751f\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u4e0a\u200b\uff0chmean\u200b\u4ece\u200b76.97%\u200b\u63d0\u5347\u200b\u5230\u200b78.24%\u3002</p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv4_introduction.html#3-pp-lcnetv3","title":"(3) PP-LCNetV3\uff1a\u200b\u7cbe\u5ea6\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u9aa8\u5e72\u200b\u7f51\u7edc","text":"<p>PP-LCNetV3\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u662f\u200bPP-LCNet\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u7684\u200b\u5ef6\u7eed\u200b\uff0c\u200b\u8986\u76d6\u200b\u4e86\u200b\u66f4\u200b\u5927\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u8303\u56f4\u200b\uff0c\u200b\u80fd\u591f\u200b\u9002\u5e94\u200b\u4e0d\u540c\u200b\u4e0b\u6e38\u200b\u4efb\u52a1\u200b\u7684\u200b\u9700\u8981\u200b\u3002PP-LCNetV3\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u4ece\u200b\u591a\u4e2a\u200b\u65b9\u9762\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4f18\u5316\u200b\uff0c\u200b\u63d0\u51fa\u200b\u4e86\u200b\u53ef\u200b\u5b66\u4e60\u200b\u4eff\u5c04\u53d8\u6362\u200b\u6a21\u5757\u200b\uff0c\u200b\u5bf9\u91cd\u200b\u53c2\u6570\u200b\u5316\u200b\u7b56\u7565\u200b\u3001\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6539\u8fdb\u200b\uff0c\u200b\u540c\u65f6\u200b\u8c03\u6574\u200b\u4e86\u200b\u7f51\u7edc\u200b\u6df1\u5ea6\u200b\u4e0e\u200b\u5bbd\u5ea6\u200b\u3002\u200b\u6700\u7ec8\u200b\uff0cPP-LCNetV3\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u80fd\u591f\u200b\u5728\u200b\u6027\u80fd\u200b\u4e0e\u200b\u6548\u7387\u200b\u4e4b\u95f4\u200b\u8fbe\u5230\u6700\u4f73\u200b\u7684\u200b\u5e73\u8861\u200b\uff0c\u200b\u5728\u200b\u4e0d\u540c\u200b\u7cbe\u5ea6\u200b\u8303\u56f4\u200b\u5185\u200b\u53d6\u5f97\u200b\u6781\u81f4\u200b\u7684\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u3002\u200b\u4f7f\u7528\u200bPP-LCNetV3\u200b\u66ff\u6362\u200bMobileNetv3 backbone\uff0cPP-OCRv4\u200b\u5b66\u751f\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200bhmean\u200b\u4ece\u200b78.24%\u200b\u63d0\u5347\u200b\u5230\u200b79.08%\u3002</p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv4_introduction.html#4cml-kd","title":"\uff084\uff09CML: \u200b\u878d\u5408\u200bKD\u200b\u7684\u200b\u4e92\u200b\u5b66\u4e60\u7b56\u7565","text":"<p>PP-OCRv4\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\u5bf9\u200bPP-OCRv3\u200b\u4e2d\u200b\u7684\u200bCML\uff08Collaborative Mutual Learning) \u200b\u534f\u540c\u200b\u4e92\u200b\u5b66\u4e60\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4f18\u5316\u200b\u3002\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0c\u200b\u5728\u200b\u8ba1\u7b97\u200bStudent Model\u200b\u548c\u200bTeacher Model\u200b\u7684\u200bdistill Loss\u200b\u65f6\u200b\uff0c\u200b\u989d\u5916\u200b\u6dfb\u52a0\u200bKL div loss\uff0c\u200b\u8ba9\u200b\u4e24\u8005\u200b\u8f93\u51fa\u200b\u7684\u200bresponse maps\u200b\u5206\u5e03\u200b\u63a5\u8fd1\u200b\uff0c\u200b\u7531\u6b64\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200bStudent\u200b\u7f51\u7edc\u200b\u7684\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u68c0\u6d4b\u200bHmean\u200b\u4ece\u200b79.08%\u200b\u589e\u52a0\u200b\u5230\u200b79.56%\uff0c\u200b\u7aef\u5230\u200b\u7aef\u200b\u6307\u6807\u200b\u4ece\u200b61.31%\u200b\u589e\u52a0\u200b\u5230\u200b61.87%\u3002</p> <p></p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv4_introduction.html#3","title":"3. \u200b\u8bc6\u522b\u200b\u4f18\u5316","text":"<p>PP-OCRv4\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u5728\u200bPP-OCRv3\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\u8fdb\u4e00\u6b65\u200b\u5347\u7ea7\u200b\u3002\u200b\u5982\u4e0b\u200b\u56fe\u200b\u6240\u793a\u200b\uff0c\u200b\u6574\u4f53\u200b\u7684\u200b\u6846\u67b6\u56fe\u200b\u4fdd\u6301\u200b\u4e86\u200b\u4e0e\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u76f8\u540c\u200b\u7684\u200bpipeline\uff0c\u200b\u5206\u522b\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6570\u636e\u200b\u3001\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u3001\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\u7b49\u200b\u65b9\u9762\u200b\u7684\u200b\u4f18\u5316\u200b\u3002</p> <p></p> <p>\u200b\u7ecf\u8fc7\u200b\u5982\u56fe\u6240\u793a\u200b\u7684\u200b\u7b56\u7565\u200b\u4f18\u5316\u200b\uff0cPP-OCRv4\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u76f8\u6bd4\u200bPP-OCRv3\uff0c\u200b\u5728\u200b\u901f\u5ea6\u200b\u53ef\u6bd4\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u7cbe\u5ea6\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b4%\u3002 \u200b\u5177\u4f53\u200b\u6d88\u878d\u200b\u5b9e\u9a8c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> ID \u200b\u7b56\u7565\u200b \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u7cbe\u5ea6\u200b \u200b\u9884\u6d4b\u200b\u8017\u65f6\u200b\uff08CPU openvino) 01 PP-OCRv3 12M 71.50% 8.54ms 02 +DF 12M 72.70% 8.54ms 03 + LiteNeck + GTC 9.6M 73.21% 9.09ms 04 + PP-LCNetV3 11M 74.18% 9.8ms 05 + multi-scale 11M 74.20% 9.8ms 06 + TextConAug 11M 74.72% 9.8ms 08 + UDML 11M 75.45% 9.8ms <p>\u200b\u6ce8\u200b\uff1a \u200b\u6d4b\u8bd5\u200b\u901f\u5ea6\u200b\u65f6\u200b\uff0c\u200b\u8f93\u5165\u200b\u56fe\u7247\u5c3a\u5bf8\u200b\u5747\u200b\u4e3a\u200b(3,48,320)\u3002\u200b\u5728\u200b\u5b9e\u9645\u200b\u9884\u6d4b\u200b\u65f6\u200b\uff0c\u200b\u56fe\u50cf\u200b\u4e3a\u200b\u53d8\u957f\u200b\u8f93\u5165\u200b\uff0c\u200b\u901f\u5ea6\u200b\u4f1a\u200b\u6709\u6240\u200b\u53d8\u5316\u200b\u3002\u200b\u6d4b\u8bd5\u73af\u5883\u200b\uff1a Intel Gold 6148 CPU\uff0c\u200b\u9884\u6d4b\u200b\u65f6\u200b\u4f7f\u7528\u200bOpenvino\u200b\u9884\u6d4b\u200b\u5f15\u64ce\u200b\u3002</p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv4_introduction.html#1df","title":"\uff081\uff09DF\uff1a\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848","text":"<p>DF(Data Filter) \u200b\u662f\u200b\u4e00\u79cd\u200b\u7b80\u5355\u200b\u6709\u6548\u200b\u7684\u200b\u6570\u636e\u6316\u6398\u200b\u65b9\u6848\u200b\u3002\u200b\u6838\u5fc3\u601d\u60f3\u200b\u662f\u200b\u5229\u7528\u200b\u5df2\u6709\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff0c\u200b\u901a\u8fc7\u200b\u7f6e\u4fe1\u5ea6\u200b\u548c\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u7b49\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5bf9\u200b\u5168\u91cf\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u7b5b\u9009\u200b\u3002\u200b\u5177\u4f53\u200b\u7684\u200b\uff1a\u200b\u9996\u5148\u200b\u4f7f\u7528\u200b\u5c11\u91cf\u200b\u6570\u636e\u200b\u5feb\u901f\u200b\u8bad\u7ec3\u200b\u5f97\u5230\u200b\u4e00\u4e2a\u200b\u4f4e\u200b\u7cbe\u5ea6\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f7f\u7528\u200b\u8be5\u200b\u4f4e\u200b\u7cbe\u5ea6\u200b\u6a21\u578b\u200b\u5bf9\u200b\u5343\u4e07\u7ea7\u200b\u7684\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u53bb\u9664\u200b\u7f6e\u4fe1\u5ea6\u200b\u5927\u4e8e\u200b0.95\u200b\u7684\u200b\u6837\u672c\u200b\uff0c\u200b\u8be5\u200b\u90e8\u5206\u200b\u88ab\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u5bf9\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\u7cbe\u5ea6\u200b\u65e0\u6548\u200b\u7684\u200b\u5197\u4f59\u200b\u6837\u672c\u200b\u3002\u200b\u5176\u6b21\u200b\u4f7f\u7528\u200bPP-OCRv3\u200b\u4f5c\u4e3a\u200b\u9ad8\u7cbe\u5ea6\u200b\u6a21\u578b\u200b\uff0c\u200b\u5bf9\u200b\u5269\u4f59\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u53bb\u9664\u200b\u7f6e\u4fe1\u5ea6\u200b\u5c0f\u4e8e\u200b0.15\u200b\u7684\u200b\u6837\u672c\u200b\uff0c\u200b\u8be5\u200b\u90e8\u5206\u200b\u88ab\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u96be\u4ee5\u200b\u8bc6\u522b\u200b\u6216\u200b\u8d28\u91cf\u200b\u5f88\u5dee\u200b\u7684\u200b\u6837\u672c\u200b\u3002 \u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\uff0c\u200b\u5343\u4e07\u200b\u7ea7\u522b\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u88ab\u200b\u7cbe\u7b80\u200b\u81f3\u200b\u767e\u4e07\u200b\u7ea7\u200b\uff0c\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u4ece\u200b2\u200b\u5468\u200b\u51cf\u5c11\u200b\u5230\u200b5\u200b\u5929\u200b\uff0c\u200b\u663e\u8457\u200b\u63d0\u5347\u200b\u4e86\u200b\u8bad\u7ec3\u200b\u6548\u7387\u200b\uff0c\u200b\u540c\u65f6\u200b\u7cbe\u5ea6\u200b\u63d0\u5347\u200b\u81f3\u200b72.7%(+1.2%)\u3002</p> <p></p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv4_introduction.html#2pp-lcnetv3","title":"\uff082\uff09PP-LCNetV3\uff1a\u200b\u7cbe\u5ea6\u200b\u66f4\u4f18\u200b\u7684\u200b\u9aa8\u5e72\u200b\u7f51\u7edc","text":"<p>PP-LCNetV3\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u662f\u200bPP-LCNet\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u7684\u200b\u5ef6\u7eed\u200b\uff0c\u200b\u8986\u76d6\u200b\u4e86\u200b\u66f4\u200b\u5927\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u8303\u56f4\u200b\uff0c\u200b\u80fd\u591f\u200b\u9002\u5e94\u200b\u4e0d\u540c\u200b\u4e0b\u6e38\u200b\u4efb\u52a1\u200b\u7684\u200b\u9700\u8981\u200b\u3002PP-LCNetV3\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u4ece\u200b\u591a\u4e2a\u200b\u65b9\u9762\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4f18\u5316\u200b\uff0c\u200b\u63d0\u51fa\u200b\u4e86\u200b\u53ef\u200b\u5b66\u4e60\u200b\u4eff\u5c04\u53d8\u6362\u200b\u6a21\u5757\u200b\uff0c\u200b\u5bf9\u91cd\u200b\u53c2\u6570\u200b\u5316\u200b\u7b56\u7565\u200b\u3001\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6539\u8fdb\u200b\uff0c\u200b\u540c\u65f6\u200b\u8c03\u6574\u200b\u4e86\u200b\u7f51\u7edc\u200b\u6df1\u5ea6\u200b\u4e0e\u200b\u5bbd\u5ea6\u200b\u3002\u200b\u6700\u7ec8\u200b\uff0cPP-LCNetV3\u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u80fd\u591f\u200b\u5728\u200b\u6027\u80fd\u200b\u4e0e\u200b\u6548\u7387\u200b\u4e4b\u95f4\u200b\u8fbe\u5230\u6700\u4f73\u200b\u7684\u200b\u5e73\u8861\u200b\uff0c\u200b\u5728\u200b\u4e0d\u540c\u200b\u7cbe\u5ea6\u200b\u8303\u56f4\u200b\u5185\u200b\u53d6\u5f97\u200b\u6781\u81f4\u200b\u7684\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u3002</p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv4_introduction.html#3lite-neckneck","title":"\uff083\uff09Lite-Neck\uff1a\u200b\u7cbe\u7b80\u200b\u53c2\u6570\u200b\u7684\u200bNeck\u200b\u7ed3\u6784","text":"<p>Lite-Neck\u200b\u6574\u4f53\u200b\u7ed3\u6784\u200b\u6cbf\u7528\u200bPP-OCRv3\u200b\u7248\u672c\u200b\u7684\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5728\u200b\u53c2\u6570\u200b\u4e0a\u200b\u7a0d\u4f5c\u200b\u7cbe\u7b80\u200b\uff0c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u6574\u4f53\u200b\u7684\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u53ef\u200b\u4ece\u200b12M\u200b\u964d\u4f4e\u200b\u5230\u200b8.5M\uff0c\u200b\u800c\u200b\u7cbe\u5ea6\u200b\u4e0d\u53d8\u200b\uff1b\u200b\u5728\u200bCTCHead\u200b\u4e2d\u200b\uff0c\u200b\u5c06\u200bNeck\u200b\u8f93\u51fa\u200b\u7279\u5f81\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u4ece\u200b64\u200b\u63d0\u5347\u200b\u5230\u200b120\uff0c\u200b\u6b64\u65f6\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u4ece\u200b8.5M\u200b\u63d0\u5347\u200b\u5230\u200b9.6M\u3002</p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv4_introduction.html#4gtc-nrtrattentionctc","title":"\uff084\uff09GTC-NRTR\uff1aAttention\u200b\u6307\u5bfc\u200bCTC\u200b\u8bad\u7ec3\u200b\u7b56\u7565","text":"<p>GTC\uff08Guided Training of CTC\uff09\uff0c\u200b\u662f\u200bPP-OCRv3\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u6700\u200b\u6709\u6548\u200b\u7684\u200b\u7b56\u7565\u200b\u4e4b\u4e00\u200b\uff0c\u200b\u878d\u5408\u200b\u591a\u79cd\u200b\u6587\u672c\u200b\u7279\u5f81\u200b\u7684\u200b\u8868\u8fbe\u200b\uff0c\u200b\u6709\u6548\u200b\u7684\u200b\u63d0\u5347\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u7cbe\u5ea6\u200b\u3002\u200b\u5728\u200bPP-OCRv4\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u66f4\u200b\u7a33\u5b9a\u200b\u7684\u200bTransformer\u200b\u6a21\u578b\u200bNRTR\u200b\u4f5c\u4e3a\u200b\u6307\u5bfc\u200b\u5206\u652f\u200b\uff0c\u200b\u76f8\u6bd4\u200bV3\u200b\u7248\u672c\u200b\u4e2d\u200b\u7684\u200bSAR\u200b\u57fa\u4e8e\u200b\u5faa\u73af\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u7ed3\u6784\u200b\uff0cNRTR\u200b\u57fa\u4e8e\u200bTransformer\u200b\u5b9e\u73b0\u200b\u89e3\u7801\u200b\u8fc7\u7a0b\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b\u66f4\u5f3a\u200b\uff0c\u200b\u80fd\u200b\u6709\u6548\u200b\u6307\u5bfc\u200bCTC\u200b\u5206\u652f\u200b\u5b66\u4e60\u200b\uff0c\u200b\u89e3\u51b3\u200b\u7b80\u5355\u200b\u573a\u666f\u200b\u4e0b\u200b\u5feb\u901f\u200b\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u95ee\u9898\u200b\u3002\u200b\u4f7f\u7528\u200bLite-Neck\u200b\u548c\u200bGTC-NRTR\u200b\u4e24\u4e2a\u200b\u7b56\u7565\u200b\uff0c\u200b\u8bc6\u522b\u200b\u7cbe\u5ea6\u200b\u63d0\u5347\u200b\u81f3\u200b73.21%(+0.5%)\u3002</p> <p></p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv4_introduction.html#5multi-scale","title":"\uff085\uff09Multi-Scale\uff1a\u200b\u591a\u200b\u5c3a\u5ea6\u200b\u8bad\u7ec3\u200b\u7b56\u7565","text":"<p>\u200b\u52a8\u6001\u200b\u5c3a\u5ea6\u200b\u8bad\u7ec3\u200b\u7b56\u7565\u200b\uff0c\u200b\u662f\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u968f\u673a\u200bresize\u200b\u8f93\u5165\u200b\u56fe\u7247\u200b\u7684\u200b\u9ad8\u5ea6\u200b\uff0c\u200b\u4ee5\u200b\u589e\u5f3a\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u5728\u200b\u7aef\u200b\u5230\u200b\u7aef\u200b\u4e32\u8054\u200b\u4f7f\u7528\u200b\u65f6\u200b\u7684\u200b\u9c81\u68d2\u6027\u200b\u3002\u200b\u5728\u200b\u8bad\u7ec3\u200b\u65f6\u200b\uff0c\u200b\u6bcf\u4e2a\u200biter\u200b\u4ece\u200b\uff0832\uff0c48\uff0c64\uff09\u200b\u4e09\u79cd\u200b\u9ad8\u5ea6\u200b\u4e2d\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u4e00\u79cd\u200b\u9ad8\u5ea6\u200b\u8fdb\u884c\u200bresize\u3002\u200b\u5b9e\u9a8c\u200b\u8bc1\u660e\u200b\uff0c\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\uff0c\u200b\u5c3d\u7ba1\u200b\u5728\u200b\u8bc6\u522b\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u51c6\u786e\u7387\u200b\u6ca1\u6709\u200b\u63d0\u5347\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u7aef\u200b\u5230\u200b\u7aef\u200b\u4e32\u8054\u200b\u8bc4\u4f30\u200b\u65f6\u200b\uff0c\u200b\u6307\u6807\u200b\u63d0\u5347\u200b0.5%\u3002</p> <p></p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv4_introduction.html#6dkd","title":"\uff086\uff09DKD\uff1a\u200b\u84b8\u998f\u200b\u7b56\u7565","text":"<p>\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u7684\u200b\u84b8\u998f\u200b\u5305\u542b\u200b\u4e24\u4e2a\u200b\u90e8\u5206\u200b\uff0cNRTRhead\u200b\u84b8\u998f\u200b\u548c\u200bCTCHead\u200b\u84b8\u998f\u200b;</p> <p>\u200b\u5bf9\u4e8e\u200bNRTR head\uff0c\u200b\u4f7f\u7528\u200b\u4e86\u200bDKD loss\u200b\u84b8\u998f\u200b\uff0c\u200b\u62c9\u8fd1\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u548c\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u7684\u200bNRTR head logits\u3002\u200b\u6700\u7ec8\u200bNRTR head\u200b\u7684\u200bloss\u200b\u662f\u200b\u5b66\u751f\u200b\u4e0e\u200b\u6559\u5e08\u200b\u95f4\u200b\u7684\u200bDKD loss\u200b\u548c\u200b\u4e0e\u200bground truth\u200b\u7684\u200bcross entropy loss\u200b\u7684\u200b\u52a0\u6743\u200b\u548c\u200b\uff0c\u200b\u7528\u4e8e\u200b\u76d1\u7763\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200bbackbone\u200b\u8bad\u7ec3\u200b\u3002\u200b\u901a\u8fc7\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u6211\u4eec\u200b\u53d1\u73b0\u200b\u52a0\u5165\u200bDKD loss\u200b\u540e\u200b\uff0c\u200b\u8ba1\u7b97\u200b\u4e0e\u200bground truth\u200b\u7684\u200bcross entropy loss\u200b\u65f6\u200b\u53bb\u9664\u200blabel smoothing\u200b\u53ef\u4ee5\u200b\u8fdb\u4e00\u6b65\u63d0\u9ad8\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u5728\u200b\u8fd9\u91cc\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u4e0d\u5e26\u200blabel smoothing\u200b\u7684\u200bcross entropy loss\u3002</p> <p>\u200b\u5bf9\u4e8e\u200bCTCHead\uff0c\u200b\u7531\u4e8e\u200bCTC\u200b\u7684\u200b\u8f93\u51fa\u200b\u4e2d\u200b\u5b58\u5728\u200bBlank\u200b\u4f4d\u200b\uff0c\u200b\u5373\u4f7f\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u548c\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4e00\u6837\u200b\uff0c\u200b\u4e8c\u8005\u200b\u7684\u200b\u8f93\u51fa\u200b\u7684\u200blogits\u200b\u5206\u5e03\u200b\u4e5f\u200b\u4f1a\u200b\u5b58\u5728\u200b\u5dee\u5f02\u200b\uff0c\u200b\u5f71\u54cd\u200b\u6559\u5e08\u200b\u6a21\u578b\u200b\u5411\u200b\u5b66\u751f\u200b\u6a21\u578b\u200b\u7684\u200b\u77e5\u8bc6\u200b\u4f20\u9012\u200b\u3002PP-OCRv4\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\u4e2d\u200b\uff0c\u200b\u5c06\u200bCTC\u200b\u8f93\u51fa\u200blogits\u200b\u6cbf\u7740\u200b\u6587\u672c\u200b\u957f\u5ea6\u200b\u7ef4\u5ea6\u200b\u8ba1\u7b97\u200b\u5747\u503c\u200b\uff0c\u200b\u5c06\u200b\u591a\u200b\u5b57\u7b26\u8bc6\u522b\u200b\u95ee\u9898\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u591a\u200b\u5b57\u7b26\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff0c\u200b\u7528\u4e8e\u200b\u76d1\u7763\u200bCTC Head\u200b\u7684\u200b\u8bad\u7ec3\u200b\u3002\u200b\u4f7f\u7528\u200b\u8be5\u200b\u7b56\u7565\u200b\u878d\u5408\u200bNRTRhead DKD\u200b\u84b8\u998f\u200b\u7b56\u7565\u200b\uff0c\u200b\u6307\u6807\u200b\u4ece\u200b74.72%\u200b\u63d0\u5347\u200b\u5230\u200b75.45%\u3002</p>"},{"location":"en/version2.x/ppocr/blog/PP-OCRv4_introduction.html#4","title":"4. \u200b\u7aef\u5230\u200b\u7aef\u200b\u8bc4\u4f30","text":"<p>\u200b\u7ecf\u8fc7\u200b\u4ee5\u4e0a\u200b\u4f18\u5316\u200b\uff0c\u200b\u6700\u7ec8\u200bPP-OCRv4\u200b\u5728\u200b\u901f\u5ea6\u200b\u53ef\u6bd4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4e2d\u6587\u200b\u573a\u666f\u200b\u7aef\u5230\u200b\u7aef\u200bHmean\u200b\u6307\u6807\u200b\u76f8\u6bd4\u200b\u4e8e\u200bPP-OCRv3\u200b\u63d0\u5347\u200b4.5%\uff0c\u200b\u6548\u679c\u200b\u5927\u5e45\u200b\u63d0\u5347\u200b\u3002\u200b\u5177\u4f53\u200b\u6307\u6807\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\uff1a</p> Model Hmean Model Size (M) Time Cost (CPU, ms) PP-OCRv3 57.99% 15.6 78 PP-OCRv4 62.24% 15.8 76 <p>\u200b\u6d4b\u8bd5\u73af\u5883\u200b\uff1aCPU\u200b\u578b\u53f7\u200b\u4e3a\u200bIntel Gold 6148\uff0cCPU\u200b\u9884\u6d4b\u200b\u65f6\u200b\u4f7f\u7528\u200bopenvino\u3002</p> <p>\u200b\u9664\u4e86\u200b\u66f4\u65b0\u200b\u4e2d\u6587\u200b\u6a21\u578b\u200b\uff0c\u200b\u672c\u6b21\u200b\u5347\u7ea7\u200b\u4e5f\u200b\u4f18\u5316\u200b\u4e86\u200b\u82f1\u6587\u200b\u6570\u5b57\u6a21\u578b\u200b\uff0c\u200b\u5728\u200b\u81ea\u6709\u200b\u8bc4\u4f30\u200b\u96c6\u4e0a\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u51c6\u786e\u7387\u200b\u63d0\u5347\u200b6%\uff0c\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\uff1a</p> Model ACC PP-OCR_en 54.38% PP-OCRv3_en 64.04% PP-OCRv4_en 70.1% <p>\u200b\u540c\u65f6\u200b\uff0c\u200b\u5bf9\u200b\u5df2\u200b\u652f\u6301\u200b\u7684\u200b80\u200b\u4f59\u79cd\u200b\u8bed\u8a00\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5347\u7ea7\u200b\u66f4\u65b0\u200b\uff0c\u200b\u5728\u200b\u6709\u200b\u8bc4\u4f30\u200b\u96c6\u200b\u7684\u200b\u56db\u79cd\u200b\u8bed\u7cfb\u200b\u8bc6\u522b\u200b\u51c6\u786e\u7387\u200b\u5e73\u5747\u200b\u63d0\u5347\u200b8%\u200b\u4ee5\u4e0a\u200b\uff0c\u200b\u5982\u4e0b\u200b\u8868\u200b\u6240\u793a\u200b\uff1a</p> Model \u200b\u62c9\u4e01\u8bed\u7cfb\u200b \u200b\u963f\u62c9\u4f2f\u8bed\u200b\u7cfb\u200b \u200b\u65e5\u8bed\u200b \u200b\u97e9\u8bed\u200b PP-OCR_mul 69.60% 40.50% 38.50% 55.40% PP-OCRv3_mul 71.57% 72.90% 45.85% 77.23% PP-OCRv4_mul 80.00% 75.48% 56.50% 83.25%"},{"location":"en/version2.x/ppocr/blog/clone.html","title":"Project Clone","text":""},{"location":"en/version2.x/ppocr/blog/clone.html#1-clone-paddleocr","title":"1. Clone PaddleOCR","text":"<pre><code># Recommend\ngit clone https://github.com/PaddlePaddle/PaddleOCR\n\n# If you cannot pull successfully due to network problems, you can switch to the mirror hosted on Gitee:\n\ngit clone https://gitee.com/paddlepaddle/PaddleOCR\n\n# Note: The mirror on Gitee may not keep in synchronization with the latest project on GitHub. There might be a delay of 3-5 days. Please try GitHub at first.\n</code></pre>"},{"location":"en/version2.x/ppocr/blog/clone.html#2-install-third-party-libraries","title":"2. Install third-party libraries","text":"<pre><code>cd PaddleOCR\npip3 install -r requirements.txt\n</code></pre> <p>If you getting this error <code>OSError: [WinError 126] The specified module could not be found</code> when you install shapely on windows.</p> <p>Please try to download Shapely whl file from http://www.lfd.uci.edu/~gohlke/pythonlibs/#shapely.</p> <p>Reference: Solve shapely installation on windows</p>"},{"location":"en/version2.x/ppocr/blog/config.html","title":"Configuration","text":""},{"location":"en/version2.x/ppocr/blog/config.html#1-optional-parameter-list","title":"1. Optional Parameter List","text":"<p>The following list can be viewed through <code>--help</code></p> FLAG Supported script Use Defaults Note -c ALL Specify configuration file to use None Please refer to the parameter introduction for configuration file usage -o ALL set configuration options None Configuration using -o has higher priority than the configuration file selected with -c. E.g: -o Global.use_gpu=false"},{"location":"en/version2.x/ppocr/blog/config.html#2-introduction-to-global-parameters-of-configuration-file","title":"2. Introduction to Global Parameters of Configuration File","text":"<p>Take rec_chinese_lite_train_v2.0.yml as an example</p>"},{"location":"en/version2.x/ppocr/blog/config.html#global","title":"Global","text":"Parameter Use Defaults Note use_gpu Set using GPU or not true \\ epoch_num Maximum training epoch number 500 \\ log_smooth_window Log queue length, the median value in the queue each time will be printed 20 \\ print_batch_step Set print log interval 10 \\ save_model_dir Set model save path output/{\u200b\u7b97\u6cd5\u200b\u540d\u79f0\u200b} \\ save_epoch_step Set model save interval 3 \\ eval_batch_step Set the model evaluation interval 2000 or [1000, 2000] running evaluation every 2000 iters or evaluation is run every 2000 iterations after the 1000th iteration cal_metric_during_train Set whether to evaluate the metric during the training process. At this time, the metric of the model under the current batch is evaluated true \\ load_static_weights Set whether the pre-training model is saved in static graph mode (currently only required by the detection algorithm) true \\ pretrained_model Set the path of the pre-trained model ./pretrain_models/CRNN/best_accuracy \\ checkpoints set model parameter path None Used to load parameters after interruption to continue training use_visualdl Set whether to enable visualdl for visual log display False Tutorial use_wandb Set whether to enable W&amp;B for visual log display False Documentation infer_img Set inference image path or folder path ./infer_img \\ character_dict_path Set dictionary path ./ppocr/utils/ppocr_keys_v1.txt If the character_dict_path is None, model can only recognize number and lower letters max_text_length Set the maximum length of text 25 \\ use_space_char Set whether to recognize spaces True \\ label_list Set the angle supported by the direction classifier ['0','180'] Only valid in angle classifier model save_res_path Set the save address of the test model results ./output/det_db/predicts_db.txt Only valid in the text detection model"},{"location":"en/version2.x/ppocr/blog/config.html#optimizer-ppocroptimizer","title":"Optimizer (ppocr/optimizer)","text":"Parameter Use Defaults Note name Optimizer class name Adam Currently supports<code>Momentum</code>,<code>Adam</code>,<code>RMSProp</code>, see ppocr/optimizer/optimizer.py beta1 Set the exponential decay rate for the 1st moment estimates 0.9 \\ beta2 Set the exponential decay rate for the 2nd moment estimates 0.999 \\ clip_norm The maximum norm value - \\ lr Set the learning rate decay method - \\ name Learning rate decay class name Cosine Currently supports<code>Linear</code>,<code>Cosine</code>,<code>Step</code>,<code>Piecewise</code>, see ppocr/optimizer/learning_rate.py learning_rate Set the base learning rate 0.001 \\ regularizer Set network regularization method - \\ name Regularizer class name L2 Currently support<code>L1</code>,<code>L2</code>, see ppocr/optimizer/regularizer.py factor Regularizer coefficient 0.00001 \\"},{"location":"en/version2.x/ppocr/blog/config.html#architecture-ppocrmodeling","title":"Architecture (ppocr/modeling)","text":"<p>In PaddleOCR, the network is divided into four stages: Transform, Backbone, Neck and Head</p> Parameter Use Defaults Note model_type Network Type rec Currently support<code>rec</code>,<code>det</code>,<code>cls</code> algorithm Model name CRNN See algorithm_overview for the support list Transform Set the transformation method - Currently only recognition algorithms are supported, see ppocr/modeling/transform for details name Transformation class name TPS Currently supports <code>TPS</code> num_fiducial Number of TPS control points 20 Ten on the top and bottom loc_lr Localization network learning rate 0.1 \\ model_name Localization network size small Currently support<code>small</code>,<code>large</code> Backbone Set the network backbone class name - see ppocr/modeling/backbones name backbone class name ResNet Currently support<code>MobileNetV3</code>,<code>ResNet</code> layers resnet layers 34 Currently support18,34,50,101,152,200 model_name MobileNetV3 network size small Currently support<code>small</code>,<code>large</code> Neck Set network neck - see ppocr/modeling/necks name neck class name SequenceEncoder Currently support<code>SequenceEncoder</code>,<code>DBFPN</code> encoder_type SequenceEncoder encoder type rnn Currently support<code>reshape</code>,<code>fc</code>,<code>rnn</code> hidden_size rnn number of internal units 48 \\ out_channels Number of DBFPN output channels 256 \\ Head Set the network head - see ppocr/modeling/heads name head class name CTCHead Currently support<code>CTCHead</code>,<code>DBHead</code>,<code>ClsHead</code> fc_decay CTCHead regularization coefficient 0.0004 \\ k DBHead binarization coefficient 50 \\ class_dim ClsHead output category number 2 \\"},{"location":"en/version2.x/ppocr/blog/config.html#loss-ppocrlosses","title":"Loss (ppocr/losses)","text":"Parameter Use Defaults Note name loss class name CTCLoss Currently support<code>CTCLoss</code>,<code>DBLoss</code>,<code>ClsLoss</code> balance_loss Whether to balance the number of positive and negative samples in DBLossloss (using OHEM) True \\ ohem_ratio The negative and positive sample ratio of OHEM in DBLossloss 3 \\ main_loss_type The loss used by shrink_map in DBLossloss DiceLoss Currently support<code>DiceLoss</code>,<code>BCELoss</code> alpha The coefficient of shrink_map_loss in DBLossloss 5 \\ beta The coefficient of threshold_map_loss in DBLossloss 10 \\"},{"location":"en/version2.x/ppocr/blog/config.html#postprocess-ppocrpostprocess","title":"PostProcess (ppocr/postprocess)","text":"Parameter Use Defaults Note name Post-processing class name CTCLabelDecode Currently support<code>CTCLoss</code>,<code>AttnLabelDecode</code>,<code>DBPostProcess</code>,<code>ClsPostProcess</code> thresh The threshold for binarization of the segmentation map in DBPostProcess 0.3 \\ box_thresh The threshold for filtering output boxes in DBPostProcess. Boxes below this threshold will not be output 0.7 \\ max_candidates The maximum number of text boxes output in DBPostProcess 1000 unclip_ratio The unclip ratio of the text box in DBPostProcess 2.0 \\"},{"location":"en/version2.x/ppocr/blog/config.html#metric-ppocrmetrics","title":"Metric (ppocr/metrics)","text":"Parameter Use Defaults Note name Metric method name CTCLabelDecode Currently support<code>DetMetric</code>,<code>RecMetric</code>,<code>ClsMetric</code> main_indicator Main indicators, used to select the best model acc For the detection method is hmean, the recognition and classification method is acc"},{"location":"en/version2.x/ppocr/blog/config.html#dataset-ppocrdata","title":"Dataset  (ppocr/data)","text":"Parameter Use Defaults Note dataset Return one sample per iteration - - name dataset class name SimpleDataSet Currently support<code>SimpleDataSet</code>,<code>LMDBDataSet</code> data_dir Image folder path ./train_data \\ label_file_list Groundtruth file path [\"./train_data/train_list.txt\"] This parameter is not required when dataset is LMDBDataSet ratio_list Ratio of data set [1.0] If there are two train_lists in label_file_list and ratio_list is [0.4,0.6], 40% will be sampled from train_list1, and 60% will be sampled from train_list2 to combine the entire dataset transforms List of methods to transform images and labels [DecodeImage,CTCLabelEncode,RecResizeImg,KeepKeys] see ppocr/data/imaug loader dataloader related - shuffle Does each epoch disrupt the order of the data set True \\ batch_size_per_card Single card batch size during training 256 \\ drop_last Whether to discard the last incomplete mini-batch because the number of samples in the data set cannot be divisible by batch_size True \\ num_workers The number of sub-processes used to load data, if it is 0, the sub-process is not started, and the data is loaded in the main process 8 \\"},{"location":"en/version2.x/ppocr/blog/config.html#weights-biases-wb","title":"Weights &amp; Biases (W&amp;B)","text":"Parameter Use Defaults Note project Project to which the run is to be logged uncategorized \\ name Alias/Name of the run Randomly generated by wandb \\ id ID of the run Randomly generated by wandb \\ entity User or team to which the run is being logged The logged in user \\ save_dir local directory in which all the models and other data is saved wandb \\ config model configuration None \\"},{"location":"en/version2.x/ppocr/blog/config.html#3-multilingual-config-file-generation","title":"3. Multilingual Config File Generation","text":"<p>PaddleOCR currently supports recognition for 80 languages (besides Chinese). A multi-language configuration file template is provided under the path <code>configs/rec/multi_languages</code>: rec_multi_language_lite_train.yml.</p> <p>There are two ways to create the required configuration file:</p> <ol> <li>Automatically generated by script</li> </ol> <p>Script generate_multi_language_configs.py can help you generate configuration files for multi-language models.</p> <ul> <li> <p>Take Italian as an example, if your data is prepared in the following format:</p> <pre><code>|-train_data\n    |- it_train.txt # train_set label\n    |- it_val.txt # val_set label\n    |- data\n        |- word_001.jpg\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre> <p>You can use the default parameters to generate a configuration file:</p> <pre><code># The code needs to be run in the specified directory\ncd PaddleOCR/configs/rec/multi_language/\n# Set the configuration file of the language to be generated through the -l or --language parameter.\n# This command will write the default parameters into the configuration file\npython3 generate_multi_language_configs.py -l it\n</code></pre> </li> <li> <p>If your data is placed in another location, or you want to use your own dictionary, you can generate the configuration file by specifying the relevant parameters:</p> <pre><code># -l or --language field is required\n# --train to modify the training set\n# --val to modify the validation set\n# --data_dir to modify the data set directory\n# --dict to modify the dict path\n# -o to modify the corresponding default parameters\ncd PaddleOCR/configs/rec/multi_language/\npython3 generate_multi_language_configs.py -l it \\  # language\n--train {path/of/train_label.txt} \\ # path of train_label\n--val {path/of/val_label.txt} \\     # path of val_label\n--data_dir {train_data/path} \\      # root directory of training data\n--dict {path/of/dict} \\             # path of dict\n-o Global.use_gpu=False             # whether to use gpu\n...\n</code></pre> </li> </ul> <p>Italian is made up of Latin letters, so after executing the command, you will get the rec_latin_lite_train.yml.</p> <ol> <li>Manually modify the configuration file</li> </ol> <p>You can also manually modify the following fields in the template:</p> <pre><code> Global:\n   use_gpu: True\n   epoch_num: 500\n   ...\n   character_dict_path:  {path/of/dict} # path of dict\n\nTrain:\n   dataset:\n     name: SimpleDataSet\n     data_dir: train_data/ # root directory of training data\n     label_file_list: [\"./train_data/train_list.txt\"] # train label path\n   ...\n\nEval:\n   dataset:\n     name: SimpleDataSet\n     data_dir: train_data/ # root directory of val data\n     label_file_list: [\"./train_data/val_list.txt\"] # val label path\n   ...\n</code></pre> <p>Currently, the multi-language algorithms supported by PaddleOCR are:</p> Configuration file Algorithm name backbone trans seq pred language rec_chinese_cht_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc chinese traditional rec_en_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc English(Case sensitive) rec_french_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc French rec_ger_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc German rec_japan_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc Japanese rec_korean_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc Korean rec_latin_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc Latin rec_arabic_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc arabic rec_cyrillic_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc cyrillic rec_devanagari_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc devanagari <p>For more supported languages, please refer to: Multi-language model</p> <p>The multi-language model training method is the same as the Chinese model. The training data set is 100w synthetic data. A small amount of fonts and test data can be downloaded using the following two methods.</p> <ul> <li>Baidu Netdisk,Extraction code:frgi.</li> <li>Google drive</li> </ul>"},{"location":"en/version2.x/ppocr/blog/customize.html","title":"HOW TO MAKE YOUR OWN LIGHTWEIGHT OCR MODEL?","text":"<p>The process of making a customized ultra-lightweight OCR models can be divided into three steps: training text detection model, training text recognition model, and concatenate the predictions from previous steps.</p>"},{"location":"en/version2.x/ppocr/blog/customize.html#step1-train-text-detection-model","title":"STEP1: TRAIN TEXT DETECTION MODEL","text":"<p>PaddleOCR provides two text detection algorithms: EAST and DB. Both support MobileNetV3 and ResNet50_vd backbone networks, select the corresponding configuration file as needed and start training. For example, to train with MobileNetV3 as the backbone network for DB detection model :</p> <pre><code>python3 tools/train.py -c configs/det/det_mv3_db.yml 2&gt;&amp;1 | tee det_db.log\n</code></pre> <p>For more details about data preparation and training tutorials, refer to the documentation Text detection model training/evaluation/prediction</p>"},{"location":"en/version2.x/ppocr/blog/customize.html#step2-train-text-recognition-model","title":"STEP2: TRAIN TEXT RECOGNITION MODEL","text":"<p>PaddleOCR provides four text recognition algorithms: CRNN, Rosetta, STAR-Net, and RARE. They all support two backbone networks: MobileNetV3 and ResNet34_vd, select the corresponding configuration files as needed to start training. For example, to train a CRNN recognition model that uses MobileNetV3 as the backbone network:</p> <pre><code>python3 tools/train.py -c configs/rec/rec_chinese_lite_train.yml 2&gt;&amp;1 | tee rec_ch_lite.log\n</code></pre> <p>For more details about data preparation and training tutorials, refer to the documentation Text recognition model training/evaluation/prediction</p>"},{"location":"en/version2.x/ppocr/blog/customize.html#step3-concatenate-predictions","title":"STEP3: CONCATENATE PREDICTIONS","text":"<p>PaddleOCR provides a concatenation tool for detection and recognition models, which can connect any trained detection model and any recognition model into a two-stage text recognition system. The input image goes through four main stages: text detection, text rectification, text recognition, and score filtering to output the text position and recognition results, and at the same time, you can choose to visualize the results.</p> <p>When performing prediction, you need to specify the path of a single image or a image folder through the parameter <code>image_dir</code>, the parameter <code>det_model_dir</code> specifies the path of detection model, and the parameter <code>rec_model_dir</code> specifies the path of recognition model. The visualized results are saved to the <code>./inference_results</code> folder by default.</p> <pre><code>python3 tools/infer/predict_system.py --image_dir=\"./doc/imgs/11.jpg\" --det_model_dir=\"./inference/det/\"  --rec_model_dir=\"./inference/rec/\"\n</code></pre> <p>For more details about text detection and recognition concatenation, please refer to the document Inference</p>"},{"location":"en/version2.x/ppocr/blog/distributed_training.html","title":"Distributed training","text":""},{"location":"en/version2.x/ppocr/blog/distributed_training.html#introduction","title":"Introduction","text":"<p>The high performance of distributed training is one of the core advantages of PaddlePaddle. In the classification task, distributed training can achieve almost linear speedup ratio. Generally, OCR training task need massive training data. Such as recognition, PP-OCR v2.0 model is trained based on 1800W dataset, which is very time-consuming if using single machine. Therefore, the distributed training is used in PaddleOCR to speedup the training task. For more information about distributed training, please refer to distributed training quick start tutorial.</p>"},{"location":"en/version2.x/ppocr/blog/distributed_training.html#quick-start","title":"Quick Start","text":""},{"location":"en/version2.x/ppocr/blog/distributed_training.html#training-with-single-machine","title":"Training with single machine","text":"<p>Take recognition as an example. After the data is prepared locally, start the training task with the interface of <code>paddle.distributed.launch</code>. The start command as follows:</p> <pre><code>python3 -m paddle.distributed.launch \\\n    --log_dir=./log/ \\\n    --gpus \"0,1,2,3,4,5,6,7\" \\\n    tools/train.py \\\n    -c configs/rec/rec_mv3_none_bilstm_ctc.yml\n</code></pre>"},{"location":"en/version2.x/ppocr/blog/distributed_training.html#training-with-multi-machine","title":"Training with multi machine","text":"<p>Compared with single machine, training with multi machine only needs to add the parameter <code>--ips</code> to start command, which represents the IP list of machines used for distributed training, and the IP of different machines are separated by commas. The start command as follows:</p> <pre><code>ip_list=\"192.168.0.1,192.168.0.2\"\npython3 -m paddle.distributed.launch \\\n    --log_dir=./log/ \\\n    --ips=\"${ip_list}\" \\\n    --gpus=\"0,1,2,3,4,5,6,7\" \\\n    tools/train.py \\\n    -c configs/rec/rec_mv3_none_bilstm_ctc.yml\n</code></pre> <p>Notice:</p> <ul> <li>The IP addresses of different machines need to be separated by commas, which can be queried through <code>ifconfig</code> or <code>ipconfig</code>.</li> <li>Different machines need to be set to be secret free and can <code>ping</code> success with others directly, otherwise communication cannot establish between them.</li> <li>The code, data and start command between different machines must be completely consistent and then all machines need to run start command. The first machine in the <code>ip_list</code> is set to <code>trainer0</code>, and so on.</li> </ul>"},{"location":"en/version2.x/ppocr/blog/distributed_training.html#performance-comparison","title":"Performance comparison","text":"<p>We conducted model training on 2x8 P40 GPUs. Accuracy, training time, and multi machine acceleration ratio of different models are shown below.</p> Model Configuration Configuration 8 GPU training time / Accuracy 3x8 GPU training time / Accuracy Acceleration ratio CRNN rec_chinese_lite_train_v2.0.yml 260k Chinese dataset 2.50d/66.70% 1.67d/67.00% 1.5 <p>We conducted model training on 3x8 V100 GPUs. Accuracy, training time, and multi machine acceleration ratio of different models are shown below.</p> Model Configuration Configuration 8 GPU training time / Accuracy 3x8 GPU training time / Accuracy Acceleration ratio SLANet SLANet.yml PubTabNet 49.80h/76.20% 19.75h/74.77% 2.52 <p>Note: when training with 3x8 GPUs, the single card batch size is unchanged compared with the 1x8 GPUs' training process, and the learning rate is multiplied by 2 (if it is multiplied by 3 by default, the accuracy is only 73.42%).</p> <p>We conducted model training on 4x8 V100 GPUs. Accuracy, training time, and multi machine acceleration ratio of different models are shown below.</p> Model Configuration Configuration 8 GPU training time / Accuracy 4x8 GPU training time / Accuracy Acceleration ratio SVTR PP-OCRv3_mobile_rec_distillation.yml PP-OCRv3_rec data 10d/- 2.84d/74.00% 3.5"},{"location":"en/version2.x/ppocr/blog/enhanced_ctc_loss.html","title":"Enhanced CTC Loss","text":"<p>In OCR recognition, CRNN is a text recognition algorithm widely applied in the industry. In the training phase, it uses CTCLoss to calculate the network loss. In the inference phase, it uses CTCDecode to obtain the decoding result. Although the CRNN algorithm has been proven to achieve reliable recognition results in actual business, users have endless requirements for recognition accuracy. So how to improve the accuracy of text recognition? Taking CTCLoss as the starting point, this paper explores the improved fusion scheme of CTCLoss from three different perspectives: Hard Example Mining, Multi-task Learning, and Metric Learning. Based on the exploration, we propose EnhancedCTCLoss, which includes the following 3 components: Focal-CTC Loss, A-CTC Loss, C-CTC Loss.</p>"},{"location":"en/version2.x/ppocr/blog/enhanced_ctc_loss.html#1-focal-ctc-loss","title":"1. Focal-CTC Loss","text":"<p>Focal Loss was proposed by the paper, \"Focal Loss for Dense Object Detection\". When the loss was first proposed, it was mainly to solve the problem of a serious imbalance in the ratio of positive and negative samples in one-stage target detection. This loss function reduces the weight of a large number of simple negative samples in training and also can be understood as a kind of difficult sample mining. The form of the loss function is as follows:</p> \\[ \\begin{equation} L_{fl}=\\left\\{ \\begin{array}{cl} -\\alpha(1 - y^{'})^{\\gamma}logy^{'} ,&amp;  y = 1 \\\\ -(1 - \\alpha)y^{'\\gamma}log(1 - y^{'}), &amp;  y = 0 \\\\ \\end{array} \\right. \\end{equation} \\] <p>Among them, y' is the output of the activation function, and the value is between 0-1. It adds a modulation factor (1-y\u2019)^\u03b3 and a balance factor \u03b1 on the basis of the original cross-entropy loss. When \u03b1 = 1, y = 1, the comparison between the loss function and the cross-entropy loss is shown in the following figure:</p> <p></p> <p>As can be seen from the above figure, when \u03b3\u00a0&gt; 0, the adjustment coefficient (1-y\u2019)^\u03b3 gives smaller weight to the easy-to-classify sample loss, making the network pay more attention to the difficult and misclassified samples. The adjustment factor \u03b3 is used to adjust the rate at which the weight of simple samples decreases. When \u03b3 = 0, it is the cross-entropy loss function. When \u03b3 increases, the influence of the adjustment factor will also increase. Experiments revealed that 2 is the optimal value of \u03b3. The balance factor \u03b1 is used to balance the uneven proportions of the positive and negative samples. In the text, \u03b1 is taken as 0.25.</p> <p>For the classic CTC algorithm, suppose a certain feature sequence (f<sub>1</sub>, f<sub>2</sub>, ......f<sub>t</sub>), after CTC decoding, the probability that the result is equal to label is y', then the probability that the CTC decoding result is not equal to label is (1-y'); it is not difficult to find that the CTCLoss value and y' have the following relationship:</p> \\[ L_{CTC} = -log(y^{'}) \\] <p>Combining the idea of Focal Loss, assigning larger weights to difficult samples and smaller weights to simple samples can make the network focus more on the mining of difficult samples and further improve the accuracy of recognition. Therefore, we propose Focal-CTC Loss. Its definition is as follows:</p> \\[ L_{Focal\\_CTC} = \\alpha * (1 - y^{'})^{\\gamma} * L_{CTC} \\] <p>In the experiment, the value of \u03b3 is 2, \u03b1\u00a0= 1, see this for specific implementation: rec_ctc_loss.py</p>"},{"location":"en/version2.x/ppocr/blog/enhanced_ctc_loss.html#2-a-ctc-loss","title":"2. A-CTC Loss","text":"<p>A-CTC Loss is short for CTC Loss + ACE Loss. Among them, ACE Loss was proposed by the paper, \u201cAggregation Cross-Entropy for Sequence Recognition\u201d. Compared with CTCLoss, ACE Loss has the following two advantages: + ACE Loss can solve the recognition problem of 2-D text, while CTCLoss can only process 1-D text + ACE Loss is better than CTC loss in time complexity and space complexity</p> <p>The advantages and disadvantages of the OCR recognition algorithm summarized by the predecessors are shown in the following figure:</p> <p></p> <p>Although ACELoss does handle 2D predictions, as shown in the figure above, and has advantages in memory usage and inference speed, in practice, we found that using ACELoss alone, the recognition effect is not as good as CTCLoss. Consequently, we tried to combine CTCLoss and ACELoss, and CTCLoss is the mainstay while ACELoss acts as an auxiliary supervision loss. This attempt has achieved better results. On our internal experimental data set, compared to using CTCLoss alone, the recognition accuracy can be improved by about 1%. A_CTC Loss is defined as follows:</p> \\[ L_{A-CTC} = L_{CTC} + \\lambda * L_{ACE} \\] <p>In the experiment, \u03bb = 0.1. See the ACE loss implementation code: ace_loss.py</p>"},{"location":"en/version2.x/ppocr/blog/enhanced_ctc_loss.html#3-c-ctc-loss","title":"3. C-CTC Loss","text":"<p>C-CTC Loss is short for CTC Loss + Center Loss. Among them, Center Loss was proposed by the paper, \u201cA Discriminative Feature Learning Approach for Deep Face Recognition\u201c. It was first used in face recognition tasks to increase the distance between classes and reduce the distance within classes. It is an earlier and also widely used algorithm.</p> <p>In the task of Chinese OCR recognition, through the analysis of bad cases, we found that a major difficulty in Chinese recognition is that there are many similar characters, which are easy to misunderstand. From this, we thought about whether we can learn from the idea of n to increase the class spacing of similar characters, to improve recognition accuracy. However, Metric Learning is mainly used in the field of image recognition, and the label of the training data is a fixed value; for OCR recognition, it is a sequence recognition task essentially, and there is no explicit alignment between features and labels. Therefore, how to combine the two is still a direction worth exploring.</p> <p>By trying Arcmargin, Cosmargin and other methods, we finally found that Centerloss can help further improve the accuracy of recognition. C_CTC Loss is defined as follows:</p> \\[ L_{C-CTC} = L_{CTC} + \\lambda * L_{center} \\] <p>In the experiment, we set \u03bb=0.25. See the center_loss implementation code: center_loss.py</p> <p>It is worth mentioning that in C-CTC Loss, choosing to initialize the Center randomly does not bring significant improvement. Our Center initialization method is as follows: + Based on the original CTCLoss, a network N is obtained by training + Select the training set, identify the completely correct part, and form the set G + Send each sample in G to the network, perform forward calculation, and extract the correspondence between the input of the last FC layer (ie feature) and the result of argmax calculation (ie index) + Aggregate features with the same index, calculate the average, and get the initial center of each character.</p> <p>Taking the configuration file <code>configs/rec/ch_PP-OCRv2/ch_PP-OCRv2_rec.yml</code> as an example, the center extraction command is as follows:</p> <pre><code>python tools/export_center.py -c configs/rec/ch_PP-OCRv2/ch_PP-OCRv2_rec.yml -o Global.pretrained_model=\"./output/rec_mobile_pp-OCRv2/best_accuracy\"\n</code></pre> <p>After running, <code>train_center.pkl</code> will be generated in the main directory of PaddleOCR.</p>"},{"location":"en/version2.x/ppocr/blog/enhanced_ctc_loss.html#4-experiment","title":"4. Experiment","text":"<p>For the above three solutions, we conducted training and evaluation based on Baidu's internal data set. The experimental conditions are shown in the following table:</p> algorithm Focal_CTC A_CTC C-CTC gain +0.3% +0.7% +1.7% <p>Based on the above experimental conclusions, we adopted the C-CTC strategy in PP-OCRv2. It is worth mentioning that, because PP-OCRv2 deals with the recognition task of 6625 Chinese characters, the character set is relatively large and there are many similar characters, so the C-CTC solution brings a significant improvement on this task. But if you switch to other OCR recognition tasks, the conclusion may be different. You can try Focal-CTC, A-CTC, C-CTC, and the combined solution EnhancedCTC. We believe it will bring different degrees of improvement.</p> <p>The unified combined plan is shown in the following file: rec_enhanced_ctc_loss.py</p>"},{"location":"en/version2.x/ppocr/blog/inference_args.html","title":"PaddleOCR Model Inference Parameter Explanation","text":"<p>When using PaddleOCR for model inference, you can customize the modification parameters to modify the model, data, preprocessing, postprocessing, etc. (parameter file: utility.py)\uff0cThe detailed parameter explanation is as follows:</p> <ul> <li>Global parameters</li> </ul> parameters type default implication image_dir str None, must be specified explicitly Image or folder path page_num int 0 Valid when the input type is pdf file, specify to predict the previous page_num pages, all pages are predicted by default vis_font_path str \"./doc/fonts/simfang.ttf\" font path for visualization drop_score float 0.5 Results with a recognition score less than this value will be discarded and will not be returned as results use_pdserving bool False Whether to use Paddle Serving for prediction warmup bool False Whether to enable warmup, this method can be used when statistical prediction time draw_img_save_dir str \"./inference_results\" The saving folder of the system's tandem prediction OCR results save_crop_res bool False Whether to save the recognized text image for OCR crop_res_save_dir str \"./output\" Save the text image path recognized by OCR use_mp bool False Whether to enable multi-process prediction total_process_num int 6 The number of processes, which takes effect when <code>use_mp</code> is <code>True</code> process_id int 0 The id number of the current process, no need to modify it yourself benchmark bool False Whether to enable benchmark, and make statistics on prediction speed, memory usage, etc. save_log_path str \"./log_output/\" Folder where log results are saved when <code>benchmark</code> is enabled show_log bool True Whether to show the log information in the inference use_onnx bool False Whether to enable onnx prediction <ul> <li>Prediction engine related parameters</li> </ul> parameters type default implication use_gpu bool True Whether to use GPU for prediction ir_optim bool True Whether to analyze and optimize the calculation graph. The prediction process can be accelerated when <code>ir_optim</code> is enabled use_tensorrt bool False Whether to enable tensorrt min_subgraph_size int 15 The minimum subgraph size in tensorrt. When the size of the subgraph is greater than this value, it will try to use the trt engine to calculate the subgraph. precision str fp32 The precision of prediction, supports <code>fp32</code>, <code>fp16</code>, <code>int8</code> enable_mkldnn bool True Whether to enable mkldnn cpu_threads int 10 When mkldnn is enabled, the number of threads predicted by the cpu <ul> <li>Text detection model related parameters</li> </ul> parameters type default implication det_algorithm str \"DB\" Text detection algorithm name, currently supports <code>DB</code>, <code>EAST</code>, <code>SAST</code>, <code>PSE</code>, <code>DB++</code>, <code>FCE</code> det_model_dir str xx Detection inference model paths det_limit_side_len int 960 image side length limit det_limit_type str \"max\" The side length limit type, currently supports <code>min</code>and <code>max</code>. <code>min</code> means to ensure that the shortest side of the image is not less than <code>det_limit_side_len</code>, <code>max</code> means to ensure that the longest side of the image is not greater than <code>det_limit_side_len</code> <p>The relevant parameters of the DB algorithm are as follows</p> parameters type default implication det_db_thresh float 0.3 In the probability map output by DB, only pixels with a score greater than this threshold will be considered as text pixels det_db_box_thresh float 0.6 Within the detection box, when the average score of all pixels is greater than the threshold, the result will be considered as a text area det_db_unclip_ratio float 1.5 The expansion factor of the <code>Vatti clipping</code> algorithm, which is used to expand the text area max_batch_size int 10 max batch size use_dilation bool False Whether to inflate the segmentation results to obtain better detection results det_db_score_mode str \"fast\" DB detection result score calculation method, supports <code>fast</code> and <code>slow</code>, <code>fast</code> calculates the average score according to all pixels within the bounding rectangle of the polygon, <code>slow</code> calculates the average score according to all pixels within the original polygon, The calculation speed is relatively slower, but more accurate. <p>The relevant parameters of the EAST algorithm are as follows</p> parameters type default implication det_east_score_thresh float 0.8 Threshold for score map in EAST postprocess det_east_cover_thresh float 0.1 Average score threshold for text boxes in EAST postprocess det_east_nms_thresh float 0.2 Threshold of nms in EAST postprocess <p>The relevant parameters of the SAST algorithm are as follows</p> parameters type default implication det_sast_score_thresh float 0.5 Score thresholds in SAST postprocess det_sast_nms_thresh float 0.5 Thresholding of nms in SAST postprocess det_box_type str 'quad' Whether polygon detection, curved text scene (such as Total-Text) is set to 'poly' <p>The relevant parameters of the PSE algorithm are as follows</p> parameters type default implication det_pse_thresh float 0.0 Threshold for binarizing the output image det_pse_box_thresh float 0.85 Threshold for filtering boxes, below this threshold is discarded det_pse_min_area float 16 The minimum area of the box, below this threshold is discarded det_box_type str \"quad\" The type of the returned box, quad: four point coordinates, poly: all point coordinates of the curved text det_pse_scale int 1 The ratio of the input image relative to the post-processed image, such as an image of <code>640*640</code>, the network output is <code>160*160</code>, and when the scale is 2, the shape of the post-processed image is <code>320*320</code>. Increasing this value can speed up the post-processing speed, but it will bring about a decrease in accuracy <ul> <li>Text recognition model related parameters</li> </ul> parameters type default implication rec_algorithm str \"CRNN\" Text recognition algorithm name, currently supports <code>CRNN</code>, <code>SRN</code>, <code>RARE</code>, <code>NETR</code>, <code>SAR</code>, <code>ViTSTR</code>, <code>ABINet</code>, <code>VisionLAN</code>, <code>SPIN</code>, <code>RobustScanner</code>, <code>SVTR</code>, <code>SVTR_LCNet</code> rec_model_dir str None, it is required if using the recognition model recognition inference model paths rec_image_shape str \"3,48,320\" ] Image size at the time of recognition rec_batch_num int 6 batch size max_text_length int 25 The maximum length of the recognition result, valid in <code>SRN</code> rec_char_dict_path str \"./ppocr/utils/ppocr_keys_v1.txt\" character dictionary file use_space_char bool True Whether to include spaces, if <code>True</code>, the <code>space</code> character will be added at the end of the character dictionary <ul> <li>End-to-end text detection and recognition model related parameters</li> </ul> parameters type default implication e2e_algorithm str \"PGNet\" End-to-end algorithm name, currently supports <code>PGNet</code> e2e_model_dir str None, it is required if using the end-to-end model end-to-end model inference model path e2e_limit_side_len int 768 End-to-end input image side length limit e2e_limit_type str \"max\" End-to-end side length limit type, currently supports <code>min</code> and <code>max</code>. <code>min</code> means to ensure that the shortest side of the image is not less than <code>e2e_limit_side_len</code>, <code>max</code> means to ensure that the longest side of the image is not greater than <code>e2e_limit_side_len</code> e2e_pgnet_score_thresh float 0.5 End-to-end score threshold, results below this threshold are discarded e2e_char_dict_path str \"./ppocr/utils/ic15_dict.txt\" Recognition dictionary file path e2e_pgnet_valid_set str \"totaltext\" The name of the validation set, currently supports <code>totaltext</code>, <code>partvgg</code>, the post-processing methods corresponding to different data sets are different, and it can be consistent with the training process e2e_pgnet_mode str \"fast\" PGNet's detection result score calculation method, supports <code>fast</code> and <code>slow</code>, <code>fast</code> calculates the average score according to all pixels within the bounding rectangle of the polygon, <code>slow</code> calculates the average score according to all pixels within the original polygon, The calculation speed is relatively slower, but more accurate. <ul> <li>Angle classifier model related parameters</li> </ul> parameters type default implication use_angle_cls bool False whether to use an angle classifier cls_model_dir str None, if you need to use, you must specify the path explicitly angle classifier inference model path cls_image_shape str \"3,48,192\" prediction shape label_list list ['0', '180'] The angle value corresponding to the class id cls_batch_num int 6 batch size cls_thresh float 0.9 Prediction threshold, when the model prediction result is 180 degrees, and the score is greater than the threshold, the final prediction result is considered to be 180 degrees and needs to be flipped <ul> <li>OCR image preprocessing parameters</li> </ul> parameters type default implication invert bool False whether to invert image before processing binarize bool False whether to threshold binarize image before processing alphacolor tuple \"255,255,255\" Replacement color for the alpha channel, if the latter is present; R,G,B integers"},{"location":"en/version2.x/ppocr/blog/multi_languages.html","title":"Multi-language model","text":"<p>Recent Update</p> <ul> <li>2022.5.8 update the <code>PP-OCRv3</code> version of the multi-language detection and recognition model, and the average recognition accuracy has increased by more than 5%.</li> <li>2021.4.9 supports the detection and recognition of 80 languages</li> <li>2021.4.9 supports lightweight high-precision English model detection and recognition</li> </ul> <p>PaddleOCR aims to create a rich, leading, and practical OCR tool library, which not only provides Chinese and English models in general scenarios, but also provides models specifically trained in English scenarios. And multilingual models covering 80 languages.</p> <p>Among them, the English model supports the detection and recognition of uppercase and lowercase letters and common punctuation, and the recognition of space characters is optimized:</p> <p></p> <p>The multilingual models cover Latin, Arabic, Traditional Chinese, Korean, Japanese, etc.:</p> <p></p> <p></p> <p></p> <p></p> <p>This document will briefly introduce how to use the multilingual model.</p>"},{"location":"en/version2.x/ppocr/blog/multi_languages.html#1-installation","title":"1 Installation","text":""},{"location":"en/version2.x/ppocr/blog/multi_languages.html#11-paddle-installation","title":"1.1 Paddle installation","text":"<pre><code># cpu\npip install \"paddlepaddle&lt;=2.6\"\n\n# gpu\npip install \"paddlepaddle-gpu&lt;=2.6\"\n</code></pre>"},{"location":"en/version2.x/ppocr/blog/multi_languages.html#12-paddleocr-package-installation","title":"1.2 PaddleOCR package installation","text":"<pre><code>pip install \"paddleocr&lt;3.0\"\n</code></pre> <p>Build and install locally</p> <pre><code>python3 -m build\npip3 install dist/paddleocr-x.x.x-py3-none-any.whl # x.x.x is the version number of paddleocr\n</code></pre>"},{"location":"en/version2.x/ppocr/blog/multi_languages.html#2-quick-use","title":"2 Quick use","text":""},{"location":"en/version2.x/ppocr/blog/multi_languages.html#21-command-line-operation","title":"2.1 Command line operation","text":"<p>View help information</p> <pre><code>paddleocr -h\n</code></pre> <ul> <li>Whole image prediction (detection + recognition)</li> </ul> <p>PaddleOCR currently supports 80 languages, which can be specified by the --lang parameter. The supported languages are listed in the table.</p> <pre><code>paddleocr --image_dir doc/imgs_en/254.jpg --lang=en\n</code></pre> <p></p> <p></p> <p>The result is a list. Each item contains a text box, text and recognition confidence</p> <pre><code>[('PHO CAPITAL', 0.95723116), [[66.0, 50.0], [327.0, 44.0], [327.0, 76.0], [67.0, 82.0]]]\n[('107 State Street', 0.96311164), [[72.0, 90.0], [451.0, 84.0], [452.0, 116.0], [73.0, 121.0]]]\n[('Montpelier Vermont', 0.97389287), [[69.0, 132.0], [501.0, 126.0], [501.0, 158.0], [70.0, 164.0]]]\n[('8022256183', 0.99810505), [[71.0, 175.0], [363.0, 170.0], [364.0, 202.0], [72.0, 207.0]]]\n[('REG 07-24-201706:59 PM', 0.93537045), [[73.0, 299.0], [653.0, 281.0], [654.0, 318.0], [74.0, 336.0]]]\n[('045555', 0.99346405), [[509.0, 331.0], [651.0, 325.0], [652.0, 356.0], [511.0, 362.0]]]\n[('CT1', 0.9988654), [[535.0, 367.0], [654.0, 367.0], [654.0, 406.0], [535.0, 406.0]]]\n......\n</code></pre> <ul> <li>Recognition</li> </ul> <pre><code>paddleocr --image_dir doc/imgs_words_en/word_308.png --det false --lang=en\n</code></pre> <p></p> <p>The result is a 2-tuple, which contains the recognition result and recognition confidence</p> <pre><code>(0.99879867, 'LITTLE')\n</code></pre> <ul> <li>Detection</li> </ul> <pre><code>paddleocr --image_dir PaddleOCR/doc/imgs/11.jpg --rec false\n</code></pre> <p>The result is a list. Each item represents the coordinates of a text box.</p> <pre><code>[[26.0, 457.0], [137.0, 457.0], [137.0, 477.0], [26.0, 477.0]]\n[[25.0, 425.0], [372.0, 425.0], [372.0, 448.0], [25.0, 448.0]]\n[[128.0, 397.0], [273.0, 397.0], [273.0, 414.0], [128.0, 414.0]]\n......\n</code></pre>"},{"location":"en/version2.x/ppocr/blog/multi_languages.html#22-run-with-python-script","title":"2.2 Run with Python script","text":"<p>PPOCR is able to run with Python scripts for easy integration with your own code:</p> <ul> <li>Whole image prediction (detection + recognition)</li> </ul> <pre><code>from paddleocr import PaddleOCR, draw_ocr\n\n# Also switch the language by modifying the lang parameter\nocr = PaddleOCR(lang=\"korean\") # The model file will be downloaded automatically when executed for the first time\nimg_path ='doc/imgs/korean_1.jpg'\nresult = ocr.ocr(img_path)\n# Recognition and detection can be performed separately through parameter control\n# result = ocr.ocr(img_path, det=False)  Only perform recognition\n# result = ocr.ocr(img_path, rec=False)  Only perform detection\n# Print detection frame and recognition result\nfor line in result:\n    print(line)\n\n# Visualization\nfrom PIL import Image\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='/path/to/PaddleOCR/doc/fonts/korean.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre> <p>Visualization of results:</p> <p></p> <p>PPOCR also supports direction classification. For more detailed usage, please refer to: whl package instructions.</p>"},{"location":"en/version2.x/ppocr/blog/multi_languages.html#3-custom-training","title":"3 Custom training","text":"<p>PPOCR supports using your own data for custom training or fine-tune, where the recognition model can refer to French configuration file Modify the training data path, dictionary and other parameters.</p> <p>For specific data preparation and training process, please refer to: Text Detection, Text Recognition, more functions such as predictive deployment, For functions such as data annotation, you can read the complete Document Tutorial.</p>"},{"location":"en/version2.x/ppocr/blog/multi_languages.html#4-inference-and-deployment","title":"4 Inference and Deployment","text":"<p>In addition to installing the whl package for quick forecasting, PPOCR also provides a variety of forecasting deployment methods. If necessary, you can read related documents:</p> <ul> <li>Python Inference</li> <li>C++ Inference</li> <li>Serving</li> <li>Mobile</li> <li>Benchmark</li> </ul>"},{"location":"en/version2.x/ppocr/blog/multi_languages.html#5-support-languages-and-abbreviations","title":"5 Support languages and abbreviations","text":"Language Abbreviation Language Abbreviation Chinese &amp; English ch Arabic ar English en Hindi hi French fr Uyghur ug German german Persian fa Japanese japan Urdu ur Korean korean Serbian(latin) rs_latin Chinese Traditional chinese_cht Occitan oc Italian it Marathi mr Spanish es Nepali ne Portuguese pt Serbian(cyrillic) rs_cyrillic Russian ru Bulgarian bg Ukranian uk Estonian et Belarusian be Irish ga Telugu te Croatian hr Sanskrit sa Hungarian hu Tamil ta Indonesian id Afrikaans af Icelandic is Azerbaijani az Kurdish ku Bosnian bs Lithuanian lt Czech cs Latvian lv Welsh cy Maori mi Danish da Malay ms Maltese mt Adyghe ady Dutch nl Kabardian kbd Norwegian no Avar ava Polish pl Dargwa dar Romanian ro Ingush inh Slovak sk Lak lbe Slovenian sl Lezghian lez Albanian sq Tabassaran tab Swedish sv Bihari bh Swahili sw Maithili mai Tagalog tl Angika ang Turkish tr Bhojpuri bho Uzbek uz Magahi mah Vietnamese vi Nagpur sck Mongolian mn Newari new Abaza abq Goan Konkani gom Chechen che Pali pi Haryanvi bgc Latin la"},{"location":"en/version2.x/ppocr/blog/ocr_book.html","title":"E-book: Dive Into OCR","text":"<p>\"Dive Into OCR\" is a textbook that combines OCR theory and practice, written by the PaddleOCR community. The main features are as follows:</p> <ul> <li>OCR full-stack technology covering text detection, recognition and document analysis</li> <li>Closely integrate theory and practice, cross the code implementation gap, and supporting instructional videos</li> <li>Jupyter Notebook textbook, flexibly modifying code for instant results</li> </ul>"},{"location":"en/version2.x/ppocr/blog/ocr_book.html#structure","title":"Structure","text":"<ul> <li> <p>The first part is the preliminary knowledge of the book, including the knowledge index and resource links needed in the process of positioning and using the book content of the book</p> </li> <li> <p>The second part is chapters 4-8 of the book, which introduce the concepts, applications, and industry practices related to the detection and identification capabilities of the OCR engine. In the \"Introduction to OCR Technology\", the application scenarios and challenges of OCR, the basic concepts of technology, and the pain points in industrial applications are comprehensively explained. Then, in the two chapters of \"Text Detection\" and \"Text Recognition\", the two basic tasks of OCR are introduced. In each chapter, an algorithm is accompanied by a detailed explanation of the code and practical exercises. Chapters 6 and 7 are a detailed introduction to the PP-OCR series model, PP-OCR is a set of OCR systems for industrial applications, on the basis of the basic detection and identification model, after a series of optimization strategies to achieve the general field of industrial SOTA model, while opening up a variety of predictive deployment solutions, enabling enterprises to quickly land OCR applications.</p> </li> <li> <p>The third part is chapter 9-12 of the book, which introduces applications other than the two-stage OCR engine, including data synthesis, preprocessing algorithm, and end-to-end model, focusing on OCR's layout analysis, table recognition, visual document question and answer capabilities in the document scene, and also through the combination of algorithm and code, so that readers can deeply understand and apply.</p> </li> </ul>"},{"location":"en/version2.x/ppocr/blog/ocr_book.html#address","title":"Address","text":"<ul> <li>E-book: Dive Into OCR (PDF)</li> <li>Notebook (.ipynb)</li> <li>Videos (Chinese only)</li> </ul>"},{"location":"en/version2.x/ppocr/blog/slice.html","title":"Slice Operator","text":"<p>If you have a very large image/document that you would like to run PaddleOCR (detection and recognition) on, you can use the slice operation as follows:</p> <p><code>ocr_inst  =  PaddleOCR(**ocr_settings)</code> <code>results  =  ocr_inst.ocr(img, det=True,rec=True, slice=slice, cls=False,bin=False,inv=False,alpha_color=False)</code></p> <p>where <code>slice  = {'horizontal_stride': h_stride, 'vertical_stride':v_stride, 'merge_x_thres':x_thres, 'merge_y_thres': y_thres}</code></p> <p>Here, <code>h_stride</code>, <code>v_stride</code>, <code>x_thres</code>, and <code>y_thres</code> are user-configurable values and need to be set manually. The way the <code>slice</code> operator works is that it runs a sliding window across the large input image, creating slices of it and runs the OCR algorithms on it.</p> <p>The fragmented slice-level results are then merged together to output image-level detection and recognition results. The horizontal and vertical strides cannot be lower than a certain limit (as too low values would create so many slices it would be very computationally expensive to get results for each of them). However, as an example the recommended values for an image with dimensions 6616x14886 would be as follows.</p> <p><code>slice = {'horizontal_stride': 300, 'vertical_stride':500, 'merge_x_thres':50, 'merge_y_thres': 35}</code></p> <p>All slice-level detections with bounding boxes as close as <code>merge_x_thres</code> and <code>merge_y_thres</code> will be merged together.</p>"},{"location":"en/version2.x/ppocr/blog/whl.html","title":"Paddleocr Package","text":""},{"location":"en/version2.x/ppocr/blog/whl.html#1-get-started-quickly","title":"1 Get started quickly","text":""},{"location":"en/version2.x/ppocr/blog/whl.html#11-install-package","title":"1.1 Install package","text":"<p>install by pypi</p> <pre><code>pip install \"paddleocr&gt;=2.0.1\" # Recommend to use version 2.0.1+\n</code></pre> <p>build own whl package and install</p> <pre><code>python3 -m build\npip3 install dist/paddleocr-x.x.x-py3-none-any.whl # x.x.x is the version of paddleocr\n</code></pre>"},{"location":"en/version2.x/ppocr/blog/whl.html#2-use","title":"2 Use","text":""},{"location":"en/version2.x/ppocr/blog/whl.html#21-use-by-code","title":"2.1 Use by code","text":"<p>The paddleocr whl package will automatically download the ppocr lightweight model as the default model, which can be customized and replaced according to the section 3 Custom Model.</p> <ul> <li>detection angle classification and recognition</li> </ul> <pre><code>from paddleocr import PaddleOCR,draw_ocr\n# Paddleocr supports Chinese, English, French, German, Korean and Japanese.\n# You can set the parameter `lang` as `ch`, `en`, `french`, `german`, `korean`, `japan`\n# to switch the language model in order.\nocr = PaddleOCR(use_angle_cls=True, lang='en') # need to run only once to download and load model into memory\nimg_path = 'PaddleOCR/doc/imgs_en/img_12.jpg'\nresult = ocr.ocr(img_path, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n# draw result\nfrom PIL import Image\nresult = result[0]\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='/path/to/PaddleOCR/doc/fonts/simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre> <p>Output will be a list, each item contains bounding box, text and recognition confidence</p> <pre><code>[[[442.0, 173.0], [1169.0, 173.0], [1169.0, 225.0], [442.0, 225.0]], ['ACKNOWLEDGEMENTS', 0.99283075]]\n[[[393.0, 340.0], [1207.0, 342.0], [1207.0, 389.0], [393.0, 387.0]], ['We would like to thank all the designers and', 0.9357758]]\n[[[399.0, 398.0], [1204.0, 398.0], [1204.0, 433.0], [399.0, 433.0]], ['contributors whohave been involved in the', 0.9592447]]\n......\n</code></pre> <p>Visualization of results</p> <p></p> <ul> <li>detection and recognition</li> </ul> <pre><code>from paddleocr import PaddleOCR,draw_ocr\nocr = PaddleOCR(lang='en') # need to run only once to download and load model into memory\nimg_path = 'PaddleOCR/doc/imgs_en/img_12.jpg'\nresult = ocr.ocr(img_path, cls=False)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n# draw result\nfrom PIL import Image\nresult = result[0]\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='/path/to/PaddleOCR/doc/fonts/simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre> <p>Output will be a list, each item contains bounding box, text and recognition confidence</p> <pre><code>[[[442.0, 173.0], [1169.0, 173.0], [1169.0, 225.0], [442.0, 225.0]], ['ACKNOWLEDGEMENTS', 0.99283075]]\n[[[393.0, 340.0], [1207.0, 342.0], [1207.0, 389.0], [393.0, 387.0]], ['We would like to thank all the designers and', 0.9357758]]\n[[[399.0, 398.0], [1204.0, 398.0], [1204.0, 433.0], [399.0, 433.0]], ['contributors whohave been involved in the', 0.9592447]]\n......\n</code></pre> <p>Visualization of results</p> <p></p> <ul> <li>classification and recognition</li> </ul> <pre><code>from paddleocr import PaddleOCR\nocr = PaddleOCR(use_angle_cls=True, lang='en') # need to run only once to load model into memory\nimg_path = 'PaddleOCR/doc/imgs_words_en/word_10.png'\nresult = ocr.ocr(img_path, det=False, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n</code></pre> <p>Output will be a list, each item contains recognition text and confidence</p> <pre><code>['PAIN', 0.990372]\n</code></pre> <ul> <li>only detection</li> </ul> <pre><code>from paddleocr import PaddleOCR,draw_ocr\nocr = PaddleOCR() # need to run only once to download and load model into memory\nimg_path = 'PaddleOCR/doc/imgs_en/img_12.jpg'\nresult = ocr.ocr(img_path,rec=False)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n# draw result\nfrom PIL import Image\nresult = result[0]\nimage = Image.open(img_path).convert('RGB')\nim_show = draw_ocr(image, result, txts=None, scores=None, font_path='/path/to/PaddleOCR/doc/fonts/simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre> <p>Output will be a list, each item only contains bounding box</p> <pre><code>[[756.0, 812.0], [805.0, 812.0], [805.0, 830.0], [756.0, 830.0]]\n[[820.0, 803.0], [1085.0, 801.0], [1085.0, 836.0], [820.0, 838.0]]\n[[393.0, 801.0], [715.0, 805.0], [715.0, 839.0], [393.0, 836.0]]\n......\n</code></pre> <p>Visualization of results</p> <p></p> <ul> <li>only recognition</li> </ul> <pre><code>from paddleocr import PaddleOCR\nocr = PaddleOCR(lang='en') # need to run only once to load model into memory\nimg_path = 'PaddleOCR/doc/imgs_words_en/word_10.png'\nresult = ocr.ocr(img_path, det=False, cls=False)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n</code></pre> <p>Output will be a list, each item contains recognition text and confidence</p> <pre><code>['PAIN', 0.990372]\n</code></pre> <ul> <li>only classification</li> </ul> <pre><code>from paddleocr import PaddleOCR\nocr = PaddleOCR(use_angle_cls=True) # need to run only once to load model into memory\nimg_path = 'PaddleOCR/doc/imgs_words_en/word_10.png'\nresult = ocr.ocr(img_path, det=False, rec=False, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n</code></pre> <p>Output will be a list, each item contains classification result and confidence</p> <pre><code>['0', 0.99999964]\n</code></pre>"},{"location":"en/version2.x/ppocr/blog/whl.html#22-use-by-command-line","title":"2.2 Use by command line","text":"<p>show help information</p> <pre><code>paddleocr -h\n</code></pre> <ul> <li>detection classification and recognition</li> </ul> <pre><code>paddleocr --image_dir PaddleOCR/doc/imgs_en/img_12.jpg --use_angle_cls true --lang en\n</code></pre> <p>Output will be a list, each item contains bounding box, text and recognition confidence</p> <pre><code>[[[441.0, 174.0], [1166.0, 176.0], [1165.0, 222.0], [441.0, 221.0]], ('ACKNOWLEDGEMENTS', 0.9971134662628174)]\n[[[403.0, 346.0], [1204.0, 348.0], [1204.0, 384.0], [402.0, 383.0]], ('We would like to thank all the designers and', 0.9761400818824768)]\n[[[403.0, 396.0], [1204.0, 398.0], [1204.0, 434.0], [402.0, 433.0]], ('contributors who have been involved in the', 0.9791957139968872)]\n......\n</code></pre> <p>pdf file is also supported, you can infer the first few pages by using the <code>page_num</code> parameter, the default is 0, which means infer all pages</p> <pre><code>paddleocr --image_dir ./xxx.pdf --use_angle_cls true --use_gpu false --page_num 2\n</code></pre> <ul> <li>detection and recognition</li> </ul> <pre><code>paddleocr --image_dir PaddleOCR/doc/imgs_en/img_12.jpg --lang en\n</code></pre> <p>Output will be a list, each item contains bounding box, text and recognition confidence</p> <pre><code>[[[441.0, 174.0], [1166.0, 176.0], [1165.0, 222.0], [441.0, 221.0]], ('ACKNOWLEDGEMENTS', 0.9971134662628174)]\n[[[403.0, 346.0], [1204.0, 348.0], [1204.0, 384.0], [402.0, 383.0]], ('We would like to thank all the designers and', 0.9761400818824768)]\n[[[403.0, 396.0], [1204.0, 398.0], [1204.0, 434.0], [402.0, 433.0]], ('contributors who have been involved in the', 0.9791957139968872)]\n......\n</code></pre> <ul> <li>classification and recognition</li> </ul> <pre><code>paddleocr --image_dir PaddleOCR/doc/imgs_words_en/word_10.png --use_angle_cls true --det false --lang en\n</code></pre> <p>Output will be a list, each item contains text and recognition confidence</p> <pre><code>['PAIN', 0.9934559464454651]\n</code></pre> <ul> <li>only detection</li> </ul> <pre><code>paddleocr --image_dir PaddleOCR/doc/imgs_en/img_12.jpg --rec false\n</code></pre> <p>Output will be a list, each item only contains bounding box</p> <pre><code>[[397.0, 802.0], [1092.0, 802.0], [1092.0, 841.0], [397.0, 841.0]]\n[[397.0, 750.0], [1211.0, 750.0], [1211.0, 789.0], [397.0, 789.0]]\n[[397.0, 702.0], [1209.0, 698.0], [1209.0, 734.0], [397.0, 738.0]]\n......\n</code></pre> <ul> <li>only recognition</li> </ul> <pre><code>paddleocr --image_dir PaddleOCR/doc/imgs_words_en/word_10.png --det false --lang en\n</code></pre> <p>Output will be a list, each item contains text and recognition confidence</p> <pre><code>['PAIN', 0.9934559464454651]\n</code></pre> <ul> <li>only classification</li> </ul> <pre><code>paddleocr --image_dir PaddleOCR/doc/imgs_words_en/word_10.png --use_angle_cls true --det false --rec false\n</code></pre> <p>Output will be a list, each item contains classification result and confidence</p> <pre><code>['0', 0.99999964]\n</code></pre>"},{"location":"en/version2.x/ppocr/blog/whl.html#3-use-custom-model","title":"3 Use custom model","text":"<p>When the built-in model cannot meet the needs, you need to use your own trained model. First, refer to export doc to convert your det and rec model to inference model, and then use it as follows</p>"},{"location":"en/version2.x/ppocr/blog/whl.html#31-use-by-code","title":"3.1 Use by code","text":"<pre><code>from paddleocr import PaddleOCR,draw_ocr\n# The path of detection and recognition model must contain model and params files\nocr = PaddleOCR(det_model_dir='{your_det_model_dir}', rec_model_dir='{your_rec_model_dir}', rec_char_dict_path='{your_rec_char_dict_path}', cls_model_dir='{your_cls_model_dir}', use_angle_cls=True)\nimg_path = 'PaddleOCR/doc/imgs_en/img_12.jpg'\nresult = ocr.ocr(img_path, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n# draw result\nfrom PIL import Image\nresult = result[0]\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='/path/to/PaddleOCR/doc/fonts/simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre>"},{"location":"en/version2.x/ppocr/blog/whl.html#32-use-by-command-line","title":"3.2 Use by command line","text":"<pre><code>paddleocr --image_dir PaddleOCR/doc/imgs/11.jpg --det_model_dir {your_det_model_dir} --rec_model_dir {your_rec_model_dir} --rec_char_dict_path {your_rec_char_dict_path} --cls_model_dir {your_cls_model_dir} --use_angle_cls true\n</code></pre>"},{"location":"en/version2.x/ppocr/blog/whl.html#4-use-web-images-or-numpy-array-as-input","title":"4 Use web images or numpy array as input","text":""},{"location":"en/version2.x/ppocr/blog/whl.html#41-web-image","title":"4.1 Web image","text":"<ul> <li>Use by code</li> </ul> <pre><code>from paddleocr import PaddleOCR, draw_ocr\nocr = PaddleOCR(use_angle_cls=True, lang=\"ch\") # need to run only once to download and load model into memory\nimg_path = 'http://n.sinaimg.cn/ent/transform/w630h933/20171222/o111-fypvuqf1838418.jpg'\nresult = ocr.ocr(img_path, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n# show result\nfrom PIL import Image\nresult = result[0]\nimage = Image.open(img_path).convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='/path/to/PaddleOCR/doc/fonts/simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre> <ul> <li>Use by command line</li> </ul> <pre><code>paddleocr --image_dir http://n.sinaimg.cn/ent/transform/w630h933/20171222/o111-fypvuqf1838418.jpg --use_angle_cls=true\n</code></pre>"},{"location":"en/version2.x/ppocr/blog/whl.html#42-numpy-array","title":"4.2 Numpy array","text":"<p>Support numpy array as input only when used by code</p> <pre><code>import cv2\nfrom paddleocr import PaddleOCR, draw_ocr, download_with_progressbar\nocr = PaddleOCR(use_angle_cls=True, lang=\"ch\") # need to run only once to download and load model into memory\nimg_path = 'PaddleOCR/doc/imgs/11.jpg'\nimg = cv2.imread(img_path)\n# img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY), If your own training model supports grayscale images, you can uncomment this line\nresult = ocr.ocr(img, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n# show result\nfrom PIL import Image\nresult = result[0]\ndownload_with_progressbar(img_path, 'tmp.jpg')\nimage = Image.open('tmp.jpg').convert('RGB')\nboxes = [line[0] for line in result]\ntxts = [line[1][0] for line in result]\nscores = [line[1][1] for line in result]\nim_show = draw_ocr(image, boxes, txts, scores, font_path='/path/to/PaddleOCR/doc/fonts/simfang.ttf')\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre>"},{"location":"en/version2.x/ppocr/blog/whl.html#5-pdf-file","title":"5 PDF file","text":"<ul> <li>Use by command line</li> </ul> <p>you can infer the first few pages by using the <code>page_num</code> parameter, the default is 0, which means infer all pages</p> <pre><code>paddleocr --image_dir ./xxx.pdf --use_angle_cls true --use_gpu false --page_num 2\n</code></pre> <ul> <li>Use by code</li> </ul> <pre><code>from paddleocr import PaddleOCR, draw_ocr\n\n# Paddleocr supports Chinese, English, French, German, Korean and Japanese.\n# You can set the parameter `lang` as `ch`, `en`, `fr`, `german`, `korean`, `japan`\n# to switch the language model in order.\nocr = PaddleOCR(use_angle_cls=True, lang=\"ch\"\uff0c page_num=2)  # need to run only once to download and load model into memory\nimg_path = './xxx.pdf'\nresult = ocr.ocr(img_path, cls=True)\nfor idx in range(len(result)):\n    res = result[idx]\n    for line in res:\n        print(line)\n\n# draw result\nimport fitz\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimgs = []\nwith fitz.open(img_path) as pdf:\n    for pg in range(0, pdf.pageCount):\n        page = pdf[pg]\n        mat = fitz.Matrix(2, 2)\n        pm = page.getPixmap(matrix=mat, alpha=False)\n        # if width or height &gt; 2000 pixels, don't enlarge the image\n        if pm.width &gt; 2000 or pm.height &gt; 2000:\n            pm = page.getPixmap(matrix=fitz.Matrix(1, 1), alpha=False)\n\n        img = Image.frombytes(\"RGB\", [pm.width, pm.height], pm.samples)\n        img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n        imgs.append(img)\nfor idx in range(len(result)):\n    res = result[idx]\n    image = imgs[idx]\n    boxes = [line[0] for line in res]\n    txts = [line[1][0] for line in res]\n    scores = [line[1][1] for line in res]\n    im_show = draw_ocr(image, boxes, txts, scores, font_path='doc/fonts/simfang.ttf')\n    im_show = Image.fromarray(im_show)\n    im_show.save('result_page_{}.jpg'.format(idx))\n</code></pre>"},{"location":"en/version2.x/ppocr/blog/whl.html#6-parameter-description","title":"6 Parameter Description","text":"Parameter Description Default value use_gpu use GPU or not TRUE gpu_mem GPU memory size used for initialization 8000M image_dir The images path or folder path for predicting when used by the command line page_num Valid when the input type is pdf file, specify to predict the previous page_num pages, all pages are predicted by default 0 det_algorithm Type of detection algorithm selected DB det_model_dir the text detection inference model folder. There are two ways to transfer parameters, 1. None: Automatically download the built-in model to <code>~/.paddleocr/det</code>; 2. The path of the inference model converted by yourself, the model and params files must be included in the model path None det_max_side_len The maximum size of the long side of the image. When the long side exceeds this value, the long side will be resized to this size, and the short side will be scaled proportionally 960 det_db_thresh Binarization threshold value of DB output map 0.3 det_db_box_thresh The threshold value of the DB output box. Boxes score lower than this value will be discarded 0.5 det_db_unclip_ratio The expanded ratio of DB output box 2 det_db_score_mode The parameter that control how the score of the detection frame is calculated. There are 'fast' and 'slow' options. If the text to be detected is curved, it is recommended to use 'slow' 'fast' det_east_score_thresh Binarization threshold value of EAST output map 0.8 det_east_cover_thresh The threshold value of the EAST output box. Boxes score lower than this value will be discarded 0.1 det_east_nms_thresh The NMS threshold value of EAST model output box 0.2 rec_algorithm Type of recognition algorithm selected CRNN rec_model_dir the text recognition inference model folder. There are two ways to transfer parameters, 1. None: Automatically download the built-in model to <code>~/.paddleocr/rec</code>; 2. The path of the inference model converted by yourself, the model and params files must be included in the model path None rec_image_shape image shape of recognition algorithm \"3,32,320\" rec_batch_num When performing recognition, the batchsize of forward images 30 max_text_length The maximum text length that the recognition algorithm can recognize 25 rec_char_dict_path the alphabet path which needs to be modified to your own path when <code>rec_model_Name</code> use mode 2 ./ppocr/utils/ppocr_keys_v1.txt use_space_char Whether to recognize spaces TRUE drop_score Filter the output by score (from the recognition model), and those below this score will not be returned 0.5 use_angle_cls Whether to load classification model FALSE cls_model_dir the classification inference model folder. There are two ways to transfer parameters, 1. None: Automatically download the built-in model to <code>~/.paddleocr/cls</code>; 2. The path of the inference model converted by yourself, the model and params files must be included in the model path None cls_image_shape image shape of classification algorithm \"3,48,192\" label_list label list of classification algorithm ['0','180'] cls_batch_num When performing classification, the batchsize of forward images 30 enable_mkldnn Whether to enable mkldnn FALSE use_zero_copy_run Whether to forward by zero_copy_run FALSE lang The support language, now only Chinese(ch)\u3001English(en)\u3001French(french)\u3001German(german)\u3001Korean(korean)\u3001Japanese(japan) are supported ch det Enable detection when <code>ppocr.ocr</code> func exec TRUE rec Enable recognition when <code>ppocr.ocr</code> func exec TRUE cls Enable classification when <code>ppocr.ocr</code> func exec((Use use_angle_cls in command line mode to control whether to start classification in the forward direction) FALSE show_log Whether to print log FALSE type Perform ocr or table structuring, the value is selected in ['ocr','structure'] ocr ocr_version OCR Model version number, the current model support list is as follows: PP-OCRv3 supports Chinese and English detection, recognition, multilingual recognition, direction classifier models, PP-OCRv2 support Chinese detection and recognition model, PP-OCR support Chinese detection, recognition and direction classifier, multilingual recognition model PP-OCRv3"},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html","title":"Knowledge Distillation","text":""},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#1-introduction","title":"1. Introduction","text":""},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#11-introduction-to-knowledge-distillation","title":"1.1 Introduction to Knowledge Distillation","text":"<p>In recent years, deep neural networks have been proved to be an extremely effective method for solving problems in the fields of computer vision and natural language processing. By constructing a suitable neural network and training it, the performance metrics of the final network model will basically exceed the traditional algorithm. When the amount of data is large enough, increasing the amount of parameters by constructing a reasonable network model can significantly improve the performance of the model, but this brings about the problem of a sharp increase in the complexity of the model. Large models are more expensive to use in actual scenarios. Deep neural networks generally have more parameter redundancy. At present, there are several main methods to compress the model and reduce the amount of its parameters. Such as pruning, quantification, knowledge distillation, etc., where knowledge distillation refers to the use of teacher models to guide student models to learn specific tasks, to ensure that the small model obtains a relatively large performance improvement under the condition of unchanged parameters. In addition, in the knowledge distillation task, a mutual learning model training method was also derived. The paper Deep Mutual Learning pointed out that using two identical models to supervise each other during the training process can achieve better results than a single model training.</p>"},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#12-introduction-to-paddleocr-knowledge-distillation","title":"1.2 Introduction to PaddleOCR Knowledge Distillation","text":"<p>Whether it is a large model distilling a small model, or a small model learning from each other and updating parameters, they are essentially the output between different models or mutual supervision between feature maps. The only difference is (1) whether the model requires fixed parameters. (2) Whether the model needs to be loaded with a pre-trained model. For the case where a large model distills a small model, the large model generally needs to load the pre-trained model and fix the parameters. For the situation where small models distill each other, the small models generally do not load the pre-trained model, and the parameters are also in a learnable state.</p> <p>In the task of knowledge distillation, it is not only the distillation between two models, but also the situation where multiple models learn from each other. Therefore, in the knowledge distillation code framework, it is also necessary to support this type of distillation method.</p> <p>The algorithm of knowledge distillation is integrated in PaddleOCR. Specifically, it has the following main features:</p> <ul> <li>It supports mutual learning of any network, and does not require the sub-network structure to be completely consistent or to have a pre-trained model. At the same time, there is no limit to the number of sub-networks, just add it in the configuration file.</li> <li>Support arbitrarily configuring the loss function through the configuration file, not only can use a certain loss, but also a combination of multiple losses.</li> <li>Support all model-related environments such as knowledge distillation training, prediction, evaluation, and export, which is convenient for use and deployment.</li> </ul> <p>Through knowledge distillation, in the common Chinese and English text recognition task, without adding any time-consuming prediction, the accuracy of the model can be improved by more than 3%. Combining the learning rate adjustment strategy and the model structure fine-tuning strategy, the final improvement is more than 5%.</p>"},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#2-configuration-file-analysis","title":"2. Configuration File Analysis","text":"<p>In the process of knowledge distillation training, there is no change in data preprocessing, optimizer, learning rate, and some global attributes. The configuration files of the model structure, loss function, post-processing, metric calculation and other modules need to be fine-tuned.</p> <p>The following takes the knowledge distillation configuration file for recognition and detection as an example to analyze the training and configuration of knowledge distillation.</p>"},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#21-recognition-model-configuration-file-analysis","title":"2.1 Recognition Model Configuration File Analysis","text":"<p>The configuration file is in ch_PP-OCRv2_rec_distillation.yml.</p>"},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#211-model-structure","title":"2.1.1 Model Structure","text":"<p>In the knowledge distillation task, the model structure configuration is as follows.</p> <pre><code>Architecture:\n  model_type: &amp;model_type \"rec\"    # Model category, recognition, detection, etc.\n  name: DistillationModel          # Structure name, in the distillation task, it is DistillationModel\n  algorithm: Distillation          # Algorithm name\n  Models:                          # Model, including the configuration information of the subnet\n    Teacher:                       # The name of the subnet, it must include at least the `pretrained` and `freeze_params` parameters, and the other parameters are the construction parameters of the subnet\n      pretrained:                  # Does this sub-network need to load pre-training weights\n      freeze_params: false         # Do you need fixed parameters\n      return_all_feats: true       # Do you need to return all features, if it is False, only the final output is returned\n      model_type: *model_type      # Model category\n      algorithm: SVTR              # The algorithm name of the sub-network. The remaining parameters of the sub-network are consistent with the general model training configuration\n      Transform:\n      Backbone:\n        name: MobileNetV1Enhance\n        scale: 0.5\n        last_conv_stride: [1, 2]\n        last_pool_type: avg\n      Head:\n        name: MultiHead\n        head_list:\n          - CTCHead:\n              Neck:\n                name: svtr\n                dims: 64\n                depth: 2\n                hidden_dims: 120\n                use_guide: True\n              Head:\n                fc_decay: 0.00001\n          - SARHead:\n              enc_dim: 512\n              max_text_length: *max_text_length\n    Student:                       # Another sub-network, here is a distillation example of DML, the two sub-networks have the same structure, and both need to learn parameters\n      pretrained:                  # The following parameters are the same as above\n      freeze_params: false\n      return_all_feats: true\n      model_type: *model_type\n      algorithm: SVTR\n      Transform:\n      Backbone:\n        name: MobileNetV1Enhance\n        scale: 0.5\n        last_conv_stride: [1, 2]\n        last_pool_type: avg\n      Head:\n        name: MultiHead\n        head_list:\n          - CTCHead:\n              Neck:\n                name: svtr\n                dims: 64\n                depth: 2\n                hidden_dims: 120\n                use_guide: True\n              Head:\n                fc_decay: 0.00001\n          - SARHead:\n              enc_dim: 512\n              max_text_length: *max_text_length\n</code></pre> <p>If you want to add more sub-networks for training, you can also add the corresponding fields in the configuration file according to the way of adding <code>Student</code> and <code>Teacher</code>. For example, if you want 3 models to supervise each other and train together, then <code>Architecture</code> can be written in the following format.</p> <pre><code>Architecture:\n  model_type: &amp;model_type \"rec\"\n  name: DistillationModel\n  algorithm: Distillation\n  Models:\n    Teacher:\n      pretrained:\n      freeze_params: false\n      return_all_feats: true\n      model_type: *model_type\n      algorithm: SVTR\n      Transform:\n      Backbone:\n        name: MobileNetV1Enhance\n        scale: 0.5\n        last_conv_stride: [1, 2]\n        last_pool_type: avg\n      Head:\n        name: MultiHead\n        head_list:\n          - CTCHead:\n              Neck:\n                name: svtr\n                dims: 64\n                depth: 2\n                hidden_dims: 120\n                use_guide: True\n              Head:\n                fc_decay: 0.00001\n          - SARHead:\n              enc_dim: 512\n              max_text_length: *max_text_length\n    Student:\n      pretrained:\n      freeze_params: false\n      return_all_feats: true\n      model_type: *model_type\n      algorithm: SVTR\n      Transform:\n      Backbone:\n        name: MobileNetV1Enhance\n        scale: 0.5\n        last_conv_stride: [1, 2]\n        last_pool_type: avg\n      Head:\n        name: MultiHead\n        head_list:\n          - CTCHead:\n              Neck:\n                name: svtr\n                dims: 64\n                depth: 2\n                hidden_dims: 120\n                use_guide: True\n              Head:\n                fc_decay: 0.00001\n          - SARHead:\n              enc_dim: 512\n              max_text_length: *max_text_length\n    Student2:\n      pretrained:\n      freeze_params: false\n      return_all_feats: true\n      model_type: *model_type\n      algorithm: SVTR\n      Transform:\n      Backbone:\n        name: MobileNetV1Enhance\n        scale: 0.5\n        last_conv_stride: [1, 2]\n        last_pool_type: avg\n      Head:\n        name: MultiHead\n        head_list:\n          - CTCHead:\n              Neck:\n                name: svtr\n                dims: 64\n                depth: 2\n                hidden_dims: 120\n                use_guide: True\n              Head:\n                fc_decay: 0.00001\n          - SARHead:\n              enc_dim: 512\n              max_text_length: *max_text_length\n</code></pre> <p>When the model is finally trained, it contains 3 sub-networks: <code>Teacher</code>, <code>Student</code>, <code>Student2</code>.</p> <p>The specific implementation code of the <code>DistillationModel</code> class can refer to distillation_model.py. The final model output is a dictionary, the key is the name of all the sub-networks, for example, here are <code>Student</code> and <code>Teacher</code>, and the value is the output of the corresponding sub-network, which can be <code>Tensor</code> (only the last layer of the network is returned) and <code>dict</code> (also returns the characteristic information in the middle). In the recognition task, in order to add more loss functions and ensure the scalability of the distillation method, the output of each sub-network is saved as a <code>dict</code>, which contains the sub-module output. Take the recognition model as an example. The output result of each sub-network is <code>dict</code>, the key contains <code>backbone_out</code>, <code>neck_out</code>, <code>head_out</code>, and <code>value</code> is the tensor of the corresponding module. Finally, for the above configuration file, <code>DistillationModel</code> The output format is as follows.</p> <pre><code>{\n  \"Teacher\": {\n    \"backbone_out\": tensor,\n    \"neck_out\": tensor,\n    \"head_out\": tensor,\n  },\n  \"Student\": {\n    \"backbone_out\": tensor,\n    \"neck_out\": tensor,\n    \"head_out\": tensor,\n  }\n}\n</code></pre>"},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#212-loss-function","title":"2.1.2 Loss Function","text":"<p>In the knowledge distillation task, the loss function configuration is as follows.</p> <pre><code>Loss:\n  name: CombinedLoss                           # Loss function name\n  loss_config_list:                            # List of loss function configuration files, mandatory functions for CombinedLoss\n  - DistillationCTCLoss:                       # CTC loss function based on distillation, inherited from standard CTC loss\n      weight: 1.0                              # The weight of the loss function. In loss_config_list, each loss function must include this field\n      model_name_list: [\"Student\", \"Teacher\"]  # For the prediction results of the distillation model, extract the output of these two sub-networks and calculate the CTC loss with gt\n      key: head_out                            # In the sub-network output dict, take the corresponding tensor\n  - DistillationDMLLoss:                       # DML loss function, inherited from the standard DMLLoss\n      weight: 1.0\n      act: \"softmax\"                           # Activation function, use it to process the input, can be softmax, sigmoid or None, the default is None\n      model_name_pairs:                        # The subnet name pair used to calculate DML loss. If you want to calculate the DML loss of other subnets, you can continue to add it below the list\n      - [\"Student\", \"Teacher\"]\n      key: head_out\n      multi_head: True                         # whether to use mult_head\n      dis_head: ctc                            # assign the head name to calculate loss\n      name: dml_ctc                            # prefix name of the loss\n  - DistillationDMLLoss:                       # DML loss function, inherited from the standard DMLLoss\n      weight: 0.5\n      act: \"softmax\"                           # Activation function, use it to process the input, can be softmax, sigmoid or None, the default is None\n      model_name_pairs:                        # The subnet name pair used to calculate DML loss. If you want to calculate the DML loss of other subnets, you can continue to add it below the list\n      - [\"Student\", \"Teacher\"]\n      key: head_out\n      multi_head: True                         # whether to use mult_head\n      dis_head: sar                            # assign the head name to calculate loss\n      name: dml_sar                            # prefix name of the loss\n  - DistillationDistanceLoss:                  # Distilled distance loss function\n      weight: 1.0\n      mode: \"l2\"                               # Support l1, l2 or smooth_l1\n      model_name_pairs:                        # Calculate the distance loss of the subnet name pair\n      - [\"Student\", \"Teacher\"]\n      key: backbone_out\n  - DistillationSARLoss:                       # SAR loss function based on distillation, inherited from standard SAR loss\n      weight: 1.0                              # The weight of the loss function. In loss_config_list, each loss function must include this field\n      model_name_list: [\"Student\", \"Teacher\"]  # For the prediction results of the distillation model, extract the output of these two sub-networks and calculate the SAR loss with gt\n      key: head_out                            # In the sub-network output dict, take the corresponding tensor\n      multi_head: True                         # whether it is multi-head or not, if true, SAR branch is used to calculate the loss\n</code></pre> <p>Among the above loss functions, all distillation loss functions are inherited from the standard loss function class. The main functions are: Analyze the output of the distillation model, find the intermediate node (tensor) used to calculate the loss, and then use the standard loss function class to calculate.</p> <p>Taking the above configuration as an example, the final distillation training loss function contains the following five parts.</p> <ul> <li>CTC branch of the final output <code>head_out</code> for <code>Student</code> and <code>Teacher</code> calculates the CTC loss with gt (loss weight equals 1.0). Here, because both sub-networks need to update the parameters, both of them need to calculate the loss with gt.</li> <li>SAR branch of the final output <code>head_out</code> for <code>Student</code> and <code>Teacher</code> calculates the SAR loss with gt (loss weight equals 1.0). Here, because both sub-networks need to update the parameters, both of them need to calculate the loss with gt.</li> <li>DML loss between CTC branch of  <code>Student</code> and <code>Teacher</code>'s final output <code>head_out</code> (loss weight equals 1.0).</li> <li>DML loss between SAR branch of <code>Student</code> and <code>Teacher</code>'s final output <code>head_out</code> (loss weight equals 0.5).</li> <li>L2 loss between <code>Student</code> and <code>Teacher</code>'s backbone network output <code>backbone_out</code> (loss weight equals 1.0).</li> </ul> <p>For more specific implementation of <code>CombinedLoss</code>, please refer to: combined_loss.py. For more specific implementations of distillation loss functions such as <code>DistillationCTCLoss</code>, please refer to distillation_loss.py</p>"},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#213-post-processing","title":"2.1.3 Post-processing","text":"<p>In the knowledge distillation task, the post-processing configuration is as follows.</p> <pre><code>PostProcess:\n  name: DistillationCTCLabelDecode       # CTC decoding post-processing of distillation tasks, inherited from the standard CTCLabelDecode class\n  model_name: [\"Student\", \"Teacher\"]     # For the prediction results of the distillation model, extract the outputs of these two sub-networks and decode them\n  key: head_out                          # Take the corresponding tensor in the subnet output dict\n  multi_head: True                       # whether it is multi-head or not, if true, CTC branch is used to calculate the loss\n</code></pre> <p>Taking the above configuration as an example, the CTC decoding output of the two sub-networks <code>Student</code> and <code>Teacher</code> will be calculated at the same time. Among them, <code>key</code> is the name of the subnet, and <code>value</code> is the list of subnets.</p> <p>For more specific implementation of <code>DistillationCTCLabelDecode</code>, please refer to: rec_postprocess.py</p>"},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#214-metric-calculation","title":"2.1.4 Metric Calculation","text":"<p>In the knowledge distillation task, the metric calculation configuration is as follows.</p> <pre><code>Metric:\n  name: DistillationMetric         # CTC decoding post-processing of distillation tasks, inherited from the standard CTCLabelDecode class\n  base_metric_name: RecMetric      # The base class of indicator calculation. For the output of the model, the indicator will be calculated based on this class\n  main_indicator: acc              # The name of the indicator\n  key: \"Student\"                   # Select the main_indicator of this subnet as the criterion for saving the best model\n  ignore_space: False              # whether to ignore space during evaluation\n</code></pre> <p>Taking the above configuration as an example, the accuracy metric of the <code>Student</code> subnet will be used as the judgment metric for saving the best model. At the same time, the accuracy metric of all subnets will be printed out in the log.</p> <p>For more specific implementation of <code>DistillationMetric</code>, please refer to: distillation_metric.py.</p>"},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#215-fine-tuning-distillation-model","title":"2.1.5 Fine-tuning Distillation Model","text":"<p>There are two ways to fine-tune the recognition distillation task.</p> <ol> <li>Fine-tuning based on knowledge distillation: this situation is relatively simple, download the pre-trained model. Then configure the pre-training model path and your own data path in PP-OCRv3_mobile_rec_distillation.yml to perform fine-tuning training of the model.</li> <li> <p>Do not use knowledge distillation in fine-tuning: In this case, you need to first extract the student model parameters from the pre-training model. The specific steps are as follows.</p> </li> <li> <p>First download the pre-trained model and unzip it.</p> </li> </ol> <pre><code>wget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_train.tar\ntar -xf ch_PP-OCRv3_rec_train.tar\n</code></pre> <ul> <li>Then use python to extract the student model parameters</li> </ul> <pre><code>import paddle\n# Load the pre-trained model\nall_params = paddle.load(\"ch_PP-OCRv3_rec_train/best_accuracy.pdparams\")\n# View the keys of the weight parameter\nprint(all_params.keys())\n# Weight extraction of student model\ns_params = {key[len(\"Student.\"):]: all_params[key] for key in all_params if \"Student.\" in key}\n# View the keys of the weight parameters of the student model\nprint(s_params.keys())\n# Save weight parameters\npaddle.save(s_params, \"ch_PP-OCRv3_rec_train/student.pdparams\")\n</code></pre> <p>After the extraction is complete, use PP-OCRv3_mobile_rec.yml to modify the path of the pre-trained model (the path of the exported <code>student.pdparams</code> model) and your own data path to fine-tune the model.</p>"},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#22-detection-model-configuration-file-analysis","title":"2.2 Detection Model Configuration File Analysis","text":"<p>The configuration file of the detection model distillation is in the <code>PaddleOCR/configs/det/ch_PP-OCRv3/</code> directory, which contains three distillation configuration files:</p> <ul> <li><code>PP-OCRv3_det_cml.yml</code>, Use one large model to distill two small models, and the two small models learn from each other</li> <li><code>PP-OCRv3_det_dml.yml</code>, Method of mutual distillation of two student models</li> </ul>"},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#221-model-structure","title":"2.2.1 Model Structure","text":"<p>In the knowledge distillation task, the model structure configuration is as follows:</p> <pre><code>Architecture:\n  name: DistillationModel          # Structure name, in the distillation task, it is DistillationModel\n  algorithm: Distillation          # Algorithm name\n  Models:                          # Model, including the configuration information of the subnet\n    Student:                       # The name of the subnet, it must include at least the `pretrained` and `freeze_params` parameters, and the other parameters are the construction parameters of the subnet\n      pretrained: ./pretrain_models/MobileNetV3_large_x0_5_pretrained  # Does this sub-network need to load pre-training weights\n      freeze_params: false         # Do you need fixed parameters\n      return_all_feats: false      # Do you need to return all features, if it is False, only the final output is returned\n      model_type: det\n      algorithm: DB\n      Backbone:\n        name: ResNet\n        in_channels: 3\n        layers: 50\n      Neck:\n        name: LKPAN\n        out_channels: 256\n      Head:\n        name: DBHead\n        kernel_list: [7,2,2]\n        k: 50\n    Teacher:                      # Another sub-network, here is a distillation example of a large model distill a small model\n      pretrained: ./pretrain_models/ch_ppocr_server_v2.0_det_train/best_accuracy\n      return_all_feats: false\n      model_type: det\n      algorithm: DB\n      Transform:\n      Backbone:\n        name: ResNet\n        in_channels: 3\n        layers: 50\n      Neck:\n        name: LKPAN\n        out_channels: 256\n      Head:\n        name: DBHead\n        kernel_list: [7,2,2]\n        k: 50\n</code></pre> <p>If DML is used, that is, the method of two small models learning from each other, the Teacher network structure in the above configuration file needs to be set to the same configuration as the Student model. Refer to the configuration file for details. PP-OCRv3_det_dml.yml</p> <p>The following describes the configuration file parameters PP-OCRv3_det_cml.yml:</p> <pre><code>Architecture:\n  name: DistillationModel\n  algorithm: Distillation\n  model_type: det\n  Models:\n    Teacher:                         # Teacher model configuration of CML distillation\n      pretrained: ./pretrain_models/ch_ppocr_server_v2.0_det_train/best_accuracy\n      freeze_params: true            # Teacher does not train\n      return_all_feats: false\n      model_type: det\n      algorithm: DB\n      Transform:\n      Backbone:\n        name: ResNet\n        in_channels: 3\n        layers: 50\n      Neck:\n        name: LKPAN\n        out_channels: 256\n      Head:\n        name: DBHead\n        kernel_list: [7,2,2]\n        k: 50\n    Student:                         # Student model configuration for CML distillation\n      pretrained: ./pretrain_models/MobileNetV3_large_x0_5_pretrained\n      freeze_params: false\n      return_all_feats: false\n      model_type: det\n      algorithm: DB\n      Backbone:\n        name: MobileNetV3\n        scale: 0.5\n        model_name: large\n        disable_se: true\n      Neck:\n        name: RSEFPN\n        out_channels: 96\n        shortcut: True\n      Head:\n        name: DBHead\n        k: 50\n    Student2:                          # Student2 model configuration for CML distillation\n      pretrained: ./pretrain_models/MobileNetV3_large_x0_5_pretrained\n      freeze_params: false\n      return_all_feats: false\n      model_type: det\n      algorithm: DB\n      Transform:\n      Backbone:\n        name: MobileNetV3\n        scale: 0.5\n        model_name: large\n        disable_se: true\n      Neck:\n        name: RSEFPN\n        out_channels: 96\n        shortcut: True\n      Head:\n        name: DBHead\n        k: 50\n</code></pre> <p>The specific implementation code of the distillation model <code>DistillationModel</code> class can refer to distillation_model.py.</p> <p>The final model output is a dictionary, the key is the name of all the sub-networks, for example, here are <code>Student</code> and <code>Teacher</code>, and the value is the output of the corresponding sub-network, which can be <code>Tensor</code> (only the last layer of the network is returned) and <code>dict</code> (also returns the characteristic information in the middle).</p> <p>In the distillation task, in order to facilitate the addition of the distillation loss function, the output of each network is saved as a <code>dict</code>, which contains the sub-module output. The key contains <code>backbone_out</code>, <code>neck_out</code>, <code>head_out</code>, and <code>value</code> is the tensor of the corresponding module. Finally, for the above configuration file, the output format of <code>DistillationModel</code> is as follows.</p> <pre><code>{\n  \"Teacher\": {\n    \"backbone_out\": tensor,\n    \"neck_out\": tensor,\n    \"head_out\": tensor,\n  },\n  \"Student\": {\n    \"backbone_out\": tensor,\n    \"neck_out\": tensor,\n    \"head_out\": tensor,\n  }\n}\n</code></pre>"},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#222-loss-function","title":"2.2.2 Loss Function","text":"<p>The distillation loss function configuration(<code>PP-OCRv3_det_cml.yml</code>) is shown below.</p> <pre><code>Loss:\n  name: CombinedLoss\n  loss_config_list:\n  - DistillationDilaDBLoss:\n      weight: 1.0\n      model_name_pairs:\n      - [\"Student\", \"Teacher\"]\n      - [\"Student2\", \"Teacher\"]                  # 1. Calculate the loss of two Student and Teacher\n      key: maps\n      balance_loss: true\n      main_loss_type: DiceLoss\n      alpha: 5\n      beta: 10\n      ohem_ratio: 3\n  - DistillationDMLLoss:                         # 2. Add to calculate the loss between two students\n      model_name_pairs:\n      - [\"Student\", \"Student2\"]\n      maps_name: \"thrink_maps\"\n      weight: 1.0\n      # act: None\n      key: maps\n  - DistillationDBLoss:\n      weight: 1.0\n      model_name_list: [\"Student\", \"Student2\"]   # 3. Calculate the loss between two students and GT\n      balance_loss: true\n      main_loss_type: DiceLoss\n      alpha: 5\n      beta: 10\n      ohem_ratio: 3\n</code></pre> <p>For more specific implementation of <code>DistillationDilaDBLoss</code>, please refer to: distillation_loss.py. For more specific implementations of distillation loss functions such as <code>DistillationDBLoss</code>, please refer to: distillation_loss.py</p>"},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#223-post-processing","title":"2.2.3 Post-processing","text":"<p>In the task of detecting knowledge distillation, the post-processing configuration of detecting distillation is as follows.</p> <pre><code>PostProcess:\n  name: DistillationDBPostProcess                  # The post-processing of the DB detection distillation task, inherited from the standard DBPostProcess class\n  model_name: [\"Student\", \"Student2\", \"Teacher\"]   # Extract the output of multiple sub-networks and decode them. The network that does not require post-processing is not set in model_name\n  thresh: 0.3\n  box_thresh: 0.6\n  max_candidates: 1000\n  unclip_ratio: 1.5\n</code></pre> <p>Taking the above configuration as an example, the output of the three subnets <code>Student</code>, <code>Student2</code> and <code>Teacher</code> will be calculated at the same time for post-processing calculations. Since there are multiple inputs, there are also multiple outputs returned by post-processing. For a more specific implementation of <code>DistillationDBPostProcess</code>, please refer to: db_postprocess.py</p>"},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#224-metric-calculation","title":"2.2.4 Metric Calculation","text":"<p>In the knowledge distillation task, the metric calculation configuration is as follows.</p> <pre><code>Metric:\n  name: DistillationMetric\n  base_metric_name: DetMetric\n  main_indicator: hmean\n  key: \"Student\"\n</code></pre> <p>Since distillation needs to include multiple networks, only one network metrics needs to be calculated when calculating the metrics. The <code>key</code> field is set to <code>Student</code>, it means that only the metrics of the <code>Student</code> network is calculated. Model Structure</p>"},{"location":"en/version2.x/ppocr/model_compress/knowledge_distillation.html#225-fine-tuning-distillation-model","title":"2.2.5 Fine-tuning Distillation Model","text":"<p>There are three ways to fine-tune the detection distillation task:</p> <ul> <li><code>ch_PP-OCRv3_det_distill.yml</code>, The teacher model is set to the model provided by PaddleOCR or the large model you have trained.</li> <li><code>PP-OCRv3_det_cml.yml</code>, Use cml distillation. Similarly, the Teacher model is set to the model provided by PaddleOCR or the large model you have trained.</li> <li><code>PP-OCRv3_det_dml.yml</code>, Distillation using DML. The method of mutual distillation of the two Student models has an accuracy improvement of about 1.7% on the data set used by PaddleOCR.</li> </ul> <p>In fine-tune, you need to set the pre-trained model to be loaded in the <code>pretrained</code> parameter of the network structure.</p> <p>In terms of accuracy improvement, <code>cml</code> &gt; <code>dml</code> &gt; <code>distill</code>. When the amount of data is insufficient or the accuracy of the teacher model is similar to that of the student, this conclusion may change.</p> <p>In addition, since the distillation pre-training model provided by PaddleOCR contains multiple model parameters, if you want to extract the parameters of the student model, you can refer to the following code:</p> <pre><code># Download the parameters of the distillation training model\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-OCRv3_mobile_det_pretrained.pdparams\n</code></pre> <pre><code>import paddle\n# Load the pre-trained model\nall_params = paddle.load(\"ch_PP-OCRv3_det_distill_train/best_accuracy.pdparams\")\n# View the keys of the weight parameter\nprint(all_params.keys())\n# Extract the weights of the student model\ns_params = {key[len(\"Student.\"):]: all_params[key] for key in all_params if \"Student.\" in key}\n# View the keys of the weight parameters of the student model\nprint(s_params.keys())\n# Save\npaddle.save(s_params, \"ch_PP-OCRv3_det_distill_train/student.pdparams\")\n</code></pre> <p>Finally, the parameters of the student model will be saved in <code>ch_PP-OCRv3_det_distill_train/student.pdparams</code> for the fine-tune of the model.</p>"},{"location":"en/version2.x/ppocr/model_compress/prune.html","title":"PP-OCR Models Pruning","text":"<p>Generally, a more complex model would achieve better performance in the task, but it also leads to some redundancy in the model. Model Pruning is a technique that reduces this redundancy by removing the sub-models in the neural network model, so as to reduce model calculation complexity and improve model inference performance.</p> <p>This example uses PaddleSlim provided APIs of Pruning to compress the OCR model. PaddleSlim, an open source library which integrates model pruning, quantization (including quantization training and offline quantization), distillation, neural network architecture search, and many other commonly used and leading model compression technique in the industry.</p> <p>It is recommended that you could understand following pages before reading this example\uff1a</p> <ol> <li>PaddleOCR training methods</li> <li>The demo of prune</li> </ol>"},{"location":"en/version2.x/ppocr/model_compress/prune.html#quick-start","title":"Quick start","text":""},{"location":"en/version2.x/ppocr/model_compress/prune.html#1-install-paddleslim","title":"1. Install PaddleSlim","text":"<pre><code>git clone https://github.com/PaddlePaddle/PaddleSlim.git\ncd PaddleSlim\ngit checkout develop\npython3 setup.py install\n</code></pre>"},{"location":"en/version2.x/ppocr/model_compress/prune.html#2-download-pre-trained-model","title":"2. Download Pre-trained Model","text":"<p>Model prune needs to load pre-trained models. PaddleOCR also provides a series of models. Developers can choose their own models or use their own models according to their needs.</p>"},{"location":"en/version2.x/ppocr/model_compress/prune.html#3-pruning-sensitivity-analysis","title":"3. Pruning sensitivity analysis","text":"<p>After the pre-trained model is loaded, sensitivity analysis is performed on each network layer of the model to understand the redundancy of each network layer, and save a sensitivity file which named: sen.pickle.  After that, user could load the sensitivity file via the methods provided by PaddleSlim and determining the pruning ratio of each network layer automatically. For specific details of sensitivity analysis, see\uff1aSensitivity analysis The data format of sensitivity file\uff1a</p> <pre><code>sen.pickle(Dict){\n              'layer_weight_name_0': sens_of_each_ratio(Dict){'pruning_ratio_0': acc_loss, 'pruning_ratio_1': acc_loss}\n              'layer_weight_name_1': sens_of_each_ratio(Dict){'pruning_ratio_0': acc_loss, 'pruning_ratio_1': acc_loss}\n          }\n</code></pre> <p>example\uff1a</p> <pre><code>{\n    'conv10_expand_weights': {0.1: 0.006509952684312718, 0.2: 0.01827734339798862, 0.3: 0.014528405644659832, 0.6: 0.06536008804270439, 0.8: 0.11798612250664964, 0.7: 0.12391408417493704, 0.4: 0.030615754498018757, 0.5: 0.047105205602406594}\n    'conv10_linear_weights': {0.1: 0.05113190831455035, 0.2: 0.07705573833558801, 0.3: 0.12096721757739311, 0.6: 0.5135061352930738, 0.8: 0.7908166677143281, 0.7: 0.7272187676899062, 0.4: 0.1819252083008504, 0.5: 0.3728054727792405}\n}\n</code></pre> <p>The function would return a dict after loading the sensitivity file. The keys of the dict are name of parameters in each layer. And the value of key is the information about pruning sensitivity of corresponding layer. In example, pruning 10% filter of the layer corresponding to conv10_expand_weights would lead to 0.65% degradation of model performance. The details could be seen at: Sensitivity analysis</p> <p>The function would return a dict after loading the sensitivity file. The keys of the dict are name of parameters in each layer. And the value of key is the information about pruning sensitivity of corresponding layer. In example, pruning 10% filter of the layer corresponding to conv10_expand_weights would lead to 0.65% degradation of model performance. The details could be seen at: Sensitivity analysis</p> <p>Enter the PaddleOCR root directory\uff0cperform sensitivity analysis on the model with the following command\uff1a</p> <pre><code>python3 deploy/slim/prune/sensitivity_anal.py -c configs/det/ch_ppocr_v2.0/ch_det_mv3_db_v2.0.yml -o Global.pretrained_model=\"your trained model\"  Global.save_model_dir=./output/prune_model/\n</code></pre>"},{"location":"en/version2.x/ppocr/model_compress/prune.html#5-export-inference-model-and-deploy-it","title":"5. Export inference model and deploy it","text":"<p>We can export the pruned model as inference_model for deployment:</p> <pre><code>python deploy/slim/prune/export_prune_model.py -c configs/det/ch_ppocr_v2.0/ch_det_mv3_db_v2.0.yml  -o Global.pretrained_model=./output/det_db/best_accuracy  Global.save_inference_dir=./prune/prune_inference_model\n</code></pre> <p>Reference for prediction and deployment of inference model:</p> <ol> <li>inference model python prediction</li> <li>inference model C++ prediction</li> </ol>"},{"location":"en/version2.x/ppocr/model_compress/quantization.html","title":"PP-OCR Models Quantization","text":"<p>Generally, a more complex model would achieve better performance in the task, but it also leads to some redundancy in the model. Quantization is a technique that reduces this redundancy by reducing the full precision data to a fixed number, so as to reduce model calculation complexity and improve model inference performance.</p> <p>This example uses PaddleSlim provided APIs of Quantization to compress the OCR model.</p> <p>It is recommended that you could understand following pages before reading this example\uff1a</p> <ul> <li>The training strategy of OCR model</li> <li>PaddleSlim Document</li> </ul>"},{"location":"en/version2.x/ppocr/model_compress/quantization.html#quick-start","title":"Quick Start","text":"<p>Quantization is mostly suitable for the deployment of lightweight models on mobile terminals. After training, if you want to further compress the model size and accelerate the prediction, you can use quantization methods to compress the model according to the following steps.</p> <ol> <li>Install PaddleSlim</li> <li>Prepare trained model</li> <li>Quantization-Aware Training</li> <li>Export inference model</li> <li>Deploy quantization inference model</li> </ol>"},{"location":"en/version2.x/ppocr/model_compress/quantization.html#1-install-paddleslim","title":"1. Install PaddleSlim","text":"<pre><code>pip3 install paddleslim==2.3.2\n</code></pre>"},{"location":"en/version2.x/ppocr/model_compress/quantization.html#2-download-pre-trained-model","title":"2. Download Pre-trained Model","text":"<p>PaddleOCR provides a series of pre-trained models. If the model to be quantified is not in the list, you need to follow the Regular Training method to get the trained model.</p>"},{"location":"en/version2.x/ppocr/model_compress/quantization.html#3-quant-aware-training","title":"3. Quant-Aware Training","text":"<p>Quantization training includes offline quantization training and online quantization training. Online quantization training is more effective. It is necessary to load the pre-trained model. After the quantization strategy is defined, the model can be quantified.</p> <p>The code for quantization training is located in <code>slim/quantization/quant.py</code>. For example, the training instructions of slim PPOCRv3 detection model are as follows:</p> <pre><code># download provided model\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar\ntar xf ch_PP-OCRv3_det_distill_train.tar\n\npython deploy/slim/quantization/quant.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml -o Global.pretrained_model='./ch_PP-OCRv3_det_distill_train/best_accuracy'   Global.save_model_dir=./output/quant_model_distill/\n</code></pre> <p>If you want to quantify the text recognition model, you can modify the configuration file and loaded model parameters.</p>"},{"location":"en/version2.x/ppocr/model_compress/quantization.html#4-export-inference-model","title":"4. Export inference model","text":"<p>Once we got the model after pruning and fine-tuning, we can export it as an inference model for the deployment of predictive tasks:</p> <pre><code>python deploy/slim/quantization/export_model.py -c configs/det/ch_PP-OCRv3/ch_PP-OCRv3_det_cml.yml -o Global.checkpoints=output/quant_model/best_accuracy Global.save_inference_dir=./output/quant_inference_model\n</code></pre>"},{"location":"en/version2.x/ppocr/model_compress/quantization.html#5-deploy","title":"5. Deploy","text":"<p>The numerical range of the quantized model parameters derived from the above steps is still FP32, but the numerical range of the parameters is int8. The derived model can be converted through the <code>opt tool</code> of PaddleLite.</p> <p>For quantitative model deployment, please refer to Mobile terminal model deployment</p>"},{"location":"en/version2.x/ppocr/model_train/PPOCRv3_det_train.html","title":"PP-OCRv3 text detection model training","text":""},{"location":"en/version2.x/ppocr/model_train/PPOCRv3_det_train.html#1-introduction","title":"1. Introduction","text":"<p>PP-OCRv3 is a further upgrade of PP-OCRv2. This section introduces the training steps of the PP-OCRv3 detection model. For an introduction to the PP-OCRv3 strategy, refer to document.</p>"},{"location":"en/version2.x/ppocr/model_train/PPOCRv3_det_train.html#2-detection-training","title":"2. Detection training","text":"<p>The PP-OCRv3 detection model is an upgrade of the CML (Collaborative Mutual Learning) collaborative mutual learning text detection distillation strategy in PP-OCRv2. PP-OCRv3 further optimizes the detection teacher model and student model. Among them, when optimizing the teacher model, the PAN structure LK-PAN with a large receptive field and the DML (Deep Mutual Learning) distillation strategy are proposed; when optimizing the student model, the FPN structure RSE-FPN with a residual attention mechanism is proposed.</p> <p>PP-OCRv3 detection training includes two steps:</p> <ul> <li> <p>Step 1: Use DML distillation method to train detection teacher model</p> </li> <li> <p>Step 2: Use the teacher model obtained in step 1 to train a lightweight student model using CML method</p> </li> </ul>"},{"location":"en/version2.x/ppocr/model_train/PPOCRv3_det_train.html#21-prepare-data-and-operating-environment","title":"2.1 Prepare data and operating environment","text":"<p>The training data uses icdar2015 data. For the steps of preparing the training set, refer to ocr_dataset.</p> <p>For the preparation of the operating environment, refer to document.</p>"},{"location":"en/version2.x/ppocr/model_train/PPOCRv3_det_train.html#22-train-the-teacher-model","title":"2.2 Train the teacher model","text":"<p>The configuration file for teacher model training is PP-OCRv3_det_dml.yml. The Backbone, Neck, and Head of the teacher model structure are Resnet50, LKPAN, and DBHead respectively, and are trained using the DML distillation method. For a detailed introduction to the configuration file, refer to Document.</p> <p>Download ImageNet pre-trained model:</p> <pre><code># Download ResNet50_vd pre-trained model\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/pretrained/ResNet50_vd_ssld_pretrained.pdparams\n</code></pre> <p>Start training</p> <pre><code># Single card training\npython3 tools/train.py -c configs/det/PP-OCRv3/PP-OCRv3_det_dml.yml \\\n-o Architecture.Models.Student.pretrained=./pretrain_models/ResNet50_vd_ssld_pretrained \\\nArchitecture.Models.Student2.pretrained=./pretrain_models/ResNet50_vd_ssld_pretrained \\\nGlobal.save_model_dir=./output/\n# If you want to use multi-GPU distributed training, please use the following command:\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/det/PP-OCRv3/PP-OCRv3_det_dml.yml \\\n-o Architecture.Models.Student.pretrained=./pretrain_models/ResNet50_vd_ssld_pretrained \\\nArchitecture.Models.Student2.pretrained=./pretrain_models/ResNet50_vd_ssld_pretrained \\\nGlobal.save_model_dir=./output/\n</code></pre> <p>The model saved during training is in the output directory, which contains the following files:</p> <pre><code>best_accuracy.states\nbest_accuracy.pdparams # The model parameters with the best accuracy are saved by default\nbest_accuracy.pdopt # The optimizer-related parameters with the best accuracy are saved by default\nlatest.states\nlatest.pdparams # The latest model parameters saved by default\nlatest.pdopt # The optimizer-related parameters of the latest model saved by default\n</code></pre> <p>Among them, best_accuracy is the model parameter with the highest accuracy saved, and the model can be directly used for evaluation.</p> <p>The model evaluation command is as follows:</p> <pre><code>python3 tools/eval.py -c configs/det/PP-OCRv3/PP-OCRv3_det_dml.yml -o Global.checkpoints=./output/best_accuracy\n</code></pre> <p>The trained teacher model has a larger structure and higher accuracy, which is used to improve the accuracy of the student model.</p> <p>Extract teacher model parameters best_accuracy contains the parameters of two models, corresponding to Student and Student2 in the configuration file. The method to extract the parameters of Student is as follows:</p> <pre><code>import paddle\n# Load pre-trained model\nall_params = paddle.load(\"output/best_accuracy.pdparams\")\n# View the keys of weight parameters\nprint(all_params.keys())\n# Model weight extraction\ns_params = {key[len(\"Student.\"):]: all_params[key] for key in all_params if \"Student.\" in key}\n# View the keys of model weight parameters\nprint(s_params.keys())\n# Save\npaddle.save(s_params, \"./pretrain_models/dml_teacher.pdparams\")\n</code></pre> <p>The extracted model parameters can be used for further fine-tuning or distillation training of the model.</p>"},{"location":"en/version2.x/ppocr/model_train/PPOCRv3_det_train.html#23-training-the-student-model","title":"2.3 Training the student model","text":"<p>The configuration file for training the student model is PP-OCRv3_det_cml.yml The teacher model trained in the previous section is used as supervision, and the CML method is used to train a lightweight student model.</p> <p>Download the ImageNet pre-trained model of the student model:</p> <pre><code># Download the pre-trained model of MobileNetV3\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/pretrained/MobileNetV3_large_x0_5_pretrained.pdparams\n</code></pre> <p>Start training</p> <pre><code># Single card training\npython3 tools/train.py -c configs/det/PP-OCRv3/PP-OCRv3_det_cml.yml \\\n-o Architecture.Models.Student.pretrained=./pretrain_models/MobileNetV3_large_x0_5_pretrained \\\nArchitecture.Models.Student2.pretrained=./pretrain_models/MobileNetV3_large_x0_5_pretrained \\\nArchitecture.Models.Teacher.pretrained=./pretrain_models/dml_teacher \\\nGlobal.save_model_dir=./output/\n# If you want to use multi-GPU distributed training, please use the following command:\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/det/PP-OCRv3/PP-OCRv3_det_cml.yml \\\n-o Architecture.Models.Student.pretrained=./pretrain_models/MobileNetV3_large_x0_5_pretrained \\\nArchitecture.Models.Student2.pretrained=./pretrain_models/MobileNetV3_large_x0_5_pretrained \\\nArchitecture.Models.Teacher.pretrained=./pretrain_models/dml_teacher \\\nGlobal.save_model_dir=./output/\n</code></pre> <p>The model saved during the training process is in the output directory. The model evaluation command is as follows:</p> <pre><code>python3 tools/eval.py -c configs/det/PP-OCRv3/PP-OCRv3_det_cml.yml -o Global.checkpoints=./output/best_accuracy\n</code></pre> <p>best_accuracy contains the parameters of three models, corresponding to Student, Student2, and Teacher in the configuration file. The method to extract Student parameters is as follows:</p> <pre><code>import paddle\n# Load pre-trained model\nall_params = paddle.load(\"output/best_accuracy.pdparams\")\n# View the keys of weight parameters\nprint(all_params.keys())\n# Model weight extraction\ns_params = {key[len(\"Student.\"):]: all_params[key] for key in all_params if \"Student.\" in key}\n# View the keys of model weight parameters\nprint(s_params.keys())\n# Save\npaddle.save(s_params, \"./pretrain_models/cml_student.pdparams\")\n</code></pre> <p>The extracted Student parameters can be used for model deployment or further fine-tuning training.</p>"},{"location":"en/version2.x/ppocr/model_train/PPOCRv3_det_train.html#3-fine-tune-training-based-on-pp-ocrv3-detection","title":"3. Fine-tune training based on PP-OCRv3 detection","text":"<p>This section describes how to use the PP-OCRv3 detection model for fine-tune training in other scenarios.</p> <p>Fine-tune training is applicable to three scenarios:</p> <ul> <li> <p>Fine-tune training based on the CML distillation method is applicable to scenarios where the teacher model has higher accuracy than the PP-OCRv3 detection model in the usage scenario and a lightweight detection model is desired.</p> </li> <li> <p>Fine-tune training based on the PP-OCRv3 lightweight detection model does not require the training of the teacher model and is intended to improve the accuracy of the usage scenario based on the PP-OCRv3 detection model.</p> </li> <li> <p>Fine-tune training based on the DML distillation method is applicable to scenarios where the DML method is used to further improve accuracy.</p> </li> </ul> <p>Finetune training based on CML distillation method</p> <p>Download PP-OCRv3 training model:</p> <pre><code>wget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar\ntar xf ch_PP-OCRv3_det_distill_train.tar\n</code></pre> <p>ch_PP-OCRv3_det_distill_train/best_accuracy.pdparams contains the parameters of Student, Student2, and Teacher models in the CML configuration file.</p> <p>Start training:</p> <pre><code># Single card training\npython3 tools/train.py -c configs/det/PP-OCRv3/PP-OCRv3_det_cml.yml \\\n-o Global.pretrained_model=./ch_PP-OCRv3_det_distill_train/best_accuracy \\\nGlobal.save_model_dir=./output/\n# If you want to use multi-GPU distributed training, please use the following command:\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/det/PP-OCRv3/PP-OCRv3_det_cml.yml \\\n-o Global.pretrained_model=./ch_PP-OCRv3_det_distill_train/best_accuracy \\\nGlobal.save_model_dir=./output/\n</code></pre> <p>Finetune training based on PP-OCRv3 lightweight detection model</p> <p>Download PP-OCRv3 training model and extract model parameters of Student structure:</p> <pre><code>wget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar\ntar xf ch_PP-OCRv3_det_distill_train.tar\n</code></pre> <p>The method to extract Student parameters is as follows:</p> <pre><code>import paddle\n# Load pre-trained model\nall_params = paddle.load(\"output/best_accuracy.pdparams\")\n# View the keys of weight parameters\nprint(all_params.keys())\n# Model weight extraction\ns_params = {key[len(\"Student.\"):]: all_params[key] for key in all_params if \"Student.\" in key}\n# View the keys of the model weight parameters\nprint(s_params.keys())\n# Save\npaddle.save(s_params, \"./student.pdparams\")\n</code></pre> <p>Train using the configuration file PP-OCRv3_mobile_det.yml.</p> <p>Start training</p> <pre><code># Single card training\npython3 tools/train.py -c configs/det/PP-OCRv3/PP-OCRv3_mobile_det.yml \\\n-o Global.pretrained_model=./student \\\nGlobal.save_model_dir=./output/\n# If you want to use multi-GPU distributed training, please use the following command:\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/det/PP-OCRv3/PP-OCRv3_mobile_det.yml \\\n-o Global.pretrained_model=./student \\\nGlobal.save_model_dir=./output/\n</code></pre> <p>Finetune training based on DML distillation method</p> <p>Take the Teacher model in ch_PP-OCRv3_det_distill_train as an example. First, extract the parameters of the Teacher structure. The method is as follows:</p> <pre><code>import paddle\n# Load pre-trained model\nall_params = paddle.load(\"ch_PP-OCRv3_det_distill_train/best_accuracy.pdparams\")\n# View the keys of weight parameters\nprint(all_params.keys())\n# Model weight extraction\ns_params = {key[len(\"Teacher.\"):]: all_params[key] for key in all_params if \"Teacher.\" in key}\n# View the keys of model weight parameters\nprint(s_params.keys())\n# Save\npaddle.save(s_params, \"./teacher.pdparams\")\n</code></pre> <p>Start training</p> <pre><code># Single card training\npython3 tools/train.py -c configs/det/PP-OCRv3/PP-OCRv3_det_dml.yml \\\n-o Architecture.Models.Student.pretrained=./teacher \\\nArchitecture.Models.Student2.pretrained=./teacher \\\nGlobal.save_model_dir=./output/\n# If you want to use multi-GPU distributed training, please use the following command:\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/det/PP-OCRv3/PP-OCRv3_det_dml.yml \\\n-o Architecture.Models.Student.pretrained=./teacher \\\nArchitecture.Models.Student2.pretrained=./teacher \\\nGlobal.save_model_dir=./output/\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/angle_class.html","title":"Text Direction Classification","text":""},{"location":"en/version2.x/ppocr/model_train/angle_class.html#1-method-introduction","title":"1. Method Introduction","text":"<p>The angle classification is used in the scene where the image is not 0 degrees. In this scene, it is necessary to perform a correction operation on the text line detected in the picture. In the PaddleOCR system, The text line image obtained after text detection is sent to the recognition model after affine transformation. At this time, only a 0 and 180 degree angle classification of the text is required, so the built-in PaddleOCR text angle classifier only supports 0 and 180 degree classification. If you want to support more angles, you can modify the algorithm yourself to support.</p> <p>Example of 0 and 180 degree data samples\uff1a</p> <p></p>"},{"location":"en/version2.x/ppocr/model_train/angle_class.html#2-data-preparation","title":"2. Data Preparation","text":"<p>Please organize the dataset as follows:</p> <p>The default storage path for training data is <code>PaddleOCR/train_data/cls</code>, if you already have a dataset on your disk, just create a soft link to the dataset directory:</p> <pre><code>ln -sf &lt;path/to/dataset&gt; &lt;path/to/paddle_ocr&gt;/train_data/cls/dataset\n</code></pre> <p>please refer to the following to organize your data.</p>"},{"location":"en/version2.x/ppocr/model_train/angle_class.html#training-set","title":"Training set","text":"<p>First put the training images in the same folder (train_images), and use a txt file (cls_gt_train.txt) to store the image path and label.</p> <ul> <li>Note: by default, the image path and image label are split with <code>\\t</code>, if you use other methods to split, it will cause training error</li> </ul> <p>0 and 180 indicate that the angle of the image is 0 degrees and 180 degrees, respectively.</p> <pre><code>\" Image file name           Image annotation \"\n\ntrain/word_001.jpg   0\ntrain/word_002.jpg   180\n</code></pre> <p>The final training set should have the following file structure:</p> <pre><code>|-train_data\n    |-cls\n        |- cls_gt_train.txt\n        |- train\n            |- word_001.png\n            |- word_002.jpg\n            |- word_003.jpg\n            | ...\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/angle_class.html#test-set","title":"Test set","text":"<p>Similar to the training set, the test set also needs to be provided a folder containing all images (test) and a cls_gt_test.txt. The structure of the test set is as follows:</p> <pre><code>|-train_data\n    |-cls\n        |- cls_gt_test.txt\n        |- test\n            |- word_001.jpg\n            |- word_002.jpg\n            |- word_003.jpg\n            | ...\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/angle_class.html#3-training","title":"3. Training","text":"<p>Write the prepared txt file and image folder path into the configuration file under the <code>Train/Eval.dataset.label_file_list</code> and <code>Train/Eval.dataset.data_dir</code> fields, the absolute path of the image consists of the <code>Train/Eval.dataset.data_dir</code> field and the image name recorded in the txt file.</p> <p>PaddleOCR provides training scripts, evaluation scripts, and prediction scripts.</p>"},{"location":"en/version2.x/ppocr/model_train/angle_class.html#start-training","title":"Start training","text":"<pre><code># Set PYTHONPATH path\nexport PYTHONPATH=$PYTHONPATH:.\n# GPU training Support single card and multi-card training, specify the card number through --gpus.\n# Start training, the following command has been written into the train.sh file, just modify the configuration file path in the file\npython3 -m paddle.distributed.launch --gpus '0,1,2,3,4,5,6,7'  tools/train.py -c configs/cls/cls_mv3.yml\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/angle_class.html#data-augmentation","title":"Data Augmentation","text":"<p>PaddleOCR provides a variety of data augmentation methods. If you want to add disturbance during training, Please uncomment the <code>RecAug</code> and <code>RandAugment</code> fields under <code>Train.dataset.transforms</code> in the configuration file.</p> <p>The default perturbation methods are: cvtColor, blur, jitter, Gauss noise, random crop, perspective, color reverse, RandAugment.</p> <p>Except for RandAugment, each disturbance method is selected with a 50% probability during the training process. For specific code implementation, please refer to: rec_img_aug.py randaugment.py</p>"},{"location":"en/version2.x/ppocr/model_train/angle_class.html#training","title":"Training","text":"<p>PaddleOCR supports alternating training and evaluation. You can modify <code>eval_batch_step</code> in <code>configs/cls/cls_mv3.yml</code> to set the evaluation frequency. By default, it is evaluated every 1000 iter. The following content will be saved during training:</p> <pre><code>\u251c\u2500\u2500 best_accuracy.pdopt # Optimizer parameters for the best model\n\u251c\u2500\u2500 best_accuracy.pdparams # Parameters of the best model\n\u251c\u2500\u2500 best_accuracy.states # Metric info and epochs of the best model\n\u251c\u2500\u2500 config.yml # Configuration file for this experiment\n\u251c\u2500\u2500 latest.pdopt # Optimizer parameters for the latest model\n\u251c\u2500\u2500 latest.pdparams # Parameters of the latest model\n\u251c\u2500\u2500 latest.states # Metric info and epochs of the latest model\n\u2514\u2500\u2500 train.log # Training log\n</code></pre> <p>If the evaluation set is large, the test will be time-consuming. It is recommended to reduce the number of evaluations, or evaluate after training.</p> <p>Note that the configuration file for prediction/evaluation must be consistent with the training.</p>"},{"location":"en/version2.x/ppocr/model_train/angle_class.html#4-evaluation","title":"4. Evaluation","text":"<p>The evaluation dataset can be set by modifying the <code>Eval.dataset.label_file_list</code> field in the <code>configs/cls/cls_mv3.yml</code> file.</p> <pre><code>export CUDA_VISIBLE_DEVICES=0\n# GPU evaluation, Global.checkpoints is the weight to be tested\npython3 tools/eval.py -c configs/cls/cls_mv3.yml -o Global.checkpoints={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/angle_class.html#5-prediction","title":"5. Prediction","text":""},{"location":"en/version2.x/ppocr/model_train/angle_class.html#training-engine-prediction","title":"Training engine prediction","text":"<p>Using the model trained by paddleocr, you can quickly get prediction through the following script.</p> <p>Use <code>Global.infer_img</code> to specify the path of the predicted picture or folder, and use <code>Global.checkpoints</code> to specify the weight:</p> <pre><code># Predict English results\npython3 tools/infer_cls.py -c configs/cls/cls_mv3.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.load_static_weights=false Global.infer_img=doc/imgs_words_en/word_10.png\n</code></pre> <p>Input image:</p> <p></p> <p>Get the prediction result of the input image:</p> <pre><code>infer_img: doc/imgs_words_en/word_10.png\n     result: ('0', 0.9999995)\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/detection.html","title":"Text Detection","text":"<p>This section uses the icdar2015 dataset as an example to introduce the training, evaluation, and testing of the detection model in PaddleOCR.</p>"},{"location":"en/version2.x/ppocr/model_train/detection.html#1-data-and-weights-preparation","title":"1. Data and Weights Preparation","text":""},{"location":"en/version2.x/ppocr/model_train/detection.html#11-data-preparation","title":"1.1 Data Preparation","text":"<p>To prepare datasets, refer to ocr_datasets.</p>"},{"location":"en/version2.x/ppocr/model_train/detection.html#12-download-pre-trained-model","title":"1.2 Download Pre-trained Model","text":"<p>First download the pre-trained model. The detection model of PaddleOCR currently supports 3 backbones, namely MobileNetV3, ResNet18_vd and ResNet50_vd. You can use the model in PaddleClas to replace backbone according to your needs. And the responding download link of backbone pre-trained weights can be found in (https://github.com/PaddlePaddle/PaddleClas/blob/release%2F2.0/README_cn.md#resnet%E5%8F%8A%E5%85%B6vd%E7%B3%BB%E5%88%97).</p> <pre><code>cd PaddleOCR/\n# Download the pre-trained model of MobileNetV3\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/pretrained/MobileNetV3_large_x0_5_pretrained.pdparams\n# or, download the pre-trained model of ResNet18_vd\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/pretrained/ResNet18_vd_pretrained.pdparams\n# or, download the pre-trained model of ResNet50_vd\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/pretrained/ResNet50_vd_ssld_pretrained.pdparams\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/detection.html#2-training","title":"2. Training","text":""},{"location":"en/version2.x/ppocr/model_train/detection.html#21-start-training","title":"2.1 Start Training","text":"<p>If CPU version installed, please set the parameter <code>use_gpu</code> to <code>false</code> in the configuration.</p> <pre><code>python3 tools/train.py -c configs/det/det_mv3_db.yml  \\\n         -o Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained\n</code></pre> <p>In the above instruction, use <code>-c</code> to select the training to use the <code>configs/det/det_mv3_db.yml</code> configuration file. For a detailed explanation of the configuration file, please refer to config.</p> <p>You can also use <code>-o</code> to change the training parameters without modifying the yml file. For example, adjust the training learning rate to 0.0001.</p> <pre><code># single GPU training\npython3 tools/train.py -c configs/det/det_mv3_db.yml -o   \\\n         Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained  \\\n         Optimizer.base_lr=0.0001\n\n# multi-GPU training\n# Set the GPU ID used by the '--gpus' parameter.\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/det/det_mv3_db.yml -o Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained\n\n# multi-Node, multi-GPU training\n# Set the IPs of your nodes used by the '--ips' parameter. Set the GPU ID used by the '--gpus' parameter.\npython3 -m paddle.distributed.launch --ips=\"xx.xx.xx.xx,xx.xx.xx.xx\" --gpus '0,1,2,3' tools/train.py -c configs/det/det_mv3_db.yml \\\n     -o Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained\n</code></pre> <p>Note: For multi-Node multi-GPU training, you need to replace the <code>ips</code> value in the preceding command with the address of your machine, and the machines must be able to ping each other. In addition, it requires activating commands separately on multiple machines when we start the training. The command for viewing the IP address of the machine is <code>ifconfig</code>.</p> <p>If you want to further speed up the training, you can use automatic mixed precision training. for single card training, the command is as follows:</p> <pre><code>python3 tools/train.py -c configs/det/det_mv3_db.yml \\\n     -o Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained \\\n     Global.use_amp=True Global.scale_loss=1024.0 Global.use_dynamic_loss_scaling=True\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/detection.html#22-load-trained-model-and-continue-training","title":"2.2 Load Trained Model and Continue Training","text":"<p>If you expect to load trained model and continue the training again, you can specify the parameter <code>Global.checkpoints</code> as the model path to be loaded.</p> <p>For example:</p> <pre><code>python3 tools/train.py -c configs/det/det_mv3_db.yml -o Global.checkpoints=./your/trained/model\n</code></pre> <p>Note: The priority of <code>Global.checkpoints</code> is higher than that of <code>Global.pretrained_model</code>, that is, when two parameters are specified at the same time, the model specified by <code>Global.checkpoints</code> will be loaded first. If the model path specified by <code>Global.checkpoints</code> is wrong, the one specified by <code>Global.pretrained_model</code> will be loaded.</p>"},{"location":"en/version2.x/ppocr/model_train/detection.html#23-training-with-new-backbone","title":"2.3 Training with New Backbone","text":"<p>The network part completes the construction of the network, and PaddleOCR divides the network into four parts, which are under ppocr/modeling. The data entering the network will pass through these four parts in sequence(transforms-&gt;backbones-&gt; necks-&gt;heads).</p> <pre><code>\u251c\u2500\u2500 architectures # Code for building network\n\u251c\u2500\u2500 transforms    # Image Transformation Module\n\u251c\u2500\u2500 backbones     # Feature extraction module\n\u251c\u2500\u2500 necks         # Feature enhancement module\n\u2514\u2500\u2500 heads         # Output module\n</code></pre> <p>If the Backbone to be replaced has a corresponding implementation in PaddleOCR, you can directly modify the parameters in the <code>Backbone</code> part of the configuration yml file.</p> <p>However, if you want to use a new Backbone, an example of replacing the backbones is as follows:</p> <ol> <li>Create a new file under the ppocr/modeling/backbones folder, such as my_backbone.py.</li> <li>Add code in the my_backbone.py file, the sample code is as follows:</li> </ol> <pre><code>import paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n\nclass MyBackbone(nn.Layer):\n    def __init__(self, *args, **kwargs):\n        super(MyBackbone, self).__init__()\n        # your init code\n        self.conv = nn.xxxx\n\n    def forward(self, inputs):\n        # your network forward\n        y = self.conv(inputs)\n        return y\n</code></pre> <ol> <li>Import the added module in the ppocr/modeling/backbones/_init_.py file.</li> </ol> <p>After adding the four-part modules of the network, you only need to configure them in the configuration file to use, such as:</p> <pre><code>  Backbone:\n    name: MyBackbone\n    args1: args1\n</code></pre> <p>NOTE: More details about replace Backbone and other module can be found in doc.</p>"},{"location":"en/version2.x/ppocr/model_train/detection.html#24-mixed-precision-training","title":"2.4 Mixed Precision Training","text":"<p>If you want to speed up your training further, you can use Auto Mixed Precision Training, taking a single machine and a single gpu as an example, the commands are as follows:</p> <pre><code>python3 tools/train.py -c configs/det/det_mv3_db.yml \\\n     -o Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained \\\n     Global.use_amp=True Global.scale_loss=1024.0 Global.use_dynamic_loss_scaling=True\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/detection.html#25-distributed-training","title":"2.5 Distributed Training","text":"<p>During multi-machine multi-gpu training, use the <code>--ips</code> parameter to set the used machine IP address, and the <code>--gpus</code> parameter to set the used GPU ID:</p> <pre><code>python3 -m paddle.distributed.launch --ips=\"xx.xx.xx.xx,xx.xx.xx.xx\" --gpus '0,1,2,3' tools/train.py -c configs/det/det_mv3_db.yml \\\n     -o Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained\n</code></pre> <p>Note: (1) When using multi-machine and multi-gpu training, you need to replace the ips value in the above command with the address of your machine, and the machines need to be able to ping each other. (2) Training needs to be launched separately on multiple machines. The command to view the ip address of the machine is <code>ifconfig</code>. (3) For more details about the distributed training speedup ratio, please refer to Distributed Training Tutorial.</p>"},{"location":"en/version2.x/ppocr/model_train/detection.html#26-training-with-knowledge-distillation","title":"2.6 Training with knowledge distillation","text":"<p>Knowledge distillation is supported in PaddleOCR for text detection training process. For more details, please refer to doc.</p>"},{"location":"en/version2.x/ppocr/model_train/detection.html#27-training-on-other-platformwindowsmacoslinux-dcu","title":"2.7 Training on other platform(Windows/macOS/Linux DCU)","text":"<ul> <li> <p>Windows GPU/CPU The Windows platform is slightly different from the Linux platform: Windows platform only supports <code>single gpu</code> training and inference, specify GPU for training <code>set CUDA_VISIBLE_DEVICES=0</code> On the Windows platform, DataLoader only supports single-process mode, so you need to set <code>num_workers</code> to 0;</p> </li> <li> <p>macOS GPU mode is not supported, you need to set <code>use_gpu</code> to False in the configuration file, and the rest of the training evaluation prediction commands are exactly the same as Linux GPU.</p> </li> <li> <p>Linux DCU Running on a DCU device requires setting the environment variable <code>export HIP_VISIBLE_DEVICES=0,1,2,3</code>, and the rest of the training and evaluation prediction commands are exactly the same as the Linux GPU.</p> </li> </ul>"},{"location":"en/version2.x/ppocr/model_train/detection.html#28-fine-tuning","title":"2.8 Fine-tuning","text":"<p>In actual use, it is recommended to load the official pre-trained model and fine-tune it in your own data set. For the fine-tuning method of the detection model, please refer to: Model Fine-tuning Tutorial.</p>"},{"location":"en/version2.x/ppocr/model_train/detection.html#3-evaluation-and-test","title":"3. Evaluation and Test","text":""},{"location":"en/version2.x/ppocr/model_train/detection.html#31-evaluation","title":"3.1 Evaluation","text":"<p>PaddleOCR calculates three indicators for evaluating performance of OCR detection task: Precision, Recall, and Hmean(F-Score).</p> <p>Run the following code to calculate the evaluation indicators. The result will be saved in the test result file specified by <code>save_res_path</code> in the configuration file <code>det_db_mv3.yml</code></p> <p>When evaluating, set post-processing parameters <code>box_thresh=0.6</code>, <code>unclip_ratio=1.5</code>. If you use different datasets, different models for training, these two parameters should be adjusted for better result.</p> <p>The model parameters during training are saved in the <code>Global.save_model_dir</code> directory by default. When evaluating indicators, you need to set <code>Global.checkpoints</code> to point to the saved parameter file.</p> <pre><code>python3 tools/eval.py -c configs/det/det_mv3_db.yml  -o Global.checkpoints=\"{path/to/weights}/best_accuracy\" PostProcess.box_thresh=0.6 PostProcess.unclip_ratio=1.5\n</code></pre> <ul> <li>Note: <code>box_thresh</code> and <code>unclip_ratio</code> are parameters required for DB post-processing, and not need to be set when evaluating the EAST and SAST model.</li> </ul>"},{"location":"en/version2.x/ppocr/model_train/detection.html#32-test","title":"3.2 Test","text":"<p>Test the detection result on a single image:</p> <pre><code>python3 tools/infer_det.py -c configs/det/det_mv3_db.yml -o Global.infer_img=\"./doc/imgs_en/img_10.jpg\" Global.pretrained_model=\"./output/det_db/best_accuracy\"\n</code></pre> <p>When testing the DB model, adjust the post-processing threshold:</p> <pre><code>python3 tools/infer_det.py -c configs/det/det_mv3_db.yml -o Global.infer_img=\"./doc/imgs_en/img_10.jpg\" Global.pretrained_model=\"./output/det_db/best_accuracy\"  PostProcess.box_thresh=0.6 PostProcess.unclip_ratio=2.0\n</code></pre> <p>Test the detection result on all images in the folder:</p> <pre><code>python3 tools/infer_det.py -c configs/det/det_mv3_db.yml -o Global.infer_img=\"./doc/imgs_en/\" Global.pretrained_model=\"./output/det_db/best_accuracy\"\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/detection.html#4-inference","title":"4. Inference","text":"<p>The inference model (the model saved by <code>paddle.jit.save</code>) is generally a solidified model saved after the model training is completed, and is mostly used to give prediction in deployment.</p> <p>The model saved during the training process is the checkpoints model, which saves the parameters of the model and is mostly used to resume training.</p> <p>Compared with the checkpoints model, the inference model will additionally save the structural information of the model. Therefore, it is easier to deploy because the model structure and model parameters are already solidified in the inference model file, and is suitable for integration with actual systems.</p> <p>Firstly, we can convert DB trained model to inference model:</p> <pre><code>python3 tools/export_model.py -c configs/det/det_mv3_db.yml -o Global.pretrained_model=\"./output/det_db/best_accuracy\" Global.save_inference_dir=\"./output/det_db_inference/\"\n</code></pre> <p>The detection inference model prediction\uff1a</p> <pre><code>python3 tools/infer/predict_det.py --det_algorithm=\"DB\" --det_model_dir=\"./output/det_db_inference/\" --image_dir=\"./doc/imgs/\" --use_gpu=True\n</code></pre> <p>If it is other detection algorithms, such as the EAST, the det_algorithm parameter needs to be modified to EAST, and the default is the DB algorithm:</p> <pre><code>python3 tools/infer/predict_det.py --det_algorithm=\"EAST\" --det_model_dir=\"./output/det_db_inference/\" --image_dir=\"./doc/imgs/\" --use_gpu=True\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/detection.html#5-faq","title":"5. FAQ","text":"<p>Q1: The prediction results of trained model and inference model are inconsistent?</p> <p>A: Most of the problems are caused by the inconsistency of the pre-processing and post-processing parameters during the prediction of the trained model and the pre-processing and post-processing parameters during the prediction of the inference model. Taking the model trained by the det_mv3_db.yml configuration file as an example, the solution to the problem of inconsistent prediction results between the training model and the inference model is as follows:</p> <ul> <li>Check whether the trained model preprocessing is consistent with the prediction preprocessing function of the inference model. When the algorithm is evaluated, the input image size will affect the accuracy. In order to be consistent with the paper, the image is resized to [736, 1280] in the training icdar15 configuration file, but there is only a set of default parameters when the inference model predicts, which will be considered. To predict the speed problem, the longest side of the image is limited to 960 for resize by default. The preprocessing function of the training model preprocessing and the inference model is located in ppocr/data/imaug/operators.py.</li> <li>Check whether the post-processing of the trained model is consistent with the post-processing parameters of the inference.</li> </ul>"},{"location":"en/version2.x/ppocr/model_train/finetune.html","title":"Fine-tune","text":""},{"location":"en/version2.x/ppocr/model_train/finetune.html#1-background-and-meaning","title":"1. Background and meaning","text":"<p>The PP-OCR series models provided by PaddleOCR have excellent performance in general scenarios and can solve detection and recognition problems in most cases. In vertical scenarios, if you want to obtain better model, you can further improve the accuracy of the PP-OCR series detection and recognition models through fine-tune.</p> <p>This article mainly introduces some precautions when fine-tuning the text detection and recognition model. Finally, you can obtain a text detection and recognition model with higher accuracy through model fine-tuning in your own scenarios.</p> <p>The core points of this article are as follows:</p> <ol> <li>The pre-trained model provided by PP-OCR has better generalization ability</li> <li>Adding a small amount of real data (detection:&gt;=500, recognition:&gt;=5000) will greatly improve the detection and recognition effect of vertical scenes</li> <li>When fine-tuning the model, adding real general scene data can further improve the model accuracy and generalization performance</li> <li>In the text detection task, increasing the prediction shape of the image can further improve the detection effect of the smaller text area</li> <li>When fine-tuning the model, it is necessary to properly adjust the hyperparameters (learning rate and batch size are the most important) to obtain a better fine-tuning effect.</li> </ol> <p>For more details, please refer to Chapter 2 and Chapter 3.</p>"},{"location":"en/version2.x/ppocr/model_train/finetune.html#2-text-detection-model-fine-tuning","title":"2. Text detection model fine-tuning","text":""},{"location":"en/version2.x/ppocr/model_train/finetune.html#21-dataset","title":"2.1 Dataset","text":"<ul> <li> <p>Dataset: It is recommended to prepare at least 500 text detection datasets for model fine-tuning.</p> </li> <li> <p>Dataset annotation: single-line text annotation format, it is recommended that the labeled detection frame be consistent with the actual semantic content. For example, in the train ticket scene, the surname and first name may be far apart, but they belong to the same detection field semantically. Here, the entire name also needs to be marked as a detection frame.</p> </li> </ul>"},{"location":"en/version2.x/ppocr/model_train/finetune.html#22-model","title":"2.2 Model","text":"<p>It is recommended to choose the PP-OCRv3 model (configuration file: PP-OCRv3_mobile_det.yml\uff0cpre-trained model: ch_PP-OCRv3_det_distill_train.tar, its accuracy and generalization performance is the best pre-training model currently available.</p> <p>For more PP-OCR series models, please refer to PP-OCR Series Model Library.</p> <p>Note: When using the above pre-trained model, you need to use the <code>student.pdparams</code> file in the folder as the pre-trained model, that is, only use the student model.</p>"},{"location":"en/version2.x/ppocr/model_train/finetune.html#23-training-hyperparameter","title":"2.3 Training hyperparameter","text":"<p>When fine-tuning the model, the most important hyperparameter is the pre-training model path <code>pretrained_model</code>, <code>learning_rate</code> and <code>batch_size</code>\uff0csome hyperparameters are as follows:</p> <pre><code>Global:\n  pretrained_model: ./ch_PP-OCRv3_det_distill_train/student.pdparams # pre-training model path\nOptimizer:\n  lr:\n    name: Cosine\n    learning_rate: 0.001 # learning_rate\n    warmup_epoch: 2\n  regularizer:\n    name: 'L2'\n    factor: 0\n\nTrain:\n  loader:\n    shuffle: True\n    drop_last: False\n    batch_size_per_card: 8  # single gpu batch size\n    num_workers: 4\n</code></pre> <p>In the above configuration file, you need to specify the <code>pretrained_model</code> field as the <code>student.pdparams</code> file path.</p> <p>The configuration file provided by PaddleOCR is for 8-gpu training (equivalent to a total batch size of <code>8*8=64</code>) and no pre-trained model is loaded. Therefore, in your scenario, the learning rate is the same as the total The batch size needs to be adjusted linearly, for example</p> <ul> <li>If your scenario is single-gpu training, single gpu batch_size=8, then the total batch_size=8, it is recommended to adjust the learning rate to about <code>1e-4</code>.</li> <li>If your scenario is for single-gpu training, due to memory limitations, you can only set batch_size=4 for a single gpu, and the total batch_size=4. It is recommended to adjust the learning rate to about <code>5e-5</code>.</li> </ul>"},{"location":"en/version2.x/ppocr/model_train/finetune.html#24-prediction-hyperparameter","title":"2.4 Prediction hyperparameter","text":"<p>When exporting and inferring the trained model, you can further adjust the predicted image scale to improve the detection effect of small-area text. The following are some hyperparameters during DBNet inference, which can be adjusted appropriately to improve the effect.</p> hyperparameter type default meaning det_db_thresh float 0.3 In the probability map output by DB, pixels with a score greater than the threshold will be considered as text pixels det_db_box_thresh float 0.6 When the average score of all pixels within the frame of the detection result is greater than the threshold, the result will be considered as a text area det_db_unclip_ratio float 1.5 The expansion coefficient of <code>Vatti clipping</code>, using this method to expand the text area max_batch_size int 10 batch size use_dilation bool False Whether to expand the segmentation results to obtain better detection results det_db_score_mode str \"fast\" DB's detection result score calculation method supports <code>fast</code> and <code>slow</code>. <code>fast</code> calculates the average score based on all pixels in the polygon\u2019s circumscribed rectangle border, and <code>slow</code> calculates the average score based on all pixels in the original polygon. The calculation speed is relatively slower, but more accurate. <p>For more information on inference methods, please refer toPaddle Inference doc.</p>"},{"location":"en/version2.x/ppocr/model_train/finetune.html#3-text-recognition-model-fine-tuning","title":"3. Text recognition model fine-tuning","text":""},{"location":"en/version2.x/ppocr/model_train/finetune.html#31-dataset","title":"3.1 Dataset","text":"<ul> <li> <p>Dataset\uff1aIf the dictionary is not changed, it is recommended to prepare at least 5,000 text recognition datasets for model fine-tuning; if the dictionary is changed (not recommended), more quantities are required.</p> </li> <li> <p>Data distribution: It is recommended that the distribution be as consistent as possible with the actual measurement scenario. If the actual scene contains a lot of short text, it is recommended to include more short text in the training data. If the actual scene has high requirements for the recognition effect of spaces, it is recommended to include more text content with spaces in the training data.</p> </li> <li> <p>Data synthesis: In the case of some character recognition errors, it is recommended to obtain a batch of specific character dataset, add it to the original dataset and use a small learning rate for fine-tuning. The ratio of original dataset to new dataset can be 10:1 to 5:1 to avoid overfitting of the model caused by too much data in a single scene. At the same time, try to balance the word frequency of the corpus to ensure that the frequency of common words will not be too low.</p> </li> </ul> <p>Specific characters can be generated using the TextRenderer tool, for synthesis examples, please refer to data synthesis   . The synthetic data corpus should come from real usage scenarios as much as possible, and keep the richness of fonts and backgrounds on the basis of being close to the real scene, which will help improve the model effect.</p> <ul> <li>Common Chinese and English data: During training, common real data can be added to the training set (for example, in the fine-tuning scenario without changing the dictionary, it is recommended to add real data such as LSVT, RCTW, MTWI) to further improve the generalization performance of the model.</li> </ul>"},{"location":"en/version2.x/ppocr/model_train/finetune.html#32-model","title":"3.2 Model","text":"<p>It is recommended to choose the PP-OCRv3 model (configuration file: PP-OCRv3_mobile_rec_distillation.yml\uff0cpre-trained model: PP-OCRv3_mobile_rec_train.tar\uff0cits accuracy and generalization performance is the best pre-training model currently available.</p> <p>For more PP-OCR series models, please refer to PP-OCR Series Model Library.</p> <p>The PP-OCRv3 model uses the GTC strategy. The SAR branch has a large number of parameters. When the training data is a simple scene, the model is easy to overfit, resulting in poor fine-tuning effect. It is recommended to remove the GTC strategy. The configuration file of the model structure is modified as follows:</p> <pre><code>Architecture:\n  model_type: rec\n  algorithm: SVTR\n  Transform:\n  Backbone:\n    name: MobileNetV1Enhance\n    scale: 0.5\n    last_conv_stride: [1, 2]\n    last_pool_type: avg\n  Neck:\n    name: SequenceEncoder\n    encoder_type: svtr\n    dims: 64\n    depth: 2\n    hidden_dims: 120\n    use_guide: False\n  Head:\n    name: CTCHead\n    fc_decay: 0.00001\nLoss:\n  name: CTCLoss\n\nTrain:\n  dataset:\n  ......\n    transforms:\n    # remove RecConAug\n    # - RecConAug:\n    #     prob: 0.5\n    #     ext_data_num: 2\n    #     image_shape: [48, 320, 3]\n    #     max_text_length: *max_text_length\n    - RecAug:\n    # modify Encode\n    - CTCLabelEncode:\n    - KeepKeys:\n        keep_keys:\n        - image\n        - label\n        - length\n...\n\nEval:\n  dataset:\n  ...\n    transforms:\n    ...\n    - CTCLabelEncode:\n    - KeepKeys:\n        keep_keys:\n        - image\n        - label\n        - length\n...\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/finetune.html#33-training-hyperparameter","title":"3.3 Training hyperparameter","text":"<p>Similar to text detection task fine-tuning, when fine-tuning the recognition model, the most important hyperparameters are the pre-trained model path <code>pretrained_model</code>, <code>learning_rate</code> and <code>batch_size</code>, some default configuration files are shown below.</p> <pre><code>Global:\n  pretrained_model:  # pre-training model path\nOptimizer:\n  lr:\n    name: Piecewise\n    decay_epochs : [700, 800]\n    values : [0.001, 0.0001]  # learning_rate\n    warmup_epoch: 5\n  regularizer:\n    name: 'L2'\n    factor: 0\n\nTrain:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/\n    label_file_list:\n    - ./train_data/train_list.txt\n    ratio_list: [1.0] # Sampling ratio, the default value is [1.0]\n  loader:\n    shuffle: True\n    drop_last: False\n    batch_size_per_card: 128 # single gpu batch size\n    num_workers: 8\n</code></pre> <p>In the above configuration file, you first need to specify the <code>pretrained_model</code> field as the <code>ch_PP-OCRv3_rec_train/best_accuracy.pdparams</code> file path decompressed in Chapter 3.2.</p> <p>The configuration file provided by PaddleOCR is for 8-gpu training (equivalent to a total batch size of <code>8*128=1024</code>) and no pre-trained model is loaded. Therefore, in your scenario, the learning rate is the same as the total The batch size needs to be adjusted linearly, for example:</p> <ul> <li>If your scenario is single-gpu training, single gpu batch_size=128, then the total batch_size=128, in the case of loading the pre-trained model, it is recommended to adjust the learning rate to about <code>[1e-4, 2e-5]</code> (For the piecewise learning rate strategy, two values need to be set, the same below).</li> <li>If your scenario is for single-gpu training, due to memory limitations, you can only set batch_size=64 for a single gpu, and the total batch_size=64. When loading the pre-trained model, it is recommended to adjust the learning rate to <code>[5e-5 , 1e-5]</code>about.</li> </ul> <p>If there is general real scene data added, it is recommended that in each epoch, the amount of vertical scene data and real scene data should be kept at about 1:1.</p> <p>For example: your own vertical scene recognition data volume is 1W, the data label file is <code>vertical.txt</code>, the collected general scene recognition data volume is 10W, and the data label file is <code>general.txt</code>.</p> <p>Then, the <code>label_file_list</code> and <code>ratio_list</code> parameters can be set as shown below. In each epoch, <code>vertical.txt</code> will be fully sampled (sampling ratio is 1.0), including 1W pieces of data; <code>general.txt</code> will be sampled according to a sampling ratio of 0.1, including <code>10W*0.1=1W</code> pieces of data, the final ratio of the two is <code>1:1</code>.</p> <pre><code>Train:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/\n    label_file_list:\n    - vertical.txt\n    - general.txt\n    ratio_list: [1.0, 0.1]\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/finetune.html#34-training-optimization","title":"3.4 Training optimization","text":"<p>The training process does not happen overnight. After completing a stage of training evaluation, it is recommended to collect and analyze the badcase of the current model in the real scene, adjust the proportion of training data in a targeted manner, or further add synthetic data. Through multiple iterations of training, the model effect is continuously optimized.</p> <p>If you modify the custom dictionary during training, since the parameters of the last layer of FC cannot be loaded, it is normal for acc=0 at the beginning of the iteration. Don't worry, loading the pre-trained model can still speed up the model convergence.</p>"},{"location":"en/version2.x/ppocr/model_train/kie.html","title":"Key Information Extraction","text":"<p>This tutorial provides a guide to the whole process of key information extraction using PaddleOCR, including data preparation, model training, optimization, evaluation, prediction of semantic entity recognition (SER) and relationship extraction (RE) tasks.</p>"},{"location":"en/version2.x/ppocr/model_train/kie.html#1-data-preparation","title":"1. Data Preparation","text":""},{"location":"en/version2.x/ppocr/model_train/kie.html#11-prepare-for-dataset","title":"1.1. Prepare for dataset","text":"<p>PaddleOCR supports the following data format when training KIE models.</p> <ul> <li><code>general data</code> is used to train a dataset whose annotation is stored in a text file (SimpleDataset).</li> </ul> <p>The default storage path of training data is <code>PaddleOCR/train_data</code>. If you already have datasets on your disk, you only need to create a soft link to the dataset directory.</p> <pre><code># linux and mac os\nln -sf &lt;path/to/dataset&gt; &lt;path/to/paddle_ocr&gt;/train_data/dataset\n# windows\nmklink /d &lt;path/to/paddle_ocr&gt;/train_data/dataset &lt;path/to/dataset&gt;\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/kie.html#12-custom-dataset","title":"1.2. Custom Dataset","text":"<p>The training process generally includes the training set and the evaluation set. The data formats of the two sets are same.</p>"},{"location":"en/version2.x/ppocr/model_train/kie.html#1-training-set","title":"(1) Training set","text":"<p>It is recommended to put the training images into the same folder, record the path and annotation of images in a text file. The contents of the text file are as follows:</p> <pre><code>\" image path                 annotation information \"\nzh_train_0.jpg   [{\"transcription\": \"\u200b\u6c47\u4e30\u200b\u664b\u4fe1\u200b\", \"label\": \"other\", \"points\": [[104, 114], [530, 114], [530, 175], [104, 175]], \"id\": 1, \"linking\": []}, {\"transcription\": \"\u200b\u53d7\u7406\u200b\u65f6\u95f4\u200b:\", \"label\": \"question\", \"points\": [[126, 267], [266, 267], [266, 305], [126, 305]], \"id\": 7, \"linking\": [[7, 13]]}, {\"transcription\": \"2020.6.15\", \"label\": \"answer\", \"points\": [[321, 239], [537, 239], [537, 285], [321, 285]], \"id\": 13, \"linking\": [[7, 13]]}]\nzh_train_1.jpg   [{\"transcription\": \"\u200b\u4e2d\u56fd\u200b\u4eba\u4f53\u5668\u5b98\u200b\u6350\u732e\u200b\", \"label\": \"other\", \"points\": [[544, 459], [954, 459], [954, 517], [544, 517]], \"id\": 1, \"linking\": []}, {\"transcription\": \"&gt;\u200b\u7f16\u53f7\u200b:MC545715483585\", \"label\": \"other\", \"points\": [[1462, 470], [2054, 470], [2054, 543], [1462, 543]], \"id\": 10, \"linking\": []}, {\"transcription\": \"CHINAORGANDONATION\", \"label\": \"other\", \"points\": [[543, 516], [958, 516], [958, 551], [543, 551]], \"id\": 14, \"linking\": []}, {\"transcription\": \"\u200b\u4e2d\u56fd\u200b\u4eba\u4f53\u5668\u5b98\u200b\u6350\u732e\u200b\u5fd7\u613f\u200b\u767b\u8bb0\u8868\u200b\", \"label\": \"header\", \"points\": [[635, 793], [1892, 793], [1892, 904], [635, 904]], \"id\": 18, \"linking\": []}]\n...\n</code></pre> <p>Note: In the text file, please split the image path and annotation with <code>\\t</code>. Otherwise, error will happen when training.</p> <p>The annotation can be parsed by <code>json</code> into a list of sub-annotations. Each element in the list is a dict, which stores the required information of each text line. The required fields are as follows.</p> <ul> <li>transcription: stores the text content of the text line</li> <li>label: the category of the text line content</li> <li>points: stores the four point position information of the text line</li> <li>id: stores the ID information of the text line for RE model training</li> <li>linking: stores the connection information between text lines for RE model training</li> </ul>"},{"location":"en/version2.x/ppocr/model_train/kie.html#2-evaluation-set","title":"(2) Evaluation set","text":"<p>The evaluation set is constructed in the same way as the training set.</p>"},{"location":"en/version2.x/ppocr/model_train/kie.html#3-dictionary-file","title":"(3) Dictionary file","text":"<p>The textlines in the training set and the evaluation set contain label information. The list of all labels is stored in the dictionary file (such as <code>class_list.txt</code>). Each line in the dictionary file is represented as a label name.</p> <p>For example, FUND_zh data contains four categories. The contents of the dictionary file are as follows.</p> <pre><code>OTHER\nQUESTION\nANSWER\nHEADER\n</code></pre> <p>In the annotation file, the annotation information of the <code>label</code> field of the text line content of each annotation needs to belong to the dictionary content.</p> <p>The final dataset shall have the following file structure.</p> <pre><code>|-train_data\n  |-data_name\n    |- train.json\n    |- train\n        |- zh_train_0.png\n        |- zh_train_1.jpg\n        | ...\n    |- val.json\n    |- val\n        |- zh_val_0.png\n        |- zh_val_1.jpg\n        | ...\n</code></pre> <p>Note:</p> <p>-The category information in the annotation file is not case sensitive. For example, 'HEADER' and 'header' will be seen as the same category ID.</p> <ul> <li>In the dictionary file, it is recommended to put the <code>other</code> category (other textlines that need not be paid attention to can be labeled as <code>other</code>) on the first line. When parsing, the category ID of the 'other' category will be resolved to 0, and the textlines predicted as <code>other</code> will not be visualized later.</li> </ul>"},{"location":"en/version2.x/ppocr/model_train/kie.html#13-download-data","title":"1.3. Download data","text":"<p>If you do not have local dataset, you can download the source files of XFUND or FUNSD and use the scripts of XFUND or FUNSD for transform them into PaddleOCR format. Then you can use the public dataset to quick experience KIE.</p> <p>For more information about public KIE datasets, please refer to KIE dataset tutorial.</p> <p>PaddleOCR also supports the annotation of KIE models. Please refer to PPOCRLabel tutorial.</p>"},{"location":"en/version2.x/ppocr/model_train/kie.html#2-training","title":"2. Training","text":"<p>PaddleOCR provides training scripts, evaluation scripts and inference scripts. We will introduce based on VI-LayoutXLM model in this section. This section will take the VI layoutxlm multimodal pre training model as an example to explain.</p> <p>If you want to use the SDMGR based KIE algorithm, please refer to: SDMGR tutorial.</p>"},{"location":"en/version2.x/ppocr/model_train/kie.html#21-start-training","title":"2.1. Start Training","text":"<p>If you do not use a custom dataset, you can use XFUND_zh that has been processed in PaddleOCR dataset for quick experience.</p> <pre><code>mkdir train_data\ncd train_data\nwget https://paddleocr.bj.bcebos.com/ppstructure/dataset/XFUND.tar &amp;&amp; tar -xf XFUND.tar\ncd ..\n</code></pre> <p>If you don't want to train, and want to directly experience the process of model evaluation, prediction, and inference, you can download the training model provided in PaddleOCR and skip section 2.1.</p> <p>Use the following command to download the trained model.</p> <pre><code>mkdir pretrained_model\ncd pretrained_model\n# download and uncompress SER model\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/ser_vi_layoutxlm_xfund_pretrained.tar &amp; tar -xf ser_vi_layoutxlm_xfund_pretrained.tar\n\n# download and uncompress RE model\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/re_vi_layoutxlm_xfund_pretrained.tar &amp; tar -xf re_vi_layoutxlm_xfund_pretrained.tar\n</code></pre> <p>Start training:</p> <ul> <li>If your paddlepaddle version is <code>CPU</code>, you need to set <code>Global.use_gpu=False</code> in your config file.</li> <li>During training, PaddleOCR will download the VI-LayoutXLM pretraining model by default. There is no need to download it in advance.</li> </ul> <pre><code># GPU training, support single card and multi-cards\n# The training log will be save in \"{Global.save_model_dir}/train.log\"\n\n# train SER model using single card\npython3 tools/train.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml\n\n# train SER model using multi-cards, you can use --gpus to assign the GPU ids.\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml\n\n# train RE model using single card\npython3 tools/train.py -c configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml\n</code></pre> <p>Take the SER model training as an example. After the training is started, you will see the following log output.</p> <pre><code>[2022/08/08 16:28:28] ppocr INFO: epoch: [1/200], global_step: 10, lr: 0.000006, loss: 1.871535, avg_reader_cost: 0.28200 s, avg_batch_cost: 0.82318 s, avg_samples: 8.0, ips: 9.71838 samples/s, eta: 0:51:59\n[2022/08/08 16:28:33] ppocr INFO: epoch: [1/200], global_step: 19, lr: 0.000018, loss: 1.461939, avg_reader_cost: 0.00042 s, avg_batch_cost: 0.32037 s, avg_samples: 6.9, ips: 21.53773 samples/s, eta: 0:37:55\n[2022/08/08 16:28:39] ppocr INFO: cur metric, precision: 0.11526348939743859, recall: 0.19776657060518732, hmean: 0.14564265817747712, fps: 34.008392345050055\n[2022/08/08 16:28:45] ppocr INFO: save best model is to ./output/ser_vi_layoutxlm_xfund_zh/best_accuracy\n[2022/08/08 16:28:45] ppocr INFO: best metric, hmean: 0.14564265817747712, precision: 0.11526348939743859, recall: 0.19776657060518732, fps: 34.008392345050055, best_epoch: 1\n[2022/08/08 16:28:51] ppocr INFO: save model in ./output/ser_vi_layoutxlm_xfund_zh/latest\n</code></pre> <p>The following information will be automatically printed.</p> Field meaning epoch current iteration round iter current iteration times lr current learning rate loss current loss function reader_cost current batch data processing time batch_ Cost total current batch time samples number of samples in the current batch ips number of samples processed per second <p>PaddleOCR supports evaluation during training. you can modify <code>eval_batch_step</code> in the config file <code>configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml</code> (default as 19 iters). Trained model with best hmean will be saved as <code>output/ser_vi_layoutxlm_xfund_zh/best_accuracy/</code>.</p> <p>If the evaluation dataset is very large, it's recommended to enlarge the eval interval or evaluate the model after training.</p> <p>Note: for more KIE models training and configuration files, you can go into <code>configs/kie/</code> or refer to Frontier KIE algorithms.</p> <p>If you want to train model on your own dataset, you need to modify the data path, dictionary file and category number in the configuration file.</p> <p>Take <code>configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml</code> as an example, contents we need to fix is as follows.</p> <pre><code>Architecture:\n  # ...\n  Backbone:\n    name: LayoutXLMForSer\n    pretrained: True\n    mode: vi\n    # Assuming that n categories are included in the dictionary file (other is included), the the num_classes is set as 2n-1\n    num_classes: &amp;num_classes 7\n\nPostProcess:\n  name: kieSerTokenLayoutLMPostProcess\n  # Modify the dictionary file path for your custom dataset\n  class_path: &amp;class_path train_data/XFUND/class_list_xfun.txt\n\nTrain:\n  dataset:\n    name: SimpleDataSet\n    # Modify the data path for your training dataset\n    data_dir: train_data/XFUND/zh_train/image\n    # Modify the data annotation path for your training dataset\n    label_file_list:\n      - train_data/XFUND/zh_train/train.json\n    ...\n  loader:\n    # batch size for single card when training\n    batch_size_per_card: 8\n    ...\n\nEval:\n  dataset:\n    name: SimpleDataSet\n    # Modify the data path for your evaluation dataset\n    data_dir: train_data/XFUND/zh_val/image\n    # Modify the data annotation path for your evaluation dataset\n    label_file_list:\n      - train_data/XFUND/zh_val/val.json\n    ...\n  loader:\n    # batch size for single card when evaluation\n    batch_size_per_card: 8\n</code></pre> <p>Note that the configuration file for prediction/evaluation must be consistent with the training file.</p>"},{"location":"en/version2.x/ppocr/model_train/kie.html#22-resume-training","title":"2.2. Resume Training","text":"<p>If the training process is interrupted and you want to load the saved model to resume training, you can specify the path of the model to be loaded by specifying <code>Architecture.Backbone.checkpoints</code>.</p> <pre><code>python3 tools/train.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./output/ser_vi_layoutxlm_xfund_zh/best_accuracy\n</code></pre> <p>Note:</p> <ul> <li>Priority of <code>Architecture.Backbone.checkpoints</code> is higher than<code>Architecture.Backbone.pretrained</code>. You need to set <code>Architecture.Backbone.checkpoints</code> for model finetuning, resume and evaluation. If you want to train with the NLP pretrained model, you need to set <code>Architecture.Backbone.pretrained</code> as <code>True</code> and set <code>Architecture.Backbone.checkpoints</code> as null (<code>null</code>).</li> <li>PaddleNLP pretrained models are used here for LayoutXLM series models, the model loading and saving logic is same as those in PaddleNLP. Therefore we do not need to set <code>Global.pretrained_model</code> or <code>Global.checkpoints</code> here.</li> <li>If you use knowledge distillation to train the LayoutXLM series models, resuming training is not supported now.</li> </ul>"},{"location":"en/version2.x/ppocr/model_train/kie.html#23-mixed-precision-training","title":"2.3. Mixed Precision Training","text":"<p>coming soon!</p>"},{"location":"en/version2.x/ppocr/model_train/kie.html#24-distributed-training","title":"2.4. Distributed Training","text":"<p>During multi-machine multi-gpu training, use the <code>--ips</code> parameter to set the used machine IP address, and the <code>--gpus</code> parameter to set the used GPU ID:</p> <pre><code>python3 -m paddle.distributed.launch --ips=\"xx.xx.xx.xx,xx.xx.xx.xx\" --gpus '0,1,2,3' tools/train.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml\n</code></pre> <p>Note: (1) When using multi-machine and multi-gpu training, you need to replace the ips value in the above command with the address of your machine, and the machines need to be able to ping each other. (2) Training needs to be launched separately on multiple machines. The command to view the ip address of the machine is <code>ifconfig</code>. (3) For more details about the distributed training speedup ratio, please refer to Distributed Training Tutorial.</p>"},{"location":"en/version2.x/ppocr/model_train/kie.html#25-train-with-knowledge-distillation","title":"2.5. Train with Knowledge Distillation","text":"<p>Knowledge distillation is supported in PaddleOCR for KIE model training process. The configuration file is ser_vi_layoutxlm_xfund_zh_udml.yml. For more information, please refer to doc.</p> <p>Note: The saving and loading logic of the LayoutXLM series KIE models in PaddleOCR is consistent with PaddleNLP, so only the parameters of the student model are saved in the distillation process. If you want to use the saved model for evaluation, you need to use the configuration of the student model (the student model corresponding to the distillation file above is ser_vi_layoutxlm_xfund_zh.yml.</p>"},{"location":"en/version2.x/ppocr/model_train/kie.html#26-training-on-other-platform","title":"2.6. Training on other platform","text":"<ul> <li> <p>Windows GPU/CPU The Windows platform is slightly different from the Linux platform: Windows platform only supports <code>single gpu</code> training and inference, specify GPU for training <code>set CUDA_VISIBLE_DEVICES=0</code> On the Windows platform, DataLoader only supports single-process mode, so you need to set <code>num_workers</code> to 0;</p> </li> <li> <p>macOS GPU mode is not supported, you need to set <code>use_gpu</code> to False in the configuration file, and the rest of the training evaluation prediction commands are exactly the same as Linux GPU.</p> </li> <li> <p>Linux DCU Running on a DCU device requires setting the environment variable <code>export HIP_VISIBLE_DEVICES=0,1,2,3</code>, and the rest of the training and evaluation prediction commands are exactly the same as the Linux GPU.</p> </li> </ul>"},{"location":"en/version2.x/ppocr/model_train/kie.html#3-evaluation-and-test","title":"3. Evaluation and Test","text":""},{"location":"en/version2.x/ppocr/model_train/kie.html#31-evaluation","title":"3.1. Evaluation","text":"<p>The trained model will be saved in <code>Global.save_model_dir</code>. When evaluation, you need to set <code>Architecture.Backbone.checkpoints</code> as your model directory. The evaluation dataset can be set by modifying the <code>Eval.dataset.label_file_list</code> field in the <code>configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml</code> file.</p> <pre><code># GPU evaluation, Global.checkpoints is the weight to be tested\npython3 tools/eval.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./output/ser_vi_layoutxlm_xfund_zh/best_accuracy\n</code></pre> <p>The following information will be printed such as precision, recall, hmean and so on.</p> <pre><code>[2022/08/09 07:59:28] ppocr INFO: metric eval ***************\n[2022/08/09 07:59:28] ppocr INFO: precision:0.697476609016161\n[2022/08/09 07:59:28] ppocr INFO: recall:0.8861671469740634\n[2022/08/09 07:59:28] ppocr INFO: hmean:0.7805806758686339\n[2022/08/09 07:59:28] ppocr INFO: fps:17.367364606899105\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/kie.html#32-test","title":"3.2. Test","text":"<p>Using the model trained by PaddleOCR, we can quickly get prediction through the following script.</p> <p>The default prediction image is stored in <code>Global.infer_img</code>, and the trained model weight is specified via <code>-o Global.checkpoints</code>.</p> <p>According to the <code>Global.save_model_dir</code> and <code>save_epoch_step</code> fields set in the configuration file, the following parameters will be saved.</p> <pre><code>output/ser_vi_layoutxlm_xfund_zh/\n\u251c\u2500\u2500 best_accuracy\n       \u251c\u2500\u2500 metric.states\n       \u251c\u2500\u2500 model_config.json\n       \u251c\u2500\u2500 model_state.pdparams\n\u251c\u2500\u2500 best_accuracy.pdopt\n\u251c\u2500\u2500 config.yml\n\u251c\u2500\u2500 train.log\n\u251c\u2500\u2500 latest\n       \u251c\u2500\u2500 metric.states\n       \u251c\u2500\u2500 model_config.json\n       \u251c\u2500\u2500 model_state.pdparams\n\u251c\u2500\u2500 latest.pdopt\n</code></pre> <p>Among them, best_accuracy.is the best model on the evaluation set; latest. is the model of the last epoch.</p> <p>The configuration file for prediction must be consistent with the training file. If you finish the training process using <code>python3 tools/train.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml</code>. You can use the following command for prediction.</p> <pre><code>python3 tools/infer_kie_token_ser.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./output/ser_vi_layoutxlm_xfund_zh/best_accuracy Global.infer_img=./ppstructure/docs/kie/input/zh_val_42.jpg\n</code></pre> <p>The output image is as follows, which is also saved in <code>Global.save_res_path</code>.</p> <p></p> <p>During the prediction process, the detection and recognition model of PP-OCRv3 will be loaded by default for information extraction of OCR. If you want to load the OCR results obtained in advance, you can use the following method to predict, and specify <code>Global.infer_img</code> as the annotation file, which contains the image path and OCR information, and specifies <code>Global.infer_mode</code> as False, indicating that the OCR inference engine is not used at this time.</p> <pre><code>python3 tools/infer_kie_token_ser.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./output/ser_vi_layoutxlm_xfund_zh/best_accuracy Global.infer_img=./train_data/XFUND/zh_val/val.json Global.infer_mode=False\n</code></pre> <p>For the above image, if information extraction is performed using the labeled OCR results, the prediction results are as follows.</p> <p></p> <p>It can be seen that part of the detection information is more accurate, but the overall information extraction results are basically the same.</p> <p>In RE model prediction, the SER model result needs to be given first, so the configuration file and model weight of SER need to be loaded at the same time, as shown in the following example.</p> <pre><code>python3 ./tools/infer_kie_token_ser_re.py \\\n  -c configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml \\\n  -o Architecture.Backbone.checkpoints=./pretrain_models/re_vi_layoutxlm_udml_xfund_zh/best_accuracy/ \\\n  Global.infer_img=./train_data/XFUND/zh_val/image/ \\\n  -c_ser configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml \\\n  -o_ser Architecture.Backbone.checkpoints=pretrain_models/ \\\n  ser_vi_layoutxlm_udml_xfund_zh/best_accuracy/\n</code></pre> <p>The result is as follows.</p> <p></p> <p>If you want to load the OCR results obtained in advance, you can use the following method to predict, and specify <code>Global.infer_img</code> as the annotation file, which contains the image path and OCR information, and specifies <code>Global.infer_mode</code> as False, indicating that the OCR inference engine is not used at this time.</p> <pre><code>python3 ./tools/infer_kie_token_ser_re.py \\\n  -c configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml \\\n  -o Architecture.Backbone.checkpoints=./pretrain_models/re_vi_layoutxlm_udml_xfund_zh/best_accuracy/ \\\n  Global.infer_img=./train_data/XFUND/zh_val/val.json \\\n  Global.infer_mode=False \\\n  -c_ser configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml \\\n  -o_ser Architecture.Backbone.checkpoints=pretrain_models/ser_vi_layoutxlm_udml_xfund_zh/best_accuracy/\n</code></pre> <p><code>c_ser</code> denotes SER configurations file, <code>o_ser</code> denotes the SER model configurations that will override corresponding content in the file.</p> <p>The result is as follows.</p> <p></p> <p>It can be seen that the re prediction results directly using the annotated OCR results are more accurate.</p>"},{"location":"en/version2.x/ppocr/model_train/kie.html#4-model-inference","title":"4. Model inference","text":""},{"location":"en/version2.x/ppocr/model_train/kie.html#41-export-the-model","title":"4.1 Export the model","text":"<p>The inference model (the model saved by <code>paddle.jit.save</code>) is generally a solidified model saved after the model training is completed, and is mostly used to give prediction in deployment.</p> <p>The model saved during the training process is the checkpoints model, which saves the parameters of the model and is mostly used to resume training.</p> <p>Compared with the checkpoints model, the inference model will additionally save the structural information of the model. Therefore, it is easier to deploy because the model structure and model parameters are already solidified in the inference model file, and is suitable for integration with actual systems.</p> <p>The SER model can be converted to the inference model using the following command.</p> <pre><code># -c Set the training algorithm yml configuration file.\n# -o Set optional parameters.\n# Architecture.Backbone.checkpoints Set the training model address.\n# Global.save_inference_dir Set the address where the converted model will be saved.\npython3 tools/export_model.py -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./output/ser_vi_layoutxlm_xfund_zh/best_accuracy Global.save_inference_dir=./inference/ser_vi_layoutxlm\n</code></pre> <p>After the conversion is successful, there are three files in the model save directory:</p> <pre><code>inference/ser_vi_layoutxlm/\n    \u251c\u2500\u2500 inference.pdiparams         # The parameter file of recognition inference model\n    \u251c\u2500\u2500 inference.pdiparams.info    # The parameter information of recognition inference model, which can be ignored\n    \u2514\u2500\u2500 inference.pdmodel           # The program file of recognition\n</code></pre> <p>The RE model can be converted to the inference model using the following command.</p> <pre><code># -c Set the training algorithm yml configuration file.\n# -o Set optional parameters.\n# Architecture.Backbone.checkpoints Set the training model address.\n# Global.save_inference_dir Set the address where the converted model will be saved.\npython3 tools/export_model.py -c configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml -o Architecture.Backbone.checkpoints=./output/re_vi_layoutxlm_xfund_zh/best_accuracy Global.save_inference_dir=./inference/re_vi_layoutxlm\n</code></pre> <p>After the conversion is successful, there are three files in the model save directory:</p> <pre><code>inference/re_vi_layoutxlm/\n    \u251c\u2500\u2500 inference.pdiparams         # The parameter file of recognition inference model\n    \u251c\u2500\u2500 inference.pdiparams.info    # The parameter information of recognition inference model, which can be ignored\n    \u2514\u2500\u2500 inference.pdmodel           # The program file of recognition\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/kie.html#42-model-inference","title":"4.2 Model inference","text":"<p>The VI layoutxlm model performs reasoning based on the ser task, and can execute the following commands:</p> <p>Using the following command to infer the VI-LayoutXLM SER model.</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser.py \\\n  --kie_algorithm=LayoutXLM \\\n  --ser_model_dir=../inference/ser_vi_layoutxlm \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\"\n</code></pre> <p>The visualized result will be saved in <code>./output</code>, which is shown as follows.</p> <p></p> <p>Using the following command to infer the VI-LayoutXLM RE model.</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser_re.py \\\n  --kie_algorithm=LayoutXLM \\\n  --re_model_dir=../inference/re_vi_layoutxlm \\\n  --ser_model_dir=../inference/ser_vi_layoutxlm \\\n  --use_visual_backbone=False \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\"\n</code></pre> <p>The visualized result will be saved in <code>./output</code>, which is shown as follows.</p> <p></p>"},{"location":"en/version2.x/ppocr/model_train/kie.html#5-faq","title":"5. FAQ","text":"<p>Q1: After the training model is transferred to the inference model, the prediction effect is inconsistent?</p> <p>A\uff1aThe problems are mostly caused by inconsistent preprocessing and postprocessing parameters when the trained model predicts and the preprocessing and postprocessing parameters when the inference model predicts. You can compare whether there are differences in preprocessing, postprocessing, and prediction in the configuration files used for training.</p>"},{"location":"en/version2.x/ppocr/model_train/recognition.html","title":"Text Recognition","text":"<p>This article provides a comprehensive guide for the PaddleOCR text recognition task, covering the entire workflow including data preparation, model training, fine-tuning, evaluation, and prediction, with detailed explanations for each phase.</p>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#1-data-preparation","title":"1. Data Preparation","text":""},{"location":"en/version2.x/ppocr/model_train/recognition.html#11-prepare-the-dataset","title":"1.1. Prepare the Dataset","text":"<p>PaddleOCR supports two data formats:</p> <ul> <li><code>lmdb</code>: Used for training with datasets stored in LMDB format (LMDBDataSet);</li> <li><code>General Data</code>: Used for training with datasets stored in text files (SimpleDataSet);</li> </ul> <p>The default storage path for training data is <code>PaddleOCR/train_data</code>. If you already have a dataset on your disk, simply create a symbolic link to the dataset directory:</p> <pre><code># Linux and macOS\nln -sf &lt;path/to/dataset&gt; &lt;path/to/paddle_ocr&gt;/train_data/dataset\n# Windows\nmklink /d &lt;path/to/paddle_ocr&gt;/train_data/dataset &lt;path/to/dataset&gt;\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#12-custom-dataset","title":"1.2. Custom Dataset","text":"<p>Here, we will use a general dataset as an example to explain how to prepare the dataset:</p> <ul> <li>Training Dataset</li> </ul> <p>It is recommended to place the training images in the same folder and record the image paths and labels in a txt file (<code>rec_gt_train.txt</code>). The content of the txt file should be as follows:</p> <p>Note: In the txt file, please use <code>\\t</code> to separate the image path and the label. Using any other separator will cause errors during training.</p> <pre><code>\" Image Filename                   Image Label \"\n\ntrain_data/rec/train/word_001.jpg   Simple and reliable\ntrain_data/rec/train/word_002.jpg   Making the complex world simpler with technology\n...\n</code></pre> <p>The final structure of the training dataset should look like this:</p> <pre><code>|-train_data\n  |-rec\n    |- rec_gt_train.txt\n    |- train\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre> <p>In addition to the single-image-per-line format described above, PaddleOCR also supports training on data augmented offline. To avoid sampling the same sample multiple times in the same batch, we can list image paths with the same label on one line. During training, PaddleOCR will randomly select one image from the list. The corresponding format of the label file is as follows:</p> <pre><code>[\"11.jpg\", \"12.jpg\"]   Simple and reliable\n[\"21.jpg\", \"22.jpg\", \"23.jpg\"]   Making the complex world simpler with technology\n3.jpg   ocr\n</code></pre> <p>In the above example, both \"11.jpg\" and \"12.jpg\" have the same label <code>Simple and reliable</code>. During training, one of these images will be randomly chosen for training.</p> <ul> <li>Validation Dataset</li> </ul> <p>Similarly to the training dataset, the validation dataset should also provide a folder containing all the images (test) and a <code>rec_gt_test.txt</code> file. The structure of the validation dataset is as follows:</p> <pre><code>|-train_data\n  |-rec\n    |- rec_gt_test.txt\n    |- test\n        |- word_001.jpg\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#13-data-download","title":"1.3. Data Download","text":"<ul> <li>ICDAR2015</li> </ul> <p>If you don't have a dataset locally, you can download the ICDAR2015 dataset from the official website for quick testing. You can also refer to DTRB to download the LMDB formatted dataset needed for benchmarking.</p> <p>If you're using the public ICDAR2015 dataset, PaddleOCR provides a label file for training the ICDAR2015 dataset. You can download it as follows:</p> <pre><code># Training set label\nwget -P ./train_data/ic15_data  https://paddleocr.bj.bcebos.com/dataset/rec_gt_train.txt\n# Test Set Label\nwget -P ./train_data/ic15_data  https://paddleocr.bj.bcebos.com/dataset/rec_gt_test.txt\n</code></pre> <p>PaddleOCR also provides a data format conversion script, which can convert ICDAR official website label to a data format supported by PaddleOCR. The data conversion tool is in <code>ppocr/utils/gen_label.py</code>, here is the training set as an example:</p> <pre><code># convert the official gt to rec_gt_label.txt\npython gen_label.py --mode=\"rec\" --input_path=\"{path/of/origin/label}\" --output_label=\"rec_gt_label.txt\"\n</code></pre> <p>The data format is as follows, (a) is the original picture, (b) is the Ground Truth text file corresponding to each picture:</p> <p></p> <ul> <li>Multilingual Datasets</li> </ul> <p>The multi-language model training method is the same as the Chinese model. The training data set is 100w synthetic data. A small amount of fonts and test data can be downloaded using the following two methods.</p> <ul> <li>Baidu Netdisk ,Extraction code:frgi.</li> <li>Google drive</li> </ul>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#14-dictionary","title":"1.4. Dictionary","text":"<p>Finally, a dictionary ({word_dict_name}.txt) needs to be provided so that when the model is trained, all the characters that appear can be mapped to the dictionary index.</p> <p>Therefore, the dictionary needs to contain all the characters that you want to be recognized correctly. {word_dict_name}.txt needs to be written in the following format and saved in the <code>utf-8</code> encoding format:</p> <pre><code>l\nd\na\nd\nr\nn\n</code></pre> <p>In <code>word_dict.txt</code>, there is a single word in each line, which maps characters and numeric indexes together, e.g \"and\" will be mapped to [2 5 1]</p> <p>PaddleOCR includes several built-in dictionaries that can be used as needed:</p> <ul> <li><code>ppocr/utils/ppocr_keys_v1.txt</code>: A Chinese dictionary containing 6623 characters.</li> <li><code>ppocr/utils/ic15_dict.txt</code>: An English dictionary containing 36 characters.</li> <li><code>ppocr/utils/dict/french_dict.txt</code>: A French dictionary containing 118 characters.</li> <li><code>ppocr/utils/dict/japan_dict.txt</code>: A Japanese dictionary containing 4399 characters.</li> <li><code>ppocr/utils/dict/korean_dict.txt</code>: A Korean dictionary containing 3636 characters.</li> <li><code>ppocr/utils/dict/german_dict.txt</code>: A German dictionary containing 131 characters.</li> <li><code>ppocr/utils/en_dict.txt</code>: An English dictionary containing 96 characters.</li> </ul> <p>Currently, the multilingual models are still in the demo stage, and we are continuously improving the models and adding new languages. We highly welcome you to provide dictionaries and fonts for other languages. If you are willing, you can submit your dictionary files to the dict directory, and we will credit you in the repo. To customize the dict file, please modify the <code>character_dict_path</code> field in <code>configs/rec/rec_icdar15_train.yml</code>.</p> <ul> <li>Custom Dictionary</li> </ul> <p>If you need to customize dic file, please add character_dict_path field in configs/rec/rec_icdar15_train.yml to point to your dictionary path. And set character_type to ch.</p>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#15-add-space-category","title":"1.5. Add Space Category","text":"<p>To support recognition of the \"space\" category, set the <code>use_space_char</code> field in the YAML file to <code>True</code>.</p>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#16-data-augmentation","title":"1.6. Data Augmentation","text":"<p>PaddleOCR provides a variety of data augmentation methods. All the augmentation methods are enabled by default.</p> <p>The default perturbation methods are: cvtColor, blur, jitter, Gasuss noise, random crop, perspective, color reverse, TIA augmentation.</p> <p>Each disturbance method is selected with a 40% probability during the training process. For specific code implementation, please refer to: rec_img_aug.py</p>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#2-training","title":"2. Training","text":"<p>PaddleOCR provides training scripts, evaluation scripts, and prediction scripts. This section will take the PP-OCRv3 English recognition model as an example:</p>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#21-start-training","title":"2.1. Start Training","text":"<p>First download the pretrain model, you can download the trained model to finetune on the icdar2015 data:</p> <pre><code>cd PaddleOCR/\n# Download the pre-trained model of en_PP-OCRv3\nwget -P ./pretrain_models/ https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_train.tar\n# Decompress model parameters\ncd pretrain_models\ntar -xf en_PP-OCRv3_rec_train.tar &amp;&amp; rm -rf en_PP-OCRv3_rec_train.tar\n</code></pre> <p>Start training:</p> <pre><code># GPU training Support single card and multi-card training\n# Training icdar15 English data and The training log will be automatically saved as train.log under \"{save_model_dir}\"\n\n#specify the single card training(Long training time, not recommended)\npython3 tools/train.py -c configs/rec/PP-OCRv3/en_PP-OCRv3_mobile_rec.yml -o Global.pretrained_model=en_PP-OCRv3_rec_train/best_accuracy\n\n#specify the card number through --gpus\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py -c configs/rec/PP-OCRv3/en_PP-OCRv3_mobile_rec.yml -o Global.pretrained_model=en_PP-OCRv3_rec_train/best_accuracy\n</code></pre> <p>PaddleOCR supports alternating training and evaluation. You can modify <code>eval_batch_step</code> in <code>configs/rec/rec_icdar15_train.yml</code> to set the evaluation frequency. By default, it is evaluated every 500 iter and the best acc model is saved under <code>output/rec_CRNN/best_accuracy</code> during the evaluation process.</p> <p>If the evaluation set is large, the test will be time-consuming. It is recommended to reduce the number of evaluations, or evaluate after training.</p> <ul> <li>Tip: You can use the <code>-c</code> parameter to select multiple model configurations under the <code>configs/rec/</code> path for training. The recognition algorithms supported at rec_algorithm:</li> </ul> <p>For training Chinese data, it is recommended to use PP-OCRv3_mobile_rec_distillation.yml. If you want to try the result of other algorithms on the Chinese data set, please refer to the following instructions to modify the configuration file:</p> <p>Take <code>PP-OCRv3_mobile_rec_distillation.yml</code> as an example:</p> <pre><code>Global:\n  ...\n  # Add a custom dictionary, such as modify the dictionary, please point the path to the new dictionary\n  character_dict_path: ppocr/utils/ppocr_keys_v1.txt\n  # Modify character type\n  ...\n  # Whether to recognize spaces\n  use_space_char: True\n\n\nOptimizer:\n  ...\n  # Add learning rate decay strategy\n  lr:\n    name: Cosine\n    learning_rate: 0.001\n  ...\n\n...\n\nTrain:\n  dataset:\n    # Type of dataset\uff0cwe support LMDBDataSet and SimpleDataSet\n    name: SimpleDataSet\n    # Path of dataset\n    data_dir: ./train_data/\n    # Path of train list\n    label_file_list: [\"./train_data/train_list.txt\"]\n    transforms:\n      ...\n      - RecResizeImg:\n          # Modify image_shape to fit long text\n          image_shape: [3, 48, 320]\n      ...\n  loader:\n    ...\n    # Train batch_size for Single card\n    batch_size_per_card: 256\n    ...\n\nEval:\n  dataset:\n    # Type of dataset\uff0cwe support LMDBDataSet and SimpleDataSet\n    name: SimpleDataSet\n    # Path of dataset\n    data_dir: ./train_data\n    # Path of eval list\n    label_file_list: [\"./train_data/val_list.txt\"]\n    transforms:\n      ...\n      - RecResizeImg:\n          # Modify image_shape to fit long text\n          image_shape: [3, 48, 320]\n      ...\n  loader:\n    # Eval batch_size for Single card\n    batch_size_per_card: 256\n    ...\n</code></pre> <p>Note that the configuration file for prediction/evaluation must be consistent with the training.</p>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#22-load-trained-model-and-continue-training","title":"2.2 Load Trained Model and Continue Training","text":"<p>If you expect to load trained model and continue the training again, you can specify the parameter <code>Global.checkpoints</code> as the model path to be loaded.</p> <p>For example:</p> <pre><code>python3 tools/train.py -c configs/rec/rec_icdar15_train.yml -o Global.checkpoints=./your/trained/model\n</code></pre> <p>Note: The priority of <code>Global.checkpoints</code> is higher than that of <code>Global.pretrained_model</code>, that is, when two parameters are specified at the same time, the model specified by <code>Global.checkpoints</code> will be loaded first. If the model path specified by <code>Global.checkpoints</code> is wrong, the one specified by <code>Global.pretrained_model</code> will be loaded.</p>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#23-training-with-new-backbone","title":"2.3 Training with New Backbone","text":"<p>The network part completes the construction of the network, and PaddleOCR divides the network into four parts, which are under ppocr/modeling. The data entering the network will pass through these four parts in sequence(transforms-&gt;backbones-&gt; necks-&gt;heads).</p> <pre><code>\u251c\u2500\u2500 architectures # Code for building network\n\u251c\u2500\u2500 transforms    # Image Transformation Module\n\u251c\u2500\u2500 backbones     # Feature extraction module\n\u251c\u2500\u2500 necks         # Feature enhancement module\n\u2514\u2500\u2500 heads         # Output module\n</code></pre> <p>If the Backbone to be replaced has a corresponding implementation in PaddleOCR, you can directly modify the parameters in the <code>Backbone</code> part of the configuration yml file.</p> <p>However, if you want to use a new Backbone, an example of replacing the backbones is as follows:</p> <ol> <li>Create a new file under the ppocr/modeling/backbones folder, such as my_backbone.py.</li> <li>Add code in the my_backbone.py file, the sample code is as follows:</li> </ol> <pre><code>import paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n\nclass MyBackbone(nn.Layer):\n    def __init__(self, *args, **kwargs):\n        super(MyBackbone, self).__init__()\n        # your init code\n        self.conv = nn.xxxx\n\n    def forward(self, inputs):\n        # your network forward\n        y = self.conv(inputs)\n        return y\n</code></pre> <ol> <li>Import the added module in the ppocr/modeling/backbones/_init_.py file.</li> </ol> <p>After adding the four-part modules of the network, you only need to configure them in the configuration file to use, such as:</p> <pre><code>  Backbone:\n    name: MyBackbone\n    args1: args1\n</code></pre> <p>NOTE: More details about replace Backbone and other module can be found in doc.</p>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#24-mixed-precision-training","title":"2.4. Mixed Precision Training","text":"<p>If you want to speed up your training further, you can use Auto Mixed Precision Training, taking a single machine and a single gpu as an example, the commands are as follows:</p> <pre><code>python3 tools/train.py -c configs/rec/rec_icdar15_train.yml \\\n     -o Global.pretrained_model=./pretrain_models/rec_mv3_none_bilstm_ctc_v2.0_train \\\n     Global.use_amp=True Global.scale_loss=1024.0 Global.use_dynamic_loss_scaling=True\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#25-distributed-training","title":"2.5. Distributed Training","text":"<p>During multi-machine multi-gpu training, use the <code>--ips</code> parameter to set the used machine IP address, and the <code>--gpus</code> parameter to set the used GPU ID:</p> <pre><code>python3 -m paddle.distributed.launch --ips=\"xx.xx.xx.xx,xx.xx.xx.xx\" --gpus '0,1,2,3' tools/train.py -c configs/rec/rec_icdar15_train.yml \\\n     -o Global.pretrained_model=./pretrain_models/rec_mv3_none_bilstm_ctc_v2.0_train\n</code></pre> <p>Note: 1. When using multi-machine and multi-gpu training, you need to replace the ips value in the above command with the address of your machine, and the machines need to be able to ping each other. 2. Training needs to be launched separately on multiple machines. The command to view the ip address of the machine is <code>ifconfig</code>.  3. For more details about the distributed training speedup ratio, please refer to Distributed Training Tutorial.</p>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#26-training-with-knowledge-distillation","title":"2.6. Training with Knowledge Distillation","text":"<p>Knowledge distillation is supported in PaddleOCR for text recognition training process. For more details, please refer to doc.</p>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#27-multi-language-model-training","title":"2.7. Multi-Language Model Training","text":"<p>Currently, the multi-language algorithms supported by PaddleOCR are:</p> Configuration file Algorithm name backbone trans seq pred language rec_chinese_cht_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc chinese traditional rec_en_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc English(Case sensitive) rec_french_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc French rec_ger_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc German rec_japan_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc Japanese rec_korean_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc Korean rec_latin_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc Latin rec_arabic_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc arabic rec_cyrillic_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc cyrillic rec_devanagari_lite_train.yml CRNN Mobilenet_v3 small 0.5 None BiLSTM ctc devanagari <p>For more supported languages, please refer to : Multi-language model</p> <p>If you want to finetune on the basis of the existing model effect, please refer to the following instructions to modify the configuration file:</p> <p>Take <code>rec_french_lite_train</code> as an example:</p> <pre><code>Global:\n  ...\n  # Add a custom dictionary, such as modify the dictionary, please point the path to the new dictionary\n  character_dict_path: ./ppocr/utils/dict/french_dict.txt\n  ...\n  # Whether to recognize spaces\n  use_space_char: True\n\n...\n\nTrain:\n  dataset:\n    # Type of dataset\uff0cwe support LMDBDataSet and SimpleDataSet\n    name: SimpleDataSet\n    # Path of dataset\n    data_dir: ./train_data/\n    # Path of train list\n    label_file_list: [\"./train_data/french_train.txt\"]\n    ...\n\nEval:\n  dataset:\n    # Type of dataset\uff0cwe support LMDBDataSet and SimpleDataSet\n    name: SimpleDataSet\n    # Path of dataset\n    data_dir: ./train_data\n    # Path of eval list\n    label_file_list: [\"./train_data/french_val.txt\"]\n    ...\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#28-training-on-other-platformwindowsmacoslinux-dcu","title":"2.8 Training on other platform(Windows/macOS/Linux DCU)","text":"<ul> <li> <p>Windows GPU/CPU The Windows platform is slightly different from the Linux platform: Windows platform only supports <code>single gpu</code> training and inference, specify GPU for training <code>set CUDA_VISIBLE_DEVICES=0</code> On the Windows platform, DataLoader only supports single-process mode, so you need to set <code>num_workers</code> to 0;</p> </li> <li> <p>macOS GPU mode is not supported, you need to set <code>use_gpu</code> to False in the configuration file, and the rest of the training evaluation prediction commands are exactly the same as Linux GPU.</p> </li> <li> <p>Linux DCU Running on a DCU device requires setting the environment variable <code>export HIP_VISIBLE_DEVICES=0,1,2,3</code>, and the rest of the training and evaluation prediction commands are exactly the same as the Linux GPU.</p> </li> </ul>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#29-fine-tuning","title":"2.9 Fine-tuning","text":"<p>In actual use, it is recommended to load the official pre-trained model and fine-tune it in your own data set. For the fine-tuning method of the recognition model, please refer to: Model Fine-tuning Tutorial.</p>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#3-evaluation-and-test","title":"3. Evaluation and Test","text":""},{"location":"en/version2.x/ppocr/model_train/recognition.html#31-evaluation","title":"3.1. Evaluation","text":"<p>The model parameters during training are saved in the <code>Global.save_model_dir</code> directory by default. When evaluating indicators, you need to set <code>Global.checkpoints</code> to point to the saved parameter file. The evaluation dataset can be set by modifying the <code>Eval.dataset.label_file_list</code> field in the <code>configs/rec/PP-OCRv3/en_PP-OCRv3_mobile_rec.yml</code> file.</p> <pre><code># GPU evaluation, Global.checkpoints is the weight to be tested\npython3 -m paddle.distributed.launch --gpus '0' tools/eval.py -c configs/rec/PP-OCRv3/en_PP-OCRv3_mobile_rec.yml -o Global.checkpoints={path/to/weights}/best_accuracy\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#32-test","title":"3.2 Test","text":"<p>Using the model trained by paddleocr, you can quickly get prediction through the following script.</p> <p>The default prediction picture is stored in <code>infer_img</code>, and the trained weight is specified via <code>-o Global.checkpoints</code>:</p> <p>According to the <code>save_model_dir</code> and <code>save_epoch_step</code> fields set in the configuration file, the following parameters will be saved:</p> <pre><code>output/rec/\n\u251c\u2500\u2500 best_accuracy.pdopt\n\u251c\u2500\u2500 best_accuracy.pdparams\n\u251c\u2500\u2500 best_accuracy.states\n\u251c\u2500\u2500 config.yml\n\u251c\u2500\u2500 iter_epoch_3.pdopt\n\u251c\u2500\u2500 iter_epoch_3.pdparams\n\u251c\u2500\u2500 iter_epoch_3.states\n\u251c\u2500\u2500 latest.pdopt\n\u251c\u2500\u2500 latest.pdparams\n\u251c\u2500\u2500 latest.states\n\u2514\u2500\u2500 train.log\n</code></pre> <p>Among them, best_accuracy.is the best model on the evaluation set; iter_epoch_x. is the model saved at intervals of <code>save_epoch_step</code>; latest.* is the model of the last epoch.</p> <pre><code># Predict English results\npython3 tools/infer_rec.py -c configs/rec/PP-OCRv3/en_PP-OCRv3_mobile_rec.yml -o Global.pretrained_model={path/to/weights}/best_accuracy  Global.infer_img=doc/imgs_words/en/word_1.png\n</code></pre> <p>Input image:</p> <p></p> <p>Get the prediction result of the input image:</p> <pre><code>infer_img: doc/imgs_words/en/word_1.png\n        result: ('joint', 0.9998967)\n</code></pre> <p>The configuration file used for prediction must be consistent with the training. For example, you completed the training of the Chinese model with <code>python3 tools/train.py -c configs/rec/ch_ppocr_v2.0/rec_chinese_lite_train_v2.0.yml</code>, you can use the following command to predict the Chinese model:</p> <pre><code># Predict Chinese results\npython3 tools/infer_rec.py -c configs/rec/ch_ppocr_v2.0/rec_chinese_lite_train_v2.0.yml -o Global.pretrained_model={path/to/weights}/best_accuracy Global.infer_img=doc/imgs_words/ch/word_1.jpg\n</code></pre> <p>Input image:</p> <p></p> <p>Get the prediction result of the input image:</p> <pre><code>infer_img: doc/imgs_words/ch/word_1.jpg\n        result: ('\u200b\u97e9\u56fd\u200b\u5c0f\u9986\u200b', 0.997218)\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#4-model-export-and-prediction","title":"4. Model Export and Prediction","text":"<p>Inference Model (saved using <code>paddle.jit.save</code>)</p> <p>The inference model is a \"frozen\" version of the model, where both the model structure and model parameters are saved in a file. It is typically used for prediction and deployment scenarios. In contrast, the checkpoint model only saves the model's parameters and is mostly used for training resumption, etc. Compared to the checkpoint model, the inference model also includes the model structure information, which makes it more efficient for deployment, inference acceleration, and flexible integration with systems.</p> <p>The process of converting a recognition model to an inference model is similar to the detection model conversion, as shown below:</p> <pre><code># Enable old IR mode\nexport FLAGS_enable_pir_api=0\n\n# -c Set the training algorithm yml configuration file\n# -o Set optional parameters\n# Global.pretrained_model parameter Set the training model address to be converted without adding the file suffix .pdmodel, .pdopt or .pdparams.\n# Global.save_inference_dir Set the address where the converted model will be saved.\n\npython3 tools/export_model.py -c configs/rec/PP-OCRv3/en_PP-OCRv3_mobile_rec.yml -o Global.pretrained_model=en_PP-OCRv3_rec_train/best_accuracy  Global.save_inference_dir=./inference/en_PP-OCRv3_mobile_rec/\n</code></pre> <p>If you have a model trained on your own dataset with a different dictionary file, please make sure that you modify the <code>character_dict_path</code> in the configuration file to your dictionary file path.</p> <p>After the conversion is successful, there are three files in the model save directory:</p> <pre><code>inference/en_PP-OCRv3_mobile_rec/\n    \u251c\u2500\u2500 inference.pdiparams         # The parameter file of recognition inference model\n    \u251c\u2500\u2500 inference.pdiparams.info    # The parameter information of recognition inference model, which can be ignored\n    \u2514\u2500\u2500 inference.pdmodel           # The program file of recognition model\n</code></pre> <p>Note: If you need to store the model in the new IR mode (i.e., <code>.json</code> format), use the following command to switch to the new IR mode:</p> <pre><code>export FLAGS_enable_pir_api=1\npython3 tools/export_model.py -c configs/rec/PP-OCRv3/en_PP-OCRv3_mobile_rec.yml -o Global.pretrained_model=./pretrain_models/en_PP-OCRv3_rec_train/best_accuracy Global.save_inference_dir=./inference/en_PP-OCRv3_mobile_rec/\n</code></pre> <p>Once successful, you will have two files in the directory:</p> <pre><code>inference/en_PP-OCRv3_mobile_rec/\n    \u251c\u2500\u2500 inference.pdiparams         # Model parameter file for the inference model\n    \u2514\u2500\u2500 inference.json              # Program file for the inference model\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#custom-model-inference","title":"Custom Model Inference","text":"<p>If you modified the text dictionary during training, you must specify the path to the custom dictionary when using the inference model for prediction. For more information about configuring and explaining inference hyperparameters, refer to the Inference Hyperparameters Explanation Tutorial.</p> <pre><code>python3 tools/infer/predict_rec.py --image_dir=\"./doc/imgs_words_en/word_336.png\" --rec_model_dir=\"./your_inference_model\" --rec_image_shape=\"3, 48, 320\" --rec_char_dict_path=\"your_text_dict_path\"\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/recognition.html#5-faq","title":"5. FAQ","text":"<p>Q1: Why is the prediction result inconsistent after converting a trained model to an inference model?</p> <p>A: This is a common issue. It typically arises due to differences in the preprocessing and postprocessing parameters used during training and inference. To troubleshoot, check whether the preprocessing, postprocessing, and prediction settings in the configuration file used for training match those used during inference.</p>"},{"location":"en/version2.x/ppocr/model_train/training.html","title":"Model Training","text":"<p>This article will introduce the basic concepts that is necessary for model training and tuning.</p> <p>At the same time, it will briefly introduce the structure of the training data and how to prepare the data to fine-tune model in vertical scenes.</p>"},{"location":"en/version2.x/ppocr/model_train/training.html#1-yml-configuration","title":"1. Yml Configuration","text":"<p>The PaddleOCR uses configuration files to control network training and evaluation parameters. In the configuration file, you can set the model, optimizer, loss function, and pre- and post-processing parameters of the model. PaddleOCR reads these parameters from the configuration file, and then builds a complete training process to train the model. Fine-tuning can also be completed by modifying the parameters in the configuration file, which is simple and convenient.</p> <p>For the complete configuration file description, please refer to Configuration File.</p>"},{"location":"en/version2.x/ppocr/model_train/training.html#2-basic-concepts","title":"2. Basic Concepts","text":"<p>During the model training process, some hyper-parameters can be manually specified to obtain the optimal result at the least cost. Different data volumes may require different hyper-parameters. When you want to fine-tune the model based on your own data, there are several parameter adjustment strategies for reference:</p>"},{"location":"en/version2.x/ppocr/model_train/training.html#21-learning-rate","title":"2.1 Learning Rate","text":"<p>The learning rate is one of the most important hyper-parameters for training neural networks. It represents the step length of the gradient moving towards the optimal solution of the loss function in each iteration. A variety of learning rate update strategies are provided by PaddleOCR, which can be specified in configuration files. For example:</p> <pre><code>Optimizer:\n  ...\n  lr:\n    name: Piecewise\n    decay_epochs : [700, 800]\n    values : [0.001, 0.0001]\n    warmup_epoch: 5\n</code></pre> <p><code>Piecewise</code> stands for piece-wise constant attenuation. Different learning rates are specified in different learning stages, and the learning rate stay the same in each stage.</p> <p><code>warmup_epoch</code> means that in the first 5 epochs, the learning rate will be increased gradually from 0 to base_lr. For all strategies, please refer to the code learning_rate.py.</p>"},{"location":"en/version2.x/ppocr/model_train/training.html#22-regularization","title":"2.2 Regularization","text":"<p>Regularization can effectively avoid algorithm over-fitting. PaddleOCR provides L1 and L2 regularization methods. L1 and L2 regularization are the most widely used regularization methods. L1 regularization adds a regularization term to the objective function to reduce the sum of absolute values of the parameters; while in L2 regularization, the purpose of adding a regularization term is to reduce the sum of squared parameters. The configuration method is as follows:</p> <pre><code>Optimizer:\n  ...\n  regularizer:\n    name: L2\n    factor: 2.0e-05\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/training.html#23-evaluation-indicators","title":"2.3 Evaluation Indicators","text":"<p>(1) Detection stage: First, evaluate according to the IOU of the detection frame and the labeled frame. If the IOU is greater than a certain threshold, it is judged that the detection is accurate. Here, the detection frame and the label frame are different from the general target detection frame, and they are represented by polygons. Detection accuracy: the percentage of the correct detection frame number in all detection frames is mainly used to judge the detection index. Detection recall rate: the percentage of correct detection frames in all marked frames, which is mainly an indicator of missed detection.</p> <p>(2) Recognition stage: Character recognition accuracy, that is, the ratio of correctly recognized text lines to the number of marked text lines. Only the entire line of text recognition pairs can be regarded as correct recognition.</p> <p>(3) End-to-end statistics: End-to-end recall rate: accurately detect and correctly identify the proportion of text lines in all labeled text lines; End-to-end accuracy rate: accurately detect and correctly identify the number of text lines in the detected text lines. The standard for accurate detection is that the IOU of the detection box and the labeled box is greater than a certain threshold, and the text in the correctly identified detection box is the same as the labeled text.</p>"},{"location":"en/version2.x/ppocr/model_train/training.html#3-data-and-vertical-scenes","title":"3. Data and Vertical Scenes","text":""},{"location":"en/version2.x/ppocr/model_train/training.html#31-training-data","title":"3.1 Training Data","text":"<p>The current open source models, data sets and magnitudes are as follows:</p> <ul> <li>Detection:</li> <li>English data set, ICDAR2015</li> <li> <p>Chinese data set, LSVT street view data set training data 3w pictures</p> </li> <li> <p>Identification:</p> </li> <li>English data set, MJSynth and SynthText synthetic data, the data volume is tens of millions.</li> <li>Chinese data set, LSVT street view data set crops the image according to the truth value, and performs position calibration, a total of 30w images. In addition, based on the LSVT corpus, 500w of synthesized data.</li> <li>Small language data set, using different corpora and fonts, respectively generated 100w synthetic data set, and using ICDAR-MLT as the verification set.</li> </ul> <p>Among them, the public data sets are all open source, users can search and download by themselves, or refer to Chinese data set, synthetic data is not open source, users can use open source synthesis tools to synthesize by themselves. Synthesis tools include text_renderer, SynthText, TextRecognitionDataGenerator etc.</p>"},{"location":"en/version2.x/ppocr/model_train/training.html#32-vertical-scene","title":"3.2 Vertical Scene","text":"<p>PaddleOCR mainly focuses on general OCR. If you have vertical requirements, you can use PaddleOCR + vertical data to train yourself; If there is a lack of labeled data, or if you do not want to invest in research and development costs, it is recommended to directly call the open API, which covers some of the more common vertical categories.</p>"},{"location":"en/version2.x/ppocr/model_train/training.html#33-build-your-own-dataset","title":"3.3 Build Your Own Dataset","text":"<p>There are several experiences for reference when constructing the data set:</p> <p>(1) The amount of data in the training set:</p> <p>a. The data required for detection is relatively small. For Fine-tune based on the PaddleOCR model, 500 sheets are generally required to achieve good results.</p> <p>b. Recognition is divided into English and Chinese. Generally, English scenarios require hundreds of thousands of data to achieve good results, while Chinese requires several million or more.</p> <p>(2) When the amount of training data is small, you can try the following three ways to get more data:</p> <p>a. Manually collect more training data, the most direct and effective way.</p> <p>b. Basic image processing or transformation based on PIL and opencv. For example, the three modules of ImageFont, Image, ImageDraw in PIL write text into the background, opencv's rotating affine transformation, Gaussian filtering and so on.</p> <p>c. Use data generation algorithms to synthesize data, such as algorithms such as pix2pix.</p>"},{"location":"en/version2.x/ppocr/model_train/training.html#4-faq","title":"4. FAQ","text":"<p>Q: How to choose a suitable network input shape when training CRNN recognition?</p> <pre><code>A: The general height is 32, the longest width is selected, there are two methods:\n\n(1) Calculate the aspect ratio distribution of training sample images. The selection of the maximum aspect ratio considers 80% of the training samples.\n\n(2) Count the number of texts in training samples. The selection of the longest number of characters considers the training sample that satisfies 80%. Then the aspect ratio of Chinese characters is approximately considered to be 1, and that of English is 3:1, and the longest width is estimated.\n</code></pre> <p>Q: During the recognition training, the accuracy of the training set has reached 90, but the accuracy of the verification set has been kept at 70, what should I do?</p> <pre><code>A: If the accuracy of the training set is 90 and the test set is more than 70, it should be over-fitting. There are two methods to try:\n\n(1) Add more augmentation methods or increase the [probability] of augmented prob (https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/ppocr/data/imaug/rec_img_aug.py#L341), The default is 0.4.\n\n(2) Increase the [l2 dcay value] of the system (https://github.com/PaddlePaddle/PaddleOCR/blob/a501603d54ff5513fc4fc760319472e59da25424/configs/rec/ch_ppocr_v1.1/rec_chinese_lite_train_v1.1.yml#L47)\n</code></pre> <p>Q: When the recognition model is trained, loss can drop normally, but acc is always 0</p> <pre><code>A: It is normal for the acc to be 0 at the beginning of the recognition model training, and the indicator will come up after a longer training period.\n</code></pre> <p>Click the following links for detailed training tutorial:</p> <ul> <li>text detection model training</li> <li>text recognition model training</li> <li>text direction classification model training</li> </ul>"},{"location":"en/version2.x/ppocr/model_train/tricks.html","title":"Tricks","text":"<p>Here we have sorted out some Chinese OCR training and prediction tricks, which are being updated continuously. You are welcome to contribute more OCR tricks!</p>"},{"location":"en/version2.x/ppocr/model_train/tricks.html#1-replace-backbone-network","title":"1. Replace Backbone Network","text":"<ul> <li>Problem Description</li> </ul> <p>At present, ResNet_vd series and MobileNetV3 series are the backbone networks used in PaddleOCR, whether replacing the other backbone networks will help to improve the accuracy? What should be paid attention to when replacing?</p> <ul> <li> <p>Tips</p> <ul> <li> <p>Whether text detection or text recognition, the choice of backbone network is a trade-off between prediction effect and prediction efficiency. Generally, a larger backbone network is selected, e.g. ResNet101_vd, then the performance of the detection or recognition is more accurate, but the time cost will increase accordingly. And a smaller backbone network is selected, e.g. MobileNetV3_small_x0_35, the prediction speed is faster, but the accuracy of detection or recognition will be reduced. Fortunately, the detection or recognition effect of different backbone networks is positively correlated with the performance of ImageNet 1000 classification task. PaddleClas have sorted out the 23 series of classification network structures, such as ResNet_vd\u3001Res2Net\u3001HRNet\u3001MobileNetV3\u3001GhostNet. It provides the top1 accuracy of classification, the time cost of GPU(V100 and T4) and CPU(SD 855), and the 117 pretrained models download addresses.</p> </li> <li> <p>Similar as the 4 stages of ResNet, the replacement of text detection backbone network is to determine those four stages to facilitate the integration of FPN like the object detection heads. In addition, for the text detection problem, the pre trained model in ImageNet1000 can accelerate the convergence and improve the accuracy.</p> </li> <li> <p>In order to replace the backbone network of text recognition, we need to pay attention to the descending position of network width and height stride. Since the ratio between width and height is large in chinese text recognition, the frequency of height decrease is less and the frequency of width decrease is more. You can refer the modifies of MobileNetV3 in PaddleOCR.</p> </li> </ul> </li> </ul>"},{"location":"en/version2.x/ppocr/model_train/tricks.html#2-long-chinese-text-recognition","title":"2. Long Chinese Text Recognition","text":"<ul> <li>Problem Description   The maximum resolution of Chinese recognition model during training is [3,32,320], if the text image to be recognized is too long, as shown in the figure below, how to adapt?</li> </ul> <ul> <li>Tips</li> </ul> <p>During the training, the training samples are not directly resized to [3,32,320]. At first, the height of samples are resized to 32 and keep the ratio between the width and the height. When the width is less than 320, the excess parts are padding 0. Besides, when the ratio between the width and the height of the samples is larger than 10, these samples will be ignored. When the prediction for one image, do as above, but do not limit the max ratio between the width and the height. When the prediction for an images batch, do as training, but the resized target width is the longest width of the images in the batch. Code as following\uff1a</p> <pre><code>def resize_norm_img(self, img, max_wh_ratio):\n    imgC, imgH, imgW = self.rec_image_shape\n    assert imgC == img.shape[2]\n    if self.character_type == \"ch\":\n        imgW = int((32 * max_wh_ratio))\n    h, w = img.shape[:2]\n    ratio = w / float(h)\n    if math.ceil(imgH * ratio) &gt; imgW:\n        resized_w = imgW\n    else:\n        resized_w = int(math.ceil(imgH * ratio))\n    resized_image = cv2.resize(img, (resized_w, imgH))\n    resized_image = resized_image.astype('float32')\n    resized_image = resized_image.transpose((2, 0, 1)) / 255\n    resized_image -= 0.5\n    resized_image /= 0.5\n    padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n    padding_im[:, :, 0:resized_w] = resized_image\n    return padding_im\n</code></pre>"},{"location":"en/version2.x/ppocr/model_train/tricks.html#3-space-recognition","title":"3. Space Recognition","text":"<ul> <li>Problem Description</li> </ul> <p>As shown in the figure below, for Chinese and English mixed scenes, in order to facilitate reading and using the recognition results, it is often necessary to recognize the spaces between words. How can this situation be adapted?</p> <p></p> <ul> <li>Tips</li> </ul> <p>There are two possible methods for space recognition. (1) Optimize the text detection. For splitting the text at the space in detection results, it needs to divide the text line with space into many segments when label the data for detection. (2) Optimize the text recognition. The space character is introduced into the recognition dictionary. Label the blank line in the training data for text recognition. In addition, we can also concat multiple word lines to synthesize the training data with spaces. PaddleOCR currently uses the second method.</p>"},{"location":"en/version2.x/ppstructure/models_list.html","title":"PP-Structure Model list","text":""},{"location":"en/version2.x/ppstructure/models_list.html#1-layout-analysis","title":"1. Layout Analysis","text":"model name description inference model size download dict path picodet_lcnet_x1_0_fgd_layout The layout analysis English model trained on the PubLayNet dataset based on PicoDet LCNet_x1_0 and FGD . the model can recognition 5 types of areas such as Text, Title, Table, Picture and List 9.7M inference model / trained model PubLayNet dict ppyolov2_r50vd_dcn_365e_publaynet The layout analysis English model trained on the PubLayNet dataset based on PP-YOLOv2 221.0M inference_moel / trained model same as above picodet_lcnet_x1_0_fgd_layout_cdla The layout analysis Chinese model trained on the CDLA dataset, the model can recognition 10 types of areas such as Table\u3001Figure\u3001Figure caption\u3001Table\u3001Table caption\u3001Header\u3001Footer\u3001Reference\u3001Equation 9.7M inference model / trained model CDLA dict picodet_lcnet_x1_0_fgd_layout_table The layout analysis model trained on the table dataset, the model can detect tables in Chinese and English documents 9.7M inference model / trained model Table dict ppyolov2_r50vd_dcn_365e_tableBank_word The layout analysis model trained on the TableBank Word dataset based on PP-YOLOv2, the model can detect  tables  in English documents 221.0M inference model same as above ppyolov2_r50vd_dcn_365e_tableBank_latex The layout analysis model trained on the TableBank Latex dataset based on PP-YOLOv2, the model can detect  tables  in English documents 221.0M inference model same as above"},{"location":"en/version2.x/ppstructure/models_list.html#2-ocr-and-table-recognition","title":"2. OCR and Table Recognition","text":""},{"location":"en/version2.x/ppstructure/models_list.html#21-ocr","title":"2.1 OCR","text":"model name description inference model size download en_ppocr_mobile_v2.0_table_det Text detection model of English table scenes trained on PubTabNet dataset 4.7M inference model / trained model en_ppocr_mobile_v2.0_table_rec Text recognition model of English table scenes trained on PubTabNet dataset 6.9M inference model / trained model <p>If you need to use other OCR models, you can download the model in PP-OCR model_list or use the model you trained yourself to configure to <code>det_model_dir</code>, <code>rec_model_dir</code> field.</p>"},{"location":"en/version2.x/ppstructure/models_list.html#22-table-recognition","title":"2.2 Table Recognition","text":"model description inference model size download en_ppocr_mobile_v2.0_table_structure English table recognition model trained on PubTabNet dataset based on TableRec-RARE 6.8M inference model / trained model en_ppstructure_mobile_v2.0_SLANet English table recognition model trained on PubTabNet dataset based on SLANet 9.2M inference model / trained model ch_ppstructure_mobile_v2.0_SLANet Chinese table recognition model based on SLANet 9.3M inference model / trained model"},{"location":"en/version2.x/ppstructure/models_list.html#3-kie","title":"3. KIE","text":"<p>On XFUND_zh dataset, Accuracy and time cost of different models on V100 GPU are as follows.</p> Model Backbone Task Config Hmean Time cost(ms) Download link VI-LayoutXLM VI-LayoutXLM-base SER ser_vi_layoutxlm_xfund_zh_udml.yml 93.19% 15.49 trained model LayoutXLM LayoutXLM-base SER ser_layoutxlm_xfund_zh.yml 90.38% 19.49 trained model LayoutLM LayoutLM-base SER ser_layoutlm_xfund_zh.yml 77.31% - trained model LayoutLMv2 LayoutLMv2-base SER ser_layoutlmv2_xfund_zh.yml 85.44% 31.46 trained model VI-LayoutXLM VI-LayoutXLM-base RE re_vi_layoutxlm_xfund_zh_udml.yml 83.92% 15.49 trained model LayoutXLM LayoutXLM-base RE re_layoutxlm_xfund_zh.yml 74.83% 19.49 trained model LayoutLMv2 LayoutLMv2-base RE re_layoutlmv2_xfund_zh.yml 67.77% 31.46 trained model <ul> <li>Note: The above time cost information just considers inference time without preprocess or postprocess, test environment: <code>V100 GPU + CUDA 10.2 + CUDNN 8.1.1 + TRT 7.2.3.4</code></li> </ul> <p>On wildreceipt dataset, the algorithm result is as follows:</p> Model Backbone Config Hmean Download link SDMGR VGG6 configs/kie/sdmgr/kie_unet_sdmgr.yml 86.70% trained model"},{"location":"en/version2.x/ppstructure/overview.html","title":"PP-Structure","text":""},{"location":"en/version2.x/ppstructure/overview.html#1-introduction","title":"1. Introduction","text":"<p>PP-Structure is an intelligent document analysis system developed by the PaddleOCR team, which aims to help developers better complete tasks related to document understanding such as layout analysis and table recognition.</p> <p>The pipeline of PP-StructureV2 system is shown below. The document image first passes through the image direction correction module to identify the direction of the entire image and complete the direction correction. Then, two tasks of layout information analysis and key information extraction can be completed.</p> <ul> <li>In the layout analysis task, the image first goes through the layout analysis model to divide the image into different areas such as text, table, and figure, and then analyze these areas separately. For example, the table area is sent to the form recognition module for structured recognition, and the text area is sent to the OCR engine for text recognition. Finally, the layout recovery module restores it to a word or pdf file with the same layout as the original image;</li> <li>In the key information extraction task, the OCR engine is first used to extract the text content, and then the SER(semantic entity recognition) module obtains the semantic entities in the image, and finally the RE(relationship extraction) module obtains the correspondence between the semantic entities, thereby extracting the required key information.</li> </ul> <p></p> <p>More technical details: \ud83d\udc49 PP-StructureV2 Technical Report</p> <p>PP-StructureV2 supports independent use or flexible collocation of each module. For example, you can use layout analysis alone or table recognition alone. Click the corresponding link below to get the tutorial for each independent module:</p> <ul> <li>Layout Analysis</li> <li>Table Recognition</li> <li>Key Information Extraction</li> <li>Layout Recovery</li> </ul>"},{"location":"en/version2.x/ppstructure/overview.html#2-features","title":"2. Features","text":"<p>The main features of PP-StructureV2 are as follows:</p> <ul> <li>Support layout analysis of documents in the form of images/pdfs, which can be divided into areas such as text, titles, tables, figures, formulas, etc.;</li> <li>Support common Chinese and English table detection tasks;</li> <li>Support structured table recognition, and output the final result to Excel file;</li> <li>Support multimodal-based Key Information Extraction (KIE) tasks - Semantic Entity Recognition (SER) and Relation Extraction (RE);</li> <li>Support layout recovery, that is, restore the document in word or pdf format with the same layout as the original image;</li> <li>Support customized training and multiple inference deployment methods such as python whl package quick start;</li> <li>Connect with the semi-automatic data labeling tool PPOCRLabel, which supports the labeling of layout analysis, table recognition, and SER.</li> </ul>"},{"location":"en/version2.x/ppstructure/overview.html#3-results","title":"3. Results","text":"<p>PP-StructureV2 supports the independent use or flexible collocation of each module. For example, layout analysis can be used alone, or table recognition can be used alone. Only the visualization effects of several representative usage methods are shown here.</p>"},{"location":"en/version2.x/ppstructure/overview.html#31-layout-analysis-and-table-recognition","title":"3.1 Layout analysis and table recognition","text":"<p>The figure shows the pipeline of layout analysis + table recognition. The image is first divided into four areas of image, text, title and table by layout analysis, and then OCR detection and recognition is performed on the three areas of image, text and title, and the table is performed table recognition, where the image will also be stored for use.</p> <p></p>"},{"location":"en/version2.x/ppstructure/overview.html#311-layout-recognition-returns-the-coordinates-of-a-single-word","title":"3.1.1 Layout recognition returns the coordinates of a single word","text":"<p>The following figure shows the result of layout analysis on single word\uff0cplease refer to the doc.</p> <p></p>"},{"location":"en/version2.x/ppstructure/overview.html#32-layout-recovery","title":"3.2 Layout recovery","text":"<p>The following figure shows the effect of layout recovery based on the results of layout analysis and table recognition in the previous section.</p> <p></p>"},{"location":"en/version2.x/ppstructure/overview.html#33-kie","title":"3.3 KIE","text":"<ul> <li>SER</li> </ul> <p>Different colored boxes in the figure represent different categories.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <ul> <li>RE</li> </ul> <p>In the figure, the red box represents <code>Question</code>, the blue box represents <code>Answer</code>, and <code>Question</code> and <code>Answer</code> are connected by green lines.</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"en/version2.x/ppstructure/overview.html#4-quick-start","title":"4. Quick start","text":"<p>Start from Quick Start.</p>"},{"location":"en/version2.x/ppstructure/overview.html#5-model-list","title":"5. Model List","text":"<p>Some tasks need to use both the structured analysis models and the OCR models. For example, the table recognition task needs to use the table recognition model for structured analysis, and the OCR model to recognize the text in the table. Please select the appropriate models according to your specific needs.</p> <p>For structural analysis related model downloads, please refer to:</p> <ul> <li>PP-Structure Model Zoo</li> </ul> <p>For OCR related model downloads, please refer to:</p> <ul> <li>PP-OCR Model Zoo</li> </ul>"},{"location":"en/version2.x/ppstructure/ppstructure_model.html","title":"PP-Structure \u200b\u7cfb\u5217\u200b\u6a21\u578b\u200b\u5217\u8868","text":""},{"location":"en/version2.x/ppstructure/ppstructure_model.html#1","title":"1. \u200b\u7248\u9762\u200b\u5206\u6790\u6a21\u578b","text":"\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b dict path picodet_lcnet_x1_0_fgd_layout \u200b\u57fa\u4e8e\u200bPicoDet LCNet_x1_0\u200b\u548c\u200bFGD\u200b\u84b8\u998f\u200b\u5728\u200bPubLayNet \u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u82f1\u6587\u7248\u200b\u9762\u200b\u5206\u6790\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5212\u5206\u200b\u6587\u5b57\u200b\u3001\u200b\u6807\u9898\u200b\u3001\u200b\u8868\u683c\u200b\u3001\u200b\u56fe\u7247\u200b\u4ee5\u53ca\u200b\u5217\u8868\u200b5\u200b\u7c7b\u200b\u533a\u57df\u200b 9.7M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PubLayNet dict ppyolov2_r50vd_dcn_365e_publaynet \u200b\u57fa\u4e8e\u200bPP-YOLOv2\u200b\u5728\u200bPubLayNet\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\u82f1\u6587\u7248\u200b\u9762\u200b\u5206\u6790\u6a21\u578b\u200b 221.0M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u540c\u200b\u4e0a\u200b picodet_lcnet_x1_0_fgd_layout_cdla CDLA\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u4e2d\u6587\u7248\u200b\u9762\u200b\u5206\u6790\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5212\u5206\u200b\u4e3a\u200b\u8868\u683c\u200b\u3001\u200b\u56fe\u7247\u200b\u3001\u200b\u56fe\u7247\u200b\u6807\u9898\u200b\u3001\u200b\u8868\u683c\u200b\u3001\u200b\u8868\u683c\u200b\u6807\u9898\u200b\u3001\u200b\u9875\u7709\u200b\u3001\u200b\u811a\u672c\u200b\u3001\u200b\u5f15\u7528\u200b\u3001\u200b\u516c\u5f0f\u200b10\u200b\u7c7b\u200b\u533a\u57df\u200b 9.7M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b CDLA dict picodet_lcnet_x1_0_fgd_layout_table \u200b\u8868\u683c\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u7248\u9762\u200b\u5206\u6790\u6a21\u578b\u200b\uff0c\u200b\u652f\u6301\u200b\u4e2d\u82f1\u6587\u200b\u6587\u6863\u200b\u8868\u683c\u200b\u533a\u57df\u200b\u7684\u200b\u68c0\u6d4b\u200b 9.7M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b Table dict ppyolov2_r50vd_dcn_365e_tableBank_word \u200b\u57fa\u4e8e\u200bPP-YOLOv2\u200b\u5728\u200bTableBank Word \u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u7248\u9762\u200b\u5206\u6790\u6a21\u578b\u200b\uff0c\u200b\u652f\u6301\u200b\u82f1\u6587\u200b\u6587\u6863\u200b\u8868\u683c\u200b\u533a\u57df\u200b\u7684\u200b\u68c0\u6d4b\u200b 221.0M \u200b\u63a8\u7406\u6a21\u578b\u200b \u200b\u540c\u200b\u4e0a\u200b ppyolov2_r50vd_dcn_365e_tableBank_latex \u200b\u57fa\u4e8e\u200bPP-YOLOv2\u200b\u5728\u200bTableBank Latex\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u7248\u9762\u200b\u5206\u6790\u6a21\u578b\u200b\uff0c\u200b\u652f\u6301\u200b\u82f1\u6587\u200b\u6587\u6863\u200b\u8868\u683c\u200b\u533a\u57df\u200b\u7684\u200b\u68c0\u6d4b\u200b 221.0M \u200b\u63a8\u7406\u6a21\u578b\u200b \u200b\u540c\u200b\u4e0a"},{"location":"en/version2.x/ppstructure/ppstructure_model.html#2-ocr","title":"2. OCR\u200b\u548c\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6a21\u578b","text":""},{"location":"en/version2.x/ppstructure/ppstructure_model.html#21-ocr","title":"2.1 OCR","text":"\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b en_ppocr_mobile_v2.0_table_det PubTabNet\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u82f1\u6587\u200b\u8868\u683c\u200b\u573a\u666f\u200b\u7684\u200b\u6587\u5b57\u200b\u68c0\u6d4b\u200b 4.7M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b en_ppocr_mobile_v2.0_table_rec PubTabNet\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u7684\u200b\u82f1\u6587\u200b\u8868\u683c\u200b\u573a\u666f\u200b\u7684\u200b\u6587\u5b57\u200b\u8bc6\u522b\u200b 6.9M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u5982\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b\u5176\u4ed6\u200bOCR\u200b\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5728\u200b PP-OCR model_list \u200b\u4e0b\u8f7d\u200b\u6a21\u578b\u200b\u6216\u8005\u200b\u4f7f\u7528\u200b\u81ea\u5df1\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u914d\u7f6e\u200b\u5230\u200b <code>det_model_dir</code>, <code>rec_model_dir</code>\u200b\u4e24\u4e2a\u200b\u5b57\u200b\u6bb5\u200b\u5373\u53ef\u200b\u3002</p>"},{"location":"en/version2.x/ppstructure/ppstructure_model.html#22","title":"2.2 \u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6a21\u578b","text":"\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b en_ppocr_mobile_v2.0_table_structure \u200b\u57fa\u4e8e\u200bTableRec-RARE\u200b\u5728\u200bPubTabNet\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\u82f1\u6587\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b 6.8M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b en_ppstructure_mobile_v2.0_SLANet \u200b\u57fa\u4e8e\u200bSLANet\u200b\u5728\u200bPubTabNet\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\u82f1\u6587\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b 9.2M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b ch_ppstructure_mobile_v2.0_SLANet \u200b\u57fa\u4e8e\u200bSLANet\u200b\u7684\u200b\u4e2d\u6587\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b 9.3M \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b"},{"location":"en/version2.x/ppstructure/ppstructure_model.html#3-kie","title":"3. KIE\u200b\u6a21\u578b","text":"<p>\u200b\u5728\u200bXFUND_zh\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\uff0c\u200b\u4e0d\u540c\u200b\u6a21\u578b\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u4e0e\u200bV100 GPU\u200b\u4e0a\u200b\u901f\u5ea6\u200b\u4fe1\u606f\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\u3002</p> \u200b\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u63a8\u7406\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u7cbe\u5ea6\u200b(hmean) \u200b\u9884\u6d4b\u200b\u8017\u65f6\u200b(ms) \u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b ser_VI-LayoutXLM_xfund_zh \u200b\u57fa\u4e8e\u200bVI-LayoutXLM\u200b\u5728\u200bxfund\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200bSER\u200b\u6a21\u578b\u200b 1.1G 93.19% 15.49 \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b re_VI-LayoutXLM_xfund_zh \u200b\u57fa\u4e8e\u200bVI-LayoutXLM\u200b\u5728\u200bxfund\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200bRE\u200b\u6a21\u578b\u200b 1.1G 83.92% 15.49 \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b ser_LayoutXLM_xfund_zh \u200b\u57fa\u4e8e\u200bLayoutXLM\u200b\u5728\u200bxfund\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200bSER\u200b\u6a21\u578b\u200b 1.4G 90.38% 19.49 \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b re_LayoutXLM_xfund_zh \u200b\u57fa\u4e8e\u200bLayoutXLM\u200b\u5728\u200bxfund\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200bRE\u200b\u6a21\u578b\u200b 1.4G 74.83% 19.49 \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b ser_LayoutLMv2_xfund_zh \u200b\u57fa\u4e8e\u200bLayoutLMv2\u200b\u5728\u200bxfund\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200bSER\u200b\u6a21\u578b\u200b 778.0M 85.44% 31.46 \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b re_LayoutLMv2_xfund_zh \u200b\u57fa\u4e8e\u200bLayoutLMv2\u200b\u5728\u200bxfun\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200bRE\u200b\u6a21\u578b\u200b 765.0M 67.77% 31.46 \u200b\u63a8\u7406\u6a21\u578b\u200b coming soon / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b ser_LayoutLM_xfund_zh \u200b\u57fa\u4e8e\u200bLayoutLM\u200b\u5728\u200bxfund\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200bSER\u200b\u6a21\u578b\u200b 430.0M 77.31% - \u200b\u63a8\u7406\u6a21\u578b\u200b / \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <ul> <li>\u200b\u6ce8\u200b\uff1a\u200b\u4e0a\u8ff0\u200b\u9884\u6d4b\u200b\u8017\u65f6\u200b\u4fe1\u606f\u200b\u4ec5\u200b\u5305\u542b\u200b\u4e86\u200binference\u200b\u6a21\u578b\u200b\u7684\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff0c\u200b\u6ca1\u6709\u200b\u7edf\u8ba1\u200b\u9884\u5904\u7406\u200b\u4e0e\u200b\u540e\u5904\u7406\u200b\u8017\u65f6\u200b\uff0c\u200b\u6d4b\u8bd5\u73af\u5883\u200b\u4e3a\u200b<code>V100 GPU + CUDA 10.2 + CUDNN 8.1.1 + TRT 7.2.3.4</code>\u3002</li> </ul> <p>\u200b\u5728\u200bwildreceipt\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\uff0cSDMGR\u200b\u6a21\u578b\u200b\u7cbe\u5ea6\u200b\u4e0e\u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\u3002</p> \u200b\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u6a21\u578b\u200b\u7b80\u4ecb\u200b \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b \u200b\u7cbe\u5ea6\u200b \u200b\u4e0b\u8f7d\u200b\u5730\u5740\u200b SDMGR \u200b\u5173\u952e\u200b\u4fe1\u606f\u63d0\u53d6\u200b\u6a21\u578b\u200b 78.0M 86.70% \u200b\u63a8\u7406\u6a21\u578b\u200b coming soon / \u200b\u8bad\u7ec3\u200b\u6a21\u578b"},{"location":"en/version2.x/ppstructure/quick_start.html","title":"PP-Structure Quick Start","text":""},{"location":"en/version2.x/ppstructure/quick_start.html#1-environment-preparation","title":"1. Environment Preparation","text":""},{"location":"en/version2.x/ppstructure/quick_start.html#11-install-paddlepaddle","title":"1.1 Install PaddlePaddle","text":"<p>If you do not have a Python environment, please refer to Environment Preparation.</p> <ul> <li>PaddlePaddle with CUDA 11.8</li> </ul> <pre><code>python3 -m pip install \"paddlepaddle-gpu&lt;=2.6\" -i https://www.paddlepaddle.org.cn/packages/stable/cu118/\n</code></pre> <ul> <li>PaddlePaddle with CUDA 12.3</li> </ul> <pre><code>python3 -m pip install \"paddlepaddle-gpu&lt;=2.6\" -i https://www.paddlepaddle.org.cn/packages/stable/cu123/\n</code></pre> <ul> <li>If your machine does not have an available GPU, please run the following command to install the CPU version</li> </ul> <pre><code>python3 -m pip install \"paddlepaddle&lt;=2.6\" -i https://www.paddlepaddle.org.cn/packages/stable/cpu/\n</code></pre> <p>For more software version requirements, please refer to the instructions in the Installation Document.</p>"},{"location":"en/version2.x/ppstructure/quick_start.html#12-install-paddleocr-whl-package","title":"1.2 Install PaddleOCR Whl Package","text":"<pre><code>python3 -m pip install \"paddleocr&lt;3.0\"\n\n# Install the image direction classification dependency package paddleclas (if you do not use the image direction classification, you can skip it)\npython3 -m pip install paddleclas\n</code></pre>"},{"location":"en/version2.x/ppstructure/quick_start.html#2-quick-use","title":"2. Quick Use","text":""},{"location":"en/version2.x/ppstructure/quick_start.html#21-use-by-command-line","title":"2.1 Use by command line","text":""},{"location":"en/version2.x/ppstructure/quick_start.html#211-image-orientation-layout-analysis-table-recognition","title":"2.1.1 image orientation + layout analysis + table recognition","text":"<pre><code># Temporarily disable the new IR feature\nexport FLAGS_enable_pir_api=0\npaddleocr --image_dir=ppstructure/docs/table/1.png --type=structure --image_orientation=true\n</code></pre>"},{"location":"en/version2.x/ppstructure/quick_start.html#212-layout-analysis-table-recognition","title":"2.1.2 layout analysis + table recognition","text":"<pre><code>paddleocr --image_dir=ppstructure/docs/table/1.png --type=structure\n</code></pre>"},{"location":"en/version2.x/ppstructure/quick_start.html#213-layout-analysis","title":"2.1.3 layout analysis","text":"<pre><code>paddleocr --image_dir=ppstructure/docs/table/1.png --type=structure --table=false --ocr=false\n</code></pre>"},{"location":"en/version2.x/ppstructure/quick_start.html#214-table-recognition","title":"2.1.4 table recognition","text":"<pre><code>paddleocr --image_dir=ppstructure/docs/table/table.jpg --type=structure --layout=false\n</code></pre>"},{"location":"en/version2.x/ppstructure/quick_start.html#215-key-information-extraction","title":"2.1.5 Key Information Extraction","text":"<p>Key information extraction does not currently support use by the whl package. For detailed usage tutorials, please refer to: Key Information Extraction.</p>"},{"location":"en/version2.x/ppstructure/quick_start.html#216-layout-recovery","title":"2.1.6 layout recovery","text":"<p>Two layout recovery methods are provided, For detailed usage tutorials, please refer to: Layout Recovery.</p> <ul> <li>PDF parse</li> <li>OCR</li> </ul> <p>Recovery by using PDF parse (only support pdf as input):</p> <pre><code>paddleocr --image_dir=ppstructure/docs/recovery/UnrealText.pdf --type=structure --recovery=true --use_pdf2docx_api=true\n</code></pre> <p>Recovery by using OCR\uff1a</p> <pre><code>paddleocr --image_dir=ppstructure/docs/table/1.png --type=structure --recovery=true --lang='en'\n</code></pre>"},{"location":"en/version2.x/ppstructure/quick_start.html#217-layout-recoverypdf-to-markdown","title":"2.1.7 layout recovery(PDF to Markdown)","text":"<p>Do not use LaTeXCOR model for formula recognition\uff1a</p> <pre><code>paddleocr --image_dir=ppstructure/docs/recovery/UnrealText.pdf --type=structure --recovery=true --recovery_to_markdown=true --lang='en'\n</code></pre> <p>Use LaTeXCOR model for formula recognition, where Chinese layout model must be used\uff1a</p> <pre><code>paddleocr --image_dir=ppstructure/docs/recovery/UnrealText.pdf --type=structure --recovery=true --formula=true --recovery_to_markdown=true --lang='ch'\n</code></pre>"},{"location":"en/version2.x/ppstructure/quick_start.html#22-use-by-python-script","title":"2.2 Use by python script","text":""},{"location":"en/version2.x/ppstructure/quick_start.html#221-image-orientation-layout-analysis-table-recognition","title":"2.2.1 image orientation + layout analysis + table recognition","text":"<pre><code>import os\nimport cv2\nfrom paddleocr import PPStructure,draw_structure_result,save_structure_res\n\ntable_engine = PPStructure(show_log=True, image_orientation=True)\n\nsave_folder = './output'\nimg_path = 'ppstructure/docs/table/1.png'\nimg = cv2.imread(img_path)\nresult = table_engine(img)\nsave_structure_res(result, save_folder,os.path.basename(img_path).split('.')[0])\n\nfor line in result:\n    line.pop('img')\n    print(line)\n\nfrom PIL import Image\n\nfont_path = 'doc/fonts/simfang.ttf' # PaddleOCR\u200b\u4e0b\u200b\u63d0\u4f9b\u200b\u5b57\u4f53\u200b\u5305\u200b\nimage = Image.open(img_path).convert('RGB')\nim_show = draw_structure_result(image, result,font_path=font_path)\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre>"},{"location":"en/version2.x/ppstructure/quick_start.html#222-layout-analysis-table-recognition","title":"2.2.2 layout analysis + table recognition","text":"<pre><code>import os\nimport cv2\nfrom paddleocr import PPStructure,draw_structure_result,save_structure_res\n\ntable_engine = PPStructure(show_log=True)\n\nsave_folder = './output'\nimg_path = 'ppstructure/docs/table/1.png'\nimg = cv2.imread(img_path)\nresult = table_engine(img)\nsave_structure_res(result, save_folder,os.path.basename(img_path).split('.')[0])\n\nfor line in result:\n    line.pop('img')\n    print(line)\n\nfrom PIL import Image\n\nfont_path = 'doc/fonts/simfang.ttf' # font provided in PaddleOCR\nimage = Image.open(img_path).convert('RGB')\nim_show = draw_structure_result(image, result,font_path=font_path)\nim_show = Image.fromarray(im_show)\nim_show.save('result.jpg')\n</code></pre>"},{"location":"en/version2.x/ppstructure/quick_start.html#223-layout-analysis","title":"2.2.3 layout analysis","text":"<pre><code>import os\nimport cv2\nfrom paddleocr import PPStructure,save_structure_res\n\ntable_engine = PPStructure(table=False, ocr=False, show_log=True)\n\nsave_folder = './output'\nimg_path = 'ppstructure/docs/table/1.png'\nimg = cv2.imread(img_path)\nresult = table_engine(img)\nsave_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0])\n\nfor line in result:\n    line.pop('img')\n    print(line)\n</code></pre> <pre><code>import os\nimport cv2\nfrom paddleocr import PPStructure,save_structure_res\n\nocr_engine = PPStructure(table=False, ocr=True, show_log=True)\n\nsave_folder = './output'\nimg_path = 'ppstructure/docs/recovery/UnrealText.pdf'\nresult = ocr_engine(img_path)\nfor index, res in enumerate(result):\n    save_structure_res(res, save_folder, os.path.basename(img_path).split('.')[0], index)\n\nfor res in result:\n    for line in res:\n        line.pop('img')\n        print(line)\n</code></pre> <pre><code>import os\nimport cv2\nimport numpy as np\nfrom paddleocr import PPStructure,save_structure_res\nfrom paddle.utils import try_import\nfrom PIL import Image\n\nocr_engine = PPStructure(table=False, ocr=True, show_log=True)\n\nsave_folder = './output'\nimg_path = 'ppstructure/docs/recovery/UnrealText.pdf'\n\nfitz = try_import(\"fitz\")\nimgs = []\nwith fitz.open(img_path) as pdf:\n    for pg in range(0, pdf.page_count):\n        page = pdf[pg]\n        mat = fitz.Matrix(2, 2)\n        pm = page.get_pixmap(matrix=mat, alpha=False)\n\n        # if width or height &gt; 2000 pixels, don't enlarge the image\n        if pm.width &gt; 2000 or pm.height &gt; 2000:\n            pm = page.get_pixmap(matrix=fitz.Matrix(1, 1), alpha=False)\n\n        img = Image.frombytes(\"RGB\", [pm.width, pm.height], pm.samples)\n        img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n        imgs.append(img)\n\nfor index, img in enumerate(imgs):\n    result = ocr_engine(img)\n    save_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0], index)\n    for line in result:\n        line.pop('img')\n        print(line)\n</code></pre>"},{"location":"en/version2.x/ppstructure/quick_start.html#224-table-recognition","title":"2.2.4 table recognition","text":"<pre><code>import os\nimport cv2\nfrom paddleocr import PPStructure,save_structure_res\n\ntable_engine = PPStructure(layout=False, show_log=True)\n\nsave_folder = './output'\nimg_path = 'ppstructure/docs/table/table.jpg'\nimg = cv2.imread(img_path)\nresult = table_engine(img)\nsave_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0])\n\nfor line in result:\n    line.pop('img')\n    print(line)\n</code></pre>"},{"location":"en/version2.x/ppstructure/quick_start.html#225-key-information-extraction","title":"2.2.5 Key Information Extraction","text":"<p>Key information extraction does not currently support use by the whl package. For detailed usage tutorials, please refer to: Inference.</p>"},{"location":"en/version2.x/ppstructure/quick_start.html#226-layout-recovery","title":"2.2.6 layout recovery","text":"<pre><code>import os\nimport cv2\nfrom paddleocr import PPStructure,save_structure_res\nfrom paddleocr.ppstructure.recovery.recovery_to_doc import sorted_layout_boxes, convert_info_docx\n\n# Chinese image\ntable_engine = PPStructure(recovery=True)\n# English image\n# table_engine = PPStructure(recovery=True, lang='en')\n\nsave_folder = './output'\nimg_path = 'ppstructure/docs/table/1.png'\nimg = cv2.imread(img_path)\nresult = table_engine(img)\nsave_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0])\n\nfor line in result:\n    line.pop('img')\n    print(line)\n\nh, w, _ = img.shape\nres = sorted_layout_boxes(result, w)\nconvert_info_docx(img, res, save_folder, os.path.basename(img_path).split('.')[0])\n</code></pre>"},{"location":"en/version2.x/ppstructure/quick_start.html#227-layout-recoverypdf-to-markdown","title":"2.2.7 layout recovery(PDF to Markdown)","text":"<pre><code>import os\nimport cv2\nfrom paddleocr import PPStructure,save_structure_res\nfrom paddleocr.ppstructure.recovery.recovery_to_doc import sorted_layout_boxes\nfrom paddleocr.ppstructure.recovery.recovery_to_markdown import convert_info_markdown\n\n# Chinese image\ntable_engine = PPStructure(recovery=True)\n# English image\n# table_engine = PPStructure(recovery=True, lang='en')\n\nsave_folder = './output'\nimg_path = 'ppstructure/docs/table/1.png'\nimg = cv2.imread(img_path)\nresult = table_engine(img)\nsave_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0])\n\nfor line in result:\n    line.pop('img')\n    print(line)\n\nh, w, _ = img.shape\nres = sorted_layout_boxes(result, w)\nconvert_info_markdown(res, save_folder, os.path.basename(img_path).split('.')[0])\n</code></pre>"},{"location":"en/version2.x/ppstructure/quick_start.html#23-result-description","title":"2.3 Result description","text":"<p>The return of PP-Structure is a list of dicts, the example is as follows:</p>"},{"location":"en/version2.x/ppstructure/quick_start.html#231-layout-analysis-table-recognition","title":"2.3.1 layout analysis + table recognition","text":"<pre><code>[\n  {   'type': 'Text',\n      'bbox': [34, 432, 345, 462],\n      'res': ([[36.0, 437.0, 341.0, 437.0, 341.0, 446.0, 36.0, 447.0], [41.0, 454.0, 125.0, 453.0, 125.0, 459.0, 41.0, 460.0]],\n                [('Tigure-6. The performance of CNN and IPT models using difforen', 0.90060663), ('Tent  ', 0.465441)])\n  }\n]\n</code></pre> <p>Each field in dict is described as follows:</p> field description type Type of image area. bbox The coordinates of the image area in the original image, respectively [upper left corner x, upper left corner y, lower right corner x, lower right corner y]. res OCR or table recognition result of the image area.  table: a dict with field descriptions as follows:  <code>html</code>: html str of table.\u2003\u2003\u2003\u2003\u2003\u2003\u2003 In the code usage mode, set return_ocr_result_in_table=True whrn call can get the detection and recognition results of each text in the table area, corresponding to the following fields:  <code>boxes</code>: text detection boxes. <code>rec_res</code>: text recognition results. OCR: A tuple containing the detection boxes and recognition results of each single text. <p>After the recognition is completed, each image will have a directory with the same name under the directory specified by the <code>output</code> field. Each table in the image will be stored as an excel, and the picture area will be cropped and saved. The filename of  excel and picture is their coordinates in the image.</p> <pre><code>/output/table/1/\n  \u2514\u2500 res.txt\n  \u2514\u2500 [454, 360, 824, 658].xlsx        table recognition result\n  \u2514\u2500 [16, 2, 828, 305].jpg            picture in Image\n  \u2514\u2500 [17, 361, 404, 711].xlsx        table recognition result\n</code></pre>"},{"location":"en/version2.x/ppstructure/quick_start.html#232-key-information-extraction","title":"2.3.2 Key Information Extraction","text":"<p>Please refer to: Key Information Extraction .</p>"},{"location":"en/version2.x/ppstructure/quick_start.html#24-parameter-description","title":"2.4 Parameter Description","text":"field description default output result save path ./output/table table_max_len long side of the image resize in table structure model 488 table_model_dir Table structure model inference model path None table_char_dict_path The dictionary path of table structure model ../ppocr/utils/dict/table_structure_dict.txt merge_no_span_structure In the table recognition model, whether to merge '\\' and '\\' False formula_model_dir Formula recognition model inference model path None formula_char_dict_path The dictionary path of formula recognition model ../ppocr/utils/dict/latex_ocr_tokenizer.json layout_model_dir Layout analysis model inference model path None layout_dict_path The dictionary path of layout analysis model ../ppocr/utils/dict/layout_publaynet_dict.txt layout_score_threshold The box threshold path of layout analysis model 0.5 layout_nms_threshold The nms threshold path of layout analysis model 0.5 kie_algorithm kie model algorithm LayoutXLM ser_model_dir Ser model inference model path None ser_dict_path The dictionary path of Ser model ../train_data/XFUND/class_list_xfun.txt mode structure or kie structure image_orientation Whether to perform image orientation classification in forward False layout Whether to perform layout analysis in forward True table Whether to perform table recognition in forward True formula Whether to perform formula recognition in forward False ocr Whether to perform ocr for non-table areas in layout analysis. When layout is False, it will be automatically set to False True recovery Whether to perform layout recovery in forward False recovery_to_markdown Whether to convert the layout recovery results into a markdown file False save_pdf Whether to convert docx to pdf when recovery False structure_version Structure version, optional PP-structure and PP-structurev2 PP-structure <p>Most of the parameters are consistent with the PaddleOCR whl package, see whl package documentation</p>"},{"location":"en/version2.x/ppstructure/quick_start.html#3-summary","title":"3. Summary","text":"<p>Through the content in this section, you can master the use of PP-Structure related functions through PaddleOCR whl package. Please refer to documentation tutorial for more detailed usage tutorials including model training, inference and deployment, etc.</p>"},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html","title":"Key Information Extraction Pipeline","text":""},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html#1-introduction","title":"1. Introduction","text":""},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html#11-background","title":"1.1 Background","text":"<p>Key information extraction (KIE) refers to extracting key information from text or images. As the downstream task of OCR, KIE of document image has many practical application scenarios, such as form recognition, ticket information extraction, ID card information extraction, etc. However, it is time-consuming and laborious to extract  key information from these document images by manpower. It's challengable but also valuable to combine multi-modal features (visual, layout, text, etc) together and complete KIE tasks.</p> <p>For the document images in a specific scene, the position and layout of the key information are relatively fixed. Therefore, in the early stage of the research, there are many methods based on template matching to extract the key information. This method is still widely used in many simple scenarios at present. However, it takes long time to adjust the template for different scenarios.</p> <p>The KIE in the document image generally contains 2 subtasks, which is as shown follows.</p> <ul> <li> <p>(1) SER: semantic entity recognition, which classifies each detected textline, such as dividing it into name and ID No. As shown in the red boxes in the following figure.</p> </li> <li> <p>(2) RE: relationship extraction, which matches the question and answer based on SER results. As shown in the figure below, the yellow arrows match the question and answer.</p> </li> </ul> <p></p>"},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html#12-mainstream-deep-learning-solutions","title":"1.2 Mainstream Deep-learning Solutions","text":"<p>General KIE methods are based on Named Entity Recognition (NER), but such methods only use text information and ignore location and visual feature information, which leads to limited accuracy. In recent years, most scholars have started to combine mutil-modal features to improve the accuracy of KIE model. The main methods are as follows:</p> <ul> <li> <p>(1) Grid based methods. These methods mainly focus on the fusion of multi-modal information at the image level. Most texts are of character granularity. The text and structure information embedding method is simple, such as the algorithm of chargrid [1].</p> </li> <li> <p>(2) Token based methods. These methods refer to the NLP methods such as Bert, which encode the position, vision and other feature information into the multi-modal model, and conduct pre-training on large-scale datasets, so that in downstream tasks, only a small amount of annotation data is required to obtain excellent results. The representative algorithms are layoutlm [2], layoutlmv2 [3], layoutxlm [4], structext [5], etc.</p> </li> <li> <p>(3) GCN based methods. These methods try to learn the structural information between images and characters, so as to solve the problem of extracting open set information (templates not seen in the training set), such as GCN [6], SDMGR [7] and other algorithms.</p> </li> <li> <p>(4) End to end based methods: these methods put the existing OCR character recognition and KIE information extraction tasks into a unified network for common learning, and strengthen each other in the learning process. Such as TRIE [8].</p> </li> </ul> <p>For more detailed introduction of the algorithms, please refer to Chapter 6 of Diving into OCR.</p>"},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html#2-kie-pipeline","title":"2. KIE Pipeline","text":"<p>Token based methods such as LayoutXLM are implemented in PaddleOCR. What's more, in PP-StructureV2, we simplify the LayoutXLM model and proposed VI-LayoutXLM, in which the visual feature extraction module is removed for speed-up. The textline sorting strategy conforming to the human reading order and UDML knowledge distillation strategy are utilized for higher model accuracy.</p> <p>In the non end-to-end KIE method, KIE needs at least 2 steps. Firstly, the OCR model is used to extract the text and its position. Secondly, the KIE model is used to extract the key information according to the image, text position and text content.</p>"},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html#21-train-ocr-models","title":"2.1 Train OCR Models","text":""},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html#211-text-detection","title":"2.1.1 Text Detection","text":""},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html#1-data","title":"(1) Data","text":"<p>Most of the models provided in PaddleOCR are general models. In the process of text detection, the detection of adjacent text lines is generally based on the distance of the position. As shown in the figure above, when using PP-OCRv3 general English detection model for text detection, it is easy to detect the two fields representing different properties as one. Therefore, it is suggested to finetune a detection model according to your scenario firstly during the KIE task.</p> <p>During data annotation, the different key information needs to be separated. Otherwise, it will increase the difficulty of subsequent KIE tasks.</p> <p>For downstream tasks, generally speaking, <code>200~300</code> training images can guarantee the basic training effect. If there is not too much prior knowledge, <code>200~300</code> images can be labeled firstly for subsequent text detection model training.</p>"},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html#2-model","title":"(2) Model","text":"<p>In terms of model selection, PP-OCRv3 detection model is recommended. For more information about the training methods of the detection model, please refer to: Text detection tutorial and PP-OCRv3 detection model tutorial.</p>"},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html#212-text-recognition","title":"2.1.2 Text recognition","text":"<p>Compared with the natural scene, the text recognition in the document image is generally relatively easier (the background is not too complex), so it is suggested to try the PP-OCRv3 general text recognition model provided in PaddleOCR (PP-OCRv3 model list)</p>"},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html#1-data_1","title":"(1) Data","text":"<p>However, there are also some challenges in some document scenarios, such as rare words in ID card scenarios and special fonts in invoice and other scenarios. These problems will increase the difficulty of text recognition. At this time, if you want to ensure or further improve the model accuracy, it is recommended to load PP-OCRv3 model based on the text recognition dataset of specific document scenarios for finetuning.</p> <p>In the process of model finetuning, it is recommended to prepare at least <code>5000</code> vertical scene text recognition images to ensure the basic model fine-tuning effect. If you want to improve the accuracy and generalization ability of the model, you can synthesize more text recognition images similar to the scene, collect general real text recognition data from the public data set, and add them to the text recognition training process. In the training process, it is suggested that the ratio of real data, synthetic data and general data of each epoch should be around <code>1:1:1</code>, which can be controlled by setting the sampling ratio of different data sources. If there are 3 training text files, including 10k, 20k and 50k pieces of data respectively, the data can be set in the configuration file as follows:</p> <pre><code>Train:\n  dataset:\n    name: SimpleDataSet\n    data_dir: ./train_data/\n    label_file_list:\n    - ./train_data/train_list_10k.txt\n    - ./train_data/train_list_10k.txt\n    - ./train_data/train_list_50k.txt\n    ratio_list: [1.0, 0.5, 0.2]\n    ...\n</code></pre>"},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html#2-model_1","title":"(2) Model","text":"<p>In terms of model selection, PP-OCRv3 recognition model is recommended. For more information about the training methods of the recognition model, please refer to: Text recognition tutorial and PP-OCRv3 model list.</p>"},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html#22-train-kie-models","title":"2.2 Train KIE Models","text":"<p>There are two main methods to extract the key information from the recognized texts.</p> <p>(1) Directly use SER model to obtain the key information category. For example, in the ID card scenario, we mark \"name\" and \"Geoff Sample\" as \"name_key\" and \"name_value\", respectively. The text field corresponding to the category \"name_value\" finally identified is the key information we need.</p> <p>(2) Joint use SER and RE models. For this case, we firstly use SER model to obtain all questions (keys) and questions (values) for the image text, and then use RE model to match all keys and values to find the relationship, so as to complete the extraction of key information.</p>"},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html#221-ser","title":"2.2.1 SER","text":"<p>Take the ID card scenario as an example. The key information generally includes <code>name</code>, <code>DOB</code>, etc. We can directly mark the corresponding fields as specific categories, as shown in the following figure.</p> <p>Note:</p> <ul> <li>In the labeling process, text content without key information about KIE shall be labeled as<code>other</code>, which is equivalent to background information. For example, in the ID card scenario, if we do not pay attention to <code>DOB</code> information, we can mark the categories of <code>DOB</code> and <code>Area manager</code> as <code>other</code>.</li> <li>In the annotation process of, it is required to annotate the textline position rather than the character.</li> </ul> <p>In terms of data, generally speaking, for relatively fixed scenes, 50 training images can achieve acceptable effects. You can refer to PPOCRLabel for finish the labeling process.</p> <p>In terms of model, it is recommended to use the VI-layoutXLM model proposed in PP-StructureV2. It is improved based on the LayoutXLM model, removing the visual feature extraction module, and further improving the model inference speed without the significant reduction on model accuracy. For more tutorials, please refer to VI-LayoutXLM introduction and KIE tutorial.</p>"},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html#222-ser-re","title":"2.2.2 SER + RE","text":"<p>The SER model is mainly used to identify all keys and values in the document image, and the RE model is mainly used to match all keys and values.</p> <p>Taking the ID card scenario as an example, the key information generally includes key information such as <code>name</code>, <code>DOB</code>, etc. in the SER stage, we need to identify all questions (keys) and answers (values). The demo annotation is as follows. All keys can be annotated as <code>question</code>, and all values can be annotated as <code>answer</code>.</p> <p></p> <p>In the RE stage, the ID and connection information of each field need to be marked, as shown in the following figure.</p> <p></p> <p>For each textline, you need to add 'ID' and 'linking' field information. The 'ID' records the unique identifier of the textline. Different text contents in the same images cannot be repeated. The 'linking' is a list that records the connection information between different texts. If the ID of the field \"name\" is 0 and the ID of the field \"Geoff Sample\" is 1, then they all have [[0, 1]] 'linking' marks, indicating that the fields with <code>id=0</code> and <code>id=1</code> form a key value relationship (the fields such as DOB and Expires are similar, and will not be repeated here).</p> <p>Note:</p> <p>During annotation, if value is multiple text lines, a key-value pair can be added in linking, such as <code>[[0, 1], [0, 2]]</code>.</p> <p>In terms of data, generally speaking, for relatively fixed scenes, about 50 training images can achieve acceptable effects.</p> <p>In terms of model, it is recommended to use the VI-layoutXLM model proposed in PP-StructureV2. It is improved based on the LayoutXLM model, removing the visual feature extraction module, and further improving the model inference speed without the significant reduction on model accuracy. For more tutorials, please refer to VI-LayoutXLM introduction and KIE tutorial.</p>"},{"location":"en/version2.x/ppstructure/blog/how_to_do_kie.html#3-reference","title":"3. Reference","text":"<p>[1] Katti A R, Reisswig C, Guder C, et al. Chargrid: Towards understanding 2d documents[J]. arXiv preprint arXiv:1809.08799, 2018.</p> <p>[2] Xu Y, Li M, Cui L, et al. Layoutlm: Pre-training of text and layout for document image understanding[C]//Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2020: 1192-1200.</p> <p>[3] Xu Y, Xu Y, Lv T, et al. LayoutLMv2: Multi-modal pre-training for visually-rich document understanding[J]. arXiv preprint arXiv:2012.14740, 2020.</p> <p>[4]: Xu Y, Lv T, Cui L, et al. Layoutxlm: Multimodal pre-training for multilingual visually-rich document understanding[J]. arXiv preprint arXiv:2104.08836, 2021.</p> <p>[5] Li Y, Qian Y, Yu Y, et al. StrucTexT: Structured Text Understanding with Multi-Modal Transformers[C]//Proceedings of the 29th ACM International Conference on Multimedia. 2021: 1912-1920.</p> <p>[6] Liu X, Gao F, Zhang Q, et al. Graph convolution for multimodal information extraction from visually rich documents[J]. arXiv preprint arXiv:1903.11279, 2019.</p> <p>[7] Sun H, Kuang Z, Yue X, et al. Spatial Dual-Modality Graph Reasoning for Key Information Extraction[J]. arXiv preprint arXiv:2103.14470, 2021.</p> <p>[8] Zhang P, Xu Y, Cheng Z, et al. Trie: End-to-end text reading and information extraction for document understanding[C]//Proceedings of the 28th ACM International Conference on Multimedia. 2020: 1413-1422.</p>"},{"location":"en/version2.x/ppstructure/blog/return_word_pos.html","title":"Return recognition position","text":"<p>According to the horizontal document, the recognition model not only returns the recognized content, but also the position of each word.</p>"},{"location":"en/version2.x/ppstructure/blog/return_word_pos.html#english-document-recovery","title":"English document recovery","text":""},{"location":"en/version2.x/ppstructure/blog/return_word_pos.html#download-the-inference-model-first","title":"Download the inference model first","text":"<pre><code>cd PaddleOCR/ppstructure\n\n## download model\nmkdir inference &amp;&amp; cd inference\n## Download the detection model of the ultra-lightweight English PP-OCRv3 model and unzip it\nhttps://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar &amp;&amp; tar xf en_PP-OCRv3_det_infer.tar\n## Download the recognition model of the ultra-lightweight English PP-OCRv3 model and unzip it\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0.0/en_PP-OCRv3_mobile_rec_infer.tar &amp;&amp; tar xf en_PP-OCRv3_mobile_rec_infer.tar\n## Download the ultra-lightweight English table inch model and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/paddle3.0b2/en_ppstructure_mobile_v2.0_SLANet_infer.tar\ntar xf en_ppstructure_mobile_v2.0_SLANet_infer.tar\n## Download the layout model of publaynet dataset and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_fgd_layout_infer.tar\ntar xf picodet_lcnet_x1_0_fgd_layout_infer.tar\ncd ..\n</code></pre>"},{"location":"en/version2.x/ppstructure/blog/return_word_pos.html#then-use-the-following-command-inference-in-the-ppstructure-directory","title":"Then use the following command inference in the /ppstructure/ directory","text":"<pre><code>python predict_system.py \\\n--image_dir=../docs/ppstructure/images/table_1.png \\\n--det_model_dir=inference/en_PP-OCRv3_det_infer \\\n--rec_model_dir=inference/en_PP-OCRv3_mobile_rec_infer \\\n--rec_char_dict_path=../ppocr/utils/en_dict.txt \\\n--table_model_dir=inference/en_ppstructure_mobile_v2.0_SLANet_infer \\\n--table_char_dict_path=../ppocr/utils/dict/table_structure_dict.txt \\\n--layout_model_dir=inference/picodet_lcnet_x1_0_fgd_layout_infer \\\n--layout_dict_path=../ppocr/utils/dict/layout_dict/layout_publaynet_dict.txt \\\n--vis_font_path=../doc/fonts/simfang.ttf \\\n--recovery=True \\\n--output=../output/ \\\n--return_word_box=True\n</code></pre>"},{"location":"en/version2.x/ppstructure/blog/return_word_pos.html#view-the-visualization-of-the-inference-results-under-outputstructuretable_1show_0jpg-as-shown-below","title":"View the visualization of the inference results under <code>../output/structure/table_1/show_0.jpg</code>, as shown below","text":""},{"location":"en/version2.x/ppstructure/blog/return_word_pos.html#recover-chinese-documents","title":"Recover Chinese documents","text":""},{"location":"en/version2.x/ppstructure/blog/return_word_pos.html#download-the-inference-model-first_1","title":"Download the inference model first","text":"<pre><code>cd PaddleOCR/ppstructure\n\n## download model\ncd inference\n## Download the detection model of the ultra-lightweight Chinese PP-OCRv3 model and unzip it\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0.0/PP-OCRv3_mobile_det_infer.tar &amp;&amp; tar xf PP-OCRv3_mobile_det_infer.tar\n## Download the recognition model of the ultra-lightweight Chinese PP-OCRv3 model and unzip it\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0.0/PP-OCRv3_mobile_rec_infer.tar &amp;&amp; tar xf PP-OCRv3_mobile_rec_infer.tar\n## Download the ultra-lightweight Chinese table inch model and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/paddle3.0b2/ch_ppstructure_mobile_v2.0_SLANet_infer.tar\ntar xf ch_ppstructure_mobile_v2.0_SLANet_infer.tar\n## Download the layout model of CDLA dataset and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_fgd_layout_cdla_infer.tar\ntar xf picodet_lcnet_x1_0_fgd_layout_cdla_infer.tar\ncd ..\n</code></pre>"},{"location":"en/version2.x/ppstructure/blog/return_word_pos.html#upload-the-following-test-image-2png-to-the-directory-docstable","title":"Upload the following test image \"2.png\" to the directory ./docs/table/","text":""},{"location":"en/version2.x/ppstructure/blog/return_word_pos.html#then-use-the-following-command-inference-in-the-ppstructure-directory_1","title":"Then use the following command inference in the /ppstructure/ directory","text":"<pre><code>python predict_system.py \\\n--image_dir=./docs/table/2.png \\\n--det_model_dir=inference/PP-OCRv3_mobile_det_infer \\\n--rec_model_dir=inference/PP-OCRv3_mobile_rec_infer \\\n--rec_char_dict_path=../ppocr/utils/ppocr_keys_v1.txt \\\n--table_model_dir=inference/ch_ppstructure_mobile_v2.0_SLANet_infer \\\n--table_char_dict_path=../ppocr/utils/dict/table_structure_dict_ch.txt \\\n--layout_model_dir=inference/picodet_lcnet_x1_0_fgd_layout_cdla_infer \\\n--layout_dict_path=../ppocr/utils/dict/layout_dict/layout_cdla_dict.txt \\\n--vis_font_path=../doc/fonts/chinese_cht.ttf \\\n--recovery=True \\\n--output=../output/ \\\n--return_word_box=True\n</code></pre>"},{"location":"en/version2.x/ppstructure/blog/return_word_pos.html#view-the-visualization-of-the-inference-results-under-outputstructure2show_0jpg-as-shown-below","title":"View the visualization of the inference results under <code>../output/structure/2/show_0.jpg</code>, as shown below","text":""},{"location":"en/version2.x/ppstructure/infer_deploy/index.html","title":"PP-OCR Deployment","text":""},{"location":"en/version2.x/ppstructure/infer_deploy/index.html#paddle-deployment-introduction","title":"Paddle Deployment Introduction","text":"<p>Paddle provides a variety of deployment schemes to meet the deployment requirements of different scenarios. Please choose according to the actual situation:</p> <p></p> <p>PP-OCR has supported multi deployment schemes. Click the link to get the specific tutorial.</p> <ul> <li>Python Inference</li> <li>C++ Inference</li> <li>Serving (Python/C++)</li> <li>Paddle-Lite (ARM CPU/OpenCL ARM GPU)</li> <li>Paddle2ONNX</li> </ul> <p>If you need the deployment tutorial of academic algorithm models other than PP-OCR, please directly enter the main page of corresponding algorithms, entrance\u3002</p>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html","title":"Server-side C++ Inference","text":"<p>This chapter introduces the C++ deployment steps of the PaddleOCR model. C++ is better than Python in terms of performance. Therefore, in CPU and GPU deployment scenarios, C++ deployment is mostly used. This section will introduce how to configure the C++ environment and deploy PaddleOCR in Linux (CPU\\GPU) environment. For Windows deployment please refer to Windows compilation guidelines.</p>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#1-prepare-the-environment","title":"1. Prepare the Environment","text":""},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#11-environment","title":"1.1 Environment","text":"<ul> <li>Linux, docker is recommended.</li> <li>Windows.</li> </ul>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#12-compile-opencv","title":"1.2 Compile OpenCV","text":"<ul> <li>First of all, you need to download the source code compiled package in the Linux environment from the OpenCV official website. Taking OpenCV 3.4.7 as an example, the download command is as follows.</li> </ul> <pre><code>cd deploy/cpp_infer\nwget https://paddleocr.bj.bcebos.com/libs/opencv/opencv-3.4.7.tar.gz\ntar -xf opencv-3.4.7.tar.gz\n</code></pre> <p>Finally, you will see the folder of <code>opencv-3.4.7/</code> in the current directory.</p> <ul> <li>Compile OpenCV, the OpenCV source path (<code>root_path</code>) and installation path (<code>install_path</code>) should be set by yourself. Enter the OpenCV source code path and compile it in the following way.</li> </ul> <pre><code>root_path=your_opencv_root_path\ninstall_path=${root_path}/opencv3\n\nrm -rf build\nmkdir build\ncd build\n\ncmake .. \\\n    -DCMAKE_INSTALL_PREFIX=${install_path} \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DBUILD_SHARED_LIBS=OFF \\\n    -DWITH_IPP=OFF \\\n    -DBUILD_IPP_IW=OFF \\\n    -DWITH_LAPACK=OFF \\\n    -DWITH_EIGEN=OFF \\\n    -DCMAKE_INSTALL_LIBDIR=lib64 \\\n    -DWITH_ZLIB=ON \\\n    -DBUILD_ZLIB=ON \\\n    -DWITH_JPEG=ON \\\n    -DBUILD_JPEG=ON \\\n    -DWITH_PNG=ON \\\n    -DBUILD_PNG=ON \\\n    -DWITH_TIFF=ON \\\n    -DBUILD_TIFF=ON\n\nmake -j\nmake install\n</code></pre> <p>In the above commands, <code>root_path</code> is the downloaded OpenCV source code path, and <code>install_path</code> is the installation path of OpenCV. After <code>make install</code> is completed, the OpenCV header file and library file will be generated in this folder for later OCR source code compilation.</p> <p>The final file structure under the OpenCV installation path is as follows.</p> <pre><code>opencv3/\n|-- bin\n|-- include\n|-- lib\n|-- lib64\n|-- share\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#13-compile-or-download-or-the-paddle-inference-library","title":"1.3 Compile or Download or the Paddle Inference Library","text":"<ul> <li>There are 2 ways to obtain the Paddle inference library, described in detail below.</li> </ul>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#131-direct-download-and-installation","title":"1.3.1 Direct download and installation","text":"<p>Paddle inference library official website. You can review and select the appropriate version of the inference library on the official website.</p> <ul> <li>After downloading, use the following command to extract files.</li> </ul> <pre><code>tar -xf paddle_inference.tgz\n</code></pre> <p>Finally you will see the folder of <code>paddle_inference/</code> in the current path.</p>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#132-compile-the-inference-source-code","title":"1.3.2 Compile the inference source code","text":"<ul> <li> <p>If you want to get the latest Paddle inference library features, you can download the latest code from Paddle GitHub repository and compile the inference library from the source code. It is recommended to download the inference library with paddle version greater than or equal to 2.0.1.</p> </li> <li> <p>You can refer to Paddle inference library to get the Paddle source code from GitHub, and then compile To generate the latest inference library. The method of using git to access the code is as follows.</p> </li> </ul> <pre><code>git clone https://github.com/PaddlePaddle/Paddle.git\ngit checkout develop\n</code></pre> <ul> <li>Enter the Paddle directory and run the following commands to compile the paddle inference library.</li> </ul> <pre><code>rm -rf build\nmkdir build\ncd build\n\ncmake  .. \\\n    -DWITH_CONTRIB=OFF \\\n    -DWITH_MKL=ON \\\n    -DWITH_MKLDNN=ON  \\\n    -DWITH_TESTING=OFF \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DWITH_INFERENCE_API_TEST=OFF \\\n    -DON_INFER=ON \\\n    -DWITH_PYTHON=ON\nmake -j\nmake inference_lib_dist\n</code></pre> <p>For more compilation parameter options, please refer to the document.</p> <ul> <li>After the compilation process, you can see the following files in the folder of <code>build/paddle_inference_install_dir/</code>.</li> </ul> <pre><code>build/paddle_inference_install_dir/\n|-- CMakeCache.txt\n|-- paddle\n|-- third_party\n|-- version.txt\n</code></pre> <p><code>paddle</code> is the Paddle library required for C++ prediction later, and <code>version.txt</code> contains the version information of the current inference library.</p>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#2-compile-and-run-the-demo","title":"2. Compile and Run the Demo","text":""},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#21-export-the-inference-model","title":"2.1 Export the inference model","text":"<ul> <li>You can refer to Model inference and export the inference model. After the model is exported, assuming it is placed in the <code>inference</code> directory, the directory structure is as follows.</li> </ul> <pre><code>inference/\n|-- det_db\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- rec_rcnn\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- cls\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- table\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n|-- layout\n|   |--inference.pdiparams\n|   |--inference.pdmodel\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#22-compile-paddleocr-c-inference-demo","title":"2.2 Compile PaddleOCR C++ inference demo","text":"<ul> <li>The compilation commands are as follows. The addresses of Paddle C++ inference library, opencv and other Dependencies need to be replaced with the actual addresses on your own machines.</li> </ul> <pre><code>sh tools/build.sh\n</code></pre> <p>Specifically, you should modify the paths in <code>tools/build.sh</code>. The related content is as follows.</p> <pre><code>OPENCV_DIR=your_opencv_dir\nLIB_DIR=your_paddle_inference_dir\nCUDA_LIB_DIR=your_cuda_lib_dir\nCUDNN_LIB_DIR=your_cudnn_lib_dir\n</code></pre> <p><code>OPENCV_DIR</code> is the OpenCV installation path; <code>LIB_DIR</code> is the download (<code>paddle_inference</code> folder) or the generated Paddle inference library path (<code>build/paddle_inference_install_dir</code> folder); <code>CUDA_LIB_DIR</code> is the CUDA library file path, in docker; it is <code>/usr/local/cuda/lib64</code>; <code>CUDNN_LIB_DIR</code> is the cuDNN library file path, in docker it is <code>/usr/lib/x86_64-linux-gnu/</code>.</p> <ul> <li>After the compilation is completed, an executable file named <code>ppocr</code> will be generated in the <code>build</code> folder.</li> </ul>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#23-run-the-demo","title":"2.3 Run the demo","text":"<p>Execute the built executable file:</p> <pre><code>./build/ppocr [--param1] [--param2] [...]\n</code></pre> <p>Note:ppocr uses the <code>PP-OCRv3</code> model by default, and the input shape used by the recognition model is <code>3, 48, 320</code>, if you want to use the old version model, you should add the parameter <code>--rec_img_h=32</code>.</p> <p>Specifically,</p>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#1-detclsrec","title":"1. det+cls+rec","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --cls_model_dir=inference/cls \\\n    --image_dir=../../doc/imgs/12.jpg \\\n    --use_angle_cls=true \\\n    --det=true \\\n    --rec=true \\\n    --cls=true \\\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#2-detrec","title":"2. det+rec","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --image_dir=../../doc/imgs/12.jpg \\\n    --use_angle_cls=false \\\n    --det=true \\\n    --rec=true \\\n    --cls=false \\\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#3-det","title":"3. det","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --image_dir=../../doc/imgs/12.jpg \\\n    --det=true \\\n    --rec=false\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#4-clsrec","title":"4. cls+rec","text":"<pre><code>./build/ppocr --rec_model_dir=inference/rec_rcnn \\\n    --cls_model_dir=inference/cls \\\n    --image_dir=../../doc/imgs_words/ch/word_1.jpg \\\n    --use_angle_cls=true \\\n    --det=false \\\n    --rec=true \\\n    --cls=true \\\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#5-rec","title":"5. rec","text":"<pre><code>./build/ppocr --rec_model_dir=inference/rec_rcnn \\\n    --image_dir=../../doc/imgs_words/ch/word_1.jpg \\\n    --use_angle_cls=false \\\n    --det=false \\\n    --rec=true \\\n    --cls=false \\\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#6-cls","title":"6. cls","text":"<pre><code>./build/ppocr --cls_model_dir=inference/cls \\\n    --cls_model_dir=inference/cls \\\n    --image_dir=../../doc/imgs_words/ch/word_1.jpg \\\n    --use_angle_cls=true \\\n    --det=false \\\n    --rec=false \\\n    --cls=true \\\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#7-layouttable","title":"7. layout+table","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --table_model_dir=inference/table \\\n    --image_dir=../../ppstructure/docs/table/table.jpg \\\n    --layout_model_dir=inference/layout \\\n    --type=structure \\\n    --table=true \\\n    --layout=true\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#8-layout","title":"8. layout","text":"<pre><code>./build/ppocr --layout_model_dir=inference/layout \\\n    --image_dir=../../ppstructure/docs/table/1.png \\\n    --type=structure \\\n    --table=false \\\n    --layout=true \\\n    --det=false \\\n    --rec=false\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#9-table","title":"9. table","text":"<pre><code>./build/ppocr --det_model_dir=inference/det_db \\\n    --rec_model_dir=inference/rec_rcnn \\\n    --table_model_dir=inference/table \\\n    --image_dir=../../ppstructure/docs/table/table.jpg \\\n    --type=structure \\\n    --table=true\n</code></pre> <p>More parameters are as follows,</p> <ul> <li>Common parameters</li> </ul> parameter data type default meaning use_gpu bool false Whether to use GPU gpu_id int 0 GPU id when use_gpu is true gpu_mem int 4000 GPU memory requested cpu_math_library_num_threads int 10 Number of threads when using CPU inference. When machine cores is enough, the large the value, the faster the inference speed enable_mkldnn bool true Whether to use mkdlnn library output str ./output Path where visualization results are saved <ul> <li>forward</li> </ul> parameter data type default meaning det bool true Whether to perform text detection in the forward direction rec bool true Whether to perform text recognition in the forward direction cls bool false Whether to perform text direction classification in the forward direction <ul> <li>Detection related parameters</li> </ul> parameter data type default meaning det_model_dir string - Address of detection inference model max_side_len int 960 Limit the maximum image height and width to 960 det_db_thresh float 0.3 Used to filter the binarized image of DB prediction, setting 0.-0.3 has no obvious effect on the result det_db_box_thresh float 0.5 DB post-processing filter box threshold, if there is a missing box detected, it can be reduced as appropriate det_db_unclip_ratio float 1.6 Indicates the compactness of the text box, the smaller the value, the closer the text box to the text det_db_score_mode string slow slow: use polygon box to calculate bbox score, fast: use rectangle box to calculate. Use rectangular box to calculate faster, and polygonal box more accurate for curved text area. visualize bool true Whether to visualize the results\uff0cwhen it is set as true, the prediction results will be saved in the folder specified by the <code>output</code> field on an image with the same name as the input image. <ul> <li>Classifier related parameters</li> </ul> parameter data type default meaning use_angle_cls bool false Whether to use the direction classifier cls_model_dir string - Address of direction classifier inference model cls_thresh float 0.9 Score threshold of the  direction classifier cls_batch_num int 1 batch size of classifier <ul> <li>Recognition related parameters</li> </ul> parameter data type default meaning rec_model_dir string - Address of recognition inference model rec_char_dict_path string ../../ppocr/utils/ppocr_keys_v1.txt dictionary file rec_batch_num int 6 batch size of recognition rec_img_h int 48 image height of recognition rec_img_w int 320 image width of recognition <ul> <li>Layout related parameters</li> </ul> parameter data type default meaning layout_model_dir string - Address of layout inference model layout_dict_path string ../../ppocr/utils/dict/layout_dict/layout_publaynet_dict.txt dictionary file layout_score_threshold float 0.5 Threshold of score. layout_nms_threshold float 0.5 Threshold of nms. <ul> <li>Table recognition related parameters</li> </ul> parameter data type default meaning table_model_dir string - Address of table recognition inference model table_char_dict_path string ../../ppocr/utils/dict/table_structure_dict.txt dictionary file table_max_len int 488 The size of the long side of the input image of the table recognition model, the final input image size of the network is\uff08table_max_len\uff0ctable_max_len\uff09 merge_no_span_structure bool true Whether to merge  and  to &lt;/td <ul> <li>Multi-language inference is also supported in PaddleOCR, you can refer to recognition tutorial for more supported languages and models in PaddleOCR. Specifically, if you want to infer using multi-language models, you just need to modify values of <code>rec_char_dict_path</code> and <code>rec_model_dir</code>.</li> </ul> <p>The detection results will be shown on the screen, which is as follows.</p> <pre><code>predict img: ../../doc/imgs/12.jpg\n../../doc/imgs/12.jpg\n0       det boxes: [[74,553],[427,542],[428,571],[75,582]] rec text: \u200b\u6253\u200b\u6d66\u8def\u200b252935\u200b\u53f7\u200b rec score: 0.947724\n1       det boxes: [[23,507],[513,488],[515,529],[24,548]] rec text: \u200b\u7eff\u6d32\u200b\u4ed5\u683c\u7ef4\u200b\u82b1\u56ed\u200b\u516c\u5bd3\u200b rec score: 0.993728\n2       det boxes: [[187,456],[399,448],[400,480],[188,488]] rec text: \u200b\u6253\u200b\u6d66\u8def\u200b15\u200b\u53f7\u200b rec score: 0.964994\n3       det boxes: [[42,413],[483,391],[484,428],[43,450]] rec text: \u200b\u4e0a\u6d77\u200b\u65af\u683c\u5a01\u200b\u94c2\u200b\u5c14\u200b\u5927\u9152\u5e97\u200b rec score: 0.980086\nThe detection visualized image saved in ./output//12.jpg\n</code></pre> <ul> <li>layout+table</li> </ul> <pre><code>predict img: ../../ppstructure/docs/table/1.png\n0       type: text, region: [12,729,410,848], score: 0.781044, res: count of ocr result is : 7\n********** print ocr result **********\n0       det boxes: [[4,1],[79,1],[79,12],[4,12]] rec text: CTW1500. rec score: 0.769472\n...\n6       det boxes: [[4,99],[391,99],[391,112],[4,112]] rec text: sate-of-the-artmethods[12.34.36l.ourapproachachieves rec score: 0.90414\n********** end print ocr result **********\n1       type: text, region: [69,342,342,359], score: 0.703666, res: count of ocr result is : 1\n********** print ocr result **********\n0       det boxes: [[8,2],[269,2],[269,13],[8,13]] rec text: Table6.Experimentalresults on CTW-1500 rec score: 0.890454\n********** end print ocr result **********\n2       type: text, region: [70,316,706,332], score: 0.659738, res: count of ocr result is : 2\n********** print ocr result **********\n0       det boxes: [[373,2],[630,2],[630,11],[373,11]] rec text: oroposals.andthegreencontoursarefinal rec score: 0.919729\n1       det boxes: [[8,3],[357,3],[357,11],[8,11]] rec text: Visualexperimentalresultshebluecontoursareboundar rec score: 0.915963\n********** end print ocr result **********\n3       type: text, region: [489,342,789,359], score: 0.630538, res: count of ocr result is : 1\n********** print ocr result **********\n0       det boxes: [[8,2],[294,2],[294,14],[8,14]] rec text: Table7.Experimentalresults onMSRA-TD500 rec score: 0.942251\n********** end print ocr result **********\n4       type: text, region: [444,751,841,848], score: 0.607345, res: count of ocr result is : 5\n********** print ocr result **********\n0       det boxes: [[19,3],[389,3],[389,17],[19,17]] rec text: Inthispaper,weproposeanovel adaptivebound rec score: 0.941031\n1       det boxes: [[4,22],[390,22],[390,36],[4,36]] rec text: aryproposalnetworkforarbitraryshapetextdetection rec score: 0.960172\n2       det boxes: [[4,42],[392,42],[392,56],[4,56]] rec text: whichadoptanboundaryproposalmodeltogeneratecoarse rec score: 0.934647\n3       det boxes: [[4,61],[389,61],[389,75],[4,75]] rec text: ooundaryproposals,andthenadoptanadaptiveboundary rec score: 0.946296\n4       det boxes: [[5,80],[387,80],[387,93],[5,93]] rec text: leformationmodelcombinedwithGCNandRNNtoper rec score: 0.952401\n********** end print ocr result **********\n5       type: title, region: [444,705,564,724], score: 0.785429, res: count of ocr result is : 1\n********** print ocr result **********\n0       det boxes: [[6,2],[113,2],[113,14],[6,14]] rec text: 5.Conclusion rec score: 0.856903\n********** end print ocr result **********\n6       type: table, region: [14,360,402,711], score: 0.963643, res: &lt;html&gt;&lt;body&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;td&gt;Methods&lt;/td&gt;&lt;td&gt;Ext&lt;/td&gt;&lt;td&gt;R&lt;/td&gt;&lt;td&gt;P&lt;/td&gt;&lt;td&gt;F&lt;/td&gt;&lt;td&gt;FPS&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;TextSnake [18]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;85.3&lt;/td&gt;&lt;td&gt;67.9&lt;/td&gt;&lt;td&gt;75.6&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CSE [17]&lt;/td&gt;&lt;td&gt;MiLT&lt;/td&gt;&lt;td&gt;76.1&lt;/td&gt;&lt;td&gt;78.7&lt;/td&gt;&lt;td&gt;77.4&lt;/td&gt;&lt;td&gt;0.38&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;LOMO[40]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;76.5&lt;/td&gt;&lt;td&gt;85.7&lt;/td&gt;&lt;td&gt;80.8&lt;/td&gt;&lt;td&gt;4.4&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ATRR[35]&lt;/td&gt;&lt;td&gt;Sy-&lt;/td&gt;&lt;td&gt;80.2&lt;/td&gt;&lt;td&gt;80.1&lt;/td&gt;&lt;td&gt;80.1&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SegLink++ [28]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;79.8&lt;/td&gt;&lt;td&gt;82.8&lt;/td&gt;&lt;td&gt;81.3&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextField [37]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;79.8&lt;/td&gt;&lt;td&gt;83.0&lt;/td&gt;&lt;td&gt;81.4&lt;/td&gt;&lt;td&gt;6.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MSR[38]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;79.0&lt;/td&gt;&lt;td&gt;84.1&lt;/td&gt;&lt;td&gt;81.5&lt;/td&gt;&lt;td&gt;4.3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PSENet-1s [33]&lt;/td&gt;&lt;td&gt;MLT&lt;/td&gt;&lt;td&gt;79.7&lt;/td&gt;&lt;td&gt;84.8&lt;/td&gt;&lt;td&gt;82.2&lt;/td&gt;&lt;td&gt;3.9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DB [12]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;80.2&lt;/td&gt;&lt;td&gt;86.9&lt;/td&gt;&lt;td&gt;83.4&lt;/td&gt;&lt;td&gt;22.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CRAFT [2]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;81.1&lt;/td&gt;&lt;td&gt;86.0&lt;/td&gt;&lt;td&gt;83.5&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextDragon [5]&lt;/td&gt;&lt;td&gt;MLT+&lt;/td&gt;&lt;td&gt;82.8&lt;/td&gt;&lt;td&gt;84.5&lt;/td&gt;&lt;td&gt;83.6&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PAN [34]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;81.2&lt;/td&gt;&lt;td&gt;86.4&lt;/td&gt;&lt;td&gt;83.7&lt;/td&gt;&lt;td&gt;39.8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ContourNet [36]&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;84.1&lt;/td&gt;&lt;td&gt;83.7&lt;/td&gt;&lt;td&gt;83.9&lt;/td&gt;&lt;td&gt;4.5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DRRG [41]&lt;/td&gt;&lt;td&gt;MLT&lt;/td&gt;&lt;td&gt;83.02&lt;/td&gt;&lt;td&gt;85.93&lt;/td&gt;&lt;td&gt;84.45&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextPerception[23]&lt;/td&gt;&lt;td&gt;Syn&lt;/td&gt;&lt;td&gt;81.9&lt;/td&gt;&lt;td&gt;87.5&lt;/td&gt;&lt;td&gt;84.6&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours&lt;/td&gt;&lt;td&gt; Syn&lt;/td&gt;&lt;td&gt;80.57&lt;/td&gt;&lt;td&gt;87.66&lt;/td&gt;&lt;td&gt;83.97&lt;/td&gt;&lt;td&gt;12.08&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;81.45&lt;/td&gt;&lt;td&gt;87.81&lt;/td&gt;&lt;td&gt;84.51&lt;/td&gt;&lt;td&gt;12.15&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours&lt;/td&gt;&lt;td&gt;MLT&lt;/td&gt;&lt;td&gt;83.60&lt;/td&gt;&lt;td&gt;86.45&lt;/td&gt;&lt;td&gt;85.00&lt;/td&gt;&lt;td&gt;12.21&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;\nThe table visualized image saved in ./output//6_1.png\n7       type: table, region: [462,359,820,657], score: 0.953917, res: &lt;html&gt;&lt;body&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;td&gt;Methods&lt;/td&gt;&lt;td&gt;R&lt;/td&gt;&lt;td&gt;P&lt;/td&gt;&lt;td&gt;F&lt;/td&gt;&lt;td&gt;FPS&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;SegLink [26]&lt;/td&gt;&lt;td&gt;70.0&lt;/td&gt;&lt;td&gt;86.0&lt;/td&gt;&lt;td&gt;77.0&lt;/td&gt;&lt;td&gt;8.9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PixelLink [4]&lt;/td&gt;&lt;td&gt;73.2&lt;/td&gt;&lt;td&gt;83.0&lt;/td&gt;&lt;td&gt;77.8&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextSnake [18]&lt;/td&gt;&lt;td&gt;73.9&lt;/td&gt;&lt;td&gt;83.2&lt;/td&gt;&lt;td&gt;78.3&lt;/td&gt;&lt;td&gt;1.1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TextField [37]&lt;/td&gt;&lt;td&gt;75.9&lt;/td&gt;&lt;td&gt;87.4&lt;/td&gt;&lt;td&gt;81.3&lt;/td&gt;&lt;td&gt;5.2 &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MSR[38]&lt;/td&gt;&lt;td&gt;76.7&lt;/td&gt;&lt;td&gt;87.4&lt;/td&gt;&lt;td&gt;81.7&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;FTSN[3]&lt;/td&gt;&lt;td&gt;77.1&lt;/td&gt;&lt;td&gt;87.6&lt;/td&gt;&lt;td&gt;82.0&lt;/td&gt;&lt;td&gt;:&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;LSE[30]&lt;/td&gt;&lt;td&gt;81.7&lt;/td&gt;&lt;td&gt;84.2&lt;/td&gt;&lt;td&gt;82.9&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;CRAFT [2]&lt;/td&gt;&lt;td&gt;78.2&lt;/td&gt;&lt;td&gt;88.2&lt;/td&gt;&lt;td&gt;82.9&lt;/td&gt;&lt;td&gt;8.6&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MCN [16]&lt;/td&gt;&lt;td&gt;79&lt;/td&gt;&lt;td&gt;88&lt;/td&gt;&lt;td&gt;83&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ATRR[35]&lt;/td&gt;&lt;td&gt;82.1&lt;/td&gt;&lt;td&gt;85.2&lt;/td&gt;&lt;td&gt;83.6&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PAN [34]&lt;/td&gt;&lt;td&gt;83.8&lt;/td&gt;&lt;td&gt;84.4&lt;/td&gt;&lt;td&gt;84.1&lt;/td&gt;&lt;td&gt;30.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DB[12]&lt;/td&gt;&lt;td&gt;79.2&lt;/td&gt;&lt;td&gt;91.5&lt;/td&gt;&lt;td&gt;84.9&lt;/td&gt;&lt;td&gt;32.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DRRG [41]&lt;/td&gt;&lt;td&gt;82.30&lt;/td&gt;&lt;td&gt;88.05&lt;/td&gt;&lt;td&gt;85.08&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours (SynText)&lt;/td&gt;&lt;td&gt;80.68&lt;/td&gt;&lt;td&gt;85.40&lt;/td&gt;&lt;td&gt;82.97&lt;/td&gt;&lt;td&gt;12.68&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Ours (MLT-17)&lt;/td&gt;&lt;td&gt;84.54&lt;/td&gt;&lt;td&gt;86.62&lt;/td&gt;&lt;td&gt;85.57&lt;/td&gt;&lt;td&gt;12.31&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;\nThe table visualized image saved in ./output//7_1.png\n8       type: figure, region: [14,3,836,310], score: 0.969443, res: count of ocr result is : 26\n********** print ocr result **********\n0       det boxes: [[506,14],[539,15],[539,22],[506,21]] rec text: E rec score: 0.318073\n...\n25      det boxes: [[680,290],[759,288],[759,303],[680,305]] rec text: (d) CTW1500 rec score: 0.95911\n********** end print ocr result **********\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/cpp_infer.html#3-faq","title":"3. FAQ","text":"<ol> <li>Encountered the error <code>unable to access 'https://github.com/LDOUBLEV/AutoLog.git/': gnutls_handshake() failed: The TLS connection was non-properly terminated.</code>, change the github address in <code>deploy/cpp_infer/external-cmake/auto-log.cmake</code> to the https://gitee.com/Double_V/AutoLog address.</li> </ol>"},{"location":"en/version2.x/ppstructure/infer_deploy/paddle_server.html","title":"Paddle server","text":"<p>PaddleOCR provides 2 service deployment methods:</p> <ul> <li>Based on PaddleHub Serving: Code path is <code>./deploy/hubserving</code>. Please follow this tutorial.</li> <li>Based on PaddleServing: Code path is <code>./deploy/pdserving</code>. Please refer to the tutorial for usage.</li> </ul>"},{"location":"en/version2.x/ppstructure/infer_deploy/paddle_server.html#service-deployment-based-on-paddlehub-serving","title":"Service deployment based on PaddleHub Serving","text":"<p>The hubserving service deployment directory includes seven service packages: text detection, text angle class, text recognition, text detection+text angle class+text recognition three-stage series connection, layout analysis, table recognition, and PP-Structure. Please select the corresponding service package to install and start the service according to your needs. The directory is as follows:</p> <pre><code>deploy/hubserving/\n  \u2514\u2500  ocr_det     text detection module service package\n  \u2514\u2500  ocr_cls     text angle class module service package\n  \u2514\u2500  ocr_rec     text recognition module service package\n  \u2514\u2500  ocr_system  text detection+text angle class+text recognition three-stage series connection service package\n  \u2514\u2500  structure_layout  layout analysis service package\n  \u2514\u2500  structure_table  table recognition service package\n  \u2514\u2500  structure_system  PP-Structure service package\n  \u2514\u2500  kie_ser  KIE(SER) service package\n  \u2514\u2500  kie_ser_re  KIE(SER+RE) service package\n</code></pre> <p>Each service pack contains 3 files. Take the 2-stage series connection service package as an example, the directory is as follows:</p> <pre><code>deploy/hubserving/ocr_system/\n  \u2514\u2500  __init__.py    Empty file, required\n  \u2514\u2500  config.json    Configuration file, optional, passed in as a parameter when using configuration to start the service\n  \u2514\u2500  module.py      Main module file, required, contains the complete logic of the service\n  \u2514\u2500  params.py      Parameter file, required, including parameters such as model path, pre and post-processing parameters\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/paddle_server.html#1-update","title":"1. Update","text":"<ul> <li>2022.10.09 add KIE services.</li> <li>2022.08.23 add layout analysis services.</li> <li>2022.03.30 add PP-Structure and table recognition services.</li> <li>2022.05.05 add PP-OCRv3 text detection and recognition services.</li> </ul>"},{"location":"en/version2.x/ppstructure/infer_deploy/paddle_server.html#2-quick-start-service","title":"2. Quick start service","text":"<p>The following steps take the 2-stage series service as an example. If only the detection service or recognition service is needed, replace the corresponding file path.</p>"},{"location":"en/version2.x/ppstructure/infer_deploy/paddle_server.html#21-install-paddlehub","title":"2.1 Install PaddleHub","text":"<pre><code>pip3 install paddlehub==2.1.0 --upgrade\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/paddle_server.html#22-download-inference-model","title":"2.2 Download inference model","text":"<p>Before installing the service module, you need to prepare the inference model and put it in the correct path. By default, the PP-OCRv3 models are used, and the default model path is:</p> Model Path text detection model ./inference/PP-OCRv3_mobile_det_infer/ text recognition model ./inference/PP-OCRv3_mobile_rec_infer/ text angle classifier ./inference/ch_ppocr_mobile_v2.0_cls_infer/ layout parse model ./inference/picodet_lcnet_x1_0_fgd_layout_infer/ tanle recognition ./inference/ch_ppstructure_mobile_v2.0_SLANet_infer/ KIE(SER) ./inference/ser_vi_layoutxlm_xfund_infer/ KIE(SER+RE) ./inference/re_vi_layoutxlm_xfund_infer/ <p>The model path can be found and modified in <code>params.py</code>. More models provided by PaddleOCR can be obtained from the model library. You can also use models trained by yourself.</p>"},{"location":"en/version2.x/ppstructure/infer_deploy/paddle_server.html#23-install-service-module","title":"2.3 Install Service Module","text":"<p>PaddleOCR provides 5 kinds of service modules, install the required modules according to your needs.</p> <ul> <li>On the Linux platform(replace <code>/</code> with <code>\\</code> if using Windows), the examples are as the following table:</li> </ul> <p>| Service model | Command | | text detection | <code>hub install deploy/hubserving/ocr_det</code> | | text angle class: | <code>hub install deploy/hubserving/ocr_cls</code> | | text recognition: | <code>hub install deploy/hubserving/ocr_rec</code> | | 2-stage series: | <code>hub install deploy/hubserving/ocr_system</code> | | table recognition | <code>hub install deploy/hubserving/structure_table</code> | | PP-Structure | <code>hub install deploy/hubserving/structure_system</code> | | KIE(SER) | <code>hub install deploy/hubserving/kie_ser</code> | | KIE(SER+RE) | <code>hub install deploy/hubserving/kie_ser_re</code> |</p>"},{"location":"en/version2.x/ppstructure/infer_deploy/paddle_server.html#24-start-service","title":"2.4 Start service","text":""},{"location":"en/version2.x/ppstructure/infer_deploy/paddle_server.html#241-start-with-command-line-parameters-cpu-only","title":"2.4.1 Start with command line parameters (CPU only)","text":"<p>start command:</p> <pre><code>hub serving start --modules Module1==Version1, Module2==Version2, ... \\\n                  --port 8866 \\\n                  --use_multiprocess \\\n                  --workers \\\n</code></pre> <p>Parameters: |parameters|usage| |---|---| |<code>--modules</code>/<code>-m</code>|PaddleHub Serving pre-installed model, listed in the form of multiple Module==Version key-value pairsWhen Version is not specified, the latest version is selected by default| |<code>--port</code>/<code>-p</code>|Service port, default is 8866| |<code>--use_multiprocess</code>|Enable concurrent mode, by default using the single-process mode, this mode is recommended for multi-core CPU machinesWindows operating system only supports single-process mode| |<code>--workers</code>|The number of concurrent tasks specified in concurrent mode, the default is <code>2*cpu_count-1</code>, where <code>cpu_count</code> is the number of CPU cores|</p> <p>For example, start the 2-stage series service:</p> <pre><code>hub serving start -m ocr_system\n</code></pre> <p>This completes the deployment of a service API, using the default port number 8866.</p>"},{"location":"en/version2.x/ppstructure/infer_deploy/paddle_server.html#242-start-with-configuration-filecpu-and-gpu","title":"2.4.2 Start with configuration file\uff08CPU and GPU\uff09","text":"<p>start command:</p> <pre><code>hub serving start --config/-c config.json\n</code></pre> <p>In which the format of <code>config.json</code> is as follows:</p> <pre><code>{\n    \"modules_info\": {\n        \"ocr_system\": {\n            \"init_args\": {\n                \"version\": \"1.0.0\",\n                \"use_gpu\": true\n            },\n            \"predict_args\": {\n            }\n        }\n    },\n    \"port\": 8868,\n    \"use_multiprocess\": false,\n    \"workers\": 2\n}\n</code></pre> <ul> <li>The configurable parameters in <code>init_args</code> are consistent with the <code>_initialize</code> function interface in <code>module.py</code>.</li> </ul> <p>When <code>use_gpu</code> is <code>true</code>, it means that the GPU is used to start the service. - The configurable parameters in <code>predict_args</code> are consistent with the <code>predict</code> function interface in <code>module.py</code>.</p> <p>Note:   - When using the configuration file to start the service, other parameters will be ignored.   - If you use GPU prediction (that is, <code>use_gpu</code> is set to <code>true</code>), you need to set the environment variable CUDA_VISIBLE_DEVICES before starting the service, such as:</p> <pre><code>```bash linenums=\"1\"\nexport CUDA_VISIBLE_DEVICES=0\n```\n</code></pre> <ul> <li><code>use_gpu</code> and <code>use_multiprocess</code> cannot be <code>true</code> at the same time.</li> </ul> <p>For example, use GPU card No. 3 to start the 2-stage series service:</p> <pre><code>export CUDA_VISIBLE_DEVICES=3\nhub serving start -c deploy/hubserving/ocr_system/config.json\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/paddle_server.html#3-send-prediction-requests","title":"3. Send prediction requests","text":"<p>After the service starts, you can use the following command to send a prediction request to obtain the prediction result:</p> <pre><code>python tools/test_hubserving.py --server_url=server_url --image_dir=image_path\n</code></pre> <p>Two parameters need to be passed to the script:</p> <ul> <li>server_url:service address, the format of which is   <code>http://[ip_address]:[port]/predict/[module_name]</code></li> </ul> <p>For example, if using the configuration file to start the text angle classification, text detection, text recognition, detection+classification+recognition 3 stages, table recognition and PP-Structure service,</p> <p>also modified the port for each service, then the <code>server_url</code> to send the request will be:</p> <pre><code>http://127.0.0.1:8865/predict/ocr_det\nhttp://127.0.0.1:8866/predict/ocr_cls\nhttp://127.0.0.1:8867/predict/ocr_rec\nhttp://127.0.0.1:8868/predict/ocr_system\nhttp://127.0.0.1:8869/predict/structure_table\nhttp://127.0.0.1:8870/predict/structure_system\nhttp://127.0.0.1:8870/predict/structure_layout\nhttp://127.0.0.1:8871/predict/kie_ser\nhttp://127.0.0.1:8872/predict/kie_ser_re\n</code></pre> <ul> <li>image_dir:Test image path, which can be a single image path or an image directory path</li> <li>visualize:Whether to visualize the results, the default value is False</li> <li>output:The folder to save the Visualization result, the default value is <code>./hubserving_result</code></li> </ul> <p>Example:</p> <pre><code>python tools/test_hubserving.py --server_url=http://127.0.0.1:8868/predict/ocr_system --image_dir=./doc/imgs/ --visualize=false`\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/paddle_server.html#4-returned-result-format","title":"4. Returned result format","text":"<p>The returned result is a list. Each item in the list is a dictionary which may contain three fields. The information is as follows:</p> field name data type description angle str angle text str text content confidence float text recognition confidence text_region list text location coordinates html str table HTML string regions list The result of layout analysis + table recognition + OCR, each item is a listincluding <code>bbox</code> indicating area coordinates, <code>type</code> of area type and <code>res</code> of area results layout list The result of layout analysis, each item is a dict, including <code>bbox</code> indicating area coordinates, <code>label</code> of area type <p>The fields returned by different modules are different. For example, the results returned by the text recognition service module do not contain <code>text_region</code>, detailed table is as follows:</p> field name/module name ocr_det ocr_cls ocr_rec ocr_system structure_table structure_system structure_layout kie_ser kie_re angle \u2714 \u2714 text \u2714 \u2714 \u2714 \u2714 \u2714 confidence \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 text_region \u2714 \u2714 \u2714 \u2714 \u2714 html \u2714 \u2714 regions \u2714 \u2714 layout \u2714 ser_res \u2714 re_res \u2714 <p>Note: If you need to add, delete or modify the returned fields, you can modify the file <code>module.py</code> of the corresponding module. For the complete process, refer to the user-defined modification service module in the next section.</p>"},{"location":"en/version2.x/ppstructure/infer_deploy/paddle_server.html#5-user-defined-service-module-modification","title":"5. User-defined service module modification","text":"<p>If you need to modify the service logic, the following steps are generally required (take the modification of <code>deploy/hubserving/ocr_system</code> for example):</p> <ol> <li>Stop service:</li> </ol> <pre><code>hub serving stop --port/-p XXXX\n</code></pre> <ol> <li>Modify the code in the corresponding files under <code>deploy/hubserving/ocr_system</code>, such as <code>module.py</code> and <code>params.py</code>, to your actual needs.</li> </ol> <p>For example, if you need to replace the model used by the deployed service, you need to modify model path parameters <code>det_model_dir</code> and <code>rec_model_dir</code> in <code>params.py</code>. If you want to turn off the text direction classifier, set the parameter <code>use_angle_cls</code> to <code>False</code>.</p> <p>Of course, other related parameters may need to be modified at the same time. Please modify and debug according to the actual situation.</p> <p>It is suggested to run <code>module.py</code> directly for debugging after modification before starting the service test.</p> <p>Note The image input shape used by the PPOCR-v3 recognition model is <code>3, 48, 320</code>, so you need to modify <code>cfg.rec_image_shape = \"3, 48, 320\"</code> in <code>params.py</code>, if you do not use the PPOCR-v3 recognition model, then there is no need to modify this parameter. 3. (Optional) If you want to rename the module, the following lines should be modified:    - <code>ocr_system</code> within <code>from deploy.hubserving.ocr_system.params import read_params</code>    - <code>ocr_system</code> within <code>name=\"ocr_system\",</code> 4. (Optional) It may require you to delete the directory <code>__pycache__</code> to force flush build cache of CPython:</p> <pre><code>find deploy/hubserving/ocr_system -name '__pycache__' -exec rm -r {} \\;\n</code></pre> <ol> <li>Install modified service module:</li> </ol> <pre><code>hub install deploy/hubserving/ocr_system/\n</code></pre> <ol> <li>Restart service:</li> </ol> <pre><code>hub serving start -m ocr_system\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/python_infer.html","title":"Python Inference","text":""},{"location":"en/version2.x/ppstructure/infer_deploy/python_infer.html#1-layout-structured-analysis","title":"1. Layout Structured Analysis","text":"<p>Go to the <code>ppstructure</code> directory</p> <pre><code>cd ppstructure\n\n# download model\nmkdir inference &amp;&amp; cd inference\n# Download the PP-StructureV2 layout analysis model and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_layout_infer.tar &amp;&amp; tar xf picodet_lcnet_x1_0_layout_infer.tar\n# Download the PP-OCRv3 text detection model and unzip it\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0.0/PP-OCRv3_mobile_det_infer.tar &amp;&amp; tar xf PP-OCRv3_mobile_det_infer.tar\n# Download the PP-OCRv3 text recognition model and unzip it\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0.0/PP-OCRv3_mobile_rec_infer.tar &amp;&amp; tar xf PP-OCRv3_mobile_rec_infer.tar\n# Download the PP-StructureV2 form recognition model and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/paddle3.0b2/ch_ppstructure_mobile_v2.0_SLANet_infer.tar &amp;&amp; tar xf ch_ppstructure_mobile_v2.0_SLANet_infer.tar\ncd ..\n</code></pre>"},{"location":"en/version2.x/ppstructure/infer_deploy/python_infer.html#11-layout-analysis-table-recognition","title":"1.1 layout analysis + table recognition","text":"<pre><code>python3 predict_system.py --det_model_dir=inference/PP-OCRv3_mobile_det_infer \\\n                          --rec_model_dir=inference/PP-OCRv3_mobile_rec_infer \\\n                          --table_model_dir=inference/ch_ppstructure_mobile_v2.0_SLANet_infer \\\n                          --layout_model_dir=inference/picodet_lcnet_x1_0_layout_infer \\\n                          --image_dir=./docs/table/1.png \\\n                          --rec_char_dict_path=../ppocr/utils/ppocr_keys_v1.txt \\\n                          --table_char_dict_path=../ppocr/utils/dict/table_structure_dict_ch.txt \\\n                          --output=../output \\\n                          --vis_font_path=../doc/fonts/simfang.ttf\n</code></pre> <p>After the operation is completed, each image will have a directory with the same name in the <code>structure</code> directory under the directory specified by the <code>output</code> field. Each table in the image will be stored as an excel, and the picture area will be cropped and saved. The filename of excel and picture is their coordinates in the image. Detailed results are stored in the <code>res.txt</code> file.</p>"},{"location":"en/version2.x/ppstructure/infer_deploy/python_infer.html#12-layout-analysis","title":"1.2 layout analysis","text":"<pre><code>python3 predict_system.py --layout_model_dir=inference/picodet_lcnet_x1_0_layout_infer \\\n                          --image_dir=./docs/table/1.png \\\n                          --output=../output \\\n                          --table=false \\\n                          --ocr=false\n</code></pre> <p>After the operation is completed, each image will have a directory with the same name in the <code>structure</code> directory under the directory specified by the <code>output</code> field. Each picture in image will be cropped and saved. The filename of picture area is their coordinates in the image. Layout analysis results will be stored in the <code>res.txt</code> file</p>"},{"location":"en/version2.x/ppstructure/infer_deploy/python_infer.html#13-table-recognition","title":"1.3 table recognition","text":"<pre><code>python3 predict_system.py --det_model_dir=inference/PP-OCRv3_mobile_det_infer \\\n                          --rec_model_dir=inference/PP-OCRv3_mobile_rec_infer \\\n                          --table_model_dir=inference/ch_ppstructure_mobile_v2.0_SLANet_infer \\\n                          --image_dir=./docs/table/table.jpg \\\n                          --rec_char_dict_path=../ppocr/utils/ppocr_keys_v1.txt \\\n                          --table_char_dict_path=../ppocr/utils/dict/table_structure_dict_ch.txt \\\n                          --output=../output \\\n                          --vis_font_path=../doc/fonts/simfang.ttf \\\n                          --layout=false\n</code></pre> <p>After the operation is completed, each image will have a directory with the same name in the <code>structure</code> directory under the directory specified by the <code>output</code> field. Each table in the image will be stored as an excel. The filename of excel is their coordinates in the image.</p>"},{"location":"en/version2.x/ppstructure/infer_deploy/python_infer.html#2-key-information-extraction","title":"2. Key Information Extraction","text":""},{"location":"en/version2.x/ppstructure/infer_deploy/python_infer.html#21-ser","title":"2.1 SER","text":"<pre><code>cd ppstructure\n\nmkdir inference &amp;&amp; cd inference\n# download model\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/ser_vi_layoutxlm_xfund_infer.tar &amp;&amp; tar -xf ser_vi_layoutxlm_xfund_infer.tar\ncd ..\npython3 predict_system.py \\\n  --kie_algorithm=LayoutXLM \\\n  --ser_model_dir=./inference/ser_vi_layoutxlm_xfund_infer \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../ppocr/utils/dict/kie_dict/xfund_class_list.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\" \\\n  --mode=kie\n</code></pre> <p>After the operation is completed, each image will store the visualized image in the <code>kie</code> directory under the directory specified by the <code>output</code> field, and the image name is the same as the input image name.</p>"},{"location":"en/version2.x/ppstructure/infer_deploy/python_infer.html#22-reser","title":"2.2 RE+SER","text":"<pre><code>cd ppstructure\n\nmkdir inference &amp;&amp; cd inference\n# download model\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/ser_vi_layoutxlm_xfund_infer.tar &amp;&amp; tar -xf ser_vi_layoutxlm_xfund_infer.tar\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/re_vi_layoutxlm_xfund_infer.tar &amp;&amp; tar -xf re_vi_layoutxlm_xfund_infer.tar\ncd ..\n\npython3 predict_system.py \\\n  --kie_algorithm=LayoutXLM \\\n  --re_model_dir=./inference/re_vi_layoutxlm_xfund_infer \\\n  --ser_model_dir=./inference/ser_vi_layoutxlm_xfund_infer \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../ppocr/utils/dict/kie_dict/xfund_class_list.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\" \\\n  --mode=kie\n</code></pre> <p>After the operation is completed, each image will have a directory with the same name in the <code>kie</code> directory under the directory specified by the <code>output</code> field, where the visual images and prediction results are stored.</p>"},{"location":"en/version2.x/ppstructure/model_train/recovery_to_doc.html","title":"Layout Recovery","text":""},{"location":"en/version2.x/ppstructure/model_train/recovery_to_doc.html#1-introduction","title":"1. Introduction","text":"<p>The layout recovery module is used to restore the image or pdf to an editable Word file consistent with the original image layout.</p> <p>Two layout recovery methods are provided, you can choose by PDF format:</p> <ul> <li> <p>Standard PDF parse(the input is standard PDF): Python based PDF to word library pdf2docx is optimized, the method extracts data from PDF with PyMuPDF, then parse layout with rule, finally, generate docx with python-docx.</p> </li> <li> <p>Image format PDF parse(the input can be standard PDF or image format PDF): Layout recovery combines layout analysis\u3001table recognition to better recover images, tables, titles, etc. supports input files in PDF and document image formats in Chinese and English.</p> </li> </ul> <p>The input formats and application scenarios of the two methods are as follows:</p> method input formats application scenarios/problem Standard PDF parse pdf Advantages: Better recovery for non-paper documents, each page remains on the same page after restorationDisadvantages: English characters in some Chinese documents are garbled, some contents are still beyond the current page, the whole page content is restored to the table format, and the recovery effect of some pictures is not good Image format PDF parse( pdf\u3001picture Advantages: More suitable for paper document content recovery,  OCR recognition effect is more goodDisadvantages: Currently, the recovery is based on rules, the effect of content typesetting (spacing, fonts, etc.) need to be further improved, and the effect of layout recovery depends on layout analysis <p>The following figure shows the effect of restoring the layout of documents by using PDF parse:</p> <p></p> <p>The following figures show the effect of restoring the layout of English and Chinese documents by using OCR technique:</p> <p></p> <p></p>"},{"location":"en/version2.x/ppstructure/model_train/recovery_to_doc.html#2-install","title":"2. Install","text":""},{"location":"en/version2.x/ppstructure/model_train/recovery_to_doc.html#21-install-paddlepaddle","title":"2.1 Install PaddlePaddle","text":"<pre><code>python3 -m pip install --upgrade pip\n\n# If you have cuda9 or cuda10 installed on your machine, please run the following command to install\npython3 -m pip install \"paddlepaddle-gpu\" -i https://mirror.baidu.com/pypi/simple\n\n# CPU installation\npython3 -m pip install \"paddlepaddle\" -i https://mirror.baidu.com/pypi/simple\n````\n\nFor more requirements, please refer to the instructions in [Installation Documentation](https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/install/pip/macos-pip_en.html).\n\n### 2.2 Install PaddleOCR\n\n- **(1) Download source code**\n\n```bash linenums=\"1\"\n[Recommended] git clone https://github.com/PaddlePaddle/PaddleOCR\n\n# If the pull cannot be successful due to network problems, you can also choose to use the hosting on the code cloud:\ngit clone https://gitee.com/paddlepaddle/PaddleOCR\n\n# Note: Code cloud hosting code may not be able to synchronize the update of this github project in real time, there is a delay of 3 to 5 days, please use the recommended method first.\n````\n\n- **(2) Install recovery `requirements`**\n\nThe layout restoration is exported as docx files, so python-docx API need to be installed, and PyMuPDF api([requires Python &gt;= 3.7](https://pypi.org/project/PyMuPDF/)) need to be installed to process the input files in pdf format.\n\nInstall all the libraries by running the following command:\n\n```bash linenums=\"1\"\npython3 -m pip install -r ppstructure/recovery/requirements.txt\n````\n\n And if using pdf parse method, we need to install pdf2docx api.\n\n```bash linenums=\"1\"\nwget https://paddleocr.bj.bcebos.com/whl/pdf2docx-0.0.0-py3-none-any.whl\npip3 install pdf2docx-0.0.0-py3-none-any.whl\n</code></pre>"},{"location":"en/version2.x/ppstructure/model_train/recovery_to_doc.html#3-quick-start-using-standard-pdf-parse","title":"3. Quick Start using standard PDF parse","text":"<p><code>use_pdf2docx_api</code> use PDF parse for layout recovery, The whl package is also provided  for quick use, follow the above code, for more information please refer to quickstart for details.</p> <pre><code># install paddleocr\npip3 install \"paddleocr&gt;=2.6\"\npaddleocr --image_dir=ppstructure/docs/recovery/UnrealText.pdf --type=structure --recovery=true --use_pdf2docx_api=true\n</code></pre> <p>Command line:</p> <pre><code>python3 predict_system.py \\\n    --image_dir=ppstructure/docs/recovery/UnrealText.pdf \\\n    --recovery=True \\\n    --use_pdf2docx_api=True \\\n    --output=../output/\n</code></pre>"},{"location":"en/version2.x/ppstructure/model_train/recovery_to_doc.html#4-quick-start-using-image-format-pdf-parse","title":"4. Quick Start using image format PDF parse","text":"<p>Through layout analysis, we divided the image/PDF documents into regions, located the key regions, such as text, table, picture, etc., and recorded the location, category, and regional pixel value information of each region. Different regions are processed separately, where:</p> <ul> <li> <p>OCR detection and recognition is performed in the text area, and the coordinates of the OCR detection box and the text content information are added on the basis of the previous information</p> </li> <li> <p>The table area identifies tables and records html and text information of tables</p> </li> <li>Save the image directly</li> </ul> <p>We can restore the test picture through the layout information, OCR detection and recognition structure, table information, and saved pictures.</p> <p>The whl package is also provided for quick use, follow the above code, for more information please refer to quickstart for details.</p> <pre><code>paddleocr --image_dir=ppstructure/docs/table/1.png --type=structure --recovery=true --lang='en'\n</code></pre>"},{"location":"en/version2.x/ppstructure/model_train/recovery_to_doc.html#41-download-models","title":"4.1 Download models","text":"<p>If input is English document, download English models:</p> <pre><code>cd PaddleOCR/ppstructure\n\n# download model\nmkdir inference &amp;&amp; cd inference\n# Download the detection model of the ultra-lightweight English PP-OCRv3 model and unzip it\nwget https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar &amp;&amp; tar xf en_PP-OCRv3_det_infer.tar\n# Download the recognition model of the ultra-lightweight English PP-OCRv3 model and unzip it\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0.0/en_PP-OCRv3_mobile_rec_infer.tar &amp;&amp; tar xf en_PP-OCRv3_mobile_rec_infer.tar\n# Download the ultra-lightweight English table inch model and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/paddle3.0b2/en_ppstructure_mobile_v2.0_SLANet_infer.tar\ntar xf en_ppstructure_mobile_v2.0_SLANet_infer.tar\n# Download the layout model of publaynet dataset and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_fgd_layout_infer.tar\ntar xf picodet_lcnet_x1_0_fgd_layout_infer.tar\ncd ..\n</code></pre> <p>If input is Chinese document\uff0cdownload Chinese models: Chinese and English ultra-lightweight PP-OCRv3 model</p>"},{"location":"en/version2.x/ppstructure/model_train/recovery_to_doc.html#42-layout-recovery","title":"4.2 Layout recovery","text":"<pre><code>python3 predict_system.py \\\n    --image_dir=./docs/table/1.png \\\n    --det_model_dir=inference/en_PP-OCRv3_det_infer \\\n    --rec_model_dir=inference/en_PP-OCRv3_mobile_rec_infer \\\n    --rec_char_dict_path=../ppocr/utils/en_dict.txt \\\n    --table_model_dir=inference/en_ppstructure_mobile_v2.0_SLANet_infer \\\n    --table_char_dict_path=../ppocr/utils/dict/table_structure_dict.txt \\\n    --layout_model_dir=inference/picodet_lcnet_x1_0_fgd_layout_infer \\\n    --layout_dict_path=../ppocr/utils/dict/layout_dict/layout_publaynet_dict.txt \\\n    --vis_font_path=../doc/fonts/simfang.ttf \\\n    --recovery=True \\\n    --output=../output/\n</code></pre> <p>After running, the docx of each picture will be saved in the directory specified by the output field</p> <p>Field\uff1a</p> <ul> <li>image_dir\uff1atest file\uff0c can be picture, picture directory, pdf file, pdf file directory</li> <li>det_model_dir\uff1aOCR detection model path</li> <li>rec_model_dir\uff1aOCR recognition model path</li> <li>rec_char_dict_path\uff1aOCR recognition dict path. If the Chinese model is used, change to \"../ppocr/utils/ppocr_keys_v1.txt\". And if you trained the model on your own dataset, change to the trained dictionary</li> <li>table_model_dir\uff1atable recognition model path</li> <li>table_char_dict_path\uff1atable recognition dict path. If the Chinese model is used, no need to change</li> <li>layout_model_dir\uff1alayout analysis model path</li> <li>layout_dict_path\uff1alayout analysis dict path. If the Chinese model is used, change to \"../ppocr/utils/dict/layout_dict/layout_cdla_dict.txt\"</li> <li>recovery\uff1awhether to enable layout of recovery, default False</li> <li>output\uff1asave the recovery result path</li> </ul>"},{"location":"en/version2.x/ppstructure/model_train/recovery_to_doc.html#5-more","title":"5. More","text":"<p>For training, evaluation and inference tutorial for text detection models, please refer to text detection doc.</p> <p>For training, evaluation and inference tutorial for text recognition models, please refer to text recognition doc.</p> <p>For training, evaluation and inference tutorial for layout analysis models, please refer to layout analysis doc</p> <p>For training, evaluation and inference tutorial for table recognition models, please refer to table recognition doc</p>"},{"location":"en/version2.x/ppstructure/model_train/train_kie.html","title":"Key Information Extraction (KIE)","text":""},{"location":"en/version2.x/ppstructure/model_train/train_kie.html#1-introduction","title":"1. Introduction","text":"<p>Key information extraction (KIE) refers to extracting key information from text or images. As downstream task of OCR, the key information extraction task of document image has many practical application scenarios, such as form recognition, ticket information extraction, ID card information extraction, etc.</p> <p>PP-Structure conducts research based on the LayoutXLM multi-modal, and proposes the VI-LayoutXLM, which gets rid of visual features when finetuning the downstream tasks. An textline sorting method is also utilized to fit in reading order. What's more, UDML knowledge distillation is used for higher accuracy. Finally, the accuracy and inference speed of VI-LayoutXLM surpass those of LayoutXLM.</p> <p>The main features of the key information extraction module in PP-Structure are as follows.</p> <ul> <li>Integrate multi-modal methods such as LayoutXLM, VI-LayoutXLM, and PP-OCR inference engine.</li> <li>Supports Semantic Entity Recognition (SER) and Relation Extraction (RE) tasks based on multimodal methods. Based on the SER task, the text recognition and classification in the image can be completed; based on the RE task, the relationship extraction of the text content in the image can be completed, such as judging the problem pair (pair).</li> <li>Supports custom training for SER tasks and RE tasks.</li> <li>Supports end-to-end system prediction and evaluation of OCR+SER.</li> <li>Supports end-to-end system prediction of OCR+SER+RE.</li> <li>Support SER model export and inference using PaddleInference.</li> </ul>"},{"location":"en/version2.x/ppstructure/model_train/train_kie.html#2-performance","title":"2. Performance","text":"<p>We evaluate the methods on the Chinese dataset of XFUND, and the performance is as follows</p> Model Backbone Task Config file Hmean Inference time (ms) Download link VI-LayoutXLM VI-LayoutXLM-base SER ser_vi_layoutxlm_xfund_zh_udml.yml 93.19% 15.49 trained model LayoutXLM LayoutXLM-base SER ser_layoutxlm_xfund_zh.yml 90.38% 19.49 trained model VI-LayoutXLM VI-LayoutXLM-base RE re_vi_layoutxlm_xfund_zh_udml.yml 83.92% 15.49 trained model LayoutXLM LayoutXLM-base RE re_layoutxlm_xfund_zh.yml 74.83% 19.49 trained model <ul> <li>Note\uff1aInference environment\uff1aV100 GPU + cuda10.2 + cudnn8.1.1 + TensorRT 7.2.3.4\uff0ctested using fp16.</li> </ul> <p>For more KIE models in PaddleOCR, please refer to KIE model zoo.</p>"},{"location":"en/version2.x/ppstructure/model_train/train_kie.html#3-visualization","title":"3. Visualization","text":"<p>There are two main solutions to the key information extraction task based on VI-LayoutXLM series model.</p> <p>(1) Text detection + text recognition + semantic entity recognition (SER)</p> <p>(2) Text detection + text recognition + semantic entity recognition (SER) + relationship extraction (RE)</p> <p>The following images are demo results of the SER and RE models. For more detailed introduction to the above solutions, please refer to KIE Guide.</p>"},{"location":"en/version2.x/ppstructure/model_train/train_kie.html#31-ser","title":"3.1 SER","text":"<p>Demo results for SER task are as follows.</p> <p></p> <p></p> <p></p> <p></p> <p>Note: test pictures are from xfund dataset, invoice dataset and a composite ID card dataset.</p> <p>Boxes of different colors in the image represent different categories.</p> <p>The invoice and application form images have three categories: <code>request</code>, <code>answer</code> and <code>header</code>. The <code>question</code> and <code>answer</code> can be used to extract the relationship.</p> <p>For the ID card image, the model can directly identify the key information such as <code>name</code>, <code>gender</code>, <code>nationality</code>, so that the subsequent relationship extraction process is not required, and the key information extraction task can be completed using only one model.</p>"},{"location":"en/version2.x/ppstructure/model_train/train_kie.html#32-re","title":"3.2 RE","text":"<p>Demo results for RE task are as follows.</p> <p></p> <p></p> <p></p> <p>Red boxes are questions, blue boxes are answers. The green lines means the two connected objects are a pair.</p>"},{"location":"en/version2.x/ppstructure/model_train/train_kie.html#4-usage","title":"4. Usage","text":""},{"location":"en/version2.x/ppstructure/model_train/train_kie.html#41-prepare-for-the-environment","title":"4.1 Prepare for the environment","text":"<p>Use the following command to install KIE dependencies.</p> <pre><code>git clone https://github.com/PaddlePaddle/PaddleOCR.git\ncd PaddleOCR\npip install -r requirements.txt\npip install -r ppstructure/kie/requirements.txt\n# \u200b\u5b89\u88c5\u200bPaddleOCR\u200b\u5f15\u64ce\u200b\u7528\u4e8e\u200b\u9884\u6d4b\u200b\npip install \"paddleocr&lt;3.0\"\n</code></pre> <p>NOTE: For KIE tasks, it is necessary to downgrade the Paddle framework version (Paddle&lt;2.6) and the PaddleNLP version (PaddleNLP&lt;2.6).</p> <p>The visualized results of SER are saved in the <code>./output</code> folder by default. Examples of results are as follows.</p> <p></p>"},{"location":"en/version2.x/ppstructure/model_train/train_kie.html#42-quick-start","title":"4.2 Quick start","text":"<p>Here we use XFUND dataset to quickly experience the SER model and RE model.</p>"},{"location":"en/version2.x/ppstructure/model_train/train_kie.html#421-prepare-for-the-dataset","title":"4.2.1 Prepare for the dataset","text":"<pre><code>mkdir train_data\ncd train_data\n# download and uncompress the dataset\nwget https://paddleocr.bj.bcebos.com/ppstructure/dataset/XFUND.tar &amp;&amp; tar -xf XFUND.tar\ncd ..\n</code></pre>"},{"location":"en/version2.x/ppstructure/model_train/train_kie.html#422-predict-images-using-the-trained-model","title":"4.2.2 Predict images using the trained model","text":"<p>Use the following command to download the models.</p> <pre><code>mkdir pretrained_model\ncd pretrained_model\n# download and uncompress the SER trained model\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/ser_vi_layoutxlm_xfund_pretrained.tar &amp;&amp; tar -xf ser_vi_layoutxlm_xfund_pretrained.tar\n\n# download and uncompress the RE trained model\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/re_vi_layoutxlm_xfund_pretrained.tar &amp;&amp; tar -xf re_vi_layoutxlm_xfund_pretrained.tar\n</code></pre> <p>If you want to use OCR engine to obtain end-to-end prediction results, you can use the following command to predict.</p> <pre><code># just predict using SER trained model\npython3 tools/infer_kie_token_ser.py \\\n  -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml \\\n  -o Architecture.Backbone.checkpoints=./pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy \\\n  Global.infer_img=./ppstructure/docs/kie/input/zh_val_42.jpg\n\n# predict using SER and RE trained model at the same time\npython3 ./tools/infer_kie_token_ser_re.py \\\n  -c configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml \\\n  -o Architecture.Backbone.checkpoints=./pretrained_model/re_vi_layoutxlm_xfund_pretrained/best_accuracy \\\n  Global.infer_img=./train_data/XFUND/zh_val/image/zh_val_42.jpg \\\n  -c_ser configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml \\\n  -o_ser Architecture.Backbone.checkpoints=./pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy\n</code></pre> <p>The visual result images and the predicted text file will be saved in the <code>Global.save_res_path</code> directory.</p> <p>If you want to use a custom ocr model, you can set it through the following fields</p> <ul> <li><code>Global.kie_det_model_dir</code>: the detection inference model path</li> <li><code>Global.kie_rec_model_dir</code>: the recognition inference model path</li> </ul> <p>If you want to load the text detection and recognition results collected before, you can use the following command to predict.</p> <pre><code># just predict using SER trained model\npython3 tools/infer_kie_token_ser.py \\\n  -c configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml \\\n  -o Architecture.Backbone.checkpoints=./pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy \\\n  Global.infer_img=./train_data/XFUND/zh_val/val.json \\\n  Global.infer_mode=False\n\n# predict using SER and RE trained model at the same time\npython3 ./tools/infer_kie_token_ser_re.py \\\n  -c configs/kie/vi_layoutxlm/re_vi_layoutxlm_xfund_zh.yml \\\n  -o Architecture.Backbone.checkpoints=./pretrained_model/re_vi_layoutxlm_xfund_pretrained/best_accuracy \\\n  Global.infer_img=./train_data/XFUND/zh_val/val.json \\\n  Global.infer_mode=False \\\n  -c_ser configs/kie/vi_layoutxlm/ser_vi_layoutxlm_xfund_zh.yml \\\n  -o_ser Architecture.Backbone.checkpoints=./pretrained_model/ser_vi_layoutxlm_xfund_pretrained/best_accuracy\n</code></pre>"},{"location":"en/version2.x/ppstructure/model_train/train_kie.html#423-inference-using-paddleinference","title":"4.2.3 Inference using PaddleInference","text":"<p>Firstly, download the inference SER inference model.</p> <pre><code>mkdir inference\ncd inference\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/ser_vi_layoutxlm_xfund_infer.tar &amp;&amp; tar -xf ser_vi_layoutxlm_xfund_infer.tar\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/vi_layoutxlm/re_vi_layoutxlm_xfund_infer.tar &amp;&amp; tar -xf re_vi_layoutxlm_xfund_infer.tar\ncd ..\n</code></pre> <ul> <li>SER</li> </ul> <p>Use the following command for inference.</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser.py \\\n  --kie_algorithm=LayoutXLM \\\n  --ser_model_dir=../inference/ser_vi_layoutxlm_xfund_infer \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\"\n</code></pre> <p>The visual results and text file will be saved in directory <code>output</code>.</p> <ul> <li>RE</li> </ul> <p>Use the following command for inference.</p> <pre><code>cd ppstructure\npython3 kie/predict_kie_token_ser_re.py \\\n  --kie_algorithm=LayoutXLM \\\n  --re_model_dir=../inference/re_vi_layoutxlm_xfund_infer \\\n  --ser_model_dir=../inference/ser_vi_layoutxlm_xfund_infer \\\n  --use_visual_backbone=False \\\n  --image_dir=./docs/kie/input/zh_val_42.jpg \\\n  --ser_dict_path=../train_data/XFUND/class_list_xfun.txt \\\n  --vis_font_path=../doc/fonts/simfang.ttf \\\n  --ocr_order_method=\"tb-yx\"\n</code></pre> <p>The visual results and text file will be saved in directory <code>output</code>.</p> <p>If you want to use a custom ocr model, you can set it through the following fields</p> <ul> <li><code>--det_model_dir</code>: the detection inference model path</li> <li><code>--rec_model_dir</code>: the recognition inference model path</li> </ul>"},{"location":"en/version2.x/ppstructure/model_train/train_kie.html#43-more","title":"4.3 More","text":"<p>For training, evaluation and inference tutorial for KIE models, please refer to KIE doc.</p> <p>For training, evaluation and inference tutorial for text detection models, please refer to text detection doc.</p> <p>For training, evaluation and inference tutorial for text recognition models, please refer to text recognition doc.</p> <p>To complete the key information extraction task in your own scenario from data preparation to model selection, please refer to: Guide to End-to-end KIE\u3002</p>"},{"location":"en/version2.x/ppstructure/model_train/train_kie.html#5-reference","title":"5. Reference","text":"<ul> <li>LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding, https://arxiv.org/pdf/2104.08836.pdf</li> <li>microsoft/unilm/layoutxlm, https://github.com/microsoft/unilm/tree/master/layoutxlm</li> <li>XFUND dataset, https://github.com/doc-analysis/XFUND</li> </ul>"},{"location":"en/version2.x/ppstructure/model_train/train_kie.html#6-license","title":"6. License","text":"<p>The content of this project itself is licensed under the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</p>"},{"location":"en/version2.x/ppstructure/model_train/train_layout.html","title":"Layout analysis","text":""},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#1-introduction","title":"1. Introduction","text":"<p>Layout analysis refers to the regional division of documents in the form of pictures and the positioning of key areas, such as text, title, table, picture, etc. The layout analysis algorithm is based on the lightweight model PP-picodet of PaddleDetection, including English layout analysis, Chinese layout analysis and table layout analysis models.  English layout analysis models can detect document layout elements such as text, title, table, figure, list. Chinese layout analysis models can detect document layout elements such as text, figure, figure caption, table, table caption, header, footer, reference, and equation. Table layout analysis models can detect table regions.</p> <p></p>"},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#2-quick-start","title":"2. Quick start","text":"<p>PP-Structure currently provides layout analysis models in Chinese, English and table documents. For the model link, see models_list. The whl package is also provided for quick use, see quickstart for details.</p>"},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#3-install","title":"3. Install","text":""},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#31-install-paddlepaddle","title":"3.1. Install PaddlePaddle","text":"<ul> <li>\uff081) Install PaddlePaddle</li> </ul> <pre><code>python3 -m pip install --upgrade pip\n\n# GPU Install\npython3 -m pip install \"paddlepaddle-gpu&gt;=2.3\" -i https://mirror.baidu.com/pypi/simple\n\n# CPU Install\npython3 -m pip install \"paddlepaddle&gt;=2.3\" -i https://mirror.baidu.com/pypi/simple\n</code></pre> <p>For more requirements, please refer to the instructions in the Install file\u3002</p>"},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#32-install-paddledetection","title":"3.2. Install PaddleDetection","text":"<ul> <li>\uff081\uff09Download PaddleDetection Source code</li> </ul> <pre><code>git clone https://github.com/PaddlePaddle/PaddleDetection.git\n</code></pre> <ul> <li>\uff082\uff09Install third-party libraries</li> </ul> <pre><code>cd PaddleDetection\npython3 -m pip install -r requirements.txt\n</code></pre>"},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#4-data-preparation","title":"4. Data preparation","text":"<p>If you want to experience the prediction process directly, you can skip data preparation and download the pre-training model.</p>"},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#41-english-data-set","title":"4.1. English data set","text":"<p>Download document analysis data set PubLayNet\uff08Dataset 96G\uff09\uff0ccontains 5 classes\uff1a<code>{0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"}</code></p> <pre><code># Download data\nwget https://dax-cdn.cdn.appdomain.cloud/dax-publaynet/1.0.0/publaynet.tar.gz\n# Decompress data\ntar -xvf publaynet.tar.gz\n</code></pre> <p>Uncompressed directory structure\uff1a</p> <pre><code>|-publaynet\n  |- test\n     |- PMC1277013_00004.jpg\n     |- PMC1291385_00002.jpg\n     | ...\n  |- train.json\n  |- train\n     |- PMC1291385_00002.jpg\n     |- PMC1277013_00004.jpg\n     | ...\n  |- val.json\n  |- val\n     |- PMC538274_00004.jpg\n     |- PMC539300_00004.jpg\n     | ...\n</code></pre> <p>data distribution\uff1a</p> File or Folder Description num <code>train/</code> Training set pictures 335,703 <code>val/</code> Verification set pictures 11,245 <code>test/</code> Test set pictures 11,405 <code>train.json</code> Training set annotation files - <code>val.json</code> Validation set dimension files - <p>Data Annotation</p> <p>The JSON file contains the annotations of all images, and the data is stored in a dictionary nested manner.Contains the following keys\uff1a</p> <ul> <li> <p>info\uff0crepresents the dimension file info\u3002</p> </li> <li> <p>licenses\uff0crepresents the dimension file licenses\u3002</p> </li> <li> <p>images\uff0crepresents the list of image information in the annotation file\uff0ceach element is the information of an image\u3002The information of one of the images is as follows:</p> </li> </ul> <pre><code>{\n    'file_name': 'PMC4055390_00006.jpg',    # file_name\n    'height': 601,                      # image height\n    'width': 792,                       # image width\n    'id': 341427                        # image id\n}\n</code></pre> <ul> <li>annotations\uff0c represents the list of annotation information of the target object in the annotation file\uff0ceach element is the annotation information of a target object\u3002The following is the annotation information of one of the target objects:</li> </ul> <pre><code>{\n\n    'segmentation':             # Segmentation annotation of objects\n    'area': 60518.099043117836, # Area of object\n    'iscrowd': 0,               # iscrowd\n    'image_id': 341427,         # image id\n    'bbox': [50.58, 490.86, 240.15, 252.16], # bbox [x1,y1,w,h]\n    'category_id': 1,           # category_id\n    'id': 3322348               # image id\n}\n</code></pre>"},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#42-more-datasets","title":"4.2. More datasets","text":"<p>We provide CDLA(Chinese layout analysis), TableBank(Table layout analysis)etc. data set download links\uff0cprocess to the JSON format of the above annotation file\uff0cthat is, the training can be conducted in the same way\u3002</p> dataset \u200b\u7b80\u4ecb\u200b cTDaR2019_cTDaR For form detection (TRACKA) and form identification (TRACKB).Image types include historical data sets (beginning with cTDaR_t0, such as CTDAR_T00872.jpg) and modern data sets (beginning with cTDaR_t1, CTDAR_T10482.jpg). IIIT-AR-13K Data sets constructed by manually annotating figures or pages from publicly available annual reports, containing 5 categories:table, figure, natural image, logo, and signature. TableBank For table detection and recognition of large datasets, including Word and Latex document formats CDLA Chinese document layout analysis data set, for Chinese literature (paper) scenarios, including 10 categories:Text, Title, Figure, Figure caption, Table, Table caption, Header, Footer, Reference, Equation DocBank Large-scale dataset (500K document pages) constructed using weakly supervised methods for document layout analysis, containing 12 categories:Author, Caption, Date, Equation, Figure, Footer, List, Paragraph, Reference, Section, Table, Title"},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#5-start-training","title":"5. Start training","text":"<p>Training scripts, evaluation scripts, and prediction scripts are provided, and the PubLayNet pre-training model is used as an example in this section.</p> <p>If you do not want training and directly experience the following process of model evaluation, prediction, motion to static, and inference, you can download the provided pre-trained model (PubLayNet dataset) and skip this part.</p> <pre><code>mkdir pretrained_model\ncd pretrained_model\n# Download PubLayNet pre-training model\uff08Direct experience model evaluates, predicts, and turns static\uff09\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_fgd_layout.pdparams\n# Download the PubLaynet inference model\uff08Direct experience model reasoning\uff09\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_fgd_layout_infer.tar\n</code></pre> <p>If the test image is Chinese, the pre-trained model of Chinese CDLA dataset can be downloaded to identify 10 types of document regions\uff1aTable, Figure, Figure caption, Table, Table caption, Header, Footer, Reference, Equation\uff0cDownload the training model and inference model of Model 'picodet_lcnet_x1_0_fgd_layout_cdla' in layout analysis model\u3002If only the table area in the image is detected, you can download the pre-trained model of the table dataset, and download the training model and inference model of the 'picodet_LCnet_x1_0_FGd_layout_table' model in Layout Analysis model</p>"},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#51-train","title":"5.1. Train","text":"<p>Start training with the PaddleDetection layout analysis profile</p> <ul> <li>Modify Profile</li> </ul> <p>If you want to train your own data set, you need to modify the data configuration and the number of categories in the configuration file.</p> <p>Using 'configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml' as an example, the change is as follows:</p> <pre><code>metric: COCO\n# Number of categories\nnum_classes: 5\n\nTrainDataset:\n  !COCODataSet\n    # Modify to your own training data directory\n    image_dir: train\n    # Modify to your own training data label file\n    anno_path: train.json\n    # Modify to your own training data root directory\n    dataset_dir: /root/publaynet/\n    data_fields: ['image', 'gt_bbox', 'gt_class', 'is_crowd']\n\nEvalDataset:\n  !COCODataSet\n    # Modify to your own validation data directory\n    image_dir: val\n    # Modify to your own validation data label file\n    anno_path: val.json\n    # Modify to your own validation data root\n    dataset_dir: /root/publaynet/\n\nTestDataset:\n  !ImageFolder\n    # Modify to your own test data label file\n    anno_path: /root/publaynet/val.json\n</code></pre> <ul> <li>Start training. During training, PP picodet pre training model will be downloaded by default. There is no need to download in advance.</li> </ul> <pre><code># GPU training supports single-card and multi-card training\n# The training log is automatically saved to the log directory\n\n# Single card training\nexport CUDA_VISIBLE_DEVICES=0\npython3 tools/train.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    --eval\n\n# Multi-card training, with the -- GPUS parameter specifying the card number\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\npython3 -m paddle.distributed.launch --gpus '0,1,2,3'  tools/train.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    --eval\n</code></pre> <p>Attention\uff1aIf the video memory is out during training, adjust Batch_size in TrainReader and base_LR in LearningRate. The published config is obtained by 8-card training. If the number of GPU cards is changed to 1, then the base_LR needs to be reduced by 8 times.</p> <p>After starting training normally, you will see the following log output:</p> <pre><code>[08/15 04:02:30] ppdet.utils.checkpoint INFO: Finish loading model weights: /root/.cache/paddle/weights/LCNet_x1_0_pretrained.pdparams\n[08/15 04:02:46] ppdet.engine INFO: Epoch: [0] [   0/1929] learning_rate: 0.040000 loss_vfl: 1.216707 loss_bbox: 1.142163 loss_dfl: 0.544196 loss: 2.903065 eta: 17 days, 13:50:26 batch_cost: 15.7452 data_cost: 2.9112 ips: 1.5243 images/s\n[08/15 04:03:19] ppdet.engine INFO: Epoch: [0] [  20/1929] learning_rate: 0.064000 loss_vfl: 1.180627 loss_bbox: 0.939552 loss_dfl: 0.442436 loss: 2.628206 eta: 2 days, 12:18:53 batch_cost: 1.5770 data_cost: 0.0008 ips: 15.2184 images/s\n[08/15 04:03:47] ppdet.engine INFO: Epoch: [0] [  40/1929] learning_rate: 0.088000 loss_vfl: 0.543321 loss_bbox: 1.071401 loss_dfl: 0.457817 loss: 2.057003 eta: 2 days, 0:07:03 batch_cost: 1.3190 data_cost: 0.0007 ips: 18.1954 images/s\n[08/15 04:04:12] ppdet.engine INFO: Epoch: [0] [  60/1929] learning_rate: 0.112000 loss_vfl: 0.630989 loss_bbox: 0.859183 loss_dfl: 0.384702 loss: 1.883143 eta: 1 day, 19:01:29 batch_cost: 1.2177 data_cost: 0.0006 ips: 19.7087 images/s\n</code></pre> <ul> <li><code>--eval</code> indicates that the best model is saved as <code>output/picodet_lcnet_x1_0_layout/best_accuracy</code>  by default during the evaluation process \u3002</li> </ul> <p>Note that the configuration file for prediction / evaluation must be consistent with the training.</p>"},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#52-fgd-distillation-training","title":"5.2. FGD Distillation Training","text":"<p>PaddleDetection supports FGD-based Focal and Global Knowledge Distillation for Detectors  The training process of the target detection model of distillation, FGD distillation is divided into two parts <code>Focal</code> and <code>Global</code>.     <code>Focal</code> Distillation separates the foreground and background of the image, allowing the student model to focus on the key pixels of the foreground and background features of the teacher model respectively;<code>Global</code>Distillation section reconstructs the relationships between different pixels and transfers them from the teacher to the student to compensate for the global information lost in <code>Focal</code>Distillation.</p> <p>Change the dataset and modify the data configuration and number of categories in the [TODO] configuration, referring to 4.1. Start training:</p> <pre><code># Single Card Training\nexport CUDA_VISIBLE_DEVICES=0\npython3 tools/train.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    --slim_config configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x2_5_layout.yml \\\n    --eval\n</code></pre> <ul> <li><code>-c</code>: Specify the model configuration file.</li> <li><code>--slim_config</code>:  Specify the compression policy profile.</li> </ul>"},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#6-model-evaluation-and-prediction","title":"6. Model evaluation and prediction","text":""},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#61-indicator-evaluation","title":"6.1. Indicator evaluation","text":"<p>Model parameters in training are saved by default in <code>output/picodet_ Lcnet_ X1_ 0_ Under the layout</code> directory. When evaluating indicators, you need to set <code>weights</code> to point to the saved parameter file.Assessment datasets can be accessed via <code>configs/picodet/legacy_ Model/application/layout_ Analysis/picodet_ Lcnet_ X1_ 0_ Layout. Yml</code> . Modify <code>EvalDataset</code>  : <code>img_dir</code>,<code>anno_ Path</code>and<code>dataset_dir</code> setting.</p> <pre><code># GPU evaluation, weights as weights to be measured\npython3 tools/eval.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    -o weights=./output/picodet_lcnet_x1_0_layout/best_model\n</code></pre> <p>The following information will be printed out, such as mAP, AP0.5, etc.</p> <pre><code> Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.935\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.956\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.404\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.782\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.969\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.539\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.938\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.949\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.495\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.818\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.978\n[08/15 07:07:09] ppdet.engine INFO: Total sample number: 11245, averge FPS: 24.405059207157436\n[08/15 07:07:09] ppdet.engine INFO: Best test bbox ap is 0.935.\n</code></pre> <p>If you use the provided pre-training model for evaluation or the FGD distillation training model, replace the <code>weights</code> model path and execute the following command for evaluation:</p> <pre><code>python3 tools/eval.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    --slim_config configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x2_5_layout.yml \\\n    -o weights=output/picodet_lcnet_x2_5_layout/best_model\n</code></pre> <ul> <li><code>-c</code>: Specify the model configuration file.</li> <li><code>--slim_config</code>:  Specify the distillation policy profile.</li> <li><code>-o weights</code>: Specify the model path trained by the distillation algorithm.</li> </ul>"},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#62-test-layout-analysis-results","title":"6.2. Test Layout Analysis Results","text":"<p>The profile predicted to be used must be consistent with the training, for example, if you pass <code>python3 tools/train'. Py-c configs/picodet/legacy_ Model/application/layout_ Analysis/picodet_ Lcnet_ X1_ 0_ Layout. Yml</code> completed the training process for the model.</p> <p>With  trained PaddleDetection model, you can use the following commands to make model predictions.</p> <pre><code>python3 tools/infer.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    -o weights='output/picodet_lcnet_x1_0_layout/best_model.pdparams' \\\n    --infer_img='docs/images/layout.jpg' \\\n    --output_dir=output_dir/ \\\n    --draw_threshold=0.5\n</code></pre> <ul> <li><code>--infer_img</code>:  Reasoning for a single picture can also be done via <code>--infer_ Dir</code>Inform all pictures in the file.</li> <li><code>--output_dir</code>:  Specify the path to save the visualization results.</li> <li><code>--draw_threshold</code>:Specify the NMS threshold for drawing the result box.</li> </ul> <p>If you use the provided pre-training model for prediction or the FGD distillation training model, change the <code>weights</code> model path and execute the following command to make the prediction:</p> <pre><code>python3 tools/infer.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    --slim_config configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x2_5_layout.yml \\\n    -o weights='output/picodet_lcnet_x2_5_layout/best_model.pdparams' \\\n    --infer_img='docs/images/layout.jpg' \\\n    --output_dir=output_dir/ \\\n    --draw_threshold=0.5\n</code></pre>"},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#7-model-export-and-inference","title":"7. Model Export and Inference","text":""},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#71-model-export","title":"7.1 Model Export","text":"<p>The inference model (the model saved by <code>paddle.jit.save</code>) is generally a solidified model saved after the model training is completed, and is mostly used to give prediction in deployment.</p> <p>The model saved during the training process is the checkpoints model, which saves the parameters of the model and is mostly used to resume training.</p> <p>Compared with the checkpoints model, the inference model will additionally save the structural information of the model. Therefore, it is easier to deploy because the model structure and model parameters are already solidified in the inference model file, and is suitable for integration with actual systems.</p> <p>Layout analysis model to inference model steps are as follows\uff1a</p> <pre><code>python3 tools/export_model.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    -o weights=output/picodet_lcnet_x1_0_layout/best_model \\\n    --output_dir=output_inference/\n</code></pre> <ul> <li>If no post-export processing is required, specify\uff1a<code>-o export.benchmark=True</code>\uff08If -o already exists, delete -o here\uff09</li> <li>If you do not need to export NMS, specify\uff1a<code>-o export.nms=False</code></li> </ul> <p>After successful conversion, there are three files in the directory:</p> <pre><code>output_inference/picodet_lcnet_x1_0_layout/\n    \u251c\u2500\u2500 model.pdiparams         # inference Parameter file for model\n    \u251c\u2500\u2500 model.pdiparams.info    # inference Model parameter information, ignorable\n    \u2514\u2500\u2500 model.pdmodel           # inference Model Structure File for Model\n</code></pre> <p>If you change the <code>weights</code> model path using the provided pre-training model to the Inference model, or using the FGD distillation training model, the model to inference model steps are as follows:</p> <pre><code>python3 tools/export_model.py \\\n    -c configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x1_0_layout.yml \\\n    --slim_config configs/picodet/legacy_model/application/layout_analysis/picodet_lcnet_x2_5_layout.yml \\\n    -o weights=./output/picodet_lcnet_x2_5_layout/best_model \\\n    --output_dir=output_inference/\n</code></pre>"},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#72-model-inference","title":"7.2 Model inference","text":"<p>Replace model_with the provided inference training model for inference or the FGD distillation training <code>model_dir</code>Inference model path, execute the following commands for inference:</p> <pre><code>python3 deploy/python/infer.py \\\n    --model_dir=output_inference/picodet_lcnet_x1_0_layout/ \\\n    --image_file=docs/images/layout.jpg \\\n    --device=CPU\n</code></pre> <ul> <li>--device\uff1aSpecify the GPU or CPU device</li> </ul> <p>When model inference is complete, you will see the following log output:</p> <pre><code>------------------------------------------\n-----------  Model Configuration -----------\nModel Arch: PicoDet\nTransform Order:\n--transform op: Resize\n--transform op: NormalizeImage\n--transform op: Permute\n--transform op: PadStride\n--------------------------------------------\nclass_id:0, confidence:0.9921, left_top:[20.18,35.66],right_bottom:[341.58,600.99]\nclass_id:0, confidence:0.9914, left_top:[19.77,611.42],right_bottom:[341.48,901.82]\nclass_id:0, confidence:0.9904, left_top:[369.36,375.10],right_bottom:[691.29,600.59]\nclass_id:0, confidence:0.9835, left_top:[369.60,608.60],right_bottom:[691.38,736.72]\nclass_id:0, confidence:0.9830, left_top:[369.58,805.38],right_bottom:[690.97,901.80]\nclass_id:0, confidence:0.9716, left_top:[383.68,271.44],right_bottom:[688.93,335.39]\nclass_id:0, confidence:0.9452, left_top:[370.82,34.48],right_bottom:[688.10,63.54]\nclass_id:1, confidence:0.8712, left_top:[370.84,771.03],right_bottom:[519.30,789.13]\nclass_id:3, confidence:0.9856, left_top:[371.28,67.85],right_bottom:[685.73,267.72]\nsave result to: output/layout.jpg\nTest iter 0\n------------------ Inference Time Info ----------------------\ntotal_time(ms): 2196.0, img_num: 1\naverage latency time(ms): 2196.00, QPS: 0.455373\npreprocess_time(ms): 2172.50, inference_time(ms): 11.90, postprocess_time(ms): 11.60\n</code></pre> <ul> <li>Model\uff1amodel structure</li> <li>Transform Order\uff1aPreprocessing operation</li> <li>class_id, confidence, left_top, right_bottom\uff1aIndicates category id, confidence level, upper left coordinate, lower right coordinate, respectively</li> <li>save result to\uff1aSave path of visual layout analysis results, default save to ./output folder</li> <li>inference time info\uff1aInference time, where preprocess_time represents the preprocessing time, Inference_time represents the model prediction time, and postprocess_time represents the post-processing time</li> </ul> <p>The result of visualization layout is shown in the following figure</p> <p></p>"},{"location":"en/version2.x/ppstructure/model_train/train_layout.html#citations","title":"Citations","text":"<pre><code>@inproceedings{zhong2019publaynet,\n  title={PubLayNet: largest dataset ever for document layout analysis},\n  author={Zhong, Xu and Tang, Jianbin and Yepes, Antonio Jimeno},\n  booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},\n  year={2019},\n  volume={},\n  number={},\n  pages={1015-1022},\n  doi={10.1109/ICDAR.2019.00166},\n  ISSN={1520-5363},\n  month={Sep.},\n  organization={IEEE}\n}\n\n@inproceedings{yang2022focal,\n  title={Focal and global knowledge distillation for detectors},\n  author={Yang, Zhendong and Li, Zhe and Jiang, Xiaohu and Gong, Yuan and Yuan, Zehuan and Zhao, Danpei and Yuan, Chun},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={4643--4652},\n  year={2022}\n}\n</code></pre>"},{"location":"en/version2.x/ppstructure/model_train/train_table.html","title":"Table Recognition","text":""},{"location":"en/version2.x/ppstructure/model_train/train_table.html#1-pipeline","title":"1. pipeline","text":"<p>The table recognition mainly contains three models</p> <ol> <li>Single line text detection-DB</li> <li>Single line text recognition-CRNN</li> <li>Table structure and cell coordinate prediction-SLANet</li> </ol> <p>The table recognition flow chart is as follows</p> <p></p> <ol> <li>The coordinates of single-line text is detected by DB model, and then sends it to the recognition model to get the recognition result.</li> <li>The table structure and cell coordinates is predicted by SLANet model.</li> <li>The recognition result of the cell is combined by the coordinates, recognition result of the single line and the coordinates of the cell.</li> <li>The cell recognition result and the table structure together construct the html string of the table.</li> </ol>"},{"location":"en/version2.x/ppstructure/model_train/train_table.html#2-performance","title":"2. Performance","text":"<p>We evaluated the algorithm on the PubTabNet<sup>[1]</sup> eval dataset, and the performance is as follows:</p> Method Acc TEDS(Tree-Edit-Distance-based Similarity) Speed EDD<sup>[2]</sup> x 88.30% x TableRec-RARE(ours) 71.73% 93.88% 779ms SLANet(ours) 76.31% 95.89% 766ms <p>The performance indicators are explained as follows:</p> <ul> <li>Acc: The accuracy of the table structure in each image, a wrong token is considered an error.</li> <li>TEDS: The accuracy of the model's restoration of table information. This indicator evaluates not only the table structure, but also the text content in the table.</li> <li>Speed: The inference speed of a single image when the model runs on the CPU machine and MKL is enabled.</li> </ul>"},{"location":"en/version2.x/ppstructure/model_train/train_table.html#3-result","title":"3. Result","text":""},{"location":"en/version2.x/ppstructure/model_train/train_table.html#4-how-to-use","title":"4. How to use","text":""},{"location":"en/version2.x/ppstructure/model_train/train_table.html#41-quick-start","title":"4.1 Quick start","text":"<p>PP-Structure currently provides table recognition models in both Chinese and English. For the model link, see models_list. The whl package is also provided for quick use, see quickstart for details.</p> <p>The following takes the Chinese table recognition model as an example to introduce how to recognize a table.</p> <p>Use the following commands to quickly complete the identification of a table.</p> <pre><code>cd PaddleOCR/ppstructure\n\n# download model\nmkdir inference &amp;&amp; cd inference\n# Download the PP-OCRv3 text detection model and unzip it\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0.0/PP-OCRv3_mobile_det_infer.tar &amp;&amp; tar xf PP-OCRv3_mobile_det_infer.tar\n# Download the PP-OCRv3 text recognition model and unzip it\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0.0/PP-OCRv3_mobile_rec_infer.tar &amp;&amp; tar xf PP-OCRv3_mobile_rec_infer.tar\n# Download the PP-StructureV2 form recognition model and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/paddle3.0b2/ch_ppstructure_mobile_v2.0_SLANet_infer.tar &amp;&amp; tar xf ch_ppstructure_mobile_v2.0_SLANet_infer.tar\ncd ..\n# run\npython3 table/predict_table.py \\\n    --det_model_dir=inference/PP-OCRv3_mobile_det_infer \\\n    --rec_model_dir=inference/PP-OCRv3_mobile_rec_infer  \\\n    --table_model_dir=inference/ch_ppstructure_mobile_v2.0_SLANet_infer \\\n    --rec_char_dict_path=../ppocr/utils/ppocr_keys_v1.txt \\\n    --table_char_dict_path=../ppocr/utils/dict/table_structure_dict_ch.txt \\\n    --image_dir=docs/table/table.jpg \\\n    --output=../output/table\n</code></pre> <p>After the operation is completed, the excel table of each image will be saved to the directory specified by the output field, and an html file will be produced in the directory to visually view the cell coordinates and the recognized table.</p> <p>NOTE</p> <ol> <li>If you want to use the English table recognition model, you need to download the English text detection and recognition model and the English table recognition model in models_list, and replace <code>table_structure_dict_ch.txt</code> with <code>table_structure_dict.txt</code>.</li> <li>To use the TableRec-RARE model, you need to replace <code>table_structure_dict_ch.txt</code> with <code>table_structure_dict.txt</code>, and add parameter <code>--merge_no_span_structure=False</code></li> </ol>"},{"location":"en/version2.x/ppstructure/model_train/train_table.html#42-training-evaluation-and-inference","title":"4.2 Training, Evaluation and Inference","text":"<p>The training, evaluation and inference process of the text detection model can be referred to detection</p> <p>The training, evaluation and inference process of the text recognition model can be referred to recognition</p> <p>The training, evaluation and inference process of the table recognition model can be referred to table_recognition</p>"},{"location":"en/version2.x/ppstructure/model_train/train_table.html#43-calculate-teds","title":"4.3 Calculate TEDS","text":"<p>The table uses TEDS(Tree-Edit-Distance-based Similarity) as the evaluation metric of the model. Before the model evaluation, the three models in the pipeline need to be exported as inference models (we have provided them), and the gt for evaluation needs to be prepared. Examples of gt are as follows:</p> <pre><code>PMC5755158_010_01.png    &lt;html&gt;&lt;body&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;Weaning&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;Week 15&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;Off-test&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Weaning&lt;/td&gt;&lt;td&gt;\u2013&lt;/td&gt;&lt;td&gt;\u2013&lt;/td&gt;&lt;td&gt;\u2013&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Week 15&lt;/td&gt;&lt;td&gt;\u2013&lt;/td&gt;&lt;td&gt;0.17 \u00b1 0.08&lt;/td&gt;&lt;td&gt;0.16 \u00b1 0.03&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Off-test&lt;/td&gt;&lt;td&gt;\u2013&lt;/td&gt;&lt;td&gt;0.80 \u00b1 0.24&lt;/td&gt;&lt;td&gt;0.19 \u00b1 0.09&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;\n</code></pre> <p>Each line in gt consists of the file name and the html string of the table. The file name and the html string of the table are separated by <code>\\t</code>.</p> <p>You can also use the following command to generate an evaluation gt file from the annotation file:</p> <pre><code>python3 ppstructure/table/convert_label2html.py --ori_gt_path /path/to/your_label_file --save_path /path/to/save_file\n</code></pre> <p>Use the following command to evaluate. After the evaluation is completed, the teds indicator will be output.</p> <pre><code>python3 table/eval_table.py \\\n    --det_model_dir=path/to/det_model_dir \\\n    --rec_model_dir=path/to/rec_model_dir \\\n    --table_model_dir=path/to/table_model_dir \\\n    --image_dir=docs/table/table.jpg \\\n    --rec_char_dict_path=../ppocr/utils/dict/table_dict.txt \\\n    --table_char_dict_path=../ppocr/utils/dict/table_structure_dict.txt \\\n    --det_limit_side_len=736 \\\n    --det_limit_type=min \\\n    --gt_path=path/to/gt.txt\n</code></pre> <p>Evaluate on the PubLatNet dataset using the English model</p> <pre><code>cd PaddleOCR/ppstructure\n# Download the model\nmkdir inference &amp;&amp; cd inference\n# Download the text detection model trained on the PubTabNet dataset and unzip it\nwget https://paddleocr.bj.bcebos.com/dygraph_v2.0/table/en_ppocr_mobile_v2.0_table_det_infer.tar &amp;&amp; tar xf en_ppocr_mobile_v2.0_table_det_infer.tar\n# Download the text recognition model trained on the PubTabNet dataset and unzip it\nwget https://paddleocr.bj.bcebos.com/dygraph_v2.0/table/en_ppocr_mobile_v2.0_table_rec_infer.tar &amp;&amp; tar xf en_ppocr_mobile_v2.0_table_rec_infer.tar\n# Download the table recognition model trained on the PubTabNet dataset and unzip it\nwget https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/paddle3.0b2/en_ppstructure_mobile_v2.0_SLANet_infer.tar &amp;&amp; tar xf en_ppstructure_mobile_v2.0_SLANet_infer.tar\ncd ..\n\npython3 table/eval_table.py \\\n    --det_model_dir=inference/en_ppocr_mobile_v2.0_table_det_infer \\\n    --rec_model_dir=inference/en_ppocr_mobile_v2.0_table_rec_infer \\\n    --table_model_dir=inference/en_ppstructure_mobile_v2.0_SLANet_infer \\\n    --image_dir=train_data/table/pubtabnet/val/ \\\n    --rec_char_dict_path=../ppocr/utils/dict/table_dict.txt \\\n    --table_char_dict_path=../ppocr/utils/dict/table_structure_dict.txt \\\n    --det_limit_side_len=736 \\\n    --det_limit_type=min \\\n    --rec_image_shape=3,32,320 \\\n    --gt_path=path/to/gt.txt\n</code></pre> <p>output is</p> <pre><code>teds: 95.89\n</code></pre>"},{"location":"en/version2.x/ppstructure/model_train/train_table.html#5-reference","title":"5. Reference","text":"<ol> <li>https://github.com/ibm-aur-nlp/PubTabNet</li> <li>https://arxiv.org/pdf/1911.10683</li> </ol>"},{"location":"en/version2.x/ppstructure/model_train/training.html","title":"Model Training","text":"<p>This article will introduce the basic concepts that is necessary for model training and tuning.</p> <p>At the same time, it will briefly introduce the structure of the training data and how to prepare the data to fine-tune model in vertical scenes.</p>"},{"location":"en/version2.x/ppstructure/model_train/training.html#1-yml-configuration","title":"1. Yml Configuration","text":"<p>The PaddleOCR uses configuration files to control network training and evaluation parameters. In the configuration file, you can set the model, optimizer, loss function, and pre- and post-processing parameters of the model. PaddleOCR reads these parameters from the configuration file, and then builds a complete training process to train the model. Fine-tuning can also be completed by modifying the parameters in the configuration file, which is simple and convenient.</p> <p>For the complete configuration file description, please refer to Configuration File</p>"},{"location":"en/version2.x/ppstructure/model_train/training.html#2-basic-concepts","title":"2. Basic Concepts","text":"<p>During the model training process, some hyper-parameters can be manually specified to obtain the optimal result at the least cost. Different data volumes may require different hyper-parameters. When you want to fine-tune the model based on your own data, there are several parameter adjustment strategies for reference:</p>"},{"location":"en/version2.x/ppstructure/model_train/training.html#21-learning-rate","title":"2.1 Learning Rate","text":"<p>The learning rate is one of the most important hyper-parameters for training neural networks. It represents the step length of the gradient moving towards the optimal solution of the loss function in each iteration. A variety of learning rate update strategies are provided by PaddleOCR, which can be specified in configuration files. For example,</p> <pre><code>Optimizer:\n  ...\n  lr:\n    name: Piecewise\n    decay_epochs : [700, 800]\n    values : [0.001, 0.0001]\n    warmup_epoch: 5\n</code></pre> <p><code>Piecewise</code> stands for piece-wise constant attenuation. Different learning rates are specified in different learning stages, and the learning rate stay the same in each stage.</p> <p><code>warmup_epoch</code> means that in the first 5 epochs, the learning rate will be increased gradually from 0 to base_lr. For all strategies, please refer to the code learning_rate.py.</p>"},{"location":"en/version2.x/ppstructure/model_train/training.html#22-regularization","title":"2.2 Regularization","text":"<p>Regularization can effectively avoid algorithm over-fitting. PaddleOCR provides L1 and L2 regularization methods. L1 and L2 regularization are the most widely used regularization methods. L1 regularization adds a regularization term to the objective function to reduce the sum of absolute values of the parameters; while in L2 regularization, the purpose of adding a regularization term is to reduce the sum of squared parameters. The configuration method is as follows:</p> <pre><code>Optimizer:\n  ...\n  regularizer:\n    name: L2\n    factor: 2.0e-05\n</code></pre>"},{"location":"en/version2.x/ppstructure/model_train/training.html#23-evaluation-indicators","title":"2.3 Evaluation Indicators","text":"<p>(1) Detection stage: First, evaluate according to the IOU of the detection frame and the labeled frame. If the IOU is greater than a certain threshold, it is judged that the detection is accurate. Here, the detection frame and the label frame are different from the general general target detection frame, and they are represented by polygons. Detection accuracy: the percentage of the correct detection frame number in all detection frames is mainly used to judge the detection index. Detection recall rate: the percentage of correct detection frames in all marked frames, which is mainly an indicator of missed detection.</p> <p>(2) Recognition stage: Character recognition accuracy, that is, the ratio of correctly recognized text lines to the number of marked text lines. Only the entire line of text recognition pairs can be regarded as correct recognition.</p> <p>(3) End-to-end statistics: End-to-end recall rate: accurately detect and correctly identify the proportion of text lines in all labeled text lines; End-to-end accuracy rate: accurately detect and correctly identify the number of text lines in the detected text lines The standard for accurate detection is that the IOU of the detection box and the labeled box is greater than a certain threshold, and the text in the correctly identified detection box is the same as the labeled text.</p>"},{"location":"en/version2.x/ppstructure/model_train/training.html#3-data-and-vertical-scenes","title":"3. Data and Vertical Scenes","text":""},{"location":"en/version2.x/ppstructure/model_train/training.html#31-training-data","title":"3.1 Training Data","text":"<p>The current open source models, data sets and magnitudes are as follows:</p> <ul> <li>Detection:</li> <li>English data set, ICDAR2015</li> <li> <p>Chinese data set, LSVT street view data set training data 3w pictures</p> </li> <li> <p>Identification:</p> </li> <li>English data set, MJSynth and SynthText synthetic data, the data volume is tens of millions.</li> <li>Chinese data set, LSVT street view data set crops the image according to the truth value, and performs position calibration, a total of 30w images. In addition, based on the LSVT corpus, 500w of synthesized data.</li> <li>Small language data set, using different corpora and fonts, respectively generated 100w synthetic data set, and using ICDAR-MLT as the verification set.</li> </ul> <p>Among them, the public data sets are all open source, users can search and download by themselves, or refer to Chinese data set, synthetic data is not open source, users can use open source synthesis tools to synthesize by themselves. Synthesis tools include text_renderer, SynthText, TextRecognitionDataGenerator etc.</p>"},{"location":"en/version2.x/ppstructure/model_train/training.html#32-vertical-scene","title":"3.2 Vertical Scene","text":"<p>PaddleOCR mainly focuses on general OCR. If you have vertical requirements, you can use PaddleOCR + vertical data to train yourself; If there is a lack of labeled data, or if you do not want to invest in research and development costs, it is recommended to directly call the open API, which covers some of the more common vertical categories.</p>"},{"location":"en/version2.x/ppstructure/model_train/training.html#33-build-your-own-dataset","title":"3.3 Build Your Own Dataset","text":"<p>There are several experiences for reference when constructing the data set:</p> <p>(1) The amount of data in the training set:</p> <p>a. The data required for detection is relatively small. For Fine-tune based on the PaddleOCR model, 500 sheets are generally required to achieve good results.</p> <p>b. Recognition is divided into English and Chinese. Generally, English scenarios require hundreds of thousands of data to achieve good results, while Chinese requires several million or more.</p> <p>(2) When the amount of training data is small, you can try the following three ways to get more data:</p> <p>a. Manually collect more training data, the most direct and effective way.</p> <p>b. Basic image processing or transformation based on PIL and opencv. For example, the three modules of ImageFont, Image, ImageDraw in PIL write text into the background, opencv's rotating affine transformation, Gaussian filtering and so on.</p> <p>c. Use data generation algorithms to synthesize data, such as algorithms such as pix2pix.</p>"},{"location":"en/version2.x/ppstructure/model_train/training.html#4-faq","title":"4. FAQ","text":"<p>Q: How to choose a suitable network input shape when training CRNN recognition?</p> <pre><code>A: The general height is 32, the longest width is selected, there are two methods:\n\n(1) Calculate the aspect ratio distribution of training sample images. The selection of the maximum aspect ratio considers 80% of the training samples.\n\n(2) Count the number of texts in training samples. The selection of the longest number of characters considers the training sample that satisfies 80%. Then the aspect ratio of Chinese characters is approximately considered to be 1, and that of English is 3:1, and the longest width is estimated.\n</code></pre> <p>Q: During the recognition training, the accuracy of the training set has reached 90, but the accuracy of the verification set has been kept at 70, what should I do?</p> <pre><code>A: If the accuracy of the training set is 90 and the test set is more than 70, it should be over-fitting. There are two methods to try:\n\n(1) Add more augmentation methods or increase the [probability] of augmented prob (https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/ppocr/data/imaug/rec_img_aug.py#L341), The default is 0.4.\n\n(2) Increase the [l2 dcay value] of the system (https://github.com/PaddlePaddle/PaddleOCR/blob/a501603d54ff5513fc4fc760319472e59da25424/configs/rec/ch_ppocr_v1.1/rec_chinese_lite_train_v1.1.yml#L47)\n</code></pre> <p>Q: When the recognition model is trained, loss can drop normally, but acc is always 0</p> <pre><code>A: It is normal for the acc to be 0 at the beginning of the recognition model training, and the indicator will come up after a longer training period.\n</code></pre> <p>Click the following links for detailed training tutorial:</p> <ul> <li>text detection model training</li> <li>text recognition model training</li> <li>text direction classification model training</li> </ul>"},{"location":"en/version3.x/installation.html","title":"Installation","text":""},{"location":"en/version3.x/installation.html#1-install-paddlepaddle-framework","title":"1. Install PaddlePaddle Framework","text":"<p>Please refer to the PaddlePaddle Official Website to install PaddlePaddle framework version <code>3.0</code> or above. Using the official PaddlePaddle Docker image is recommended.</p>"},{"location":"en/version3.x/installation.html#2-install-paddleocr","title":"2. Install PaddleOCR","text":"<p>If you only want to use the inference capabilities of PaddleOCR, please refer to Install Inference Package; if you want to perform model training, exporting, etc., please refer to Install Training Dependencies. It is allowed to install both the inference package and training dependencies in the same environment without the need for environment isolation.</p>"},{"location":"en/version3.x/installation.html#21-install-inference-package","title":"2.1 Install Inference Package","text":"<p>Install the latest version of the PaddleOCR inference package from PyPI:</p> <pre><code>python -m pip install paddleocr\n</code></pre> <p>Or install from source (default is the development branch):</p> <pre><code>python -m pip install \"git+https://github.com/PaddlePaddle/PaddleOCR.git\"\n</code></pre>"},{"location":"en/version3.x/installation.html#22-install-training-dependencies","title":"2.2 Install Training Dependencies","text":"<p>To perform model training, exporting, etc., first clone the repository to your local machine:</p> <pre><code># Recommended method\ngit clone https://github.com/PaddlePaddle/PaddleOCR\n\n# (Optional) Switch to a specific branch\ngit checkout release/3.0\n\n# If you encounter network issues preventing successful cloning, you can also use the repository on Gitee:\ngit clone https://gitee.com/paddlepaddle/PaddleOCR\n\n# Note: The code hosted on Gitee may not be synchronized in real-time with updates from this GitHub project, with a delay of 3~5 days. Please prioritize using the recommended method.\n</code></pre> <p>Run the following command to install the dependencies:</p> <pre><code>python -m pip install -r requirements.txt\n</code></pre>"},{"location":"en/version3.x/logging.html","title":"Logging","text":"<p>This document mainly introduces how to configure the logging system for the PaddleOCR inference package. It's important to note that PaddleOCR's inference package uses a different logging system than the training scripts, and this document does not cover the configuration of the logging system used in the training scripts.</p> <p>PaddleOCR has built a centralized logging system based on Python's <code>logging</code> standard library. In other words, PaddleOCR uses a single logger, which can be accessed and configured via <code>paddleocr.logger</code>.</p> <p>By default, the logging level in PaddleOCR is set to <code>ERROR</code>, meaning that log messages will only be output if their level is <code>ERROR</code> or higher (e.g., <code>CRITICAL</code>). PaddleOCR also configures a <code>StreamHandler</code> for this logger, which outputs logs to the standard error stream, and sets the logger's <code>propagate</code> attribute to <code>False</code> to prevent log messages from being passed to its parent logger.</p> <p>If you wish to disable PaddleOCR's automatic logging configuration behavior, you can set the environment variable <code>DISABLE_AUTO_LOGGING_CONFIG</code> to <code>1</code>. In this case, PaddleOCR will not perform any additional configuration of the logger.</p> <p>For more flexible customization of logging behavior, refer to the relevant documentation of the <code>logging</code> standard library. Below is an example of writing logs to a file:</p> <pre><code>import logging\nfrom paddleocr import logger\n\n# Write logs to the file `paddleocr.log`\nfh = logging.FileHandler(\"paddleocr.log\")\nlogger.addHandler(fh)\n</code></pre> <p>Please note that other libraries that PaddleOCR depends on (such as PaddleX) have their own independent logging systems, and the above configuration will not affect the log output of these libraries.</p>"},{"location":"en/version3.x/model_list.html","title":"PaddleOCR\u200b\u6a21\u578b\u200b\u5217\u8868\u200b\uff08CPU/GPU\uff09","text":"<p>PaddleOCR \u200b\u5185\u7f6e\u200b\u4e86\u200b\u591a\u6761\u200b\u4ea7\u7ebf\u200b\uff0c\u200b\u6bcf\u6761\u200b\u4ea7\u7ebf\u200b\u90fd\u200b\u5305\u542b\u200b\u4e86\u200b\u82e5\u5e72\u200b\u6a21\u5757\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u6a21\u5757\u200b\u5305\u542b\u200b\u82e5\u5e72\u200b\u6a21\u578b\u200b\uff0c\u200b\u5177\u4f53\u200b\u4f7f\u7528\u200b\u54ea\u4e9b\u200b\u6a21\u578b\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u4e0b\u8fb9\u200b\u7684\u200b benchmark \u200b\u6570\u636e\u200b\u6765\u200b\u9009\u62e9\u200b\u3002\u200b\u5982\u200b\u60a8\u200b\u66f4\u200b\u8003\u8651\u200b\u6a21\u578b\u200b\u7cbe\u5ea6\u200b\uff0c\u200b\u8bf7\u200b\u9009\u62e9\u200b\u7cbe\u5ea6\u200b\u8f83\u200b\u9ad8\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5982\u200b\u60a8\u200b\u66f4\u200b\u8003\u8651\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\uff0c\u200b\u8bf7\u200b\u9009\u62e9\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u8f83\u200b\u5feb\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5982\u200b\u60a8\u200b\u66f4\u200b\u8003\u8651\u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff0c\u200b\u8bf7\u200b\u9009\u62e9\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p>"},{"location":"en/version3.x/model_list.html#_1","title":"\u6587\u672c\u200b\u68c0\u6d4b\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b \u200b\u68c0\u6d4b\u200bHmean\uff08%\uff09 GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08M) yaml \u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b PP-OCRv5_server_det - - / - - / - 101 PP-OCRv5_server_det.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-OCRv5_mobile_det - - / - - / - 20 PP-OCRv5_mobile_det.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-OCRv4_server_det 82.56 83.34 / 80.91 442.58 / 442.58 109 PP-OCRv4_server_det.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-OCRv4_mobile_det 77.35 8.79 / 3.13 51.00 / 28.58 4.7 PP-OCRv4_mobile_det.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-OCRv3_mobile_det 78.68 8.44 / 2.91 27.87 / 27.87 2.1 PP-OCRv3_mobile_det.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-OCRv3_server_det 80.11 65.41 / 13.67 305.07 / 305.07 102.1 PP-OCRv3_server_det.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b PaddleOCR \u200b\u81ea\u5efa\u200b\u7684\u200b\u4e2d\u82f1\u6587\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u8986\u76d6\u200b\u8857\u666f\u200b\u3001\u200b\u7f51\u56fe\u200b\u3001\u200b\u6587\u6863\u200b\u3001\u200b\u624b\u5199\u200b\u591a\u4e2a\u200b\u573a\u666f\u200b\uff0c\u200b\u5176\u4e2d\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u5305\u542b\u200b 593 \u200b\u5f20\u200b\u56fe\u7247\u200b\u3002</p>"},{"location":"en/version3.x/model_list.html#_2","title":"\u5370\u7ae0\u200b\u6587\u672c\u200b\u68c0\u6d4b\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b\u540d\u79f0\u200b \u200b\u68c0\u6d4b\u200bHmean\uff08%\uff09 GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b yaml \u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b PP-OCRv4_mobile_seal_det 96.47 7.82 / 3.09 48.28 / 23.97 4.7M PP-OCRv4_mobile_seal_det.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-OCRv4_server_seal_det 98.21 74.75 / 67.72 382.55 / 382.55 108.3 M PP-OCRv4_server_seal_det.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b PaddleOCR \u200b\u81ea\u5efa\u200b\u7684\u200b\u5370\u7ae0\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5305\u542b\u200b500\u200b\u5370\u7ae0\u200b\u56fe\u50cf\u200b\u3002</p>"},{"location":"en/version3.x/model_list.html#_3","title":"\u6587\u672c\u200b\u8bc6\u522b\u200b\u6a21\u5757","text":"<ul> <li>\u200b\u4e2d\u6587\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b</li> </ul> \u200b\u6a21\u578b\u200b \u200b\u8bc6\u522b\u200b Avg Accuracy(%) GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08M\uff09 yaml \u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b PP-OCRv5_server_rec 86.38 8.45/2.36 122.69/122.69 81 M PP-OCRv5_server_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-OCRv5_mobile_rec 81.29 1.46/5.43 5.32/91.79 16 M PP-OCRv5_mobile_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-OCRv4_server_rec_doc 81.53 6.65 / 2.38 32.92 / 32.92 74.7 M PP-OCRv4_server_rec_doc.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-OCRv4_mobile_rec 78.74 4.82 / 1.20 16.74 / 4.64 10.6 M PP-OCRv4_mobile_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-OCRv4_server_rec  80.61  6.58 / 2.43 33.17 / 33.17 71.2 M PP-OCRv4_server_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-OCRv3_mobile_rec 72.96 5.87 / 1.19 9.07 / 4.28 9.2 M PP-OCRv3_mobile_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b PaddleOCR \u200b\u81ea\u5efa\u200b\u7684\u200b\u4e2d\u6587\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u8986\u76d6\u200b\u8857\u666f\u200b\u3001\u200b\u7f51\u56fe\u200b\u3001\u200b\u6587\u6863\u200b\u3001\u200b\u624b\u5199\u200b\u591a\u4e2a\u200b\u573a\u666f\u200b\uff0c\u200b\u5176\u4e2d\u200b\u6587\u672c\u200b\u8bc6\u522b\u200b\u5305\u542b\u200b 8367 \u200b\u5f20\u200b\u56fe\u7247\u200b\u3002</p> \u200b\u6a21\u578b\u200b \u200b\u8bc6\u522b\u200b Avg Accuracy(%) GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08M\uff09 yaml \u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b ch_SVTRv2_rec 68.81 8.08 / 2.74 50.17 / 42.50 73.9 M ch_SVTRv2_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b PaddleOCR\u200b\u7b97\u6cd5\u200b\u6a21\u578b\u200b\u6311\u6218\u8d5b\u200b - \u200b\u8d5b\u9898\u200b\u4e00\u200b\uff1aOCR\u200b\u7aef\u5230\u200b\u7aef\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200bA\u200b\u699c\u200b\u3002 </p> \u200b\u6a21\u578b\u200b \u200b\u8bc6\u522b\u200b Avg Accuracy(%) GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08M\uff09 yaml \u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b ch_RepSVTR_rec 65.07 5.93 / 1.62 20.73 / 7.32 22.1 M ch_RepSVTR_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b PaddleOCR\u200b\u7b97\u6cd5\u200b\u6a21\u578b\u200b\u6311\u6218\u8d5b\u200b - \u200b\u8d5b\u9898\u200b\u4e00\u200b\uff1aOCR\u200b\u7aef\u5230\u200b\u7aef\u200b\u8bc6\u522b\u200b\u4efb\u52a1\u200bB\u200b\u699c\u200b\u3002 </p> <ul> <li>\u200b\u82f1\u6587\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b</li> </ul> \u200b\u6a21\u578b\u200b \u200b\u8bc6\u522b\u200b Avg Accuracy(%) GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08M\uff09 yaml \u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b en_PP-OCRv4_mobile_rec  70.39 4.81 / 0.75 16.10 / 5.31 6.8 M en_PP-OCRv4_mobile_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b en_PP-OCRv3_mobile_rec 70.69 5.44 / 0.75 8.65 / 5.57 7.8 M  en_PP-OCRv3_mobile_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b PaddleOCR \u200b\u81ea\u5efa\u200b\u7684\u200b\u82f1\u6587\u200b\u6570\u636e\u200b\u96c6\u200b\u3002 </p> <ul> <li>\u200b\u591a\u200b\u8bed\u8a00\u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b</li> </ul> \u200b\u6a21\u578b\u200b \u200b\u8bc6\u522b\u200b Avg Accuracy(%) GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08M\uff09 yaml \u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b korean_PP-OCRv3_mobile_rec 60.21 5.40 / 0.97 9.11 / 4.05 8.6 M korean_PP-OCRv3_mobile_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b japan_PP-OCRv3_mobile_rec 45.69 5.70 / 1.02 8.48 / 4.07 8.8 M  japan_PP-OCRv3_mobile_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b chinese_cht_PP-OCRv3_mobile_rec 82.06 5.90 / 1.28 9.28 / 4.34 9.7 M  chinese_cht_PP-OCRv3_mobile_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b te_PP-OCRv3_mobile_rec 95.88 5.42 / 0.82 8.10 / 6.91 7.8 M  te_PP-OCRv3_mobile_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b ka_PP-OCRv3_mobile_rec 96.96 5.25 / 0.79 9.09 / 3.86 8.0 M  ka_PP-OCRv3_mobile_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b ta_PP-OCRv3_mobile_rec 76.83 5.23 / 0.75 10.13 / 4.30 8.0 M  ta_PP-OCRv3_mobile_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b latin_PP-OCRv3_mobile_rec 76.93 5.20 / 0.79 8.83 / 7.15 7.8 M latin_PP-OCRv3_mobile_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b arabic_PP-OCRv3_mobile_rec 73.55 5.35 / 0.79 8.80 / 4.56 7.8 M arabic_PP-OCRv3_mobile_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b cyrillic_PP-OCRv3_mobile_rec 94.28 5.23 / 0.76 8.89 / 3.88 7.9 M   cyrillic_PP-OCRv3_mobile_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b devanagari_PP-OCRv3_mobile_rec 96.44 5.22 / 0.79 8.56 / 4.06 7.9 M devanagari_PP-OCRv3_mobile_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b PaddleOCR \u200b\u81ea\u5efa\u200b\u7684\u200b\u591a\u8bed\u79cd\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p>"},{"location":"en/version3.x/model_list.html#_4","title":"\u516c\u5f0f\u200b\u8bc6\u522b\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b En-BLEU(%) Zh-BLEU(%) GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b (M) yaml \u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b UniMERNet 85.91 43.50 2266.96/- -/- 1.53 G UniMERNet.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-FormulaNet-S 87.00 45.71 202.25/- -/- 224 M PP-FormulaNet-S.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-FormulaNet-L 90.36 45.78 1976.52/- -/- 695 M PP-FormulaNet-L.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-FormulaNet_plus-S 88.71 53.32 191.69/- -/- 248 M PP-FormulaNet_plus-S.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-FormulaNet_plus-M 91.45 89.76 1301.56/- -/- 592 M PP-FormulaNet_plus-M.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-FormulaNet_plus-L 92.22 90.64 1745.25/- -/- 698 M PP-FormulaNet_plus-L.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b LaTeX_OCR_rec 74.55 39.96 1244.61/- -/- 99 M LaTeX_OCR_rec.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u6d4b\u91cf\u200b\u81ea\u200b PaddleX \u200b\u5185\u90e8\u200b\u81ea\u200b\u5efa\u200b\u516c\u5f0f\u200b\u8bc6\u522b\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u3002LaTeX_OCR_rec\u200b\u5728\u200bLaTeX-OCR\u200b\u516c\u5f0f\u200b\u8bc6\u522b\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u7684\u200bBLEU score\u200b\u4e3a\u200b 0.8821\u3002</p>"},{"location":"en/version3.x/model_list.html#_5","title":"\u8868\u683c\u200b\u7ed3\u6784\u200b\u8bc6\u522b\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b \u200b\u7cbe\u5ea6\u200b\uff08%\uff09 GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b (M) yaml \u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b SLANet 59.52 103.08 / 103.08 197.99 / 197.99 6.9 M SLANet.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b SLANet_plus 63.69 140.29 / 140.29 195.39 / 195.39 6.9 M SLANet_plus.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b SLANeXt_wired 69.65 -- -- -- SLANeXt_wired.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b SLANeXt_wireless SLANeXt_wireless.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u6d4b\u91cf\u200b\u81ea\u200b PaddleX \u200b\u5185\u90e8\u200b\u81ea\u200b\u5efa\u200b\u9ad8\u96be\u5ea6\u200b\u4e2d\u6587\u200b\u8868\u683c\u200b\u8bc6\u522b\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p>"},{"location":"en/version3.x/model_list.html#_6","title":"\u8868\u683c\u200b\u5355\u5143\u683c\u200b\u68c0\u6d4b\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b mAP(%) GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b (M) yaml \u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b RT-DETR-L_wired_table_cell_det 82.7 35.00 / 10.45 495.51 / 495.51 124M RT-DETR-L_wired_table_cell_det.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b RT-DETR-L_wireless_table_cell_det RT-DETR-L_wireless_table_cell_det.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u6d4b\u91cf\u200b\u81ea\u200b PaddleX \u200b\u5185\u90e8\u200b\u81ea\u200b\u5efa\u200b\u8868\u683c\u200b\u5355\u5143\u683c\u200b\u68c0\u6d4b\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p>"},{"location":"en/version3.x/model_list.html#_7","title":"\u8868\u683c\u200b\u5206\u7c7b\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b Top1 Acc(%) GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b (M) yaml\u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b PP-LCNet_x1_0_table_cls 94.2 2.35 / 0.47 4.03 / 1.35 6.6M PP-LCNet_x1_0_table_cls.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u6d4b\u91cf\u200b\u81ea\u200b PaddleX \u200b\u5185\u90e8\u200b\u81ea\u200b\u5efa\u200b\u8868\u683c\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p>"},{"location":"en/version3.x/model_list.html#_8","title":"\u6587\u672c\u200b\u56fe\u50cf\u200b\u77eb\u6b63\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b\u540d\u79f0\u200b MS-SSIM \uff08%\uff09 GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b yaml \u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b UVDoc 54.40 16.27 / 7.76 176.97 / 80.60 30.3 M UVDoc.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u6d4b\u91cf\u200b\u81ea\u200b PaddleX\u200b\u81ea\u5efa\u200b\u7684\u200b\u56fe\u50cf\u200b\u77eb\u6b63\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p>"},{"location":"en/version3.x/model_list.html#_9","title":"\u7248\u9762\u200b\u533a\u57df\u200b\u68c0\u6d4b\u200b\u6a21\u5757","text":"<ul> <li>\u200b\u7248\u9762\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c\u200b\u5305\u542b\u200b20\u200b\u4e2a\u200b\u5e38\u89c1\u200b\u7684\u200b\u7c7b\u522b\u200b\uff1a\u200b\u6587\u6863\u200b\u6807\u9898\u200b\u3001\u200b\u6bb5\u843d\u200b\u6807\u9898\u200b\u3001\u200b\u6587\u672c\u200b\u3001\u200b\u9875\u7801\u200b\u3001\u200b\u6458\u8981\u200b\u3001\u200b\u76ee\u5f55\u200b\u3001\u200b\u53c2\u8003\u6587\u732e\u200b\u3001\u200b\u811a\u6ce8\u200b\u3001\u200b\u9875\u7709\u200b\u3001\u200b\u9875\u811a\u200b\u3001\u200b\u7b97\u6cd5\u200b\u3001\u200b\u516c\u5f0f\u200b\u3001\u200b\u516c\u5f0f\u200b\u7f16\u53f7\u200b\u3001\u200b\u56fe\u50cf\u200b\u3001\u200b\u8868\u683c\u200b\u3001\u200b\u56fe\u548c\u8868\u200b\u6807\u9898\u200b\uff08\u200b\u56fe\u200b\u6807\u9898\u200b\u3001\u200b\u8868\u683c\u200b\u6807\u9898\u200b\u548c\u200b\u56fe\u8868\u200b\u6807\u9898\u200b\uff09\u3001\u200b\u5370\u7ae0\u200b\u3001\u200b\u56fe\u8868\u200b\u3001\u200b\u4fa7\u680f\u200b\u6587\u672c\u200b\u548c\u200b\u53c2\u8003\u6587\u732e\u200b\u5185\u5bb9\u200b</li> </ul> \u200b\u6a21\u578b\u200b mAP(0.5)\uff08%\uff09 GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08M\uff09 yaml\u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b PP-DocLayout_plus-L 83.2 34.6244 / 10.3945 510.57 / -  126.01  PP-DocLayout_plus-L.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b\u81ea\u5efa\u200b\u7684\u200b\u7248\u9762\u200b\u533a\u57df\u200b\u68c0\u6d4b\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5305\u542b\u200b\u4e2d\u82f1\u6587\u200b\u8bba\u6587\u200b\u3001\u200b\u6742\u5fd7\u200b\u3001\u200b\u62a5\u7eb8\u200b\u3001\u200b\u7814\u62a5\u200b\u3001PPT\u3001\u200b\u8bd5\u5377\u200b\u3001\u200b\u8bfe\u672c\u200b\u7b49\u200b 1300 \u200b\u5f20\u200b\u6587\u6863\u200b\u7c7b\u578b\u200b\u56fe\u7247\u200b\u3002</p> <ul> <li>\u200b\u6587\u6863\u200b\u56fe\u50cf\u200b\u7248\u200b\u9762\u5b50\u200b\u6a21\u5757\u200b\u68c0\u6d4b\u200b\uff0c\u200b\u5305\u542b\u200b1\u200b\u4e2a\u200b \u200b\u7248\u9762\u200b\u533a\u57df\u200b \u200b\u7c7b\u522b\u200b\uff0c\u200b\u80fd\u200b\u68c0\u6d4b\u200b\u591a\u680f\u200b\u7684\u200b\u62a5\u7eb8\u200b\u3001\u200b\u6742\u5fd7\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u5b50\u200b\u6587\u7ae0\u200b\u7684\u200b\u6587\u672c\u200b\u533a\u57df\u200b:</li> </ul> \u200b\u6a21\u578b\u200b mAP(0.5)\uff08%\uff09 GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08M\uff09 yaml\u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b PP-DocBlockLayout 95.9 34.6244 / 10.3945 510.57 / -  123 M PP-DocBlockLayout.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b\u81ea\u5efa\u200b\u7684\u200b\u7248\u200b\u9762\u5b50\u200b\u533a\u57df\u200b\u68c0\u6d4b\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5305\u542b\u200b\u4e2d\u82f1\u6587\u200b\u8bba\u6587\u200b\u3001\u200b\u6742\u5fd7\u200b\u3001\u200b\u62a5\u7eb8\u200b\u3001\u200b\u7814\u62a5\u200b\u3001PPT\u3001\u200b\u8bd5\u5377\u200b\u3001\u200b\u8bfe\u672c\u200b\u7b49\u200b 1000 \u200b\u5f20\u200b\u6587\u6863\u200b\u7c7b\u578b\u200b\u56fe\u7247\u200b\u3002</p> <ul> <li>\u200b\u7248\u9762\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c\u200b\u5305\u542b\u200b23\u200b\u4e2a\u200b\u5e38\u89c1\u200b\u7684\u200b\u7c7b\u522b\u200b\uff1a\u200b\u6587\u6863\u200b\u6807\u9898\u200b\u3001\u200b\u6bb5\u843d\u200b\u6807\u9898\u200b\u3001\u200b\u6587\u672c\u200b\u3001\u200b\u9875\u7801\u200b\u3001\u200b\u6458\u8981\u200b\u3001\u200b\u76ee\u5f55\u200b\u3001\u200b\u53c2\u8003\u6587\u732e\u200b\u3001\u200b\u811a\u6ce8\u200b\u3001\u200b\u9875\u7709\u200b\u3001\u200b\u9875\u811a\u200b\u3001\u200b\u7b97\u6cd5\u200b\u3001\u200b\u516c\u5f0f\u200b\u3001\u200b\u516c\u5f0f\u200b\u7f16\u53f7\u200b\u3001\u200b\u56fe\u50cf\u200b\u3001\u200b\u56fe\u8868\u200b\u6807\u9898\u200b\u3001\u200b\u8868\u683c\u200b\u3001\u200b\u8868\u683c\u200b\u6807\u9898\u200b\u3001\u200b\u5370\u7ae0\u200b\u3001\u200b\u56fe\u8868\u200b\u6807\u9898\u200b\u3001\u200b\u56fe\u8868\u200b\u3001\u200b\u9875\u7709\u200b\u56fe\u50cf\u200b\u3001\u200b\u9875\u811a\u200b\u56fe\u50cf\u200b\u3001\u200b\u4fa7\u680f\u200b\u6587\u672c\u200b</li> </ul> \u200b\u6a21\u578b\u200b mAP(0.5)\uff08%\uff09 GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08M\uff09 yaml\u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b PP-DocLayout-L 90.4 34.6244 / 10.3945 510.57 / -  123.76  PP-DocLayout-L.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-DocLayout-M 75.2 13.3259 / 4.8685 44.0680 / 44.0680 22.578 PP-DocLayout-M.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PP-DocLayout-S 70.9 8.3008 / 2.3794 10.0623 / 9.9296 4.834 PP-DocLayout-S.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b\u81ea\u5efa\u200b\u7684\u200b\u7248\u9762\u200b\u533a\u57df\u200b\u68c0\u6d4b\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5305\u542b\u200b\u4e2d\u82f1\u6587\u200b\u8bba\u6587\u200b\u3001\u200b\u6742\u5fd7\u200b\u548c\u200b\u7814\u62a5\u200b\u7b49\u200b\u5e38\u89c1\u200b\u7684\u200b 500 \u200b\u5f20\u200b\u6587\u6863\u200b\u7c7b\u578b\u200b\u56fe\u7247\u200b\u3002</p> <ul> <li>\u200b\u8868\u683c\u200b\u7248\u9762\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b</li> </ul> \u200b\u6a21\u578b\u200b mAP(0.5)\uff08%\uff09 GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08M\uff09 yaml\u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b PicoDet_layout_1x_table 97.5 8.02 / 3.09 23.70 / 20.41 7.4 M PicoDet_layout_1x_table.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b PaddleOCR \u200b\u81ea\u5efa\u200b\u7684\u200b\u7248\u9762\u200b\u8868\u683c\u200b\u533a\u57df\u200b\u68c0\u6d4b\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5305\u542b\u200b\u4e2d\u82f1\u6587\u200b 7835 \u200b\u5f20\u200b\u5e26\u6709\u200b\u8868\u683c\u200b\u7684\u200b\u8bba\u6587\u200b\u6587\u6863\u200b\u7c7b\u578b\u200b\u56fe\u7247\u200b\u3002</p> <ul> <li>3\u200b\u7c7b\u200b\u7248\u9762\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c\u200b\u5305\u542b\u200b\u8868\u683c\u200b\u3001\u200b\u56fe\u50cf\u200b\u3001\u200b\u5370\u7ae0\u200b</li> </ul> \u200b\u6a21\u578b\u200b mAP(0.5)\uff08%\uff09 GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08M\uff09 yaml\u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b PicoDet-S_layout_3cls 88.2 8.99 / 2.22 16.11 / 8.73 4.8 PicoDet-S_layout_3cls.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PicoDet-L_layout_3cls 89.0 13.05 / 4.50 41.30 / 41.30 22.6 PicoDet-L_layout_3cls.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b RT-DETR-H_layout_3cls 95.8 114.93 / 27.71 947.56 / 947.56 470.1 RT-DETR-H_layout_3cls.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b PaddleOCR \u200b\u81ea\u5efa\u200b\u7684\u200b\u7248\u9762\u200b\u533a\u57df\u200b\u68c0\u6d4b\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5305\u542b\u200b\u4e2d\u82f1\u6587\u200b\u8bba\u6587\u200b\u3001\u200b\u6742\u5fd7\u200b\u548c\u200b\u7814\u62a5\u200b\u7b49\u200b\u5e38\u89c1\u200b\u7684\u200b 1154 \u200b\u5f20\u200b\u6587\u6863\u200b\u7c7b\u578b\u200b\u56fe\u7247\u200b\u3002</p> <ul> <li>5\u200b\u7c7b\u200b\u82f1\u6587\u200b\u6587\u6863\u200b\u533a\u57df\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c\u200b\u5305\u542b\u200b\u6587\u5b57\u200b\u3001\u200b\u6807\u9898\u200b\u3001\u200b\u8868\u683c\u200b\u3001\u200b\u56fe\u7247\u200b\u4ee5\u53ca\u200b\u5217\u8868\u200b</li> </ul> \u200b\u6a21\u578b\u200b mAP(0.5)\uff08%\uff09 GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08M\uff09 yaml\u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b PicoDet_layout_1x 97.8 9.03 / 3.10 25.82 / 20.70 7.4 PicoDet_layout_1x.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b PubLayNet \u200b\u7684\u200b\u8bc4\u4f30\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5305\u542b\u200b\u82f1\u6587\u200b\u6587\u6863\u200b\u7684\u200b 11245 \u200b\u5f20\u200b\u56fe\u7247\u200b\u3002</p> <ul> <li>17\u200b\u7c7b\u200b\u533a\u57df\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c\u200b\u5305\u542b\u200b17\u200b\u4e2a\u200b\u7248\u9762\u200b\u5e38\u89c1\u200b\u7c7b\u522b\u200b\uff0c\u200b\u5206\u522b\u200b\u662f\u200b\uff1a\u200b\u6bb5\u843d\u200b\u6807\u9898\u200b\u3001\u200b\u56fe\u7247\u200b\u3001\u200b\u6587\u672c\u200b\u3001\u200b\u6570\u5b57\u200b\u3001\u200b\u6458\u8981\u200b\u3001\u200b\u5185\u5bb9\u200b\u3001\u200b\u56fe\u8868\u200b\u6807\u9898\u200b\u3001\u200b\u516c\u5f0f\u200b\u3001\u200b\u8868\u683c\u200b\u3001\u200b\u8868\u683c\u200b\u6807\u9898\u200b\u3001\u200b\u53c2\u8003\u6587\u732e\u200b\u3001\u200b\u6587\u6863\u200b\u6807\u9898\u200b\u3001\u200b\u811a\u6ce8\u200b\u3001\u200b\u9875\u7709\u200b\u3001\u200b\u7b97\u6cd5\u200b\u3001\u200b\u9875\u811a\u200b\u3001\u200b\u5370\u7ae0\u200b</li> </ul> \u200b\u6a21\u578b\u200b mAP(0.5)\uff08%\uff09 GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08M\uff09 yaml\u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b PicoDet-S_layout_17cls 87.4 9.11 / 2.12 15.42 / 9.12 4.8 PicoDet-S_layout_17cls.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b PicoDet-L_layout_17cls 89.0 13.50 / 4.69 43.32 / 43.32 22.6 PicoDet-L_layout_17cls.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b RT-DETR-H_layout_17cls 98.3 115.29 / 104.09 995.27 / 995.27 470.2 RT-DETR-H_layout_17cls.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b PaddleOCR \u200b\u81ea\u5efa\u200b\u7684\u200b\u7248\u9762\u200b\u533a\u57df\u200b\u68c0\u6d4b\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5305\u542b\u200b\u4e2d\u82f1\u6587\u200b\u8bba\u6587\u200b\u3001\u200b\u6742\u5fd7\u200b\u548c\u200b\u7814\u62a5\u200b\u7b49\u200b\u5e38\u89c1\u200b\u7684\u200b 892 \u200b\u5f20\u200b\u6587\u6863\u200b\u7c7b\u578b\u200b\u56fe\u7247\u200b\u3002</p>"},{"location":"en/version3.x/model_list.html#_10","title":"\u6587\u6863\u200b\u56fe\u50cf\u200b\u65b9\u5411\u200b\u5206\u7c7b\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b Top-1 Acc\uff08%\uff09 GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08M) yaml\u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b PP-LCNet_x1_0_doc_ori 99.06 2.31 / 0.43 3.37 / 1.27 7 PP-LCNet_x1_0_doc_ori.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b\u81ea\u5efa\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u8986\u76d6\u200b\u8bc1\u4ef6\u200b\u548c\u200b\u6587\u6863\u200b\u7b49\u200b\u591a\u4e2a\u200b\u573a\u666f\u200b\uff0c\u200b\u5305\u542b\u200b 1000 \u200b\u5f20\u200b\u56fe\u7247\u200b\u3002</p>"},{"location":"en/version3.x/model_list.html#_11","title":"\u6587\u672c\u200b\u884c\u200b\u65b9\u5411\u200b\u5206\u7c7b\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b Top-1 Acc\uff08%\uff09 GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09[\u200b\u5e38\u89c4\u200b\u6a21\u5f0f\u200b / \u200b\u9ad8\u6027\u80fd\u200b\u6a21\u5f0f\u200b] \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08M) yaml\u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b PP-LCNet_x1_0_doc_ori 99.06 2.31 / 0.43 3.37 / 1.27 7 PP-LCNet_x0_25_textline_ori.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b/\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u8bc4\u4f30\u200b\u96c6\u662f\u200b\u81ea\u5efa\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u8986\u76d6\u200b\u8bc1\u4ef6\u200b\u548c\u200b\u6587\u6863\u200b\u7b49\u200b\u591a\u4e2a\u200b\u573a\u666f\u200b\uff0c\u200b\u5305\u542b\u200b 1000 \u200b\u5f20\u200b\u56fe\u7247\u200b\u3002</p>"},{"location":"en/version3.x/model_list.html#_12","title":"\u6587\u6863\u200b\u7c7b\u200b\u89c6\u89c9\u200b\u8bed\u8a00\u200b\u6a21\u578b\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b \u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u5c3a\u5bf8\u200b\uff08B\uff09 \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b\uff08GB\uff09 yaml\u200b\u6587\u4ef6\u200b \u200b\u6a21\u578b\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b PP-DocBee-2B 2 4.2 PP-DocBee-2B.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b PP-DocBee-7B 7 15.8 PP-DocBee-7B.yaml \u200b\u63a8\u7406\u6a21\u578b\u200b PP-DocBee2-3B 3 7.6 \u200b\u63a8\u7406\u6a21\u578b"},{"location":"en/version3.x/paddleocr_and_paddlex.html","title":"PaddleOCR and PaddleX","text":"<p>PaddleX is a low-code development tool built on the PaddlePaddle framework. It integrates numerous out-of-the-box pre-trained models, supports the full-pipeline development from model training to inference, and is compatible with various mainstream hardware both domestically and internationally, empowering AI developers to efficiently deploy solutions in industrial practices.</p> <p>PaddleOCR leverages PaddleX for inference deployment, enabling seamless collaboration between the two in this regard. When installing PaddleOCR, PaddleX is also installed as a dependency. Additionally, PaddleOCR and PaddleX maintain consistency in pipeline naming conventions. For quick experience, users typically do not need to understand the specific concepts of PaddleX when using basic configurations. However, knowledge of PaddleX can be beneficial in advanced configuration scenarios, service deployment, and other use cases.</p> <p>This document introduces the relationship between PaddleOCR and PaddleX and explains how to use these two tools collaboratively.</p>"},{"location":"en/version3.x/paddleocr_and_paddlex.html#1-differences-and-connections-between-paddleocr-and-paddlex","title":"1. Differences and Connections Between PaddleOCR and PaddleX","text":"<p>PaddleOCR and PaddleX have distinct focuses and functionalities: PaddleOCR specializes in OCR-related tasks, while PaddleX covers a wide range of task types, including time-series forecasting, face recognition, and more. Furthermore, PaddleX provides rich infrastructure with underlying capabilities for multi-model combined inference, enabling the integration of different models in a unified and flexible manner and supporting the construction of complex model pipelines.</p> <p>PaddleOCR fully reuses the capabilities of PaddleX in the inference deployment phase, including:</p> <ul> <li>PaddleOCR primarily relies on PaddleX for underlying capabilities such as model inference, pre- and post-processing, and multi-model combination.</li> <li>The high-performance inference capabilities of PaddleOCR are achieved through PaddleX's Paddle2ONNX plugin and high-performance inference plugins.</li> <li>The service deployment solutions of PaddleOCR are based on PaddleX's implementations.</li> </ul> <p>It is important to note that although PaddleOCR uses PaddleX at the underlying level, thanks to PaddleX\u2019s optional dependency installation feature, installing the PaddleOCR inference package does not include all of PaddleX\u2019s dependencies\u2014only those required for OCR-related tasks are installed. Therefore, users generally do not need to worry about excessive expansion of dependency size. Tested in May 2025, in an x86-64 + Linux + Python 3.10 environment, the total size of required dependencies increased only from 717 MB to 738 MB.</p> <p>The version correspondence between PaddleOCR and PaddleX is as follows:</p> PaddleOCR Version PaddleX Version <code>3.0.0</code> <code>3.0.0</code> <code>3.0.1</code> <code>3.0.1</code> <code>3.0.2</code> <code>3.0.2</code>"},{"location":"en/version3.x/paddleocr_and_paddlex.html#2-correspondence-between-paddleocr-pipelines-and-paddlex-pipeline-registration-names","title":"2. Correspondence Between PaddleOCR Pipelines and PaddleX Pipeline Registration Names","text":"PaddleOCR Pipeline PaddleX Pipeline Registration Name General OCR <code>OCR</code> General Layout Analysis v3 <code>PP-StructureV3</code> Document Scenario Information Extraction v4 <code>PP-ChatOCRv4-doc</code> General Table Recognition v2 <code>table_recognition_v2</code> Formula Recognition <code>formula_recognition</code> Seal Text Recognition <code>seal_recognition</code> Document Image Preprocessing <code>doc_preprocessor</code> Document Understanding <code>doc_understanding</code>"},{"location":"en/version3.x/paddleocr_and_paddlex.html#3-using-paddlex-pipeline-configuration-files","title":"3. Using PaddleX Pipeline Configuration Files","text":"<p>During the inference deployment phase, PaddleOCR supports exporting and loading PaddleX pipeline configuration files. Users can deeply configure inference deployment-related parameters by editing these configuration files.</p>"},{"location":"en/version3.x/paddleocr_and_paddlex.html#31-exporting-pipeline-configuration-files","title":"3.1 Exporting Pipeline Configuration Files","text":"<p>You can call the <code>export_paddlex_config_to_yaml</code> method of the PaddleOCR pipeline object to export the current pipeline configuration to a YAML file. Here is an example:</p> <pre><code>from paddleocr import PaddleOCR\n\npipeline = PaddleOCR()\npipeline.export_paddlex_config_to_yaml(\"ocr_config.yaml\")\n</code></pre> <p>The above code will generate a pipeline configuration file named <code>ocr_config.yaml</code> in the working directory.</p> <p>You can also obtain the configuration file through the PaddleX CLI. Example:</p> <pre><code># Specify the pipeline registration name\npaddlex --get_pipeline_config OCR\n</code></pre>"},{"location":"en/version3.x/paddleocr_and_paddlex.html#32-editing-pipeline-configuration-files","title":"3.2 Editing Pipeline Configuration Files","text":"<p>The exported PaddleX pipeline configuration file not only includes parameters supported by PaddleOCR's CLI and Python API but also allows for more advanced configurations. Please refer to the corresponding pipeline usage tutorials in PaddleX Pipeline Usage Overview for detailed instructions on adjusting various configurations according to your needs.</p>"},{"location":"en/version3.x/paddleocr_and_paddlex.html#33-loading-pipeline-configuration-files-in-cli","title":"3.3 Loading Pipeline Configuration Files in CLI","text":"<p>By specifying the path to the PaddleX pipeline configuration file using the <code>--paddlex_config</code> parameter, PaddleOCR will read its contents as the default configuration for the pipeline (this takes precedence over the default values of individual initialization parameters). Here is an example:</p> <pre><code>paddleocr ocr --paddlex_config ocr_config.yaml ...\n</code></pre>"},{"location":"en/version3.x/paddleocr_and_paddlex.html#34-loading-pipeline-configuration-files-in-python-api","title":"3.4 Loading Pipeline Configuration Files in Python API","text":"<p>When initializing the pipeline object, you can pass the path to the PaddleX pipeline configuration file or a configuration dictionary through the <code>paddlex_config</code> parameter, and PaddleOCR will use it as the default configuration (this takes precedence over the default values of individual initialization parameters). Here is an example:</p> <pre><code>from paddleocr import PaddleOCR\n\npipeline = PaddleOCR(paddlex_config=\"ocr_config.yaml\")\n</code></pre>"},{"location":"en/version3.x/algorithm/PP-ChatOCRv4/PP-ChatOCRv4.html","title":"Introduction to PP-ChatOCRV4","text":"<p>PP-ChatOCRv4 is a unique document and image intelligent analysis solution from PaddlePaddle, combining LLM, MLLM, and OCR technologies to address complex document information extraction challenges such as layout analysis, rare characters, multi-page PDFs, tables, and seal recognition. Integrated with ERNIE Bot, it fuses massive data and knowledge, achieving high accuracy and wide applicability. This pipeline also provides flexible service deployment options, supporting deployment on various hardware. Furthermore, it offers custom development capabilities, allowing you to train and fine-tune models on your own datasets, with seamless integration of trained models.</p>"},{"location":"en/version3.x/algorithm/PP-ChatOCRv4/PP-ChatOCRv4.html#key-metrics","title":"Key Metrics","text":"Solution Avg Recall GPT-4o 63.47% PP-ChatOCRv3 70.08% Qwen2.5-VL-72B 80.26% PP-ChatOCRv4 85.55%"},{"location":"en/version3.x/algorithm/PP-ChatOCRv4/PP-ChatOCRv4.html#demo","title":"Demo","text":""},{"location":"en/version3.x/algorithm/PP-ChatOCRv4/PP-ChatOCRv4.html#faq","title":"FAQ","text":"<ol> <li>Does support other multimodal models?</li> </ol> <p>Yes, only set on pipeline configuration.</p> <ol> <li>How to reduce latency and improve throughput?</li> </ol> <p>Use the High-performance inference plugin, and deploy multi instances.</p> <ol> <li>How to further improve accuracy?</li> </ol> <p>Firstly, it is necessary to check whether the extracted visual information is correct. If the visual information is incorrect, it is necessary to visualize the visual prediction results to determine which model performs poorly, and then fine-tune train the model with more data. If the visual information is correct but cannot extract the correct information, the prompt needs to be adjusted according to the analysing about the question and answer.</p>"},{"location":"en/version3.x/algorithm/PP-OCRv5/PP-OCRv5.html","title":"Introduction to PP-OCRv5","text":"<p>PP-OCRv5 is the new generation text recognition solution of PP-OCR, focusing on multi-scenario and multi-text type recognition. In terms of text types, PP-OCRv5 supports 5 major mainstream text types: Simplified Chinese, Chinese Pinyin, Traditional Chinese, English, and Japanese. For scenarios, PP-OCRv5 has upgraded recognition capabilities for challenging scenarios such as complex Chinese and English handwriting, vertical text, and uncommon characters. On internal complex evaluation sets across multiple scenarios, PP-OCRv5 achieved a 13 percentage point end-to-end improvement over PP-OCRv4.</p>"},{"location":"en/version3.x/algorithm/PP-OCRv5/PP-OCRv5.html#key-metrics","title":"Key Metrics","text":""},{"location":"en/version3.x/algorithm/PP-OCRv5/PP-OCRv5.html#1-text-detection-metrics","title":"1. Text Detection Metrics","text":"Model Handwritten Chinese Handwritten English Printed Chinese Printed English Traditional Chinese Ancient Text Japanese General Scenario Pinyin Rotation Distortion Artistic Text Average PP-OCRv5_server_det 0.803 0.841 0.945 0.917 0.815 0.676 0.772 0.797 0.671 0.8 0.876 0.673 0.827 PP-OCRv4_server_det 0.706 0.249 0.888 0.690 0.759 0.473 0.685 0.715 0.542 0.366 0.775 0.583 0.662 PP-OCRv5_mobile_det 0.744 0.777 0.905 0.910 0.823 0.581 0.727 0.721 0.575 0.647 0.827 0.525 0.770 PP-OCRv4_mobile_det 0.583 0.369 0.872 0.773 0.663 0.231 0.634 0.710 0.430 0.299 0.715 0.549 0.624 <p>Compared to PP-OCRv4, PP-OCRv5 shows significant improvement in all detection scenarios, especially in handwriting, ancient texts, and Japanese detection capabilities.</p>"},{"location":"en/version3.x/algorithm/PP-OCRv5/PP-OCRv5.html#2-text-recognition-metrics","title":"2. Text Recognition Metrics","text":"Evaluation Set Category Handwritten Chinese Handwritten English Printed Chinese Printed English Traditional Chinese Ancient Text Japanese Confusable Characters General Scenario Pinyin Vertical Text Artistic Text Weighted Average PP-OCRv5_server_rec 0.5807 0.5806 0.9013 0.8679 0.7472 0.6039 0.7372 0.5946 0.8384 0.7435 0.9314 0.6397 0.8401 PP-OCRv4_server_rec 0.3626 0.2661 0.8486 0.6677 0.4097 0.3080 0.4623 0.5028 0.8362 0.2694 0.5455 0.5892 0.5735 PP-OCRv5_mobile_rec 0.4166 0.4944 0.8605 0.8753 0.7199 0.5786 0.7577 0.5570 0.7703 0.7248 0.8089 0.5398 0.8015 PP-OCRv4_mobile_rec 0.2980 0.2550 0.8398 0.6598 0.3218 0.2593 0.4724 0.4599 0.8106 0.2593 0.5924 0.5555 0.5301 <p>A single model can cover multiple languages and text types, with recognition accuracy significantly ahead of previous generation products and mainstream open-source solutions.</p>"},{"location":"en/version3.x/algorithm/PP-OCRv5/PP-OCRv5.html#pp-ocrv5-demo-examples","title":"PP-OCRv5 Demo Examples","text":"<p>More Demos</p>"},{"location":"en/version3.x/algorithm/PP-OCRv5/PP-OCRv5.html#reference-data-for-inference-performance","title":"Reference Data for Inference Performance","text":"<p>Test Environment:</p> <ul> <li>NVIDIA Tesla V100</li> <li>Intel Xeon Gold 6271C</li> <li>PaddlePaddle 3.0.0</li> </ul> <p>Tested on 200 images (including both general and document images). During testing, images are read from disk, so the image reading time and other associated overhead are also included in the total time consumption. If the images are preloaded into memory, the average time per image can be further reduced by approximately 25 ms.</p> <p>Unless otherwise specified:</p> <ul> <li>PP-OCRv4_mobile_det and PP-OCRv4_mobile_rec models are used.</li> <li>Document orientation classification, image correction, and text line orientation classification are not used.</li> <li><code>text_det_limit_type</code> is set to <code>\"min\"</code> and <code>text_det_limit_side_len</code> to <code>732</code>.</li> </ul>"},{"location":"en/version3.x/algorithm/PP-OCRv5/PP-OCRv5.html#1-comparison-of-inference-performance-between-pp-ocrv5-and-pp-ocrv4","title":"1. Comparison of Inference Performance Between PP-OCRv5 and PP-OCRv4","text":"Config Description v5_mobile Uses PP-OCRv5_mobile_det and PP-OCRv5_mobile_rec models. v4_mobile Uses PP-OCRv4_mobile_det and PP-OCRv4_mobile_rec models. v5_server Uses PP-OCRv5_server_det and PP-OCRv5_server_rec models. v4_server Uses PP-OCRv4_server_det and PP-OCRv4_server_rec models. <p>GPU, without high-performance inference:</p> Configuration Avg. Time per Image (s) Avg. Characters Predicted per Second Avg. CPU Utilization (%) Peak RAM Usage (MB) Avg. RAM Usage (MB) Avg. GPU Utilization (%) Peak VRAM Usage (MB) Avg. VRAM Usage (MB) v5_mobile 0.56 1162 106.02 1576.43 1420.83 18.95 4342.00 3258.95 v4_mobile 0.27 2246 111.20 1392.22 1318.76 28.90 1304.00 1166.46 v5_server 0.70 929 105.31 1634.85 1428.55 36.21 5402.00 4685.13 v4_server 0.44 1418 106.96 1455.34 1346.95 58.82 6760.00 5817.46 <p>GPU, with high-performance inference:</p> Configuration Avg. Time per Image (s) Avg. Characters Predicted per Second Avg. CPU Utilization (%) Peak RAM Usage (MB) Avg. RAM Usage (MB) Avg. GPU Utilization (%) Peak VRAM Usage (MB) Avg. VRAM Usage (MB) v5_mobile 0.50 1301 106.50 1338.12 1155.86 11.97 4112.00 3536.36 v4_mobile 0.21 2887 114.09 1113.27 1054.46 15.22 2072.00 1840.59 v5_server 0.60 1084 105.73 1980.73 1776.20 22.10 12150.00 11849.40 v4_server 0.36 1687 104.15 1186.42 1065.67 38.12 13058.00 12679.00 <p>CPU, without high-performance inference:</p> Configuration Avg. Time per Image (s) Avg. Characters Predicted per Second Avg. CPU Utilization (%) Peak RAM Usage (MB) Avg. RAM Usage (MB) v5_mobile 1.43 455 798.93 11695.40 6829.09 v4_mobile 1.09 556 813.16 11996.30 6834.25 v5_server 3.79 172 799.24 50216.00 27902.40 v4_server 4.22 148 803.74 51428.70 28593.60 <p>CPU, with high-performance inference:</p> Configuration Avg. Time per Image (s) Avg. Characters Predicted per Second Avg. CPU Utilization (%) Peak RAM Usage (MB) Avg. RAM Usage (MB) v5_mobile 1.14 571 339.68 3245.17 2560.55 v4_mobile 0.68 892 443.00 3057.38 2329.44 v5_server 3.56 183 797.03 45664.70 26905.90 v4_server 4.22 148 803.74 51428.70 28593.60 <p>Note: PP-OCRv5 uses a larger dictionary in the recognition model, which increases inference time and causes slower performance compared to PP-OCRv4.</p>"},{"location":"en/version3.x/algorithm/PP-OCRv5/PP-OCRv5.html#2-impact-of-auxiliary-features-on-pp-ocrv5-inference-performance","title":"2. Impact of Auxiliary Features on PP-OCRv5 Inference Performance","text":"Config Description base No document orientation classification, no image correction, no text line orientation classification. with_textline Includes text line orientation classification only. with_all Includes document orientation classification, image correction, and text line orientation classification. <p>GPU, without high-performance inference:</p> Configuration Avg. Time per Image (s) Avg. Characters Predicted per Second Avg. CPU Utilization (%) Peak RAM Usage (MB) Avg. RAM Usage (MB) Avg. GPU Utilization (%) Peak VRAM Usage (MB) Avg. VRAM Usage (MB) base 0.56 1162 106.02 1576.43 1420.83 18.95 4342.00 3258.95 with_textline 0.59 1104 105.58 1765.64 1478.53 19.48 4350.00 3267.77 with_all 1.02 600 104.92 1924.23 1628.50 10.96 2632.00 2217.01 <p>CPU, without high-performance inference:</p> Configuration Avg. Time per Image (s) Avg. Characters Predicted per Second Avg. CPU Utilization (%) Peak RAM Usage (MB) Avg. RAM Usage (MB) base 1.43 455 798.93 11695.40 6829.09 with_textline 1.50 434 799.47 12007.20 6882.22 with_all 1.93 316 646.49 11759.60 6940.54 <p>Note: Auxiliary features such as image unwarping can impact inference accuracy. More features do not necessarily yield better results and may increase resource usage.</p>"},{"location":"en/version3.x/algorithm/PP-OCRv5/PP-OCRv5.html#3-impact-of-input-scaling-strategy-in-text-detection-module-on-pp-ocrv5-inference-performance","title":"3. Impact of Input Scaling Strategy in Text Detection Module on PP-OCRv5 Inference Performance","text":"Config Description mobile_min_1280 Uses <code>min</code> limit type and <code>text_det_limit_side_len=1280</code> with PP-OCRv5_mobile models. mobile_min_736 Same as default, <code>min</code>, <code>side_len=736</code>. mobile_max_960 Uses <code>max</code> limit type and <code>side_len=960</code>. mobile_max_640 Uses <code>max</code> limit type and <code>side_len=640</code>. server_min_1280 Uses <code>min</code>, <code>side_len=1280</code> with PP-OCRv5_server models. server_min_736 Same as default, <code>min</code>, <code>side_len=736</code>. server_max_960 Uses <code>max</code>, <code>side_len=960</code>. server_max_640 Uses <code>max</code>, <code>side_len=640</code>. <p>GPU, without high-performance inference:</p> Configuration Avg. Time per Image (s) Avg. Characters Predicted per Second Avg. CPU Utilization (%) Peak RAM Usage (MB) Avg. RAM Usage (MB) Avg. GPU Utilization (%) Peak VRAM Usage (MB) Avg. VRAM Usage (MB) mobile_min_1280 0.61 1071 109.12 1663.71 1439.72 19.27 4202.00 3550.32 mobile_min_736 0.56 1162 106.02 1576.43 1420.83 18.95 4342.00 3258.95 mobile_max_960 0.48 1313 103.49 1587.25 1395.48 19.37 2642.00 2319.03 mobile_max_640 0.42 1436 103.07 1651.14 1422.62 18.95 2530.00 2149.11 server_min_1280 0.82 795 107.17 1678.16 1428.94 40.43 10368.00 8320.43 server_min_736 0.70 929 105.31 1634.85 1428.55 36.21 5402.00 4685.13 server_max_960 0.59 1073 103.03 1590.19 1383.62 33.42 2928.00 2079.47 server_max_640 0.54 1099 102.63 1602.09 1416.49 30.77 3152.00 2737.81 <p>CPU, without high-performance inference:</p> Configuration Avg. Time per Image (s) Avg. Characters Predicted per Second Avg. CPU Utilization (%) Peak RAM Usage (MB) Avg. RAM Usage (MB) mobile_min_1280 1.64 398 799.45 12344.10 7100.60 mobile_min_736 1.43 455 798.93 11695.40 6829.09 mobile_max_960 1.21 521 800.13 11099.10 6369.49 mobile_max_640 1.01 597 802.52 9585.48 5573.52 server_min_1280 4.48 145 800.49 50683.10 28273.30 server_min_736 3.79 172 799.24 50216.00 27902.40 server_max_960 2.67 237 797.63 49362.50 26075.60 server_max_640 2.36 251 795.18 45656.10 24900.80"},{"location":"en/version3.x/algorithm/PP-OCRv5/PP-OCRv5.html#deployment-and-secondary-development","title":"Deployment and Secondary Development","text":"<ul> <li>Multiple System Support: Compatible with mainstream operating systems including Windows, Linux, and Mac.</li> <li>Multiple Hardware Support: Besides NVIDIA GPUs, it also supports inference and deployment on Intel CPU, Kunlun chips, Ascend, and other new hardware.</li> <li>High-Performance Inference Plugin: Recommended to combine with high-performance inference plugins to further improve inference speed. See High-Performance Inference Guide for details.</li> <li>Service Deployment: Supports highly stable service deployment solutions. See Service Deployment Guide for details.</li> <li>Secondary Development Capability: Supports custom dataset training, dictionary extension, and model fine-tuning. Example: To add Korean recognition, you can extend the dictionary and fine-tune the model, seamlessly integrating into existing pipelines. See Text Detection Module Usage Tutorial and Text Recognition Module Usage Tutorial for details.</li> </ul>"},{"location":"en/version3.x/algorithm/PP-StructureV3/PP-StructureV3.html","title":"Introduction to PP-StructureV3","text":"<p>PP-StructureV3 pipeline, based on the Layout Parsing v1 pipeline, has strengthened the ability of layout detection, table recognition, and formula recognition. It has also added the ability to understand charts and restore reading order, as well as the ability to convert results into Markdown files. In various document data, it performs excellently and can handle more complex document data. This pipeline also provides flexible service-oriented deployment methods, supporting the use of multiple programming languages on various hardware. Moreover, it also provides the ability for secondary development. You can train and optimize on your own dataset based on this pipeline, and the trained model can be seamlessly integrated.</p>"},{"location":"en/version3.x/algorithm/PP-StructureV3/PP-StructureV3.html#key-metrics","title":"Key Metrics","text":"Method Type Methods Overall<sup>Edit</sup>\u2193 Text<sup>Edit</sup>\u2193 Formula<sup>Edit</sup>\u2193 Table<sup>Edit</sup>\u2193 Read Order<sup>Edit</sup>\u2193 EN ZH EN ZH EN ZH EN ZH EN ZH Pipeline Tools PP-structureV3 0.147 0.212 0.059 0.09 0.295 0.535 0.159 0.109 0.075 0.114 MinerU-0.9.3 0.15 0.357 0.061 0.215 0.278 0.577 0.18 0.344 0.079 0.292 MinerU-1.3.11 0.166 0.310 0.0826 0.2000 0.3368 0.6236 0.1613 0.1833 0.0834 0.2316 Marker-1.2.3 0.336 0.556 0.08 0.315 0.53 0.883 0.619 0.685 0.114 0.34 Mathpix 0.191 0.365 0.105 0.384 0.306 0.454 0.243 0.32 0.108 0.304 Docling-2.14.0 0.589 0.909 0.416 0.987 0.999 1 0.627 0.81 0.313 0.837 Pix2Text-1.1.2.3 0.32 0.528 0.138 0.356 0.276 0.611 0.584 0.645 0.281 0.499 Unstructured-0.17.2 0.586 0.716 0.198 0.481 0.999 1 1 0.998 0.145 0.387 OpenParse-0.7.0 0.646 0.814 0.681 0.974 0.996 1 0.284 0.639 0.595 0.641 Expert VLMs GOT-OCR 0.287 0.411 0.189 0.315 0.36 0.528 0.459 0.52 0.141 0.28 Nougat 0.452 0.973 0.365 0.998 0.488 0.941 0.572 1 0.382 0.954 Mistral OCR 0.268 0.439 0.072 0.325 0.318 0.495 0.6 0.65 0.083 0.284 OLMOCR-sglang 0.326 0.469 0.097 0.293 0.455 0.655 0.608 0.652 0.145 0.277 SmolDocling-256M_transformer 0.493 0.816 0.262 0.838 0.753 0.997 0.729 0.907 0.227 0.522 General VLMs Gemini2.0-flash 0.191 0.264 0.091 0.139 0.389 0.584 0.193 0.206 0.092 0.128 Gemini2.5-Pro 0.148 0.212 0.055 0.168 0.356 0.439 0.13 0.119 0.049 0.121 GPT4o 0.233 0.399 0.144 0.409 0.425 0.606 0.234 0.329 0.128 0.251 Qwen2-VL-72B 0.252 0.327 0.096 0.218 0.404 0.487 0.387 0.408 0.119 0.193 Qwen2.5-VL-72B 0.214 0.261 0.092 0.18 0.315 0.434 0.341 0.262 0.106 0.168 InternVL2-76B 0.44 0.443 0.353 0.29 0.543 0.701 0.547 0.555 0.317 0.228 <p>The above data is from: * OmniDocBench * OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations</p>"},{"location":"en/version3.x/algorithm/PP-StructureV3/PP-StructureV3.html#end-to-end-benchmark","title":"End to End Benchmark","text":"<p>The performance of PP-StructureV3 and MinerU with different configurations under different GPU environments are as follows.</p> <p>Requirements: * Paddle 3.0 * PaddleOCR 3.0.0 * MinerU 1.3.10 * CUDA 11.8 * cuDNN 8.9</p>"},{"location":"en/version3.x/algorithm/PP-StructureV3/PP-StructureV3.html#local-inference","title":"Local inference","text":"<p>Local inference was tested with both V100 and A100 GPU, evaluating the performance of PP-StructureV3 under 6 different configurations. The test data consists of 15 PDF files, totaling 925 pages, including elements such as tables, formulas, seals, and charts.</p> <p>In the following PP-StructureV3 configuration, please refer to PP-OCRv5 for OCR model details, see Formula Recognition for formula recognition model details, and refer to Text Detection for the max_side_limit setting of the text detection module.</p>"},{"location":"en/version3.x/algorithm/PP-StructureV3/PP-StructureV3.html#env-nvidia-tesla-v100-intel-xeon-gold-6271c","title":"Env: NVIDIA Tesla V100 + Intel Xeon Gold 6271C","text":"Methods        Configurations        Average time per   page     (s)        Average CPU     \uff08%\uff09        Peak RAM Usage     \uff08MB\uff09        Average RAM   Usage     \uff08MB\uff09        Average GPU     \uff08%\uff09        Peak VRAM Usage     \uff08MB\uff09        Average VRAM   Usage     \uff08MB\uff09        PP-StructureV3        OCR Models        Formula Recognition Model        Chart Recognition Model        text detection module max_side_limit        Server        PP-FormulaNet-L        \u2717        4096        1.77        111.4        6.7        5.2        38.9        17.0        16.5        Server        PP-FormulaNet-L        \u2714        4096        4.09        105.3        5.5        4.0        24.7        17.0        16.6        Mobile        PP-FormulaNet-L        \u2717        4096        1.56        113.7        6.6        4.9        29.1        10.7        10.6        Server        PP-FormulaNet-M        \u2717        4096        1.42        112.9        6.8        5.1        38        16.0        15.5        Mobile        PP-FormulaNet-M        \u2717        4096        1.15        114.8        6.5        5.0        26.1        8.4        8.3        Mobile        PP-FormulaNet-M        \u2717        1200        0.99        113        7.0        5.6        29.2        8.6        8.5        MinerU        -        1.57        142.9        13.3        11.8        43.3        31.6        9.7"},{"location":"en/version3.x/algorithm/PP-StructureV3/PP-StructureV3.html#nvidia-a100-intel-xeon-platinum-8350c","title":"NVIDIA A100 + Intel Xeon Platinum 8350C","text":"Methods        Configurations        Average time per   page     (s)        Average CPU     \uff08%\uff09        Peak RAM Usage     \uff08MB\uff09        Average RAM   Usage     \uff08MB\uff09        Average GPU     \uff08%\uff09        Peak VRAM Usage     \uff08MB\uff09        Average VRAM   Usage     \uff08MB\uff09        PP-StructureV3        OCR Models        Formula Recognition Model        Chart Recognition Model        text detection module max_side_limit        Server        PP-FormulaNet-L        \u2717        4096        1.12        109.8        9.2        7.8        29.8        21.8        21.1        Server        PP-FormulaNet-L        \u2714        4096        2.76        103.7        9.0        7.7        24        21.8        21.1        Mobile        PP-FormulaNet-L        \u2717        4096        1.04        110.7        9.3        7.8        22        12.2        12.1        Server        PP-FormulaNet-M        \u2717        4096        0.95        111.4        9.1        7.8        28.1        21.8        21.0        Mobile        PP-FormulaNet-M        \u2717        4096        0.89        112.1        9.2        7.8        18.5        11.4        11.2        Mobile        PP-FormulaNet-M        \u2717        1200        0.64        113.5        10.2        8.5        23.7        11.4        11.2        MinerU        -        1.06        168.3        18.3        16.8        27.5        76.9        14.8"},{"location":"en/version3.x/algorithm/PP-StructureV3/PP-StructureV3.html#serving-inference","title":"Serving Inference","text":"<p>The serving inference test is based on the NVIDIA A100 + Intel Xeon Platinum 8350C environment, with test data consisting of 1500 images, including tables, formulas, seals, charts, and other elements.</p> Instances Number Concurrent Requests Number Throughput Average Latency (s) Success Number/Total Number 4 GPUs \u2716\ufe0f 1\u200b\u5b9e\u4f8b\u200b/\u200b\u5361\u200b 4 1.69 2.36 100% 4 GPUs \u2716\ufe0f 4\u200b\u5b9e\u4f8b\u200b/\u200b\u5361\u200b 16 4.05 3.87 100%"},{"location":"en/version3.x/algorithm/PP-StructureV3/PP-StructureV3.html#pp-structurev3-demo","title":"PP-StructureV3 Demo","text":"<p>More Demos</p>"},{"location":"en/version3.x/algorithm/PP-StructureV3/PP-StructureV3.html#faq","title":"FAQ","text":"<ol> <li>What is the default configuration? How to get higher accuracy, faster speed, or smaller GPU memory?</li> </ol> <p>When using mobile OCR models + PP-FormulaNet_plus-M, and max length of text detection set to 1200, if set use_chart_recognition to False and dont not load the chart recognition model, the GPU memory would be reduced. </p> <p>On the V100, the peak and average GPU memory would be reduced from 8776.0 MB and 8680.8 MB to 6118.0 MB and 6016.7 MB, respectively; On the A100, the peak and average GPU memory would be reduced from 11716.0 MB and 11453.9 MB to 9850.0 MB and 9593.5 MB, respectively.</p> <p>You can using multi-gpus by setting <code>device</code> to <code>gpu:&lt;no.&gt;,&lt;no.&gt;</code>, such as <code>gpu:0,1,2,3</code>. And about multi-process parallel inference, you can refer: Multi-Process Parallel Inference.</p> <ol> <li>About serving deployment</li> </ol> <p>(1) Can the service handle requests concurrently?</p> <p>For the basic serving deployment solution, the service processes only one request at a time. This plan is mainly used for rapid verification, to establish the development chain, or for scenarios where concurrent requests are not required.</p> <p>For high-stability serving deployment solution, the service process only one request at a time by default, but you can refer to the related docs to adjust achieve scaling.</p> <p>\uff082\uff09How to reduce latency and improve throughput?</p> <p>Use the High-performance inference plugin, and deploy multi instances.</p>"},{"location":"en/version3.x/deployment/high_performance_inference.html","title":"High-Performance Inference","text":"<p>In real-world production environments, many applications have stringent performance requirements for deployment strategies, particularly regarding response speed, to ensure efficient system operation and a smooth user experience. PaddleOCR provides high-performance inference capabilities, allowing users to enhance model inference speed with a single click without worrying about complex configurations or underlying details. Specifically, PaddleOCR's high-performance inference functionality can:</p> <ul> <li>Automatically select an appropriate inference backend (e.g., Paddle Inference, OpenVINO, ONNX Runtime, TensorRT) based on prior knowledge and configure acceleration strategies (e.g., increasing the number of inference threads, setting FP16 precision inference);</li> <li>Automatically convert PaddlePaddle static graph models to ONNX format as needed to leverage better inference backends for acceleration.</li> </ul> <p>This document primarily introduces the installation and usage methods for high-performance inference.</p>"},{"location":"en/version3.x/deployment/high_performance_inference.html#1-prerequisites","title":"1. Prerequisites","text":""},{"location":"en/version3.x/deployment/high_performance_inference.html#11-install-high-performance-inference-dependencies","title":"1.1 Install High-Performance Inference Dependencies","text":"<p>Install the dependencies required for high-performance inference using the PaddleOCR CLI:</p> <pre><code>paddleocr install_hpi_deps {device_type}\n</code></pre> <p>The supported device types are:</p> <ul> <li><code>cpu</code>: For CPU-only inference. Currently supports Linux systems, x86-64 architecture processors, and Python 3.8-3.12.</li> <li><code>gpu</code>: For inference using either CPU or NVIDIA GPU. Currently supports Linux systems, x86-64 architecture processors, and Python 3.8-3.12. Refer to the next subsection for detailed instructions.</li> </ul> <p>Only one type of device dependency should exist in the same environment. For Windows systems, it is currently recommended to install within a Docker container or WSL environment.</p> <p>It is recommended to use the official PaddlePaddle Docker image to install high-performance inference dependencies. The corresponding images for each device type are as follows:</p> <ul> <li><code>cpu</code>: <code>ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/paddle:3.0.0</code></li> <li><code>gpu</code>:<ul> <li>CUDA 11.8: <code>ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/paddle:3.0.0-gpu-cuda11.8-cudnn8.9-trt8.6</code></li> </ul> </li> </ul>"},{"location":"en/version3.x/deployment/high_performance_inference.html#12-detailed-gpu-environment-instructions","title":"1.2 Detailed GPU Environment Instructions","text":"<p>First, ensure that the environment has the required CUDA and cuDNN versions installed. Currently, PaddleOCR only supports CUDA and cuDNN versions compatible with CUDA 11.8 + cuDNN 8.9. Below are the installation instructions for CUDA 11.8 and cuDNN 8.9:</p> <ul> <li>Install CUDA 11.8</li> <li>Install cuDNN 8.9</li> </ul> <p>If using the official PaddlePaddle image, the CUDA and cuDNN versions in the image already meet the requirements, and no additional installation is needed.</p> <p>If installing PaddlePaddle via pip, the relevant Python packages for CUDA and cuDNN will typically be installed automatically. In this case, you still need to install the non-Python-specific CUDA and cuDNN versions. It is also recommended to install CUDA and cuDNN versions that match the Python package versions in your environment to avoid potential issues caused by coexisting library versions. You can check the versions of the CUDA and cuDNN-related Python packages with the following commands:</p> <pre><code># CUDA-related Python package versions\npip list | grep nvidia-cuda\n# cuDNN-related Python package versions\npip list | grep nvidia-cudnn\n</code></pre> <p>Secondly, ensure that the environment has the required TensorRT version installed. Currently, PaddleOCR only supports TensorRT 8.6.1.6. If using the official PaddlePaddle image, you can install the TensorRT wheel package with the following command:</p> <pre><code>python -m pip install /usr/local/TensorRT-*/python/tensorrt-*-cp310-none-linux_x86_64.whl\n</code></pre> <p>For other environments, refer to the TensorRT documentation to install TensorRT. Here is an example:</p> <pre><code># Download the TensorRT tar file\nwget https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/secure/8.6.1/tars/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-11.8.tar.gz\n# Extract the TensorRT tar file\ntar xvf TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-11.8.tar.gz\n# Install the TensorRT wheel package\npython -m pip install TensorRT-8.6.1.6/python/tensorrt-8.6.1-cp310-none-linux_x86_64.whl\n# Add the absolute path of the TensorRT `lib` directory to LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:TensorRT-8.6.1.6/lib\"\n</code></pre>"},{"location":"en/version3.x/deployment/high_performance_inference.html#2-executing-high-performance-inference","title":"2. Executing High-Performance Inference","text":"<p>For the PaddleOCR CLI, specify <code>--enable_hpi</code> as <code>True</code> to execute high-performance inference. For example:</p> <pre><code>paddleocr ocr --enable_hpi True ...\n</code></pre> <p>For the PaddleOCR Python API, set <code>enable_hpi</code> to <code>True</code> when initializing the pipeline or module object to enable high-performance inference when calling the inference method. For example:</p> <pre><code>from paddleocr import PaddleOCR\npipeline = PaddleOCR(enable_hpi=True)\nresult = pipeline.predict(...)\n</code></pre>"},{"location":"en/version3.x/deployment/high_performance_inference.html#3-notes","title":"3. Notes","text":"<ol> <li> <p>For some models, the first execution of high-performance inference may take longer to complete the construction of the inference engine. Relevant information about the inference engine will be cached in the model directory after the first construction, and subsequent initializations can reuse the cached content to improve speed.</p> </li> <li> <p>Currently, due to reasons such as not using static graph format models or the presence of unsupported operators, some models may not achieve inference acceleration.</p> </li> <li> <p>During high-performance inference, PaddleOCR automatically handles the conversion of model formats and selects the optimal inference backend whenever possible. Additionally, PaddleOCR supports users specifying ONNX models. For information on converting PaddlePaddle static graph models to ONNX format, refer to Obtaining ONNX Models.</p> </li> <li> <p>The high-performance inference capabilities of PaddleOCR rely on PaddleX and its high-performance inference plugins. By passing in a custom PaddleX production line configuration file, you can configure the inference backend and other related settings. Please refer to Using PaddleX Production Line Configuration Files and the PaddleX High-Performance Inference Guide to learn how to adjust the high-performance inference configurations.</p> </li> </ol>"},{"location":"en/version3.x/deployment/mcp_server.html","title":"PaddleOCR MCP Server","text":"<p>This project provides a lightweight Model Context Protocol (MCP) server designed to integrate the powerful capabilities of PaddleOCR into a compatible MCP Host.</p>"},{"location":"en/version3.x/deployment/mcp_server.html#key-features","title":"Key Features","text":"<ul> <li>Currently Supported Pipelines<ul> <li>OCR: Performs text detection and recognition on images and PDF files.</li> <li>PP-StructureV3: Recognizes and extracts text blocks, titles, paragraphs, images, tables, and other layout elements from an image or PDF file, converting the input into a Markdown document.</li> </ul> </li> <li>Supports the following working modes:<ul> <li>Local: Runs the PaddleOCR pipeline directly on your machine using the installed Python library.</li> <li>AI Studio: Calls cloud services provided by the Paddle AI Studio community.</li> <li>Self-hosted: Calls a PaddleOCR service that you deploy yourself (serving).</li> </ul> </li> </ul>"},{"location":"en/version3.x/deployment/mcp_server.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Installation</li> <li>2. Quick Start</li> <li>3. Configuration</li> <li>3.1. MCP Host Configuration</li> <li>3.2. Working Modes Explained<ul> <li>Mode 1: AI Studio Service (<code>aistudio</code>)</li> <li>Mode 2: Local Python Library (<code>local</code>)</li> <li>Mode 3: Self-hosted Service (<code>self_hosted</code>)</li> </ul> </li> <li>4. Parameter Reference</li> <li>5. Configuration Examples</li> <li>5.1 AI Studio Service Configuration</li> <li>5.2 Local Python Library Configuration</li> <li>5.3 Self-hosted Service Configuration</li> </ul>"},{"location":"en/version3.x/deployment/mcp_server.html#1-installation","title":"1. Installation","text":"<pre><code># Install the wheel\npip install https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/mcp/paddleocr_mcp/releases/v0.1.0/paddleocr_mcp-0.1.0-py3-none-any.whl\n\n# Or, install from source\n# git clone https://github.com/PaddlePaddle/PaddleOCR.git\n# pip install -e mcp_server\n</code></pre> <p>Some working modes may require additional dependencies.</p>"},{"location":"en/version3.x/deployment/mcp_server.html#2-quick-start","title":"2. Quick Start","text":"<p>This section guides you through a quick setup using Claude Desktop as the MCP Host and the AI Studio mode. This mode is recommended for new users as it does not require complex local dependencies. Please refer to 3. Configuration for other working modes and more configuration options.</p> <ol> <li> <p>Prepare the AI Studio Service</p> <ul> <li>Visit the Paddle AI Studio community and log in.</li> <li>In the \"PaddleX Pipeline\" section under \"More\" on the left, navigate to [Create Pipeline] - [OCR] - [General OCR] - [Deploy Directly] - [Text Recognition Module, select PP-OCRv5_server_rec] - [Start Deployment].</li> <li>Once deployed, obtain your Service Base URL (e.g., <code>https://xxxxxx.aistudio-hub.baidu.com</code>).</li> <li>Get your Access Token from this page.</li> </ul> </li> <li> <p>Locate the MCP Configuration File - For details, refer to the Official MCP Documentation.</p> <ul> <li>macOS: <code>~/Library/Application Support/Claude/claude_desktop_config.json</code></li> <li>Windows: <code>%APPDATA%\\Claude\\claude_desktop_config.json</code></li> <li>Linux: <code>~/.config/Claude/claude_desktop_config.json</code></li> </ul> </li> <li> <p>Add MCP Server Configuration     Open the <code>claude_desktop_config.json</code> file and add the configuration by referring to 5.1 AI Studio Service Configuration.</p> <p>Note: - Do not leak your Access Token. - If <code>paddleocr_mcp</code> is not in your system's <code>PATH</code>, set <code>command</code> to the absolute path of the executable.</p> </li> <li> <p>Restart the MCP Host     Restart Claude Desktop. The new <code>paddleocr-ocr</code> tool should now be available in the application.</p> </li> </ol>"},{"location":"en/version3.x/deployment/mcp_server.html#3-configuration","title":"3. Configuration","text":""},{"location":"en/version3.x/deployment/mcp_server.html#31-mcp-host-configuration","title":"3.1. MCP Host Configuration","text":"<p>In the Host's configuration file (e.g., <code>claude_desktop_config.json</code>), you need to define how to start the tool server. Key fields are: - <code>command</code>: <code>paddleocr_mcp</code> (if the executable is in your <code>PATH</code>) or an absolute path. - <code>args</code>: Configurable command-line arguments, e.g., <code>[\"--verbose\"]</code>. See 4. Parameter Reference. - <code>env</code>: Configurable environment variables. See 4. Parameter Reference.</p>"},{"location":"en/version3.x/deployment/mcp_server.html#32-working-modes-explained","title":"3.2. Working Modes Explained","text":"<p>You can configure the MCP server to run in different modes based on your needs.</p>"},{"location":"en/version3.x/deployment/mcp_server.html#mode-1-ai-studio-service-aistudio","title":"Mode 1: AI Studio Service (<code>aistudio</code>)","text":"<p>This mode calls services from the Paddle AI Studio community. - Use Case: Ideal for quickly trying out features, validating solutions, and for no-code development scenarios. - Procedure: Please refer to 2. Quick Start. - In addition to using the platform's preset model solutions, you can also train and deploy custom models on the platform.</p>"},{"location":"en/version3.x/deployment/mcp_server.html#mode-2-local-python-library-local","title":"Mode 2: Local Python Library (<code>local</code>)","text":"<p>This mode runs the model directly on your local machine and has certain requirements for the local environment and computer performance. It relies on the installed <code>paddleocr</code> inference package. - Use Case: Suitable for offline usage and scenarios with strict data privacy requirements. - Procedure:     1.  Refer to the PaddleOCR Installation Guide to install the PaddlePaddle framework and PaddleOCR. It is strongly recommended to install them in a separate virtual environment to avoid dependency conflicts.     2.  Refer to 5.2 Local Python Library Configuration to modify the <code>claude_desktop_config.json</code> file.</p>"},{"location":"en/version3.x/deployment/mcp_server.html#mode-3-self-hosted-service-self_hosted","title":"Mode 3: Self-hosted Service (<code>self_hosted</code>)","text":"<p>This mode calls a PaddleOCR inference service that you have deployed yourself. This corresponds to the Serving solutions provided by PaddleX. - Use Case: Offers the advantages of service-oriented deployment and high flexibility, making it well-suited for production environments, especially for scenarios requiring custom service configurations. - Procedure:     1.  Refer to the PaddleOCR Installation Guide to install the PaddlePaddle framework and PaddleOCR.     2.  Refer to the PaddleOCR Serving Deployment Guide to run the server.     3.  Refer to 5.3 Self-hosted Service Configuration to modify the <code>claude_desktop_config.json</code> file.     4. Set your service address in <code>PADDLEOCR_MCP_SERVER_URL</code> (e.g., <code>\"http://127.0.0.1:8080\"</code>).</p>"},{"location":"en/version3.x/deployment/mcp_server.html#4-parameter-reference","title":"4. Parameter Reference","text":"<p>You can control the server's behavior via environment variables or command-line arguments.</p> Environment Variable Command-line Argument Type Description Options Default <code>PADDLEOCR_MCP_PIPELINE</code> <code>--pipeline</code> <code>str</code> The pipeline to run <code>\"OCR\"</code>, <code>\"PP-StructureV3\"</code> <code>\"OCR\"</code> <code>PADDLEOCR_MCP_PPOCR_SOURCE</code> <code>--ppocr_source</code> <code>str</code> The source of PaddleOCR capabilities <code>\"local\"</code>, <code>\"aistudio\"</code>, <code>\"self_hosted\"</code> <code>\"local\"</code> <code>PADDLEOCR_MCP_SERVER_URL</code> <code>--server_url</code> <code>str</code> Base URL of the underlying service (required for <code>aistudio</code> or <code>self_hosted</code> mode) - <code>None</code> <code>PADDLEOCR_MCP_AISTUDIO_ACCESS_TOKEN</code> <code>--aistudio_access_token</code> <code>str</code> AI Studio authentication token (required for <code>aistudio</code> mode) - <code>None</code> <code>PADDLEOCR_MCP_TIMEOUT</code> <code>--timeout</code> <code>int</code> Request timeout for the underlying service (in seconds) - <code>30</code> <code>PADDLEOCR_MCP_DEVICE</code> <code>--device</code> <code>str</code> Specify the device for inference (only effective in <code>local</code> mode) - <code>None</code> <code>PADDLEOCR_MCP_PIPELINE_CONFIG</code> <code>--pipeline_config</code> <code>str</code> Path to the PaddleX pipeline configuration file (only effective in <code>local</code> mode) - <code>None</code> - <code>--http</code> <code>bool</code> Use HTTP transport instead of stdio (for remote deployment and multiple clients) - <code>False</code> - <code>--host</code> <code>str</code> Host address for HTTP mode - <code>\"127.0.0.1\"</code> - <code>--port</code> <code>int</code> Port for HTTP mode - <code>8080</code> - <code>--verbose</code> <code>bool</code> Enable verbose logging for debugging - <code>False</code>"},{"location":"en/version3.x/deployment/mcp_server.html#5-configuration-examples","title":"5. Configuration Examples","text":"<p>Below are complete configuration examples for different working modes. You can copy and modify them as needed.</p>"},{"location":"en/version3.x/deployment/mcp_server.html#51-ai-studio-service-configuration","title":"5.1 AI Studio Service Configuration","text":"<pre><code>{\n  \"mcpServers\": {\n    \"paddleocr-ocr\": {\n      \"command\": \"paddleocr_mcp\",\n      \"args\": [],\n      \"env\": {\n        \"PADDLEOCR_MCP_PIPELINE\": \"OCR\",\n        \"PADDLEOCR_MCP_PPOCR_SOURCE\": \"aistudio\",\n        \"PADDLEOCR_MCP_SERVER_URL\": \"&lt;your-server-url&gt;\", \n        \"PADDLEOCR_MCP_AISTUDIO_ACCESS_TOKEN\": \"&lt;your-access-token&gt;\"\n      }\n    }\n  }\n}\n</code></pre> <p>Note: - Replace <code>&lt;your-server-url&gt;</code> with your AI Studio Service Base URL, e.g., <code>https://xxxxx.aistudio-hub.baidu.com</code>. Do not include endpoint paths (like <code>/ocr</code>). - Replace <code>&lt;your-access-token&gt;</code> with your Access Token.</p>"},{"location":"en/version3.x/deployment/mcp_server.html#52-local-python-library-configuration","title":"5.2 Local Python Library Configuration","text":"<pre><code>{\n  \"mcpServers\": {\n    \"paddleocr-ocr\": {\n      \"command\": \"paddleocr_mcp\",\n      \"args\": [],\n      \"env\": {\n        \"PADDLEOCR_MCP_PIPELINE\": \"OCR\",\n        \"PADDLEOCR_MCP_PPOCR_SOURCE\": \"local\"\n      }\n    }\n  }\n}\n</code></pre> <p>Note: - <code>PADDLEOCR_MCP_PIPELINE_CONFIG</code> is optional. If not set, the default pipeline configuration is used. To adjust settings, such as changing models, refer to the PaddleOCR and PaddleX documentation, export a pipeline configuration file, and set <code>PADDLEOCR_MCP_PIPELINE_CONFIG</code> to its absolute path.</p>"},{"location":"en/version3.x/deployment/mcp_server.html#53-self-hosted-service-configuration","title":"5.3 Self-hosted Service Configuration","text":"<pre><code>{\n  \"mcpServers\": {\n    \"paddleocr-ocr\": {\n      \"command\": \"paddleocr_mcp\",\n      \"args\": [],\n      \"env\": {\n        \"PADDLEOCR_MCP_PIPELINE\": \"OCR\",\n        \"PADDLEOCR_MCP_PPOCR_SOURCE\": \"self_hosted\",\n        \"PADDLEOCR_MCP_SERVER_URL\": \"&lt;your-server-url&gt;\"\n      }\n    }\n  }\n}\n</code></pre> <p>Note: - Replace <code>&lt;your-server-url&gt;</code> with the base URL of your underlying service (e.g., <code>http://127.0.0.1:8080</code>). </p>"},{"location":"en/version3.x/deployment/obtaining_onnx_models.html","title":"Obtaining ONNX Models","text":"<p>PaddleOCR provides a rich collection of pre-trained models, all stored in PaddlePaddle's static graph format. To use these models in ONNX format during deployment, you can convert them using the Paddle2ONNX plugin provided by PaddleX. For more information about PaddleX and its relationship with PaddleOCR, refer to Differences and Connections Between PaddleOCR and PaddleX.</p> <p>First, install the Paddle2ONNX plugin for PaddleX using the following command via the PaddleX CLI:</p> <pre><code># Windows users need to use the following command to install the dev version of paddlepaddle\n# python -m pip install --pre paddlepaddle -i https://www.paddlepaddle.org.cn/packages/nightly/cpu/\n\npaddlex --install paddle2onnx\n</code></pre> <p>Then, execute the following command to complete the model conversion:</p> <pre><code>paddlex \\\n    --paddle2onnx \\  # Use the paddle2onnx feature\n    --paddle_model_dir /your/paddle_model/dir \\  # Specify the directory containing the Paddle model\n    --onnx_model_dir /your/onnx_model/output/dir \\  # Specify the output directory for the converted ONNX model\n    --opset_version 7  # Specify the ONNX opset version to use\n</code></pre> <p>The parameters are described as follows:</p> Parameter Type Description paddle_model_dir str The directory containing the Paddle model. onnx_model_dir str The output directory for the ONNX model. It can be the same as the Paddle model directory. Defaults to <code>onnx</code>. opset_version int The ONNX opset version to use. If conversion fails with a lower opset version, a higher version will be automatically selected for conversion. Defaults to <code>7</code>."},{"location":"en/version3.x/deployment/on_device_deployment.html","title":"OCR On-Device Deployment Demo Usage Guide","text":"<ul> <li>Quick Start<ul> <li>Environment Preparation</li> <li>Deployment Steps</li> </ul> </li> <li>Code Introduction</li> <li>Project Explanation</li> <li>Advanced Usage<ul> <li>Update Prediction Library </li> <li>Convert NB Model </li> <li>Update Model, Label File, and Prediction Image<ul> <li>Update Model</li> <li>Update Label File</li> <li>Update Prediction Image</li> </ul> </li> <li>Update Input/Output Preprocessing</li> </ul> </li> </ul> <p>This guide mainly introduces how to run the PaddleX on-device deployment demo for OCR text recognition on an Android shell.</p> <p>The following OCR models are supported in this guide:</p> <ul> <li>PP-OCRv3_mobile (cpu)</li> <li>PP-OCRv4_mobile (cpu)</li> <li>PP-OCRv5_mobile (cpu)</li> </ul>"},{"location":"en/version3.x/deployment/on_device_deployment.html#quick-start","title":"Quick Start","text":""},{"location":"en/version3.x/deployment/on_device_deployment.html#environment-preparation","title":"Environment Preparation","text":"<ol> <li> <p>Install the CMAKE compilation tool in your local environment and download an NDK package for your current system from the Android NDK official website. For example, if developing on a Mac, download the NDK package for the Mac platform from the Android NDK official website.</p> <p>Environment Requirements</p> <ul> <li><code>CMake &gt;= 3.10</code> (the minimum version has not been verified; 3.20 or above is recommended)</li> <li><code>Android NDK &gt;= r17c</code> (the minimum version has not been verified; r20b or above is recommended)</li> </ul> <p>Test Environment Used in This Guide:</p> <ul> <li><code>cmake == 3.20.0</code></li> <li><code>android-ndk == r20b</code></li> </ul> </li> <li> <p>Prepare an Android phone and enable USB debugging mode. Method: <code>Phone Settings -&gt; Find Developer Options -&gt; Enable Developer Options and USB Debugging Mode</code></p> </li> <li> <p>Install the ADB tool on your computer for debugging. The installation methods for ADB are as follows:</p> <p>3.1. Install ADB on a Mac:</p> <pre><code>brew cask install android-platform-tools\n</code></pre> <p>3.2. Install ADB on Linux:</p> <pre><code>sudo apt update\nsudo apt install -y wget adb\n</code></pre> <p>3.3. Install ADB on Windows:</p> <p>For installation on Windows, download and install the ADB software package from Google's Android platform: Link</p> <p>Open a terminal, connect your phone to the computer, and enter the following command in the terminal:</p> <pre><code>adb devices\n</code></pre> <p>If there is a <code>device</code> output, the installation is successful.</p> <pre><code>List of devices attached\n744be294    device\n</code></pre> </li> </ol>"},{"location":"en/version3.x/deployment/on_device_deployment.html#material-preparation","title":"Material Preparation","text":"<ol> <li> <p>Clone the <code>feature/paddle-x</code> branch of the <code>Paddle-Lite-Demo</code> repository to the <code>PaddleX-Lite-Deploy</code> directory.</p> <pre><code>git clone -b feature/paddle-x https://github.com/PaddlePaddle/Paddle-Lite-Demo.git PaddleX-Lite-Deploy\n</code></pre> </li> <li> <p>Fill out the questionnaire to download the compressed package. Place the compressed package in the specified extraction directory, switch to the specified extraction directory, and execute the extraction command.</p> <pre><code># 1. Switch to the specified extraction directory\ncd PaddleX-Lite-Deploy/ocr/android/shell/ppocr_demo\n\n# 2. Execute the extraction command\nunzip ocr.zip\n</code></pre> </li> </ol>"},{"location":"en/version3.x/deployment/on_device_deployment.html#deployment-steps","title":"Deployment Steps","text":"<ol> <li> <p>Switch the working directory to <code>PaddleX-Lite-Deploy/libs</code> and run the <code>download.sh</code> script to download the required Paddle Lite prediction library. This step only needs to be executed once to support each demo.</p> </li> <li> <p>Switch the working directory to <code>PaddleX-Lite-Deploy/ocr/assets</code> and run the <code>download.sh</code> script to download the paddle_lite_opt tool-optimized NB model file, prediction images, dictionary files, and other materials.</p> </li> <li> <p>Switch the working directory to <code>PaddleX-Lite-Deploy/ocr/android/shell/cxx/ppocr_demo</code> and run the <code>build.sh</code> script to complete the compilation of the executable file.</p> </li> <li> <p>Switch the working directory to <code>PaddleX-Lite-Deploy/ocr/android/shell/cxx/ppocr_demo</code> and run the <code>run.sh</code> script to complete the on-device prediction.</p> </li> </ol> <p>Notes:</p> <ul> <li>Before running the <code>build.sh</code> script, change the path specified by <code>NDK_ROOT</code> to the actual installation path of NDK.</li> <li>On Windows systems, you can use Git Bash to execute the deployment steps.</li> <li>If compiling on a Windows system, set <code>CMAKE_SYSTEM_NAME</code> to <code>windows</code> in <code>CMakeLists.txt</code>.</li> <li>If compiling on a Mac system, set <code>CMAKE_SYSTEM_NAME</code> to <code>darwin</code> in <code>CMakeLists.txt</code>.</li> <li>Keep the ADB connection active when running the <code>run.sh</code> script.</li> <li>The <code>download.sh</code> and <code>run.sh</code> scripts support passing parameters to specify the model. If no model is specified, the <code>PP-OCRv5_mobile</code> model is used by default. The following models are currently supported:<ul> <li><code>PP-OCRv3_mobile</code></li> <li><code>PP-OCRv4_mobile</code></li> <li><code>PP-OCRv5_mobile</code></li> </ul> </li> </ul> <p>Here is an example of the actual operation:</p> <pre><code># 1. Download the required Paddle Lite prediction library\ncd PaddleX-Lite-Deploy/libs\nsh download.sh\n\n# 2. Download the paddle_lite_opt tool-optimized NB model file, prediction images, dictionary files, and other materials\ncd ../ocr/assets\nsh download.sh PP-OCRv5_mobile\n\n# 3. Complete the compilation of the executable file\ncd ../android/shell/ppocr_demo\nsh build.sh\n\n# 4. Prediction\nsh run.sh PP-OCRv5_mobile\n</code></pre> <p>The output is as follows:</p> <pre><code>The detection visualized image saved in ./test_img_result.jpg\n0       \u200b\u7eaf\u81fb\u200b\u8425\u517b\u200b\u62a4\u53d1\u7d20\u200b  0.998541\n1       \u200b\u4ea7\u54c1\u200b\u4fe1\u606f\u200b/\u200b\u53c2\u6570\u200b   0.999094\n2       (45\u200b\u5143\u200b/\u200b\u6bcf\u516c\u65a4\u200b\uff0c100\u200b\u516c\u65a4\u200b\u8d77\u8ba2\u200b\uff09     0.948841\n3       \u200b\u6bcf\u74f6\u200b22\u200b\u5143\u200b\uff0c1000\u200b\u74f6\u200b\u8d77\u8ba2\u200b)   0.961245\n4       \u3010\u200b\u54c1\u724c\u200b\u3011\uff1a\u200b\u4ee3\u52a0\u5de5\u200b\u65b9\u5f0f\u200b/OEMODM     0.970401\n5       \u3010\u200b\u54c1\u540d\u200b\u3011\uff1a\u200b\u7eaf\u81fb\u200b\u8425\u517b\u200b\u62a4\u53d1\u7d20\u200b        0.977496\n6       ODMOEM  0.955396\n7       \u3010\u200b\u4ea7\u54c1\u7f16\u53f7\u200b\u3011\uff1aYM-X-3011 0.977864\n8       \u3010\u200b\u51c0\u542b\u91cf\u200b\u3011\uff1a220ml       0.970538\n9       \u3010\u200b\u9002\u7528\u4eba\u7fa4\u200b\u3011\uff1a\u200b\u9002\u5408\u200b\u6240\u6709\u200b\u80a4\u8d28\u200b      0.995907\n10      \u3010\u200b\u4e3b\u8981\u200b\u6210\u5206\u200b\u3011\uff1a\u200b\u9cb8\u8721\u200b\u786c\u8102\u200b\u9187\u200b\u3001\u200b\u71d5\u9ea6\u200b\u03b2-\u200b\u8461\u805a\u200b    0.975813\n11      \u200b\u7cd6\u200b\u3001\u200b\u6930\u6cb9\u200b\u9170\u80fa\u200b\u4e19\u57fa\u200b\u751c\u83dc\u78b1\u200b\u3001\u200b\u6cdb\u918c\u200b    0.964397\n12      \uff08\u200b\u6210\u54c1\u200b\u5305\u6750\u200b\uff09    0.97298\n13      \u3010\u200b\u4e3b\u8981\u200b\u529f\u80fd\u200b\u3011\uff1a\u200b\u53ef\u200b\u7d27\u81f4\u200b\u5934\u53d1\u200b\u78f7\u5c42\u200b\uff0c\u200b\u4ece\u800c\u200b\u8fbe\u5230\u200b  0.989097\n14      \u200b\u5373\u65f6\u200b\u6301\u4e45\u200b\u6539\u5584\u200b\u5934\u53d1\u200b\u5149\u6cfd\u200b\u7684\u200b\u6548\u679c\u200b\uff0c\u200b\u7ed9\u200b\u5e72\u71e5\u200b\u7684\u200b\u5934\u200b  0.990088\n15      \u200b\u53d1\u200b\u8db3\u591f\u200b\u7684\u200b\u6ecb\u517b\u200b    0.998037\n</code></pre> <p></p>"},{"location":"en/version3.x/deployment/on_device_deployment.html#code-introduction","title":"Code Introduction","text":"<pre><code>.\n\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 ocr \n\u2502    \u251c\u2500\u2500 ...\n\u2502    \u251c\u2500\u2500 android\n\u2502    \u2502    \u251c\u2500\u2500 ...\n\u2502    \u2502    \u2514\u2500\u2500 shell\n\u2502    \u2502        \u2514\u2500\u2500 ppocr_demo\n\u2502    \u2502            \u251c\u2500\u2500 src # Contains prediction code\n\u2502    \u2502            |   \u251c\u2500\u2500 cls_process.cc # Full inference process for orientation classifier, including preprocessing, prediction, and postprocessing\n\u2502    \u2502            |   \u251c\u2500\u2500 rec_process.cc # Full inference process for recognition model CRNN, including preprocessing, prediction, and postprocessing\n\u2502    \u2502            |   \u251c\u2500\u2500 det_process.cc # Full inference process for detection model CRNN, including preprocessing, prediction, and postprocessing\n\u2502    \u2502            |   \u251c\u2500\u2500 det_post_process.cc # Postprocessing file for detection model DB\n\u2502    \u2502            |   \u251c\u2500\u2500 pipeline.cc # Full inference process code for OCR text recognition demo\n\u2502    \u2502            |   \u2514\u2500\u2500 MakeFile # MakeFile file for prediction code\n\u2502    \u2502            |   \n\u2502    \u2502            \u251c\u2500\u2500 CMakeLists.txt # CMake file that defines the compilation method for the executable\n\u2502    \u2502            \u251c\u2500\u2500 README.md\n\u2502    \u2502            \u251c\u2500\u2500 build.sh # Used for compiling the executable\n\u2502    \u2502            \u2514\u2500\u2500 run.sh # Used for prediction\n\u2502    \u2514\u2500\u2500 assets # Stores models, test images, label files, and config files\n\u2502        \u251c\u2500\u2500 images # Stores test images\n\u2502        \u251c\u2500\u2500 labels # Stores dictionary files (see remarks below for details)\n\u2502        \u251c\u2500\u2500 models # Stores nb models\n\u2502        \u251c\u2500\u2500 config.txt\n\u2502        \u2514\u2500\u2500 download.sh # Download script for paddle_lite_opt tool-optimized models\n\u2514\u2500\u2500 libs # Stores prediction libraries and OpenCV libraries for different platforms.\n    \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 download.sh # Download script for Paddle Lite prediction libraries and OpenCV libraries\n</code></pre> <p>Remarks:</p> <ul> <li>The <code>PaddleX-Lite-Deploy/ocr/assets/labels/</code> directory contains the dictionary files <code>ppocr_keys_v1.txt</code> for PP-OCRv3 and PP-OCRv4 models, and <code>ppocr_keys_ocrv5.txt</code> for the PP-OCRv5 model. The appropriate dictionary file is automatically selected during inference based on the model name, so no manual intervention is required.</li> <li>If you are using an English/numeric or other language model, you need to replace it with the corresponding language dictionary. The PaddleOCR repository provides some dictionary files.</li> </ul> <pre><code># Parameters of the executable in run.sh script:\nadb shell \"cd ${ppocr_demo_path} \\\n           &amp;&amp; chmod +x ./ppocr_demo \\\n           &amp;&amp; export LD_LIBRARY_PATH=${ppocr_demo_path}:${LD_LIBRARY_PATH} \\\n           &amp;&amp; ./ppocr_demo \\\n                \\\"./models/${MODEL_NAME}_det.nb\\\" \\\n                \\\"./models/${MODEL_NAME}_rec.nb\\\" \\\n                ./models/${CLS_MODEL_FILE} \\\n                ./images/test.jpg \\\n                ./test_img_result.jpg \\\n                ./labels/${LABEL_FILE} \\\n                ./config.txt\"\n\nFirst parameter: ppocr_demo executable\nSecond parameter: ./models/${MODEL_NAME}_det.nb  Detection model .nb file\nThird parameter: ./models/${MODEL_NAME}_rec.nb  Recognition model .nb file\nFourth parameter: ./models/${CLS_MODEL_FILE} Text line orientation classification model .nb file (automatically selected based on model name by default)\nFifth parameter: ./images/test.jpg  Test image\nSixth parameter: ./test_img_result.jpg  Result save file\nSeventh parameter: ./labels/${LABEL_FILE}  Label file (automatically selected based on model name by default)\nEighth parameter: ./config.txt  Configuration file containing hyperparameters for the detection and classification models\n</code></pre> <pre><code># List of Specific Parameters in config.txt:\nmax_side_len  960         # When the width or height of the input image is greater than 960, the image is scaled proportionally so that the longest side of the image is 960.\ndet_db_thresh  0.3        # Used to filter the binarized images predicted by DB; setting it to 0.3 has no significant impact on the results.\ndet_db_box_thresh  0.5    # Threshold for filtering boxes in the DB post-processing; if there are missing boxes in detection, you may reduce this value.\ndet_db_unclip_ratio  1.6  # Represents the compactness of the text box; the smaller the value, the closer the text box is to the text.\nuse_direction_classify  0  # Whether to use a direction classifier: 0 means not using it, 1 means using it.\n</code></pre>"},{"location":"en/version3.x/deployment/on_device_deployment.html#engineering-details","title":"Engineering Details","text":"<p>The OCR text recognition demo accomplishes the OCR text recognition function using three models collaboratively. First, the input image undergoes detection processing via the <code>${MODEL_NAME}_det.nb</code> model, followed by text direction classification using the <code>ch_ppocr_mobile_v2.0_cls_slim_opt.nb</code> model, and finally, text recognition with the <code>${MODEL_NAME}_rec.nb</code> model.</p> <ol> <li> <p><code>pipeline.cc</code>: Full-process prediction code for the OCR text recognition demo   This file handles the entire process control for serial inference of the three models, including scheduling for the entire processing flow.</p> <ul> <li>The <code>Pipeline::Pipeline(...)</code> method initializes the three model class constructors, accomplishes model loading, thread count, core binding, and predictor creation.</li> <li>The <code>Pipeline::Process(...)</code> method manages the entire process control for serial inference of the three models.</li> </ul> </li> <li> <p><code>cls_process.cc</code>: Prediction file for the direction classifier   This file handles the preprocessing, prediction, and postprocessing for the direction classifier.</p> <ul> <li>The <code>ClsPredictor::ClsPredictor()</code> method initializes model loading, thread count, core binding, and predictor creation.</li> <li>The <code>ClsPredictor::Preprocess()</code> method handles model preprocessing.</li> <li>The <code>ClsPredictor::Postprocess()</code> method handles model postprocessing.</li> </ul> </li> <li> <p><code>rec_process.cc</code>: Prediction file for the CRNN recognition model   This file handles the preprocessing, prediction, and postprocessing for the CRNN recognition model.</p> <ul> <li>The <code>RecPredictor::RecPredictor()</code> method initializes model loading, thread count, core binding, and predictor creation.</li> <li>The <code>RecPredictor::Preprocess()</code> method handles model preprocessing.</li> <li>The <code>RecPredictor::Postprocess()</code> method handles model postprocessing.</li> </ul> </li> <li> <p><code>det_process.cc</code>: Prediction file for the DB detection model   This file handles the preprocessing, prediction, and postprocessing for the DB detection model.</p> <ul> <li>The <code>DetPredictor::DetPredictor()</code> method initializes model loading, thread count, core binding, and predictor creation.</li> <li>The <code>DetPredictor::Preprocess()</code> method handles model preprocessing.</li> <li>The <code>DetPredictor::Postprocess()</code> method handles model postprocessing.</li> </ul> </li> <li> <p><code>db_post_process</code>: Postprocessing functions for the DB detection model, including calls to the clipper library   This file implements third-party library calls and other postprocessing methods for the DB detection model.</p> <ul> <li>The <code>std::vector&lt;std::vector&lt;std::vector&lt;int&gt;&gt;&gt; BoxesFromBitmap(...)</code> method retrieves detection boxes from a Bitmap.</li> <li>The <code>std::vector&lt;std::vector&lt;std::vector&lt;int&gt;&gt;&gt; FilterTagDetRes(...)</code> method retrieves target box positions based on recognition results.</li> </ul> </li> </ol>"},{"location":"en/version3.x/deployment/on_device_deployment.html#advanced-usage","title":"Advanced Usage","text":"<p>If the quick start section does not meet your needs, refer to this section for custom modifications to the demo.</p> <p>This section mainly includes four parts:</p> <ul> <li>Updating the prediction library;</li> <li>Converting <code>.nb</code> models;</li> <li>Updating models, label files, and prediction images;</li> <li>Updating input/output preprocessing.</li> </ul>"},{"location":"en/version3.x/deployment/on_device_deployment.html#updating-the-prediction-library","title":"Updating the Prediction Library","text":"<p>The prediction library used in this guide is the latest version (214rc), and manual updates are not recommended.</p> <p>If you need to use a different version, follow these steps to update the prediction library:</p> <ul> <li>Paddle Lite project: https://github.com/PaddlePaddle/Paddle-Lite</li> <li>Refer to the Paddle Lite Source Code Compilation Documentation to compile the Android prediction library.</li> <li>The final compilation output is located in <code>build.lite.xxx.xxx.xxx</code> under <code>inference_lite_lib.xxx.xxx</code>.<ul> <li>Replace the C++ library:<ul> <li>Header files:   Replace the <code>PaddleX-Lite-Deploy/libs/android/cxx/include</code> folder in the demo with the generated <code>build.lite.android.xxx.gcc/inference_lite_lib.android.xxx/cxx/include</code> folder.</li> <li>armeabi-v7a:   Replace the <code>PaddleX-Lite-Deploy/libs/android/cxx/libs/armeabi-v7a/libpaddle_lite_api_shared.so</code> library in the demo with the generated <code>build.lite.android.armv7.gcc/inference_lite_lib.android.armv7/cxx/libs/libpaddle_lite_api_shared.so</code> library.</li> <li>arm64-v8a:   Replace the <code>PaddleX-Lite-Deploy/libs/android/cxx/libs/arm64-v8a/libpaddle_lite_api_shared.so</code> library in the demo with the generated <code>build.lite.android.armv8.gcc/inference_lite_lib.android.armv8/cxx/libs/libpaddle_lite_api_shared.so</code> library.</li> </ul> </li> </ul> </li> </ul>"},{"location":"en/version3.x/deployment/on_device_deployment.html#converting-nb-models","title":"Converting .nb Models","text":"<p>If you want to use your own trained models, follow the process below to obtain <code>.nb</code> models.</p>"},{"location":"en/version3.x/deployment/on_device_deployment.html#terminal-command-method-supports-macubuntu","title":"Terminal Command Method (Supports Mac/Ubuntu)","text":"<ol> <li> <p>Navigate to the release interface of the Paddle-Lite GitHub repository and download the corresponding conversion tool, opt, for the desired version (the latest version is recommended).</p> </li> <li> <p>After downloading the opt tool, execute the following command (using the 2.14rc version of the linux_x86 opt tool to convert the PP-OCRv5_mobile_det model as an example):</p> <pre><code>./opt_linux_x86 \\\n  --model_file=PP-OCRv5_mobile_det/inference.pdmodel \\\n  --param_file=PP-OCRv5_mobile_det/inference.pdiparams \\\n  --optimize_out=PP-OCRv5_mobile_det \\\n  --valid_targets=arm\n</code></pre> </li> </ol> <p>For detailed instructions on converting <code>.nb</code> models using the terminal command method, refer to the Using the Executable opt section in the Paddle-Lite repository.</p>"},{"location":"en/version3.x/deployment/on_device_deployment.html#python-script-method-supports-windowsmacubuntu","title":"Python Script Method (Supports Windows/Mac/Ubuntu)","text":"<ol> <li> <p>Install the latest version of the paddlelite wheel package.</p> <pre><code>pip install --pre paddlelite\n</code></pre> </li> <li> <p>Use the Python script to convert the model. Below is an example code snippet for converting the PP-OCRv5_mobile_det model:</p> <pre><code>from paddlelite.lite import Opt\n\n# 1. Create an Opt instance\nopt = Opt()\n# 2. Specify the input model paths \nopt.set_model_file(\"./PP-OCRv5_mobile_det/inference.pdmodel\")\nopt.set_param_file(\"./PP-OCRv5_mobile_det/inference.pdiparams\")\n# 3. Specify the target platform for optimization\nopt.set_valid_places(\"arm\")\n# 4. Specify the output path for the optimized model\nopt.set_optimize_out(\"./PP-OCRv5_mobile_det\")\n# 5. Execute model optimization\nopt.run()\n</code></pre> </li> </ol> <p>For detailed instructions on converting <code>.nb</code> models using the Python script method, refer to the Python Script opt Usage section in the Paddle-Lite repository.</p> <p>Notes</p> <ul> <li>For detailed information about the model optimization tool <code>opt</code>, refer to Paddle-Lite's Model Optimization Tool opt.</li> <li>Currently, only static graph models in <code>.pdmodel</code> format can be converted to <code>.nb</code> format.</li> </ul>"},{"location":"en/version3.x/deployment/on_device_deployment.html#updating-models-label-files-and-prediction-images","title":"Updating Models, Label Files, and Prediction Images","text":""},{"location":"en/version3.x/deployment/on_device_deployment.html#updating-models","title":"Updating Models","text":"<p>This guide has only validated the <code>PP-OCRv3_mobile</code>, <code>PP-OCRv4_mobile</code>, and <code>PP-OCRv5_mobile</code> models. Other models may not be compatible.</p> <p>If you fine-tune the <code>PP-OCRv5_mobile</code> model and generate a new model named <code>PP-OCRv5_mobile_ft</code>, follow these steps to replace the original model with your fine-tuned model:</p> <ol> <li> <p>Place the <code>.nb</code> models of <code>PP-OCRv5_mobile_ft</code> into the directory <code>PaddleX-Lite-Deploy/ocr/assets/models/</code>. The resulting file structure should be:</p> <pre><code>.\n\u251c\u2500\u2500 ocr \n\u2502    \u251c\u2500\u2500 ...\n\u2502    \u2514\u2500\u2500 assets \n\u2502        \u251c\u2500\u2500 models\n\u2502        \u2502   \u251c\u2500\u2500 ...\n\u2502        \u2502   \u251c\u2500\u2500 PP-OCRv5_mobile_ft_det.nb \n\u2502        \u2502   \u2514\u2500\u2500 PP-OCRv5_mobile_ft_rec.nb \n\u2502        \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre> </li> <li> <p>Add the model name to the <code>MODEL_LIST</code> in the <code>run.sh</code> script.</p> <pre><code>MODEL_LIST=\"PP-OCRv3_mobile PP-OCRv4_mobile PP-OCRv5_mobile PP-OCRv5_mobile_ft\" # Models are separated by spaces\n</code></pre> </li> <li> <p>Specify the model directory name when running the <code>run.sh</code> script.</p> <pre><code>sh run.sh PP-OCRv5_mobile_ft\n</code></pre> </li> </ol> <p>Notes:</p> <ul> <li> <p>If the input Tensor, Shape, or Dtype of the model is updated:</p> <ul> <li>For the text direction classifier model, update the <code>ClsPredictor::Preprocess</code> function in <code>ppocr_demo/src/cls_process.cc</code>.</li> <li>For the detection model, update the <code>DetPredictor::Preprocess</code> function in <code>ppocr_demo/src/det_process.cc</code>.</li> <li>For the recognition model, update the <code>RecPredictor::Preprocess</code> function in <code>ppocr_demo/src/rec_process.cc</code>.</li> </ul> </li> <li> <p>If the output Tensor or Dtype of the model is updated:</p> <ul> <li>For the text direction classifier model, update the <code>ClsPredictor::Postprocess</code> function in <code>ppocr_demo/src/cls_process.cc</code>.</li> <li>For the detection model, update the <code>DetPredictor::Postprocess</code> function in <code>ppocr_demo/src/det_process.cc</code>.</li> <li>For the recognition model, update the <code>RecPredictor::Postprocess</code> function in <code>ppocr_demo/src/rec_process.cc</code>.</li> </ul> </li> </ul>"},{"location":"en/version3.x/deployment/on_device_deployment.html#updating-label-files","title":"Updating Label Files","text":"<p>To update the label file, place the new label file in the directory <code>PaddleX-Lite-Deploy/ocr/assets/labels/</code> and update the execution command in <code>PaddleX-Lite-Deploy/ocr/android/shell/ppocr_demo/run.sh</code> following the model update method.</p> <p>For example, to update to <code>new_labels.txt</code>:</p> <pre><code># File: `PaddleX-Lite-Deploy/ocr/android/shell/ppocr_demo/run.sh`\n# Original command\nadb shell \"cd ${ppocr_demo_path} \\\n          &amp;&amp; chmod +x ./ppocr_demo \\\n          &amp;&amp; export LD_LIBRARY_PATH=${ppocr_demo_path}:${LD_LIBRARY_PATH} \\\n          &amp;&amp; ./ppocr_demo \\\n                \\\"./models/${MODEL_NAME}_det.nb\\\" \\\n                \\\"./models/${MODEL_NAME}_rec.nb\\\" \\\n                ./models/${CLS_MODEL_FILE} \\\n                ./images/test.jpg \\\n                ./test_img_result.jpg \\\n                ./labels/${LABEL_FILE} \\\n                ./config.txt\"\n# Updated command\nadb shell \"cd ${ppocr_demo_path} \\\n          &amp;&amp; chmod +x ./ppocr_demo \\\n          &amp;&amp; export LD_LIBRARY_PATH=${ppocr_demo_path}:${LD_LIBRARY_PATH} \\\n          &amp;&amp; ./ppocr_demo \\\n                \\\"./models/${MODEL_NAME}_det.nb\\\" \\\n                \\\"./models/${MODEL_NAME}_rec.nb\\\" \\\n                ./models/${CLS_MODEL_FILE} \\\n                ./images/test.jpg \\\n                ./test_img_result.jpg \\\n                ./labels/new_labels.txt \\\n                ./config.txt\"\n</code></pre>"},{"location":"en/version3.x/deployment/on_device_deployment.html#updating-prediction-images","title":"Updating Prediction Images","text":"<p>If you need to update the prediction images, place the updated images in the <code>PaddleX-Lite-Deploy/ocr/assets/images/</code> directory and update the execution command in the <code>PaddleX-Lite-Deploy/ocr/android/shell/ppocr_demo/run.sh</code> file.</p> <p>Here is an example of updating to <code>new_pics.jpg</code>:</p> <pre><code># File: `PaddleX-Lite-Deploy/ocr/android/shell/ppocr_demo/run.sh`\n## Original command\nadb shell \"cd ${ppocr_demo_path} \\\n          &amp;&amp; chmod +x ./ppocr_demo \\\n          &amp;&amp; export LD_LIBRARY_PATH=${ppocr_demo_path}:${LD_LIBRARY_PATH} \\\n          &amp;&amp; ./ppocr_demo \\\n                \\\"./models/${MODEL_NAME}_det.nb\\\" \\\n                \\\"./models/${MODEL_NAME}_rec.nb\\\" \\\n                ./models/${CLS_MODEL_FILE} \\\n                ./images/test.jpg \\\n                ./test_img_result.jpg \\\n                ./labels/${LABEL_FILE} \\\n                ./config.txt\"\n# Updated command\nadb shell \"cd ${ppocr_demo_path} \\\n          &amp;&amp; chmod +x ./ppocr_demo \\\n          &amp;&amp; export LD_LIBRARY_PATH=${ppocr_demo_path}:${LD_LIBRARY_PATH} \\\n          &amp;&amp; ./ppocr_demo \\\n                \\\"./models/${MODEL_NAME}_det.nb\\\" \\\n                \\\"./models/${MODEL_NAME}_rec.nb\\\" \\\n                ./models/${CLS_MODEL_FILE} \\\n                ./images/new_pics.jpg \\\n                ./test_img_result.jpg \\\n                ./labels/${LABEL_FILE} \\\n                ./config.txt\"\n</code></pre>"},{"location":"en/version3.x/deployment/on_device_deployment.html#updating-inputoutput-preprocessing","title":"Updating Input/Output Preprocessing","text":"<ul> <li> <p>Updating Input Preprocessing</p> <ul> <li>For the text direction classifier model, update the <code>ClsPredictor::Preprocess</code> function in <code>ppocr_demo/src/cls_process.cc</code>.</li> <li>For the detection model, update the <code>DetPredictor::Preprocess</code> function in <code>ppocr_demo/src/det_process.cc</code>.</li> <li>For the recognition model, update the <code>RecPredictor::Preprocess</code> function in <code>ppocr_demo/src/rec_process.cc</code>.</li> </ul> </li> <li> <p>Updating Output Preprocessing</p> <ul> <li>For the text direction classifier model, update the <code>ClsPredictor::Postprocess</code> function in <code>ppocr_demo/src/cls_process.cc</code>.</li> <li>For the detection model, update the <code>DetPredictor::Postprocess</code> function in <code>ppocr_demo/src/det_process.cc</code>.</li> <li>For the recognition model, update the <code>RecPredictor::Postprocess</code> function in <code>ppocr_demo/src/rec_process.cc</code>.</li> </ul> </li> </ul>"},{"location":"en/version3.x/deployment/python_and_cpp_infer.html","title":"Inference with Python or C++ Prediction Engine","text":"<p>Since the 2.x branch, inference with Python or C++ prediction engines has been a significant feature. This functionality allows users to load OCR-related models and perform inference without installing the wheel package.</p> <p>Due to differences in pre-processing, post-processing, and concatenation details compared to the wheel package, the inference results may slightly vary, and the two cannot be directly interchanged.</p> <p>For specific usage instructions, please refer to the following documents:</p> <ul> <li>Inference with Python Prediction Engine</li> <li>Inference with C++ Prediction Engine</li> <li>List of Supported Models</li> </ul>"},{"location":"en/version3.x/deployment/serving.html","title":"Serving","text":"<p>Serving is a common deployment method in real-world production environments. By encapsulating inference capabilities as services, clients can access these services via network requests to obtain inference results. PaddleOCR recommends using PaddleX for serving. Please refer to Differences and Connections between PaddleOCR and PaddleX to understand the relationship between PaddleOCR and PaddleX.</p> <p>PaddleX provides the following serving solutions:</p> <ul> <li>Basic Serving: An easy-to-use serving solution with low development costs.</li> <li>High-Stability Serving: Built based on NVIDIA Triton Inference Server. Compared to the basic serving, this solution offers higher stability and allows users to adjust configurations to optimize performance.</li> </ul> <p>It is recommended to first use the basic serving solution for quick validation, and then evaluate whether to try more complex solutions based on actual needs.</p>"},{"location":"en/version3.x/deployment/serving.html#1-basic-serving","title":"1. Basic Serving","text":""},{"location":"en/version3.x/deployment/serving.html#11-install-dependencies","title":"1.1 Install Dependencies","text":"<p>Run the following command to install the PaddleX serving plugin via PaddleX CLI:</p> <pre><code>paddlex --install serving\n</code></pre>"},{"location":"en/version3.x/deployment/serving.html#12-run-the-server","title":"1.2 Run the Server","text":"<p>Run the server via PaddleX CLI:</p> <pre><code>paddlex --serve --pipeline {PaddleX pipeline registration name or pipeline configuration file path} [{other command-line options}]\n</code></pre> <p>Take the general OCR pipeline as an example:</p> <pre><code>paddlex --serve --pipeline OCR\n</code></pre> <p>You should see information similar to the following:</p> <pre><code>INFO:     Started server process [63108]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n</code></pre> <p>To adjust configurations (such as model path, batch size, deployment device, etc.), specify <code>--pipeline</code> as a custom configuration file. Refer to PaddleOCR and PaddleX for the mapping between PaddleOCR pipelines and PaddleX pipeline registration names, as well as how to obtain and modify PaddleX pipeline configuration files.</p> <p>The command-line options related to serving are as follows:</p> Name Description <code>--pipeline</code> PaddleX pipeline registration name or pipeline configuration file path. <code>--device</code> Deployment device for the pipeline. By default, a GPU will be used if available; otherwise, a CPU will be used.\" <code>--host</code> Hostname or IP address to which the server is bound. Defaults to <code>0.0.0.0</code>. <code>--port</code> Port number on which the server listens. Defaults to <code>8080</code>. <code>--use_hpip</code> If specified, uses high-performance inference. Refer to the High-Performance Inference documentation for more information. <code>--hpi_config</code> High-performance inference configuration. Refer to the High-Performance Inference documentation for more information."},{"location":"en/version3.x/deployment/serving.html#13-invoke-the-service","title":"1.3 Invoke the Service","text":"<p>The \"Development Integration/Deployment\" section in the PaddleOCR pipeline tutorial provides API references and multi-language invocation examples for the service.</p>"},{"location":"en/version3.x/deployment/serving.html#2-high-stability-serving","title":"2. High-Stability Serving","text":"<p>Please refer to the PaddleX Serving Guide. More information about PaddleX pipeline configuration files can be found in Using PaddleX Pipeline Configuration Files.</p> <p>It should be noted that, due to the lack of fine-grained optimization and other reasons, the current high-stability serving deployment solution provided by PaddleOCR may not match the performance of the 2.x version based on PaddleServing. However, this new solution fully supports the PaddlePaddle 3.0 framework. We will continue to optimize it and consider introducing more performant deployment solutions in the future.</p>"},{"location":"en/version3.x/module_usage/doc_img_orientation_classification.html","title":"Document Image Orientation Classification Module Tutorial","text":""},{"location":"en/version3.x/module_usage/doc_img_orientation_classification.html#1-overview","title":"1. Overview","text":"<p>The Document Image Orientation Classification Module is primarily designed to distinguish the orientation of document images and correct them through post-processing. During processes such as document scanning or ID photo capturing, the device might be rotated to achieve clearer images, resulting in images with various orientations. Standard OCR pipelines may not handle these images effectively. By leveraging image classification techniques, the orientation of documents or IDs containing text regions can be pre-determined and adjusted, thereby improving the accuracy of OCR processing.</p>"},{"location":"en/version3.x/module_usage/doc_img_orientation_classification.html#2-supported-models-list","title":"2. Supported Models List","text":"ModelModel Download Links Top-1 Acc (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Size (MB) Description PP-LCNet_x1_0_doc_oriInference Model/Pretrained Model 99.06 2.31 / 0.43 3.37 / 1.27 7 A document image classification model based on PP-LCNet_x1_0, with four categories: 0\u00b0, 90\u00b0, 180\u00b0, and 270\u00b0. <p>Test Environment Description:</p> <ul> <li>Performance Test Environment <ul> <li>Test Dataset: Self-built multi-scenario dataset (1000 images, including ID/document scenarios)</li> <li>Hardware Configuration: <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environment: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Description</li> </ul> Mode GPU Configuration CPU Configuration Acceleration Technology Combination Normal Mode FP32 Precision / No TRT Acceleration FP32 Precision / 8 Threads PaddleInference High-Performance Mode Optimal combination of precision type and acceleration strategy FP32 Precision / 8 Threads Optimal backend selected (Paddle/OpenVINO/TRT, etc.)"},{"location":"en/version3.x/module_usage/doc_img_orientation_classification.html#3-quick-start","title":"3. Quick Start","text":"<p>\u2757 Before starting, please install the PaddleOCR wheel package. For details, refer to the Installation Guide.</p> <p>You can quickly experience it with one command:</p> <pre><code>paddleocr doc_img_orientation_classification -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/img_rot180_demo.jpg\n</code></pre> <p>Note: The official models would be download from HuggingFace by default. If can't access to HuggingFace, please set the environment variable <code>PADDLE_PDX_MODEL_SOURCE=\"BOS\"</code> to change the model source to BOS. In the future, more model sources will be supported.</p> <p>You can also integrate the model inference of the Document Image Orientation Classification Module into your project. Before running the following code, please download the sample image to your local machine.</p> <pre><code>from paddleocr import DocImgOrientationClassification\n\nmodel = DocImgOrientationClassification(model_name=\"PP-LCNet_x1_0_doc_ori\")\noutput = model.predict(\"img_rot180_demo.jpg\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/demo.png\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> <p>After running, the result will be:</p> <pre><code>{'res': {'input_path': 'img_rot180_demo.jpg', 'page_index': None, 'class_ids': array([2], dtype=int32), 'scores': array([0.88164], dtype=float32), 'label_names': ['180']}}\n</code></pre> <p>The meaning of the output parameters is as follows: - <code>input_path</code>: Represents the path of the input image. - <code>class_ids</code>: Represents the predicted class ID, with four categories: 0\u00b0, 90\u00b0, 180\u00b0, and 270\u00b0. <code>``-</code>scores<code>: Represents the confidence level of the prediction result. -</code>label_names`: Represents the category names of the prediction results.</p> <p>Here is the visualization of the image:</p> <p></p> <p>The explanations of relevant methods and parameters are as follows:</p> <ul> <li>Instantiate the document image orientation classification model with <code>DocImgOrientationClassification</code> (taking <code>PP-LCNet_x1_0_doc_ori</code> as an example here). The specific explanations are as follows:</li> </ul> Parameter Description Type Default <code>model_name</code> Model name <code>str</code> <code>PP-LCNet_x1_0_doc_ori</code> <code>model_dir</code> Model storage path <code>str</code> <code>None</code> <code>device</code> Device(s) to use for inference. Examples: <code>cpu</code>, <code>gpu</code>, <code>npu</code>, <code>gpu:0</code>, <code>gpu:0,1</code>. If multiple devices are specified, inference will be performed in parallel. Note that parallel inference is not always supported. By default, GPU 0 will be used if available; otherwise, the CPU will be used.  <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to use the high performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Precision for TensorRT when using the Paddle Inference TensorRT subgraph engine.Options: <code>fp32</code>, <code>fp16</code>, etc. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code>  Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads to use for inference on CPUs. <code>int</code> <code>10</code> <code>top_k</code> The top-k value for prediction results. If not specified, the default value in the official PaddleOCR model configuration is used. If the value is 5, the top 5 categories and their corresponding classification probabilities will be returned. <code>int</code> <code>None</code> <ul> <li> <p>Among them, <code>model_name</code> must be specified. After specifying <code>model_name</code>, the model parameters built into PaddleX are used by default. On this basis, when <code>model_dir</code> is specified, the user-defined model is used.</p> </li> <li> <p>Call the <code>predict()</code> method of the document image orientation classification model for inference prediction. This method will return a list of results. In addition, this module also provides the <code>predict_iter()</code> method. The two methods are completely consistent in terms of parameter acceptance and result return. The difference is that <code>predict_iter()</code> returns a <code>generator</code>, which can process and obtain prediction results step by step, suitable for scenarios where large datasets need to be processed or memory needs to be saved. You can choose either of these two methods according to your actual needs. The parameters of the <code>predict()</code> method are <code>input</code> and <code>batch_size</code>, and the specific explanations are as follows:</p> </li> </ul> Parameter Description Type Default <code>input</code> Input data to be predicted. Required. Supports multiple input types: <ul> <li>Python Var: e.g., <code>numpy.ndarray</code> representing image data</li> <li>str:    - Local image or PDF file path: <code>/root/data/img.jpg</code>;   - URL of image or PDF file: e.g., example;   - Local directory: directory containing images for prediction, e.g., <code>/root/data/</code> (Note: directories containing PDF files are not supported; PDFs must be specified by exact file path)</li> <li>List: Elements must be of the above types, e.g., <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code></li> </ul> <code>Python Var|str|list</code> <code>batch_size</code> Batch size, positive integer. <code>int</code> 1 <code>top_k</code> The top-k value for prediction results. If not specified, the value provided when the model was instantiated will be used; if it was not specified at instantiation either, the default value in the official PaddleOCR model configuration is used. <code>int</code> <code>None</code> <ul> <li>Process the prediction results. The prediction result for each sample is the corresponding Result object, and it supports operations such as printing, saving as an image, and saving as a <code>json</code> file:</li> </ul> Method Description Parameter Parameter Type Description Default Value <code>print()</code> Print the result to the terminal <code>format_json</code> <code>bool</code> Whether to format the output content using <code>JSON</code> indentation <code>True</code> <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data and make it more readable. It is only valid when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters as <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; when set to <code>False</code>, the original characters will be retained. It is only valid when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_json()</code> Save the result as a file in <code>json</code> format <code>save_path</code> <code>str</code> The file path to save. When it is a directory, the saved file name is consistent with the naming of the input file type. None <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data and make it more readable. It is only valid when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters as <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; when set to <code>False</code>, the original characters will be retained. It is only valid when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_img()</code> Save the result as a file in image format <code>save_path</code> <code>str</code> The file path to save. When it is a directory, the saved file name is consistent with the naming of the input file type. None <ul> <li>In addition, it also supports obtaining the visualization image with results and the prediction results through attributes. The specifics are as follows:</li> </ul> Attribute Description <code>json</code> Get the prediction result in <code>json</code> format <code>img</code> Get the visualization image in <code>dict</code> format"},{"location":"en/version3.x/module_usage/doc_img_orientation_classification.html#iv-secondary-development","title":"IV. Secondary Development","text":"<p>Since PaddleOCR does not directly provide training functionality for document image orientation classification, if you need to train a document image orientation classification model, you can refer to the PaddleX Secondary Development for Document Image Orientation Classification section for training guidance. The trained model can be seamlessly integrated into PaddleOCR's API for inference purposes.</p>"},{"location":"en/version3.x/module_usage/doc_img_orientation_classification.html#v-faq","title":"V. FAQ","text":""},{"location":"en/version3.x/module_usage/doc_vlm.html","title":"Document Visual Language Model Module Tutorial","text":""},{"location":"en/version3.x/module_usage/doc_vlm.html#i-overview","title":"I. Overview","text":"<p>Document visual language models are a cutting-edge multimodal processing technology aimed at addressing the limitations of traditional document processing methods. Traditional methods are often limited to processing document information in specific formats or predefined categories, whereas document visual language models can integrate visual and linguistic information to understand and handle diverse document content. By combining computer vision and natural language processing technologies, these models can recognize images, text, and their relationships within documents, and even understand semantic information within complex layout structures. This makes document processing more intelligent and flexible, with stronger generalization capabilities, showing broad application prospects in automated office work, information extraction, and other fields.</p>"},{"location":"en/version3.x/module_usage/doc_vlm.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link Model Storage Size (GB) Total Score Description PP-DocBee-2BInference Model 4.2 765 PP-DocBee is a self-developed multimodal large model by the PaddlePaddle team, focusing on document understanding, and it performs excellently in Chinese document understanding tasks. The model is fine-tuned and optimized using nearly 5 million multimodal datasets for document understanding, including general VQA, OCR, charts, text-rich documents, mathematics and complex reasoning, synthetic data, and pure text data, with different training data ratios set. On several authoritative English document understanding evaluation lists in academia, PP-DocBee has basically achieved SOTA for models of the same parameter scale. In terms of internal business Chinese scenario indicators, PP-DocBee also outperforms the current popular open-source and closed-source models. PP-DocBee-7BInference Model 15.8 - PP-DocBee2-3BInference Model 7.6 852 PP-DocBee2 is a self-developed multimodal large model by the PaddlePaddle team, further optimizing the base model on the foundation of PP-DocBee and introducing a new data optimization scheme to improve data quality. Using a small amount of 470,000 data generated by a self-developed data synthesis strategy, PP-DocBee2 performs better in Chinese document understanding tasks. In terms of internal business Chinese scenario indicators, PP-DocBee2 improves by about 11.4% compared to PP-DocBee, and also outperforms the current popular open-source and closed-source models of the same scale. <p>Note: The total scores of the above models are test results from an internal evaluation set, where all images have a resolution (height, width) of (1680, 1204), with a total of 1196 data entries, covering scenarios such as financial reports, laws and regulations, scientific and technical papers, manuals, humanities papers, contracts, research reports, etc. There are no plans for public release at the moment.</p>"},{"location":"en/version3.x/module_usage/doc_vlm.html#iii-quick-start","title":"III. Quick Start","text":"<p>\u2757 Before starting quickly, please install the PaddleOCR wheel package. For details, please refer to the Installation Guide.</p> <p>You can quickly experience it with one line of command:</p> <pre><code>paddleocr doc_vlm -i \"{'image': 'https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/medal_table.png', 'query': '\u200b\u8bc6\u522b\u200b\u8fd9\u4efd\u200b\u8868\u683c\u200b\u7684\u200b\u5185\u5bb9\u200b, \u200b\u4ee5\u200bmarkdown\u200b\u683c\u5f0f\u200b\u8f93\u51fa\u200b'}\"\n</code></pre> <p>Note: The official models would be download from HuggingFace by default. If can't access to HuggingFace, please set the environment variable <code>PADDLE_PDX_MODEL_SOURCE=\"BOS\"</code> to change the model source to BOS. In the future, more model sources will be supported.</p> <p>You can also integrate the model inference from the open document visual language model module into your project. Before running the following code, please download the sample image locally.</p> <pre><code>from paddleocr import DocVLM\nmodel = DocVLM(model_name=\"PP-DocBee2-3B\")\nresults = model.predict(\n    input={\"image\": \"medal_table.png\", \"query\": \"\u200b\u8bc6\u522b\u200b\u8fd9\u4efd\u200b\u8868\u683c\u200b\u7684\u200b\u5185\u5bb9\u200b, \u200b\u4ee5\u200bmarkdown\u200b\u683c\u5f0f\u200b\u8f93\u51fa\u200b\"},\n    batch_size=1\n)\nfor res in results:\n    res.print()\n    res.save_to_json(f\"./output/res.json\")\n</code></pre> <p>After running, the result is:</p> <pre><code>{'res': {'image': 'medal_table.png', 'query': '\u200b\u8bc6\u522b\u200b\u8fd9\u4efd\u200b\u8868\u683c\u200b\u7684\u200b\u5185\u5bb9\u200b, \u200b\u4ee5\u200bmarkdown\u200b\u683c\u5f0f\u200b\u8f93\u51fa\u200b', 'result': '| \u200b\u540d\u6b21\u200b | \u200b\u56fd\u5bb6\u200b/\u200b\u5730\u533a\u200b | \u200b\u91d1\u724c\u200b | \u200b\u94f6\u724c\u200b | \u200b\u94dc\u724c\u200b | \u200b\u5956\u724c\u200b\u603b\u6570\u200b |\\n| --- | --- | --- | --- | --- | --- |\\n| 1 | \u200b\u4e2d\u56fd\u200b\uff08CHN\uff09 | 48 | 22 | 30 | 100 |\\n| 2 | \u200b\u7f8e\u56fd\u200b\uff08USA\uff09 | 36 | 39 | 37 | 112 |\\n| 3 | \u200b\u4fc4\u7f57\u65af\u200b\uff08RUS\uff09 | 24 | 13 | 23 | 60 |\\n| 4 | \u200b\u82f1\u56fd\u200b\uff08GBR\uff09 | 19 | 13 | 19 | 51 |\\n| 5 | \u200b\u5fb7\u56fd\u200b\uff08GER\uff09 | 16 | 11 | 14 | 41 |\\n| 6 | \u200b\u6fb3\u5927\u5229\u4e9a\u200b\uff08AUS\uff09 | 14 | 15 | 17 | 46 |\\n| 7 | \u200b\u97e9\u56fd\u200b\uff08KOR\uff09 | 13 | 11 | 8 | 32 |\\n| 8 | \u200b\u65e5\u672c\u200b\uff08JPN\uff09 | 9 | 8 | 8 | 25 |\\n| 9 | \u200b\u610f\u5927\u5229\u200b\uff08ITA\uff09 | 8 | 9 | 10 | 27 |\\n| 10 | \u200b\u6cd5\u56fd\u200b\uff08FRA\uff09 | 7 | 16 | 20 | 43 |\\n| 11 | \u200b\u8377\u5170\u200b\uff08NED\uff09 | 7 | 5 | 4 | 16 |\\n| 12 | \u200b\u4e4c\u514b\u5170\u200b\uff08UKR\uff09 | 7 | 4 | 11 | 22 |\\n| 13 | \u200b\u80af\u5c3c\u4e9a\u200b\uff08KEN\uff09 | 6 | 4 | 6 | 16 |\\n| 14 | \u200b\u897f\u73ed\u7259\u200b\uff08ESP\uff09 | 5 | 11 | 3 | 19 |\\n| 15 | \u200b\u7259\u4e70\u52a0\u200b\uff08JAM\uff09 | 5 | 4 | 2 | 11 |\\n'}}\n</code></pre> <p>The meaning of the result parameters is as follows: - <code>image</code>: Indicates the path of the input image to be predicted - <code>query</code>: Represents the input text information to be predicted - <code>result</code>: Information of the model's prediction result</p> <p>The visualization of the prediction result is as follows:</p> <pre><code>| \u200b\u540d\u6b21\u200b | \u200b\u56fd\u5bb6\u200b/\u200b\u5730\u533a\u200b | \u200b\u91d1\u724c\u200b | \u200b\u94f6\u724c\u200b | \u200b\u94dc\u724c\u200b | \u200b\u5956\u724c\u200b\u603b\u6570\u200b |\n| --- | --- | --- | --- | --- | --- |\n| 1 | \u200b\u4e2d\u56fd\u200b\uff08CHN\uff09 | 48 | 22 | 30 | 100 |\n| 2 | \u200b\u7f8e\u56fd\u200b\uff08USA\uff09 | 36 | 39 | 37 | 112 |\n| 3 | \u200b\u4fc4\u7f57\u65af\u200b\uff08RUS\uff09 | 24 | 13 | 23 | 60 |\n| 4 | \u200b\u82f1\u56fd\u200b\uff08GBR\uff09 | 19 | 13 | 19 | 51 |\n| 5 | \u200b\u5fb7\u56fd\u200b\uff08GER\uff09 | 16 | 11 | 14 | 41 |\n| 6 | \u200b\u6fb3\u5927\u5229\u4e9a\u200b\uff08AUS\uff09 | 14 | 15 | 17 | 46 |\n| 7 | \u200b\u97e9\u56fd\u200b\uff08KOR\uff09 | 13 | 11 | 8 | 32 |\n| 8 | \u200b\u65e5\u672c\u200b\uff08JPN\uff09 | 9 | 8 | 8 | 25 |\n| 9 | \u200b\u610f\u5927\u5229\u200b\uff08ITA\uff09 | 8 | 9 | 10 | 27 |\n| 10 | \u200b\u6cd5\u56fd\u200b\uff08FRA\uff09 | 7 | 16 | 20 | 43 |\n| 11 | \u200b\u8377\u5170\u200b\uff08NED\uff09 | 7 | 5 | 4 | 16 |\n| 12 | \u200b\u4e4c\u514b\u5170\u200b\uff08UKR\uff09 | 7 | 4 | 11 | 22 |\n| 13 | \u200b\u80af\u5c3c\u4e9a\u200b\uff08KEN\uff09 | 6 | 4 | 6 | 16 |\n| 14 | \u200b\u897f\u73ed\u7259\u200b\uff08ESP\uff09 | 5 | 11 | 3 | 19 |\n| 15 | \u200b\u7259\u4e70\u52a0\u200b\uff08JAM\uff09 | 5 | 4 | 2 | 11 |\n</code></pre> <p>Explanations of related methods, parameters, etc., are as follows:</p> <ul> <li><code>DocVLM</code> instantiates the document visual language model (taking <code>PP-DocBee-2B</code> as an example), with specific explanations as follows:</li> </ul> Parameter Description Type Default <code>model_name</code> Model name <code>str</code> <code>PP-DocBee-2B</code> <code>model_dir</code> Model storage path <code>str</code> <code>None</code> <code>device</code> Device(s) to use for inference. Examples: <code>cpu</code>, <code>gpu</code>, <code>npu</code>, <code>gpu:0</code>, <code>gpu:0,1</code>. If multiple devices are specified, inference will be performed in parallel. Note that parallel inference is not always supported. By default, GPU 0 will be used if available; otherwise, the CPU will be used.  <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to use the high performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Precision for TensorRT when using the Paddle Inference TensorRT subgraph engine.Options: <code>fp32</code>, <code>fp16</code>, etc. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code>  Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads to use for inference on CPUs. <code>int</code> <code>10</code> <ul> <li> <p>Among them, <code>model_name</code> must be specified. After specifying <code>model_name</code>, the default PaddleX built-in model parameters will be used. On this basis, when specifying <code>model_dir</code>, user-defined models will be used.</p> </li> <li> <p>Call the <code>predict()</code> method of the document visual language model for inference prediction. This method will return a result list. Additionally, this module also provides the <code>predict_iter()</code> method. Both are completely consistent in terms of parameter acceptance and result return, the difference being that <code>predict_iter()</code> returns a <code>generator</code>, capable of gradually processing and obtaining prediction results, suitable for handling large datasets or scenarios where memory saving is desired. You can choose to use either of these methods based on actual needs. The <code>predict()</code> method parameters include <code>input</code>, <code>batch_size</code>, with specific explanations as follows:</p> </li> </ul> Parameter Description Type Default <code>input</code> Input data. Required. Since multimodal models have different input requirements, please refer to the specific model for the correct format. For example, for the PP-DocBee series models, the input format should be: <code>{'image': image_path, 'query': query_text}</code> <code>dict</code> None <code>batch_size</code> Batch size, positive integer. <code>int</code> 1 <ul> <li>Process the prediction results. The prediction result for each sample is the corresponding Result object, and it supports operations such as printing and saving as <code>json</code> file:</li> </ul> Method Description Parameter Type Description Default <code>print()</code> Print results to terminal <code>format_json</code> <code>bool</code> Whether to format the output content using <code>JSON</code> indentation <code>True</code> <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data, making it more readable, effective only when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Control whether non-<code>ASCII</code> characters are escaped to <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> retains the original characters, effective only when <code>format_json</code> is <code>True</code> <code>False</code> <code>save_to_json()</code> Save the result as a json format file <code>save_path</code> <code>str</code> Path of the file to be saved. When it is a directory, the naming of the saved file is consistent with the input file type. None <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data, making it more readable, effective only when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Control whether non-<code>ASCII</code> characters are escaped to <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> retains the original characters, effective only when <code>format_json</code> is <code>True</code> <code>False</code> <ul> <li>Additionally, it also supports obtaining prediction results through attributes, as follows:</li> </ul> Attribute Description <code>json</code> Get the prediction result in <code>json</code> format"},{"location":"en/version3.x/module_usage/doc_vlm.html#iv-secondary-development","title":"IV. Secondary Development","text":"<p>The current module does not support fine-tuning training temporarily, only inference integration is supported. The fine-tuning training of this module is planned to be supported in the future.</p>"},{"location":"en/version3.x/module_usage/doc_vlm.html#v-faq","title":"V. FAQ","text":""},{"location":"en/version3.x/module_usage/formula_recognition.html","title":"Formula Recognition Module Tutorial","text":""},{"location":"en/version3.x/module_usage/formula_recognition.html#i-overview","title":"I. Overview","text":"<p>The formula recognition module is a key component of an OCR (Optical Character Recognition) system, responsible for converting mathematical formulas in images into editable text or computer-readable formats. The performance of this module directly affects the accuracy and efficiency of the entire OCR system. The formula recognition module typically outputs LaTeX or MathML code of the mathematical formulas, which will be passed as input to the text understanding module for further processing.</p>"},{"location":"en/version3.x/module_usage/formula_recognition.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link En-BLEU(%) Zh-BLEU(%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction UniMERNetInference Model/Training Model 85.91 43.50 2266.96/- -/- 1.53 G UniMERNet is a formula recognition model developed by Shanghai AI Lab. It uses Donut Swin as the encoder and MBartDecoder as the decoder. The model is trained on a dataset of one million samples, including simple formulas, complex formulas, scanned formulas, and handwritten formulas, significantly improving the recognition accuracy of real-world formulas. PP-FormulaNet-SInference Model/Training Model 87.00 45.71 202.25/- -/- 224 M PP-FormulaNet is an advanced formula recognition model developed by the Baidu PaddlePaddle Vision Team. The PP-FormulaNet-S version uses PP-HGNetV2-B4 as its backbone network. Through parallel masking and model distillation techniques, it significantly improves inference speed while maintaining high recognition accuracy, making it suitable for applications requiring fast inference. The PP-FormulaNet-L version, on the other hand, uses Vary_VIT_B as its backbone network and is trained on a large-scale formula dataset, showing significant improvements in recognizing complex formulas compared to PP-FormulaNet-S. PP-FormulaNet-LInference Model/Training Model 90.36 45.78 1976.52/- -/- 695 M PP-FormulaNet_plus-SInference Model/Training Model 88.71 53.32 191.69/- -/- 248 M PP-FormulaNet_plus is an enhanced version of the formula recognition model developed by the Baidu PaddlePaddle Vision Team, building upon the original PP-FormulaNet. Compared to the original version, PP-FormulaNet_plus utilizes a more diverse formula dataset during training, including sources such as Chinese dissertations, professional books, textbooks, exam papers, and mathematics journals. This expansion significantly improves the model\u2019s recognition capabilities. Among the models, PP-FormulaNet_plus-M and PP-FormulaNet_plus-L have added support for Chinese formulas and increased the maximum number of predicted tokens for formulas from 1,024 to 2,560, greatly enhancing the recognition performance for complex formulas. Meanwhile, the PP-FormulaNet_plus-S model focuses on improving the recognition of English formulas. With these improvements, the PP-FormulaNet_plus series models perform exceptionally well in handling complex and diverse formula recognition tasks.  PP-FormulaNet_plus-MInference Model/Training Model 91.45 89.76 1301.56/- -/- 592 M PP-FormulaNet_plus-LInference Model/Training Model 92.22 90.64 1745.25/- -/- 698 M LaTeX_OCR_recInference Model/Training Model 74.55 39.96 1244.61/- -/- 99 M LaTeX-OCR is a formula recognition algorithm based on an autoregressive large model. It uses Hybrid ViT as the backbone network and a transformer as the decoder, significantly improving the accuracy of formula recognition. <p>Test Environment Description: <ul> <li>Performance Test Environment <ul> <li>Test Dataset: PaddleOCR internal custom formula recognition test set</li> <li>Hardware Configuration: <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environment: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Description</li> </ul></p> Mode GPU Configuration CPU Configuration Acceleration Technique Combination Standard Mode FP32 precision / No TRT acceleration FP32 precision / 8 threads PaddleInference High-Performance Mode Optimal combination of predefined precision type and acceleration strategy FP32 precision / 8 threads Optimal predefined backend (Paddle/OpenVINO/TRT, etc.)"},{"location":"en/version3.x/module_usage/formula_recognition.html#iii-quick-start","title":"III. Quick Start","text":"<p>\u2757 Before getting started, please install the PaddleOCR wheel package. For details, refer to the Installation Guide.</p> <p>You can quickly try it out with a single command:</p> <pre><code>paddleocr formula_recognition -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_formula_rec_001.png\n</code></pre> <p>Note: The official models would be download from HuggingFace by default. If can't access to HuggingFace, please set the environment variable <code>PADDLE_PDX_MODEL_SOURCE=\"BOS\"</code> to change the model source to BOS. In the future, more model sources will be supported.</p> <p>You can also integrate the model inference from the formula recognition module into your own project.Before running the code below, please download the example image locally.</p> <p><pre><code>from paddleocr import FormulaRecognition\nmodel = FormulaRecognition(model_name=\"PP-FormulaNet_plus-M\")\noutput = model.predict(input=\"general_formula_rec_001.png\", batch_size=1)\nfor res in output:\n    res.print()\n    res.save_to_img(save_path=\"./output/\")\n    res.save_to_json(save_path=\"./output/res.json\")\n</code></pre> After running, the output is: <pre><code>{'res': {'input_path': '/root/.paddlex/predict_input/general_formula_rec_001.png', 'page_index': None, 'rec_formula': '\\\\zeta_{0}(\\\\nu)=-\\\\frac{\\\\nu\\\\varrho^{-2\\\\nu}}{\\\\pi}\\\\int_{\\\\mu}^{\\\\infty}d\\\\omega\\\\int_{C_{+}}d z\\\\frac{2z^{2}}{(z^{2}+\\\\omega^{2})^{\\\\nu+1}}\\\\breve{\\\\Psi}(\\\\omega;z)e^{i\\\\epsilon z}\\\\quad,'}}\n</code></pre> Explanation of the result parameters:</p> <ul> <li><code>input_path</code>\uff1a Indicates the path to the input formula image to be predicted</li> <li><code>page_index</code>\uff1a If the input is a PDF file, this represents the page number; otherwise, it is None</li> <li><code>rec_formula</code>\uff1aIndicates the predicted LaTeX source code of the formula image The visualization image is as follows. The left side is the input formula image, and the right side is the rendered formula from the prediction:</li> </ul> <p></p> <p>Note: If you need to visualize the formula recognition module, you must install the LaTeX rendering environment by running the following command. Currently, visualization is only supported on Ubuntu. Other environments are not supported for now. For complex formulas, the LaTeX result may contain advanced representations that may not render successfully in Markdown or similar environments: <pre><code>sudo apt-get update\nsudo apt-get install texlive texlive-latex-base texlive-xetex latex-cjk-all texlive-latex-extra -y\n</code></pre></p> <p>Related methods and parameter descriptions are as follows:</p> <ul> <li><code>FormulaRecognition</code> instantiates the formula recognition model (here using <code>PP-FormulaNet_plus-M</code> as an example), with detailed description as follows:</li> </ul> Parameter Description Type Default <code>model_name</code> Name of the model <code>str</code> <code>PP-FormulaNet_plus-M</code> <code>model_dir</code> Model storage path <code>str</code> <code>None</code> <code>device</code> Device(s) to use for inference. Examples: <code>cpu</code>, <code>gpu</code>, <code>npu</code>, <code>gpu:0</code>, <code>gpu:0,1</code>. If multiple devices are specified, inference will be performed in parallel. Note that parallel inference is not always supported. By default, GPU 0 will be used if available; otherwise, the CPU will be used.  <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to use the high performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Precision for TensorRT when using the Paddle Inference TensorRT subgraph engine.Options: <code>fp32</code>, <code>fp16</code>, etc. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code>  Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads to use for inference on CPUs. <code>int</code> <code>10</code> <ul> <li> <p>Among these, <code>model_name</code> must be specified. When <code>model_name</code> is provided, the built-in model parameters from PaddleX are used by default. If <code>model_dir</code> is also specified, it will use the user-defined model instead.</p> </li> <li> <p>Call the <code>predict()</code> method of the formula recognition model to perform inference, which returns a result list. Additionally, this module provides the <code>predict_iter()</code> method. Both accept the same parameters and return the same result format. The difference is that <code>predict_iter()</code> returns a <code>generator</code>, which can process and retrieve results step-by-step, suitable for large datasets or memory-efficient scenarios. You can choose either method based on your actual needs. The <code>predict()</code> method takes parameters <code>input</code> and <code>batch_size</code>, described as follows:</p> </li> </ul> Parameter Description Type Default <code>input</code> Input data to be predicted. Required. Supports multiple input types: <ul> <li>Python Var: e.g., <code>numpy.ndarray</code> representing image data</li> <li>str:    - Local image or PDF file path: <code>/root/data/img.jpg</code>;   - URL of image or PDF file: e.g., example;   - Local directory: directory containing images for prediction, e.g., <code>/root/data/</code> (Note: directories containing PDF files are not supported; PDFs must be specified by exact file path)</li> <li>List: Elements must be of the above types, e.g., <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code></li> </ul> <code>Python Var|str|list</code> <code>batch_size</code> Batch size, positive integer. <code>int</code> 1 <ul> <li>The prediction results can be processed. Each result corresponds to a <code>Result</code> object, which supports printing, saving as an image, and saving as a <code>json</code> file:</li> </ul> Method Description Parameter Type Details Default <code>print()</code> Print the result to the terminal <code>format_json</code> <code>bool</code> Whether to format the output using <code>JSON</code> indentation <code>True</code> <code>indent</code> <code>int</code> Specify the indentation level to beautify the <code>JSON</code> output; only effective when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Controls whether non-<code>ASCII</code> characters are escaped to <code>Unicode</code>. If set to <code>True</code>, all non-ASCII characters are escaped; if <code>False</code>, original characters are kept. Only effective when <code>format_json</code> is <code>True</code> <code>False</code> <code>save_to_json()</code> Save the result as a json-formatted file <code>save_path</code> <code>str</code> Path to save the file. If it is a directory, the saved file name will match the input file type None <code>indent</code> <code>int</code> Specify the indentation level to beautify the <code>JSON</code> output; only effective when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Controls whether non-<code>ASCII</code> characters are escaped to <code>Unicode</code>. If set to <code>True</code>, all non-ASCII characters are escaped; if <code>False</code>, original characters are kept. Only effective when <code>format_json</code> is <code>True</code> <code>False</code> <code>save_to_img()</code> Save the result as an image file <code>save_path</code> <code>str</code> Path to save the file. If it is a directory, the saved file name will match the input file type None <ul> <li>In addition, you can also access the visualized image and prediction result via attributes, as follows:</li> </ul> Attribute Description <code>json</code> Get the prediction result in <code>json</code> format <code>img</code> Get the visualized image in <code>dict</code> format"},{"location":"en/version3.x/module_usage/formula_recognition.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If the models above do not perform well in your scenario, you can try the following steps for custom development. Here we take training <code>PP-FormulaNet_plus-M</code> as an example. For other models, just replace the corresponding config file.  First, you need to prepare a formula recognition dataset. You can follow the format of the formula recognition demo data.  Once the data is ready, follow the steps below to train and export the model. After export, the model can be quickly integrated into the API described above.  This example uses the demo dataset. Before training the model, please ensure you have installed all PaddleOCR dependencies as described in the installation documentation.</p>"},{"location":"en/version3.x/module_usage/formula_recognition.html#41-environment-setup","title":"4.1 Environment Setup","text":"<p>To train the formula recognition model, you need to install additional Python and Linux dependencies. Run the following commands:</p> <pre><code>sudo apt-get update\nsudo apt-get install libmagickwand-dev\npip install tokenizers==0.19.1 imagesize ftfy Wand\n</code></pre>"},{"location":"en/version3.x/module_usage/formula_recognition.html#42-dataset-and-pretrained-model-preparation","title":"4.2 Dataset and Pretrained Model Preparation","text":""},{"location":"en/version3.x/module_usage/formula_recognition.html#421-prepare-the-dataset","title":"4.2.1 Prepare the Dataset","text":"<pre><code># Download the demo dataset\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/ocr_rec_latexocr_dataset_example.tar\ntar -xf ocr_rec_latexocr_dataset_example.tar\n</code></pre>"},{"location":"en/version3.x/module_usage/formula_recognition.html#422-download-the-pretrained-model","title":"4.2.2 Download the Pretrained Model","text":"<pre><code># Download the PP-FormulaNet_plus-M pre-trained model\nwget https://paddleocr.bj.bcebos.com/contribution/rec_ppformulanet_plus_m_train.tar \ntar -xf rec_ppformulanet_plus_m_train.tar\n</code></pre>"},{"location":"en/version3.x/module_usage/formula_recognition.html#43-model-training","title":"4.3 Model Training","text":"<p>PaddleOCR is modularized. To train the <code>PP-FormulaNet_plus-M</code>  model, you need to use its config file.</p> <p>Training commands are as follows: <pre><code># Single GPU training (default)\npython3 tools/train.py -c configs/rec/PP-FormuaNet/PP-FormulaNet_plus-M.yaml \\\n   -o Global.pretrained_model=./rec_ppformulanet_plus_m_train/best_accuracy.pdparams\n\n# Multi-GPU training, specify GPU IDs with --gpus\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/rec/PP-FormuaNet/PP-FormulaNet_plus-M.yaml \\\n   -o Global.pretrained_model=./rec_ppformulanet_plus_m_train/best_accuracy.pdparams\n</code></pre> Note:</p> <ul> <li>By default, evaluation is performed every 1 epoch.If you change the batch size or dataset, modify the following accordingly: <pre><code>python3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/rec/PP-FormuaNet/PP-FormulaNet_plus-M.yaml \\\n  -o Global.eval_batch_step=[0,{length_of_dataset//batch_size//4}] \\\n   Global.pretrained_model=./rec_ppformulanet_plus_m_train/best_accuracy.pdparams\n</code></pre></li> </ul>"},{"location":"en/version3.x/module_usage/formula_recognition.html#44-model-evaluation","title":"4.4 Model Evaluation","text":"<p>You can evaluate trained weights, e.g., output/xxx/xxx.pdparams, or use the downloaded model with the following command:</p> <pre><code># Make sure pretrained_model is set to the local path.\n# For custom-trained models, modify the path and file name as {path/to/weights}/{model_name}\n# Demo test set evaluation\npython3 tools/eval.py -c configs/rec/PP-FormuaNet/PP-FormulaNet_plus-M.yaml -o \\\nGlobal.pretrained_model=./rec_ppformulanet_plus_m_train/best_accuracy.pdparams\n</code></pre>"},{"location":"en/version3.x/module_usage/formula_recognition.html#45-model-export","title":"4.5 Model Export","text":"<pre><code> python3 tools/export_model.py -c configs/rec/PP-FormuaNet/PP-FormulaNet_plus-M.yaml -o \\\n Global.pretrained_model=./rec_ppformulanet_plus_m_train/best_accuracy.pdparams \\\n Global.save_inference_dir=\"./PP-FormulaNet_plus-M_infer/\"\n</code></pre> <p>After exporting, the static graph model will be saved in <code>./PP-FormulaNet_plus-M_infer/</code>, and you will see the following files:  <pre><code>./PP-FormulaNet_plus-M_infer/\n\u251c\u2500\u2500 inference.json\n\u251c\u2500\u2500 inference.pdiparams\n\u251c\u2500\u2500 inference.yml\n</code></pre> At this point, the secondary development is complete. This static graph model can be directly integrated into the PaddleOCR API.</p>"},{"location":"en/version3.x/module_usage/formula_recognition.html#v-faq","title":"V. FAQ","text":"<p>Q1: Which formula recognition model does PaddleOCR recommend?</p> <p>A1: It is recommended to use the PP-FormulaNet series. If your scenario is mainly in English and inference speed is not a concern, use PP-FormulaNet-L or PP-FormulaNet_plus-L. For mainly Chinese use cases, use PP-FormulaNet_plus-L or PP-FormulaNet_plus-M. If your device has limited computing power and you are working with English formulas, use PP-FormulaNet-S.</p> <p>Q2: Why does the inference report an error? A2: The formula recognition model depends heavily on Paddle 3.0 official release. Please ensure the correct version is installed.</p> <p>Q3: Why is there no visualization image after prediction? A3: This may be due to LaTeX not being installed.You need to refer to Section III and install the LaTeX rendering tools.</p>"},{"location":"en/version3.x/module_usage/layout_detection.html","title":"Layout Detection Module Tutorial","text":""},{"location":"en/version3.x/module_usage/layout_detection.html#i-overview","title":"I. Overview","text":"<p>The core task of structure analysis is to parse and segment the content of input document images. By identifying different elements in the image (such as text, charts, images, etc.), they are classified into predefined categories (e.g., pure text area, title area, table area, image area, list area, etc.), and the position and size of these regions in the document are determined.</p>"},{"location":"en/version3.x/module_usage/layout_detection.html#ii-supported-model-list","title":"II. Supported Model List","text":"<ul> <li>The layout detection model includes 20 common categories: document title, paragraph title, text, page number, abstract, table, references, footnotes, header, footer, algorithm, formula, formula number, image, table, seal, figure_table title, chart, and sidebar text and lists of references</li> </ul> ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-DocLayout_plus-LInference Model/Training Model 83.2 34.6244 / 10.3945 510.57 / -  126.01 M A higher-precision layout area localization model trained on a self-built dataset containing Chinese and English papers, PPT, multi-layout magazines, contracts, books, exams, ancient books and research reports using RT-DETR-L <ul> <li>The layout detection model includes 1 category: Block:</li> </ul> ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-DocBlockLayoutInference Model/Training Model 95.9 34.6244 / 10.3945 510.57 / -  123.92 M A layout block localization model trained on a self-built dataset containing Chinese and English papers, PPT, multi-layout magazines, contracts, books, exams, ancient books and research reports using RT-DETR-L <ul> <li>The layout detection model includes 23 common categories: document title, paragraph title, text, page number, abstract, table of contents, references, footnotes, header, footer, algorithm, formula, formula number, image, figure caption, table, table caption, seal, figure title, figure, header image, footer image, and sidebar text</li> </ul> ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-DocLayout-LInference Model/Training Model 90.4 34.6244 / 10.3945 510.57 / - 123.76 M A high-precision layout area localization model trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using RT-DETR-L. PP-DocLayout-MInference Model/Training Model 75.2 13.3259 / 4.8685 44.0680 / 44.0680 22.578 A layout area localization model with balanced precision and efficiency, trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using PicoDet-L. PP-DocLayout-SInference Model/Training Model 70.9 8.3008 / 2.3794 10.0623 / 9.9296 4.834 A high-efficiency layout area localization model trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using PicoDet-S. <p>\u2757 The above list includes the 4 core models that are key supported by the text recognition module. The module actually supports a total of 12 full models, including several predefined models with different categories. The complete model list is as follows:</p>  \ud83d\udc49 Details of Model List  * Table Layout Detection Model ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PicoDet_layout_1x_tableInference Model/Training Model 97.5 8.02 / 3.09 23.70 / 20.41 7.4 M A high-efficiency layout area localization model trained on a self-built dataset using PicoDet-1x, capable of detecting table regions.   * 3-Class Layout Detection Model, including Table, Image, and Stamp ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PicoDet-S_layout_3clsInference Model/Training Model 88.2 8.99 / 2.22 16.11 / 8.73 4.8 A high-efficiency layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-S. PicoDet-L_layout_3clsInference Model/Training Model 89.0 13.05 / 4.50 41.30 / 41.30 22.6 A balanced efficiency and precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-L. RT-DETR-H_layout_3clsInference Model/Training Model 95.8 114.93 / 27.71 947.56 / 947.56 470.1 A high-precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using RT-DETR-H.   * 5-Class English Document Area Detection Model, including Text, Title, Table, Image, and List ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PicoDet_layout_1xInference Model/Training Model 97.8 9.03 / 3.10 25.82 / 20.70 7.4 A high-efficiency English document layout area localization model trained on the PubLayNet dataset using PicoDet-1x.   * 17-Class Area Detection Model, including 17 common layout categories: Paragraph Title, Image, Text, Number, Abstract, Content, Figure Caption, Formula, Table, Table Caption, References, Document Title, Footnote, Header, Algorithm, Footer, and Stamp ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PicoDet-S_layout_17clsInference Model/Training Model 87.4 9.11 / 2.12 15.42 / 9.12 4.8 A high-efficiency layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-S. PicoDet-L_layout_17clsInference Model/Training Model 89.0 13.50 / 4.69 43.32 / 43.32 22.6 A balanced efficiency and precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-L. RT-DETR-H_layout_17clsInference Model/Training Model 98.3 115.29 / 104.09 995.27 / 995.27 470.2 A high-precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using RT-DETR-H. Test Environment Description: <ul> <li>Performance Test Environment <ul> <li>Test Dataset\uff1a <ul> <li>20 types of layout detection models: PaddleOCR's self built layout area detection dataset, including Chinese and English papers, magazines, newspapers, research papers PPT\u3001 1300 images of document types such as test papers and textbooks. </li> <li>Type 1 version face region detection model: PaddleOCR's self built version face region detection dataset, including Chinese and English papers, magazines, newspapers, research reports PPT\u3001 1000 document type images such as test papers and textbooks. </li> <li>23 categories Layout Detection Model: A self-built layout area detection dataset by PaddleOCR, containing 500 common document type images such as Chinese and English papers, magazines, contracts, books, exam papers, and research reports.</li> <li>Table Layout Detection Model: A self-built table area detection dataset by PaddleOCR, including 7,835 Chinese and English paper document type images with tables.</li> <li> 3-Class Layout Detection Model: A self-built layout area detection dataset by PaddleOCR, comprising 1,154 common document type images such as Chinese and English papers, magazines, and research reports.</li> <li>5-Class English Document Area Detection Model: The evaluation dataset of PubLayNet, containing 11,245 images of English documents.</li> <li>17-Class Area Detection Model: A self-built layout area detection dataset by PaddleOCR, including 892 common document type images such as Chinese and English papers, magazines, and research reports.</li> </ul> </li> <li>Hardware Configuration\uff1a <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environments: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Description</li> </ul> Mode GPU Configuration  CPU Configuration  Acceleration Technology Combination Normal Mode FP32 Precision / No TRT Acceleration FP32 Precision / 8 Threads PaddleInference High-Performance Mode Optimal combination of pre-selected precision types and acceleration strategies FP32 Precision / 8 Threads Pre-selected optimal backend (Paddle/OpenVINO/TRT, etc.)"},{"location":"en/version3.x/module_usage/layout_detection.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleOCR wheel package. For detailed instructions, refer to PaddleOCR Local Installation Tutorial\u3002</p> <p>Quickly experience with just one command:</p> <pre><code>paddleocr layout_detection -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/layout.jpg\n</code></pre> <p>Note: The official models would be download from HuggingFace by default. If can't access to HuggingFace, please set the environment variable <code>PADDLE_PDX_MODEL_SOURCE=\"BOS\"</code> to change the model source to BOS. In the future, more model sources will be supported.</p> <p>You can also integrate the model inference from the layout area detection module into your project. Before running the following code, please download Example Image Go to the local area.</p> <pre><code>from paddleocr import LayoutDetection\n\nmodel = LayoutDetection(model_name=\"PP-DocLayout_plus-L\")\noutput = model.predict(\"layout.jpg\", batch_size=1, layout_nms=True)\nfor res in output:\n    res.print()\n    res.save_to_img(save_path=\"./output/\")\n    res.save_to_json(save_path=\"./output/res.json\")\n</code></pre> <p>After running, the result obtained is:</p> <pre><code>{'res': {'input_path': 'layout.jpg', 'page_index': None, 'boxes': [{'cls_id': 2, 'label': 'text', 'score': 0.9870226979255676, 'coordinate': [34.101906, 349.85275, 358.59213, 611.0772]}, {'cls_id': 2, 'label': 'text', 'score': 0.9866003394126892, 'coordinate': [34.500324, 647.1585, 358.29367, 848.66797]}, {'cls_id': 2, 'label': 'text', 'score': 0.9846674203872681, 'coordinate': [385.71445, 497.40973, 711.2261, 697.84265]}, {'cls_id': 8, 'label': 'table', 'score': 0.984126091003418, 'coordinate': [73.76879, 105.94899, 321.95303, 298.84888]}, {'cls_id': 8, 'label': 'table', 'score': 0.9834211468696594, 'coordinate': [436.95642, 105.81531, 662.7168, 313.48462]}, {'cls_id': 2, 'label': 'text', 'score': 0.9832247495651245, 'coordinate': [385.62787, 346.2288, 710.10095, 458.77127]}, {'cls_id': 2, 'label': 'text', 'score': 0.9816061854362488, 'coordinate': [385.7802, 735.1931, 710.56134, 849.9764]}, {'cls_id': 6, 'label': 'figure_title', 'score': 0.9577341079711914, 'coordinate': [34.421448, 20.055151, 358.71283, 76.53663]}, {'cls_id': 6, 'label': 'figure_title', 'score': 0.9505634307861328, 'coordinate': [385.72278, 20.053688, 711.29333, 74.92744]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.9001723527908325, 'coordinate': [386.46344, 477.03488, 699.4023, 490.07474]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.8845751285552979, 'coordinate': [35.413048, 627.73596, 185.58383, 640.52264]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.8837394118309021, 'coordinate': [387.17603, 716.3423, 524.7841, 729.258]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.8508939743041992, 'coordinate': [35.50064, 331.18445, 141.6444, 344.81097]}]}}\n</code></pre> <p>The meanings of the parameters are as follows: - <code>input_path</code>: The path to the input image for prediction. - <code>page_index</code>: If the input is a PDF file, it indicates which page of the PDF it is; otherwise, it is <code>None</code>. - <code>boxes</code>: Information about the predicted bounding boxes, a list of dictionaries. Each dictionary represents a detected object and contains the following information:   - <code>cls_id</code>: Class ID, an integer.   - <code>label</code>: Class label, a string.   - <code>score</code>: Confidence score of the bounding box, a float.   - <code>coordinate</code>: Coordinates of the bounding box, a list of floats in the format <code>[xmin, ymin, xmax, ymax]</code>.</p> <p>The visualized image is as follows:</p> <p></p> <p>Relevant methods, parameters, and explanations are as follows:</p> <ul> <li><code>LayoutDetection</code> instantiates a target detection model (here, <code>PP-DocLayout_plus-L</code> is used as an example). The detailed explanation is as follows:</li> </ul> Parameter Description Type Default <code>model_name</code> Model name <code>str</code> <code>PP-DocLayout-L</code> <code>model_dir</code> Model storage path <code>str</code> <code>None</code> <code>device</code> Device(s) to use for inference. Examples: <code>cpu</code>, <code>gpu</code>, <code>npu</code>, <code>gpu:0</code>, <code>gpu:0,1</code>. If multiple devices are specified, inference will be performed in parallel. Note that parallel inference is not always supported. By default, GPU 0 will be used if available; otherwise, the CPU will be used.  <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to use the high performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Precision for TensorRT when using the Paddle Inference TensorRT subgraph engine.Options: <code>fp32</code>, <code>fp16</code>, etc. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code>  Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads to use for inference on CPUs. <code>int</code> <code>10</code> <code>img_size</code> Input image size; if not specified, the default 800x800 will be used by PP-DocLayout_plus-LExamples: <ul> <li>int: e.g. 640, resizes input image to 640x640</li> <li>list: e.g. [640, 512], resizes input image to 640 width and 512 height</li> <li>None: not specified, defaults to 800x800</li> </ul> <code>int/list/None</code> None <code>threshold</code> Threshold for filtering low-confidence predictions; defaults to 0.5 if not specifiedExamples: <ul> <li>float: e.g. 0.2, filters out all boxes with confidence below 0.2</li> <li>dict: key is <code>int</code> cls_id, value is<code>float</code> threshold, e.g. <code>{0: 0.45, 2: 0.48, 7: 0.4}</code></li> <li>None: not specified, defaults to PaddleOCR model config</li> </ul> <code>float/dict/None</code> None <code>layout_nms</code> Whether to use NMS post-processing to filter overlapping boxes; if not specified, the default PaddleOCR official model configuration will be usedExamples: <ul> <li>bool: indicates whether to use NMS for post-processing to filter overlapping boxes</li> <li>None:  not specified, will use the default PaddleOCR official model configuration</li> </ul> <code>bool/None</code> None <code>layout_unclip_ratio</code> Scaling factor for the side length of the detection box;  if not specified, the default PaddleX official model configuration will be usedExamples: <ul> <li>float: a float greater than 0, e.g. 1.1, expands width and height of the box by 1.1 times</li> <li>list: e.g. [1.2, 1.5], expands width by 1.2x and height by 1.5x</li> <li>dict: key is <code>int</code> cls_id, value is<code>tuple</code>, e.g. <code>{0: (1.1, 2.0)}</code></li> <li>None: not specified, will use the default PaddleOCR official model configuration</li> </ul> <code>float/list/dict/None</code> None <code>layout_merge_bboxes_mode</code> Bounding box merge mode for model output; ; if not specified, the default PaddleOCR official model configuration will be used.Examples: <ul> <li>large: keep the largest outer box, remove inner overlapping boxes</li> <li>small: keep the smallest inner box, remove outer overlapping boxes</li> <li>union: keep all boxes, no filtering</li> <li>dict: key is <code>int</code> cls_id, value is<code>str</code>, e.g. <code>{0: \"large\", 2: \"small\"}</code></li> <li>None:not specified, will use the default PaddleOCR official model configuration.</li> </ul> <code>string/dict/None</code> None <ul> <li> <p>Note that <code>model_name</code> must be specified. After specifying <code>model_name</code>, the default PaddleX built-in model parameters will be used. If <code>model_dir</code> is specified, the user-defined model will be used.</p> </li> <li> <p>The <code>predict()</code> method of the target detection model is called for inference prediction. The parameters of the <code>predict()</code> method are <code>input</code>, <code>batch_size</code>, and <code>threshold</code>, which are explained as follows:</p> </li> </ul> Parameter Description Type Default <code>input</code> Input data to be predicted. Required. Supports multiple input types: <ul> <li>Python Var: e.g., <code>numpy.ndarray</code> representing image data</li> <li>str:    - Local image or PDF file path: <code>/root/data/img.jpg</code>;   - URL of image or PDF file: e.g., example;   - Local directory: directory containing images for prediction, e.g., <code>/root/data/</code> (Note: directories containing PDF files are not supported; PDFs must be specified by exact file path)</li> <li>List: Elements must be of the above types, e.g., <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code></li> </ul> <code>Python Var|str|list</code> <code>batch_size</code> Batch size, positive integer. <code>int</code> 1 <code>threshold</code> Threshold for filtering low-confidence predictions. If not specified, the model's default will be used.Examples: <ul> <li>float: e.g., 0.2, filters out all boxes with scores below 0.2</li> <li>dict: keys are <code>int</code> representing <code>cls_id</code>, and values are <code>float</code> thresholds. For example, <code>{0: 0.45, 2: 0.48, 7: 0.4}</code> applies thresholds of 0.45 to class 0, 0.48 to class 2, and 0.4 to class 7</li> <li>None: if not specified, defaults to 0.5</li> </ul> <code>float/dict/None</code> None <code>layout_nms</code> Whether to use NMS post-processing to filter overlapping boxesExamples: <ul> <li>bool: True/False, whether to apply NMS to filter overlapping detection boxes</li> <li>None: if not specified, uses the <code>layout_nms</code> value from <code>creat_model</code>; if that is also not set, NMS will not be used by default</li> </ul> <code>bool/None</code> None <code>layout_unclip_ratio</code> Scaling ratio for the detected box size. If not specified, defaults to 1.0Examples: <ul> <li>float:a positive float number, e.g., 1.1, means expanding the width and height of the detection box by 1.1 times while keeping the center unchanged</li> <li>list: e.g., [1.2, 1.5], means expanding the width by 1.2 times and the height by 1.5 times while keeping the center unchanged</li> <li>dict: keys are <code>int</code> representing <code>cls_id</code>, values are <code>tuple</code>, e.g., <code>{0: (1.1, 2.0)}</code> means cls_id 0 expanding the width by 1.1 times and the height by 2.0 times while keeping the center unchanged</li> <li>None: if not specified, defaults to 1.0</li> </ul> <code>float/list/dict/None</code> None <code>layout_merge_bboxes_mode</code> Merge mode for detected bounding boxes. Defaults to <code>union</code> if not specifiedExamples: <ul> <li>large: keeps only the largest outer box when overlapping/contained boxes exist</li> <li>small: keeps only the smallest inner box when overlapping/contained boxes exist</li> <li>union: no filtering, keeps all overlapping boxes</li> <li>dict: keys are <code>int</code> <code>cls_id</code>, values are <code>str</code>, e.g., <code>{0: \"large\", 2: \"small\"}</code> applies different merge modes to different classes</li> <li>None: if not specified, defaults to <code>union</code></li> </ul> <code>string/dict/None</code> None <p><sup>\uff0a</sup> If <code>None</code> is passed to <code>predict()</code>, the value set during model instantiation (<code>__init__</code>) will be used; if it was also <code>None</code> there, the framework defaults are applied: <code>threshold=0.5</code>, <code>layout_nms=False</code>, <code>layout_unclip_ratio=1.0</code>, <code>layout_merge_bboxes_mode=\"union\"</code>.</p> <ul> <li>Process the prediction results, with each sample's prediction result being the corresponding Result object, and supporting operations such as printing, saving as an image, and saving as a 'json' file:</li> </ul> Method Method Description Parameters Parameter type Parameter Description Default value <code>print()</code> Print the result to the terminal <code>format_json</code> <code>bool</code> Do you want to use <code>JSON</code> indentation formatting for the output content <code>True</code> <code>indent</code> <code>int</code> Specify the indentation level to enhance the readability of the <code>JSON</code> data output, only valid when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non ASCII characters to Unicode characters. When set to <code>True</code>, all non ASCII characters will be escaped; <code>False</code> preserves the original characters and is only valid when <code>format_json</code> is <code>True</code> <code>False</code> <code>save_to_json()</code> Save the result as a JSON format file <code>save_path</code> <code>str</code> The saved file path, when it is a directory, the name of the saved file is consistent with the name of the input file type None <code>indent</code> <code>int</code> Specify the indentation level to enhance the readability of the <code>JSON</code> data output, only valid when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non ASCII characters to Unicode characters. When set to <code>True</code>, all non <code>ASCII</code> characters will be escaped; <code>False</code> preserves the original characters and is only valid when<code>format_json</code> is <code>True</code> <code>False</code> <code>save_to_img()</code> Save the results as an image format file <code>save_path</code> <code>str</code> The saved file path, when it is a directory, the name of the saved file is consistent with the name of the input file type None <ul> <li>Additionally, it also supports obtaining the visualized image with results and the prediction results via attributes, as follows:</li> </ul> Attribute Description <code>json</code> Get the prediction result in <code>json</code> format <code>img</code> Get the visualized image in <code>dict</code> format"},{"location":"en/version3.x/module_usage/layout_detection.html#iv-custom-development","title":"IV. Custom Development","text":"<p>Since PaddleOCR does not directly provide training for the layout detection module, if you need to train the layout area detection model, you can refer to PaddleX Layout Detection Module Secondary DevelopmentPartially conduct training. The trained model can be seamlessly integrated into PaddleOCR's API for inference.</p>"},{"location":"en/version3.x/module_usage/layout_detection.html#v-faq","title":"V. FAQ","text":""},{"location":"en/version3.x/module_usage/module_overview.html","title":"Module Overview","text":"<p>A module is the smallest unit that implements basic functionality. Modules typically use a single model to accomplish specific tasks, such as text detection, image classification, and other basic functions. As fundamental building blocks, modules provide the necessary functional support for more complex application scenarios. This design approach allows users to flexibly select and combine different modules according to their needs, thereby simplifying the development process and enhancing development flexibility and efficiency.</p>"},{"location":"en/version3.x/module_usage/seal_text_detection.html","title":"Seal Text Detection Module Tutorial","text":""},{"location":"en/version3.x/module_usage/seal_text_detection.html#i-overview","title":"I. Overview","text":"<p>The seal text detection module typically outputs multi-point bounding boxes around text regions, which are then passed as inputs to the distortion correction and text recognition modules for subsequent processing to identify the textual content of the seal. Recognizing seal text is an integral part of document processing and finds applications in various scenarios such as contract comparison, inventory access auditing, and invoice reimbursement verification. The seal text detection module serves as a subtask within OCR (Optical Character Recognition), responsible for locating and marking the regions containing seal text within an image. The performance of this module directly impacts the accuracy and efficiency of the entire seal text OCR system.</p>"},{"location":"en/version3.x/module_usage/seal_text_detection.html#ii-supported-model-list","title":"II. Supported Model List","text":"Model NameModel Download Link Hmean\uff08%\uff09 GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Size (M) Description PP-OCRv4_server_seal_detInference Model/Training Model 98.21 74.75 / 67.72 382.55 / 382.55 109 M The server-side seal text detection model of PP-OCRv4 boasts higher accuracy and is suitable for deployment on better-equipped servers. PP-OCRv4_mobile_seal_detInference Model/Training Model 96.47 7.82 / 3.09 48.28 / 23.97 4.6 M The mobile-side seal text detection model of PP-OCRv4, on the other hand, offers greater efficiency and is suitable for deployment on end devices. <p>Test Environment Description:</p> <ul> <li>Performance Test Environment <ul> <li>Test Dataset\uff1a A Self-built Internal Dataset, Containing 500 Images of Circular Stamps.</li> <li>Hardware Configuration\uff1a <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environments: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Description</li> </ul> Mode GPU Configuration  CPU Configuration  Acceleration Technology Combination Normal Mode FP32 Precision / No TRT Acceleration FP32 Precision / 8 Threads PaddleInference High-Performance Mode Optimal combination of pre-selected precision types and acceleration strategies FP32 Precision / 8 Threads Pre-selected optimal backend (Paddle/OpenVINO/TRT, etc.)"},{"location":"en/version3.x/module_usage/seal_text_detection.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleOCR wheel package. For detailed instructions, refer to PaddleOCR Local Installation Tutorial\u3002</p> <p>Quickly experience with just one command:</p> <pre><code>paddleocr seal_text_detection -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/seal_text_det.png\n</code></pre> <p>Note: The official models would be download from HuggingFace by default. If can't access to HuggingFace, please set the environment variable <code>PADDLE_PDX_MODEL_SOURCE=\"BOS\"</code> to change the model source to BOS. In the future, more model sources will be supported.</p> <p>You can also integrate the model inference from the layout area detection module into your project. Before running the following code, please download Example Image Go to the local area.</p> <pre><code>from paddleocr import SealTextDetection\nmodel = SealTextDetection(model_name=\"PP-OCRv4_server_seal_det\")\noutput = model.predict(\"seal_text_det.png\", batch_size=1)\nfor res in output:\n    res.print()\n    res.save_to_img(save_path=\"./output/\")\n    res.save_to_json(save_path=\"./output/res.json\")\n</code></pre> <p>After running, the result is:</p> <pre><code>{'res': {'input_path': 'seal_text_det.png', 'page_index': None, 'dt_polys': [array([[463, 477],\n       ...,\n       [428, 505]]), array([[297, 444],\n       ...,\n       [230, 443]]), array([[457, 346],\n       ...,\n       [267, 345]]), array([[325,  38],\n       ...,\n       [322,  37]])], 'dt_scores': [0.9912680344777314, 0.9906849624837963, 0.9847219455533163, 0.9914791724153904]}}\n</code></pre> <p>The meanings of the parameters are as follows: - <code>input_path</code>: represents the path of the input image to be predicted - <code>dt_polys</code>: represents the predicted text detection boxes, where each text detection box contains multiple vertices of a polygon. Each vertex is a list of two elements, representing the x and y coordinates of the vertex respectively - <code>dt_scores</code>: represents the confidence scores of the predicted text detection boxes</p> <p>The visualization image is as follows:</p> <p></p> <p>The explanations of related methods and parameters are as follows:</p> <ul> <li><code>SealTextDetection</code> instantiates a text detection model (here we take <code>PP-OCRv4_server_seal_det</code> as an example), and the specific explanations are as follows:</li> </ul> Parameter Description Type Default <code>model_name</code> Model name. All supported seal text detection model names, such as <code>PP-OCRv4_mobile_seal_det</code>. <code>str</code> <code>PP-OCRv4_mobile_seal_det</code> <code>model_dir</code> Model storage path <code>str</code> <code>None</code> <code>device</code> Device(s) to use for inference. Examples: <code>cpu</code>, <code>gpu</code>, <code>npu</code>, <code>gpu:0</code>, <code>gpu:0,1</code>. If multiple devices are specified, inference will be performed in parallel. Note that parallel inference is not always supported. By default, GPU 0 will be used if available; otherwise, the CPU will be used.  <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to use the high performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Precision for TensorRT when using the Paddle Inference TensorRT subgraph engine.Options: <code>fp32</code>, <code>fp16</code>, etc. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code>  Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads to use for inference on CPUs. <code>int</code> <code>10</code> <code>limit_side_len</code> Limit on the side length of the input image for detection. <code>int</code> specifies the value. If set to <code>None</code>, the default value from the official PaddleOCR model configuration will be used. <code>int</code> / <code>None</code> <code>None</code> <code>limit_type</code> Type of image side length limitation. <code>\"min\"</code> ensures the shortest side of the image is no less than <code>det_limit_side_len</code>; <code>\"max\"</code> ensures the longest side is no greater than <code>limit_side_len</code>. If set to <code>None</code>, the default value from the official PaddleOCR model configuration will be used. <code>str</code> / <code>None</code> <code>None</code> <code>thresh</code> Pixel score threshold. Pixels in the output probability map with scores greater than this threshold are considered text pixels. Accepts any float value greater than 0. If set to <code>None</code>, the default value from the official PaddleOCR model configuration will be used. <code>float</code> / <code>None</code> <code>None</code> <code>box_thresh</code> If the average score of all pixels inside the bounding box is greater than this threshold, the result is considered a text region. Accepts any float value greater than 0. If set to <code>None</code>, the default value from the official PaddleOCR model configuration will be used. <code>float</code> / <code>None</code> <code>None</code> <code>unclip_ratio</code> Expansion ratio for the Vatti clipping algorithm, used to expand the text region. Accepts any float value greater than 0. If set to <code>None</code>, the default value from the official PaddleOCR model configuration will be used. <code>float</code> / <code>None</code> <code>None</code> <code>input_shape</code> Input image size for the model in the format <code>(C, H, W)</code>. If set to <code>None</code>, the model's default size will be used. <code>tuple</code> / <code>None</code> <code>None</code> <ul> <li> <p>The <code>model_name</code> must be specified. After specifying <code>model_name</code>, the built-in model parameters of PaddleX will be used by default. On this basis, if <code>model_dir</code> is specified, the user-defined model will be used.</p> </li> <li> <p>The <code>predict()</code> method of the seal text detection model is called for inference prediction. The parameters of the <code>predict()</code> method include <code>input</code>, <code>batch_size</code>, <code>limit_side_len</code>, <code>limit_type</code>, <code>thresh</code>, <code>box_thresh</code>, <code>max_candidates</code>, <code>unclip_ratio</code>. The specific descriptions are as follows:</p> </li> </ul> Parameter Description Type Default <code>input</code> Input data to be predicted. Required. Supports multiple input types: <ul> <li>Python Var: e.g., <code>numpy.ndarray</code> representing image data</li> <li>str:    - Local image or PDF file path: <code>/root/data/img.jpg</code>;   - URL of image or PDF file: e.g., example;   - Local directory: directory containing images for prediction, e.g., <code>/root/data/</code> (Note: directories containing PDF files are not supported; PDFs must be specified by exact file path)</li> <li>List: Elements must be of the above types, e.g., <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code></li> </ul> <code>Python Var|str|list</code> <code>batch_size</code> Batch size, positive integer. <code>int</code> 1 <code>limit_side_len</code> Limit on the side length of the input image for detection. <code>int</code> specifies the value. If set to <code>None</code>, the parameter value initialized by the model will be used by default. <code>int</code> / <code>None</code> <code>None</code> <code>limit_type</code> Type of image side length limitation. <code>\"min\"</code> ensures the shortest side of the image is no less than <code>det_limit_side_len</code>; <code>\"max\"</code> ensures the longest side is no greater than <code>limit_side_len</code>. If set to <code>None</code>, the parameter value initialized by the model will be used by default. <code>str</code> / <code>None</code> <code>None</code> <code>thresh</code> Pixel score threshold. Pixels in the output probability map with scores greater than this threshold are considered text pixels. Accepts any float value greater than 0. If set to <code>None</code>, the parameter value initialized by the model will be used by default. <code>float</code> / <code>None</code> <code>None</code> <code>box_thresh</code> If the average score of all pixels inside the bounding box is greater than this threshold, the result is considered a text region. Accepts any float value greater than 0. If set to <code>None</code>, the parameter value initialized by the model will be used by default. <code>float</code> / <code>None</code> <code>None</code> <code>unclip_ratio</code> Expansion ratio for the Vatti clipping algorithm, used to expand the text region. Accepts any float value greater than 0. If set to <code>None</code>, the parameter value initialized by the model will be used by default. <code>float</code> / <code>None</code> <code>None</code> <ul> <li>Process the prediction results. Each sample's prediction result is a corresponding Result object, and it supports operations such as printing, saving as an image, and saving as a <code>json</code> file:</li> </ul> Method Method Description Parameter Parameter Type Parameter Description Default Value <code>print()</code> Print the result to the terminal <code>format_json</code> <code>bool</code> Whether to format the output content using <code>JSON</code> indentation <code>True</code> <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data, making it more readable. This is only effective when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> retains the original characters. This is only effective when <code>format_json</code> is <code>True</code> <code>False</code> <code>save_to_json()</code> Save the result as a file in JSON format <code>save_path</code> <code>str</code> The file path for saving. When it is a directory, the saved file name will be consistent with the input file name None <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data, making it more readable. This is only effective when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> retains the original characters. This is only effective when <code>format_json</code> is <code>True</code> <code>False</code> <code>save_to_img()</code> Save the result as a file in image format <code>save_path</code> <code>str</code> The file path for saving. When it is a directory, the saved file name will be consistent with the input file name None <ul> <li>In addition, it also supports obtaining visual images with results and prediction results through attributes, as follows:</li> </ul> Attribute Attribute Description <code>json</code> Get the prediction result in <code>json</code> format <code>img</code> Get the visual image in <code>dict</code> format"},{"location":"en/version3.x/module_usage/seal_text_detection.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If the above model is still not performing well in your scenario, you can try the following steps for secondary development. Here, we'll use training <code>PP-OCRv4_server_seal_det</code> as an example; you can replace it with the corresponding configuration files for other models. First, you need to prepare a text detection dataset. You can refer to the format of the seal text detection demo data for preparation. Once prepared, you can follow the steps below for model training and export. After export, you can quickly integrate the model into the above API. This example uses a seal text detection demo dataset. Before training the model, please ensure that you have installed the dependencies required by PaddleOCR as per the installation documentation.</p>"},{"location":"en/version3.x/module_usage/seal_text_detection.html#41-dataset-and-pre-trained-model-preparation","title":"4.1 Dataset and Pre-trained Model Preparation","text":""},{"location":"en/version3.x/module_usage/seal_text_detection.html#411-preparing-the-dataset","title":"4.1.1 Preparing the Dataset","text":"<pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/ocr_curve_det_dataset_examples.tar -P ./dataset\ntar -xf ./dataset/ocr_curve_det_dataset_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/version3.x/module_usage/seal_text_detection.html#411-preparing-the-pre-trained-model","title":"4.1.1 Preparing the pre-trained model","text":"<pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-OCRv4_server_seal_det_pretrained.pdparams\n</code></pre>"},{"location":"en/version3.x/module_usage/seal_text_detection.html#42-model-training","title":"4.2 Model Training","text":"<p>PaddleOCR has modularized the code, and when training the <code>PP-OCRv4_server_seal_det</code> model, you need to use the configuration file for <code>PP-OCRv4_server_seal_det</code>.</p> <p>The training commands are as follows:</p> <pre><code># Single GPU training (default training method)\npython3 tools/train.py -c configs/det/PP-OCRv4/PP-OCRv4_server_seal_det.yml \\\n   -o Global.pretrained_model=./PP-OCRv4_server_seal_det_pretrained.pdparams\n\n# Multi-GPU training, specify GPU ids using the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/det/PP-OCRv4/PP-OCRv4_server_seal_det.yml \\\n        -o Global.pretrained_model=./PP-OCRv4_server_seal_det_pretrained.pdparams\n</code></pre>"},{"location":"en/version3.x/module_usage/seal_text_detection.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>You can evaluate the trained weights, such as <code>output/xxx/xxx.pdparams</code>, using the following command:</p> <pre><code># Make sure to set the pretrained_model path to the local path. If using a model that was trained and saved by yourself, be sure to modify the path and filename to {path/to/weights}/{model_name}.\n# Demo test set evaluation\npython3 tools/eval.py -c configs/det/PP-OCRv4/PP-OCRv4_server_seal_det.yml -o \\\nGlobal.pretrained_model=output/xxx/xxx.pdparams\n</code></pre>"},{"location":"en/version3.x/module_usage/seal_text_detection.html#44-model-export","title":"4.4 Model Export","text":"<pre><code>python3 tools/export_model.py -c configs/det/PP-OCRv4/PP-OCRv4_server_seal_det.yml -o \\\nGlobal.pretrained_model=output/xxx/xxx.pdparams \\\nsave_inference_dir=\"./PP-OCRv4_server_seal_det_infer/\"\n</code></pre> <p>After exporting the model, the static graph model will be stored in the <code>./PP-OCRv4_server_seal_det_infer/</code> directory. In this directory, you will see the following files: <pre><code>./PP-OCRv4_server_seal_det_infer/\n\u251c\u2500\u2500 inference.json\n\u251c\u2500\u2500 inference.pdiparams\n\u251c\u2500\u2500 inference.yml\n</code></pre> With this, the secondary development is complete, and the static graph model can be directly integrated into PaddleOCR's API.</p>"},{"location":"en/version3.x/module_usage/seal_text_detection.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version3.x/module_usage/table_cells_detection.html","title":"Table Cell Detection Module Usage Tutorial","text":""},{"location":"en/version3.x/module_usage/table_cells_detection.html#i-overview","title":"I. Overview","text":"<p>The Table Cell Detection Module is a key component of the table recognition task, responsible for locating and marking each cell region in table images. The performance of this module directly affects the accuracy and efficiency of the entire table recognition process. The Table Cell Detection Module typically outputs bounding boxes for each cell region, which are then passed as input to the table recognition pipeline for further processing.</p>"},{"location":"en/version3.x/module_usage/table_cells_detection.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link mAP(%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Storage Size (M) Description RT-DETR-L_wired_table_cell_det Inference Model/Training Model 82.7 35.00 / 10.45 495.51 / 495.51 124M RT-DETR is a real-time end-to-end object detection model. The Baidu PaddlePaddle Vision team pre-trained on a self-built table cell detection dataset based on the RT-DETR-L as the base model, achieving good performance in detecting both wired and wireless table cells. RT-DETR-L_wireless_table_cell_det Inference Model/Training Model <p>Test Environment Description:</p> <ul> <li>Performance Test Environment <ul> <li>Test Dataset: Internal evaluation set built by PaddleX.</li> <li>Hardware Configuration: <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environment: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Explanation</li> </ul> Mode GPU Configuration CPU Configuration Acceleration Technology Combination Regular Mode FP32 Precision / No TRT Acceleration FP32 Precision / 8 Threads PaddleInference High-Performance Mode Optimal combination of prior precision type and acceleration strategy FP32 Precision / 8 Threads Choose the optimal prior backend (Paddle/OpenVINO/TRT, etc.)"},{"location":"en/version3.x/module_usage/table_cells_detection.html#iii-quick-start","title":"III. Quick Start","text":"<p>\u2757 Before starting quickly, please first install the PaddleOCR wheel package. For details, please refer to the installation tutorial.</p> <p>You can quickly experience it with one command:</p> <pre><code>paddleocr table_cells_detection -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/table_recognition.jpg\n</code></pre> <p>Note: The official models would be download from HuggingFace by default. If can't access to HuggingFace, please set the environment variable <code>PADDLE_PDX_MODEL_SOURCE=\"BOS\"</code> to change the model source to BOS. In the future, more model sources will be supported.</p> <p>You can also integrate model inference from the table cell detection module into your project. Before running the following code, please download the sample image locally.</p> <pre><code>from paddleocr import TableCellsDetection\nmodel = TableCellsDetection(model_name=\"RT-DETR-L_wired_table_cell_det\")\noutput = model.predict(\"table_recognition.jpg\", threshold=0.3, batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> <p>After running, the result obtained is:</p> <pre><code>{'res': {'input_path': 'table_recognition.jpg', 'page_index': None, 'boxes': [{'cls_id': 0, 'label': 'cell', 'score': 0.9698355197906494, 'coordinate': [2.3011515, 0, 546.29926, 30.530712]}, {'cls_id': 0, 'label': 'cell', 'score': 0.9690820574760437, 'coordinate': [212.37508, 64.62493, 403.58868, 95.61413]}, {'cls_id': 0, 'label': 'cell', 'score': 0.9668057560920715, 'coordinate': [212.46791, 30.311079, 403.7182, 64.62613]}, {'cls_id': 0, 'label': 'cell', 'score': 0.966505229473114, 'coordinate': [403.56082, 64.62544, 546.83215, 95.66117]}, {'cls_id': 0, 'label': 'cell', 'score': 0.9662341475486755, 'coordinate': [109.48873, 64.66485, 212.5177, 95.631294]}, {'cls_id': 0, 'label': 'cell', 'score': 0.9654079079627991, 'coordinate': [212.39197, 95.63037, 403.60852, 126.78792]}, {'cls_id': 0, 'label': 'cell', 'score': 0.9653300642967224, 'coordinate': [2.2320926, 64.62229, 109.600494, 95.59732]}, {'cls_id': 0, 'label': 'cell', 'score': 0.9639787673950195, 'coordinate': [403.5752, 30.562355, 546.98975, 64.61531]}, {'cls_id': 0, 'label': 'cell', 'score': 0.9636150002479553, 'coordinate': [2.1537683, 30.410172, 109.568306, 64.62762]}, {'cls_id': 0, 'label': 'cell', 'score': 0.9631900191307068, 'coordinate': [2.0534437, 95.57448, 109.57601, 126.71458]}, {'cls_id': 0, 'label': 'cell', 'score': 0.9631181359291077, 'coordinate': [403.65976, 95.68139, 546.84766, 126.713394]}, {'cls_id': 0, 'label': 'cell', 'score': 0.9614537358283997, 'coordinate': [109.56504, 30.391184, 212.65425, 64.6444]}, {'cls_id': 0, 'label': 'cell', 'score': 0.9607433080673218, 'coordinate': [109.525795, 95.62622, 212.44917, 126.8258]}]}}\n</code></pre> <p>The parameter meanings are as follows:</p> <ul> <li><code>input_path</code>: Path of the input image to be predicted</li> <li><code>page_index</code>: If the input is a PDF file, it indicates which page of the PDF it is; otherwise, it is <code>None</code></li> <li><code>boxes</code>: Predicted bounding box information, a list of dictionaries. Each dictionary represents a detected object and contains the following information:</li> <li><code>cls_id</code>: Class ID, an integer</li> <li><code>label</code>: Class label, a string</li> <li><code>score</code>: Confidence of the bounding box, a float</li> <li><code>coordinate</code>: Coordinates of the bounding box, a list of floats in the format <code>[xmin, ymin, xmax, ymax]</code></li> </ul> <p>The visualized image is as follows:</p> <p></p> <p>The relevant methods, parameters, etc., are described as follows:</p> <ul> <li><code>TableCellsDetection</code> instantiates the table cell detection model (taking <code>RT-DETR-L_wired_table_cell_det</code> as an example here), with specific explanations as follows:</li> </ul> Parameter Description Type Default <code>model_name</code> Model name <code>str</code> <code>PP-DocLayout-L</code> <code>model_dir</code> Model storage path <code>str</code> <code>None</code> <code>device</code> Device(s) to use for inference. Examples: <code>cpu</code>, <code>gpu</code>, <code>npu</code>, <code>gpu:0</code>, <code>gpu:0,1</code>. If multiple devices are specified, inference will be performed in parallel. Note that parallel inference is not always supported. By default, GPU 0 will be used if available; otherwise, the CPU will be used.  <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to use the high performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Precision for TensorRT when using the Paddle Inference TensorRT subgraph engine.Options: <code>fp32</code>, <code>fp16</code>, etc. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code>  Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads to use for inference on CPUs. <code>int</code> <code>10</code> <code>img_size</code> Size of the input image;If not specified, the default configuration of the PaddleOCR official model will be usedExamples: <ul> <li>int: e.g. 640, resizes input image to 640x640</li> <li>list: e.g. [640, 512], resizes input image to 640 width and 512 height</li> </ul> <code>int/list/None</code> None <code>threshold</code> Threshold to filter out low-confidence predictions; In table cell detection tasks, lowering the threshold appropriately may help to obtain more accurate results. Examples: <ul> <li>float, e.g., 0.3, indicates filtering out all bounding boxes with confidence lower than 0.3</li> <li>dictionary, where the key is of type int representing <code>cls_id</code>, and the value is of type float representing the threshold. For example, <code>{0: 0.3}</code> applies a threshold of 0.3 for category cls_id 0</li> </ul> <code>float/dict/None</code> None <ul> <li> <p>Among them, <code>model_name</code> must be specified. After specifying <code>model_name</code>, the default model parameters built into PaddleX are used. When <code>model_dir</code> is specified, the user-defined model is used.</p> </li> <li> <p>Call the <code>predict()</code> method of the table cell detection model for inference prediction. This method will return a result list. Additionally, this module also provides a <code>predict_iter()</code> method. Both methods are consistent in terms of parameter acceptance and result return. The difference is that <code>predict_iter()</code> returns a <code>generator</code>, which can process and obtain prediction results step by step, suitable for handling large datasets or scenarios where memory saving is desired. You can choose to use either of these methods according to your actual needs. The <code>predict()</code> method has parameters <code>input</code>, <code>batch_size</code>, and <code>threshold</code>, with specific explanations as follows:</p> </li> </ul> Parameter Description Type Default <code>input</code> Input data to be predicted. Required. Supports multiple input types: <ul> <li>Python Var: e.g., <code>numpy.ndarray</code> representing image data</li> <li>str:    - Local image or PDF file path: <code>/root/data/img.jpg</code>;   - URL of image or PDF file: e.g., example;   - Local directory: directory containing images for prediction, e.g., <code>/root/data/</code> (Note: directories containing PDF files are not supported; PDFs must be specified by exact file path)</li> <li>List: Elements must be of the above types, e.g., <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code></li> </ul> <code>Python Var|str|list</code> <code>batch_size</code> Batch size, positive integer. <code>int</code> 1 <code>threshold</code> Threshold for filtering out low-confidence prediction results; Examples: <ul> <li>float: e.g., 0.2, filters out all boxes with scores below 0.2</li> <li>dict: keys are <code>int</code> representing <code>cls_id</code>, and values are <code>float</code> thresholds. For example, <code>{0: 0.45, 2: 0.48, 7: 0.4}</code> applies thresholds of 0.45 to class 0, 0.48 to class 2, and 0.4 to class 7</li> <li>None: if not specified, the threshold parameter specified in creat_model will be used by default, and if creat_model also does not specify it, the default PaddleOCR official model configuration will be used</li> </ul> <code>float/dict/None</code> None <ul> <li>Process the prediction results. The prediction result for each sample is a corresponding Result object, which supports printing, saving as an image, and saving as a <code>json</code> file:</li> </ul> Method Description Parameter Type Parameter Description Default Value <code>print()</code> Print result to terminal <code>format_json</code> <code>bool</code> Whether to format the output content using <code>JSON</code> indentation <code>True</code> <code>indent</code> <code>int</code> Specifies the indentation level to beautify the output <code>JSON</code> data, making it more readable, effective only when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Controls whether to escape non-<code>ASCII</code> characters into <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> will retain the original characters, effective only when <code>format_json</code> is <code>True</code> <code>False</code> <code>save_to_json()</code> Save the result as a json format file <code>save_path</code> <code>str</code> The path to save the file. When specified as a directory, the saved file is named consistent with the input file type. None <code>indent</code> <code>int</code> Specifies the indentation level to beautify the output <code>JSON</code> data, making it more readable, effective only when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Controls whether to escape non-<code>ASCII</code> characters into <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> will retain the original characters, effective only when <code>format_json</code> is <code>True</code> <code>False</code> <code>save_to_img()</code> Save the result as an image format file <code>save_path</code> <code>str</code> The path to save the file. When specified as a directory, the saved file is named consistent with the input file type. None <ul> <li>Additionally, the result can be obtained through attributes that provide the visualized images with results and the prediction results, as follows:</li> </ul> Attribute Description <code>json</code> Get the prediction result in <code>json</code> format <code>img</code> Get the visualized image"},{"location":"en/version3.x/module_usage/table_cells_detection.html#iv-secondary-development","title":"IV. Secondary Development","text":"<p>Since PaddleOCR does not directly provide training for the table cell detection module, if you need to train a table cell detection model, you can refer to the PaddleX Table Cell Detection Module Secondary Development section for training. The trained model can be seamlessly integrated into the PaddleOCR API for inference.</p>"},{"location":"en/version3.x/module_usage/table_cells_detection.html#v-faq","title":"V. FAQ","text":""},{"location":"en/version3.x/module_usage/table_classification.html","title":"Table Classification Module Usage Tutorial","text":""},{"location":"en/version3.x/module_usage/table_classification.html#1-overview","title":"1. Overview","text":"<p>The Table Classification Module is a key component in computer vision systems, responsible for classifying input table images. The performance of this module directly affects the accuracy and efficiency of the entire table recognition process. The Table Classification Module typically receives table images as input and, using deep learning algorithms, classifies them into predefined categories based on the characteristics and content of the images, such as wired and wireless tables. The classification results from the Table Classification Module serve as output for use in table recognition pipelines.</p>"},{"location":"en/version3.x/module_usage/table_classification.html#2-supported-model-list","title":"2. Supported Model List","text":"ModelModel Download Link Top1 Acc(%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Storage Size (M) PP-LCNet_x1_0_table_clsInference Model/Training Model 94.2 2.35 / 0.47 4.03 / 1.35 6.6M <p>Test Environment Description:</p> <ul> <li>Performance Test Environment <ul> <li>Test Dataset: Internal evaluation dataset built by PaddleX.</li> <li>Hardware Configuration: <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environment: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Explanation</li> </ul> Mode GPU Configuration CPU Configuration Acceleration Technology Combination Regular Mode FP32 Precision / No TRT Acceleration FP32 Precision / 8 Threads PaddleInference High-Performance Mode Optimal combination of prior precision type and acceleration strategy FP32 Precision / 8 Threads Choose the optimal prior backend (Paddle/OpenVINO/TRT, etc.)"},{"location":"en/version3.x/module_usage/table_classification.html#3-quick-start","title":"3. Quick Start","text":"<p>\u2757 Before starting quickly, please first install the PaddleOCR wheel package. For details, please refer to the installation tutorial.</p> <p>You can quickly experience it with one command:</p> <pre><code>paddleocr table_classification -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/table_recognition.jpg\n</code></pre> <p>Note: The official models would be download from HuggingFace by default. If can't access to HuggingFace, please set the environment variable <code>PADDLE_PDX_MODEL_SOURCE=\"BOS\"</code> to change the model source to BOS. In the future, more model sources will be supported.</p> <p>You can also integrate model inference from the table classification module into your project. Before running the following code, please download the sample image locally.</p> <pre><code>from paddleocr import TableClassification\nmodel = TableClassification(model_name=\"PP-LCNet_x1_0_table_cls\")\noutput = model.predict(\"table_recognition.jpg\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_json(\"./output/res.json\")\n</code></pre> <p>After running, the result obtained is:</p> <pre><code>{'res': {'input_path': 'table_recognition.jpg', 'page_index': None, 'class_ids': array([0, 1], dtype=int32), 'scores': array([0.84421, 0.15579], dtype=float32), 'label_names': ['wired_table', 'wireless_table']}}\n</code></pre> <p>The parameter meanings are as follows: - <code>input_path</code>: Path of the input image - <code>page_index</code>: If the input is a PDF file, it indicates which page of the PDF it is; otherwise, it is <code>None</code> - <code>class_ids</code>: Class IDs of the prediction results - <code>scores</code>: Confidence scores of the prediction results - <code>label_names</code>: Class names of the prediction results</p> <p>The visualized image is as follows:</p> <p></p> <p>The relevant methods, parameters, etc., are described as follows:</p> <ul> <li><code>TableClassification</code> instantiates the table classification model (taking <code>PP-LCNet_x1_0_table_cls</code> as an example here), with specific explanations as follows:</li> </ul> Parameter Description Type Default <code>model_name</code> Name of the model <code>str</code> <code>PP-LCNet_x1_0_doc_ori</code> <code>model_dir</code> Model storage path <code>str</code> <code>None</code> <code>device</code> Device(s) to use for inference. Examples: <code>cpu</code>, <code>gpu</code>, <code>npu</code>, <code>gpu:0</code>, <code>gpu:0,1</code>. If multiple devices are specified, inference will be performed in parallel. Note that parallel inference is not always supported. By default, GPU 0 will be used if available; otherwise, the CPU will be used.  <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to use the high performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Precision for TensorRT when using the Paddle Inference TensorRT subgraph engine.Options: <code>fp32</code>, <code>fp16</code>, etc. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code>  Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads to use for inference on CPUs. <code>int</code> <code>10</code> <ul> <li> <p>Among them, <code>model_name</code> must be specified. After specifying <code>model_name</code>, the default model parameters built into PaddleX are used. When <code>model_dir</code> is specified, the user-defined model is used.</p> </li> <li> <p>Call the <code>predict()</code> method of the table classification model for inference prediction. This method will return a result list. Additionally, this module also provides a <code>predict_iter()</code> method. Both methods are consistent in terms of parameter acceptance and result return. The difference is that <code>predict_iter()</code> returns a <code>generator</code>, which can process and obtain prediction results step by step, suitable for handling large datasets or scenarios where memory saving is desired. You can choose to use either of these methods according to your actual needs. The <code>predict()</code> method has parameters <code>input</code> and <code>batch_size</code>, with specific explanations as follows:</p> </li> </ul> Parameter Description Type Default <code>input</code> Input data to be predicted. Required. Supports multiple input types: <ul> <li>Python Var: e.g., <code>numpy.ndarray</code> representing image data</li> <li>str:    - Local image or PDF file path: <code>/root/data/img.jpg</code>;   - URL of image or PDF file: e.g., example;   - Local directory: directory containing images for prediction, e.g., <code>/root/data/</code> (Note: directories containing PDF files are not supported; PDFs must be specified by exact file path)</li> <li>List: Elements must be of the above types, e.g., <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code></li> </ul> <code>Python Var|str|list</code> <code>batch_size</code> Batch size, positive integer. <code>int</code> 1 <ul> <li>Process the prediction results. The prediction result for each sample is a corresponding Result object, which supports printing, saving as an image, and saving as a <code>json</code> file:</li> </ul> Method Description Parameter Type Parameter Description Default Value <code>print()</code> Print result to terminal <code>format_json</code> <code>bool</code> Whether to format the output content using <code>JSON</code> indentation <code>True</code> <code>indent</code> <code>int</code> Specifies the indentation level to beautify the output <code>JSON</code> data, making it more readable, effective only when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Controls whether to escape non-<code>ASCII</code> characters into <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> will retain the original characters, effective only when <code>format_json</code> is <code>True</code> <code>False</code> <code>save_to_json()</code> Save the result as a json format file <code>save_path</code> <code>str</code> The path to save the file. When specified as a directory, the saved file is named consistent with the input file type. None <code>indent</code> <code>int</code> Specifies the indentation level to beautify the output <code>JSON</code> data, making it more readable, effective only when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Controls whether to escape non-<code>ASCII</code> characters into <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> will retain the original characters, effective only when <code>format_json</code> is <code>True</code> <code>False</code> <ul> <li>Additionally, the result can be obtained through attributes that provide the visualized images with results and the prediction results, as follows:</li> </ul> Attribute Description <code>json</code> Get the prediction result in <code>json</code> format <code>img</code> Get the visualized image"},{"location":"en/version3.x/module_usage/table_classification.html#4-secondary-development","title":"4. Secondary Development","text":"<p>Since PaddleOCR does not directly provide training for the table classification module, if you need to train a table classification model, you can refer to the PaddleX Table Classification Module Secondary Development section for training. The trained model can be seamlessly integrated into the PaddleOCR API for inference.</p>"},{"location":"en/version3.x/module_usage/table_classification.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version3.x/module_usage/table_structure_recognition.html","title":"Table Structure Recognition Module Tutorial","text":""},{"location":"en/version3.x/module_usage/table_structure_recognition.html#1-overview","title":"1. Overview","text":"<p>Table structure recognition is an important component of table recognition systems, capable of converting non-editable table images into editable table formats (such as HTML). The goal of table structure recognition is to identify the positions of rows, columns, and cells in tables. The performance of this module directly affects the accuracy and efficiency of the entire table recognition system. The table structure recognition module usually outputs HTML code for the table area, which is then passed as input to the tabl recognition pipeline for further processing.</p>"},{"location":"en/version3.x/module_usage/table_structure_recognition.html#2-supported-model-list","title":"2. Supported Model List","text":"ModelModel Download Link Accuracy (%) GPU Inference Time (ms)[Normal Mode / High Performance Mode] CPU Inference Time (ms)[Normal Mode / High Performance Mode] Model Storage Size (M) Description SLANetInference Model/Training Model 59.52 103.08 / 103.08 197.99 / 197.99 6.9 M SLANet is a table structure recognition model independently developed by Baidu PaddlePaddle Vision Team. By adopting a CPU-friendly lightweight backbone network PP-LCNet, high-low level feature fusion module CSP-PAN, and SLA Head, a feature decoding module aligning structure and position information, this model greatly improves the accuracy and inference speed of table structure recognition. SLANet_plusInference Model/Training Model 63.69 140.29 / 140.29 195.39 / 195.39 6.9 M SLANet_plus is an enhanced version of the table structure recognition model SLANet independently developed by the Baidu PaddlePaddle Vision Team. Compared to SLANet, SLANet_plus has greatly improved the recognition ability for wireless and complex tables, and reduced the model's sensitivity to table positioning accuracy. Even if the table positioning is offset, it can still be accurately recognized.  SLANeXt_wired Inference Model/Training Model 69.65 -- -- 351M The SLANeXt series is a new generation of table structure recognition models independently developed by the Baidu PaddlePaddle Vision Team. Compared to SLANet and SLANet_plus, SLANeXt focuses on table structure recognition, and trains dedicated weights for wired and wireless tables separately. The recognition ability for all types of tables has been significantly improved, especially for wired tables. SLANeXt_wireless Inference Model/Training Model <p>Test Environment Description:</p> <ul> <li>Performance Test Environment <ul> <li>Test Dataset: High-difficulty Chinese table recognition dataset.</li> <li>Hardware Configuration: <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environment: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Description</li> </ul> Mode GPU Configuration CPU Configuration Acceleration Technology Combination Normal Mode FP32 precision / No TRT acceleration FP32 precision / 8 threads PaddleInference High Performance Mode Optimal combination of prior precision type and acceleration strategy FP32 precision / 8 threads Selects the prior optimal backend (Paddle/OpenVINO/TRT, etc.)"},{"location":"en/version3.x/module_usage/table_structure_recognition.html#3-quick-start","title":"3. Quick Start","text":"<p>\u2757 Before getting started, please install the PaddleOCR wheel package. For details, please refer to the Installation Tutorial.</p> <p>Quickly experience with a single command:</p> <pre><code>paddleocr table_structure_recognition -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/table_recognition.jpg\n</code></pre> <p>Note: The official models would be download from HuggingFace by default. If can't access to HuggingFace, please set the environment variable <code>PADDLE_PDX_MODEL_SOURCE=\"BOS\"</code> to change the model source to BOS. In the future, more model sources will be supported.</p> <p>You can also integrate the model inference of the table structure recognition module into your own project. Before running the code below, please download the sample image to your local machine.</p> <pre><code>from paddleocr import TableStructureRecognition\nmodel = TableStructureRecognition(model_name=\"SLANet\")\noutput = model.predict(input=\"table_recognition.jpg\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_json(\"./output/res.json\")\n</code></pre> <p>After running, the result is:</p> <pre><code>{'res': {'input_path': 'table_recognition.jpg', 'page_index': None, 'bbox': [[42, 2, 390, 2, 388, 27, 40, 26], [11, 35, 89, 35, 87, 63, 11, 63], [113, 34, 192, 34, 186, 64, 109, 64], [219, 33, 399, 33, 393, 62, 212, 62], [413, 33, 544, 33, 544, 64, 407, 64], [12, 67, 98, 68, 96, 93, 12, 93], [115, 66, 205, 66, 200, 91, 111, 91], [234, 65, 390, 65, 385, 92, 227, 92], [414, 66, 537, 67, 537, 95, 409, 95], [7, 97, 106, 97, 104, 128, 7, 128], [113, 96, 206, 95, 201, 127, 109, 127], [236, 96, 386, 96, 381, 128, 230, 128], [413, 96, 534, 95, 533, 127, 408, 127]], 'structure': ['&lt;html&gt;', '&lt;body&gt;', '&lt;table&gt;', '&lt;tr&gt;', '&lt;td', ' colspan=\"4\"', '&gt;', '&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;tr&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;td&gt;&lt;/td&gt;', '&lt;/tr&gt;', '&lt;/table&gt;', '&lt;/body&gt;', '&lt;/html&gt;'], 'structure_score': 0.99948007}}\n</code></pre> <p>Parameter meanings are as follows:</p> <ul> <li><code>input_path</code>: The path of the input table image to be predicted</li> <li><code>page_index</code>: If the input is a PDF file, indicates the page number of the PDF; otherwise, it is <code>None</code></li> <li><code>boxes</code>: Predicted table cell information, a list consisting of the coordinates of predicted table cells. Notably, table cell predictions for the SLANeXt series models are invalid</li> <li><code>structure</code>: Predicted table structure HTML expressions, a list consisting of predicted HTML keywords in order</li> <li><code>structure_score</code>: Confidence of the predicted table structure</li> </ul> <p>Descriptions of related methods and parameters are as follows:</p> <ul> <li><code>TableStructureRecognition</code> instantiates a table structure recognition model (using <code>SLANet</code> as an example). Details are as follows:</li> </ul> Parameter Description Type Default <code>model_name</code> Name of the model <code>str</code> <code>None</code> <code>model_dir</code> Model storage path <code>str</code> <code>None</code> <code>device</code> Device(s) to use for inference. Examples: <code>cpu</code>, <code>gpu</code>, <code>npu</code>, <code>gpu:0</code>, <code>gpu:0,1</code>. If multiple devices are specified, inference will be performed in parallel. Note that parallel inference is not always supported. By default, GPU 0 will be used if available; otherwise, the CPU will be used.  <code>str</code> <code>None</code> <code>enable_hpi</code> EWhether to use the high performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Precision for TensorRT when using the Paddle Inference TensorRT subgraph engine.Options: <code>fp32</code>, <code>fp16</code>, etc. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code>  Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads to use for inference on CPUs. <code>int</code> <code>10</code> <ul> <li> <p>Among them, <code>model_name</code> must be specified. If <code>model_dir</code> is specified, the user's custom model is used.</p> </li> <li> <p>Call the <code>predict()</code> method of the table structure recognition model for inference prediction, which returns a result list. In addition, this module also provides the <code>predict_iter()</code> method. The two are completely consistent in parameter acceptance and result return. The difference is that <code>predict_iter()</code> returns a <code>generator</code>, which can process and obtain prediction results step by step, suitable for handling large datasets or scenarios where you want to save memory. You can choose to use either method according to your actual needs. The <code>predict()</code> method has parameters <code>input</code> and <code>batch_size</code>, described as follows:</p> </li> </ul> Parameter Description Type Default <code>input</code> Input data to be predicted. Required. Supports multiple input types: <ul> <li>Python Var: e.g., <code>numpy.ndarray</code> representing image data</li> <li>str:    - Local image or PDF file path: <code>/root/data/img.jpg</code>;   - URL of image or PDF file: e.g., example;   - Local directory: directory containing images for prediction, e.g., <code>/root/data/</code> (Note: directories containing PDF files are not supported; PDFs must be specified by exact file path)</li> <li>List: Elements must be of the above types, e.g., <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code></li> </ul> <code>Python Var|str|list</code> <code>batch_size</code> Batch size, positive integer. <code>int</code> 1 <ul> <li>For processing prediction results, the prediction result of each sample is the corresponding Result object, and supports printing and saving as a <code>json</code> file:</li> </ul> Method Description Parameter Type Parameter Description Default <code>print()</code> Print result to terminal <code>format_json</code> <code>bool</code> Whether to use <code>JSON</code> indentation formatting for the output <code>True</code> <code>indent</code> <code>int</code> Specify indentation level to beautify the output <code>JSON</code> data, making it more readable, effective only when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Controls whether to escape non-<code>ASCII</code> characters as <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> keeps the original characters. Effective only when <code>format_json</code> is <code>True</code> <code>False</code> <code>save_to_json()</code> Save result as json format file <code>save_path</code> <code>str</code> Path to save the file. If it's a directory, the saved file will be named the same as the input file type None <code>indent</code> <code>int</code> Specify indentation level to beautify the output <code>JSON</code> data, making it more readable, effective only when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Controls whether to escape non-<code>ASCII</code> characters as <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> keeps the original characters. Effective only when <code>format_json</code> is <code>True</code> <code>False</code> <ul> <li>In addition, it also supports obtaining results through attributes, as follows:</li> </ul> Attribute Description <code>json</code> Get the prediction result in <code>json</code> format"},{"location":"en/version3.x/module_usage/table_structure_recognition.html#4-secondary-development","title":"4. Secondary Development","text":"<p>If the above models are still not ideal for your scenario, you can try the following steps for secondary development. Here, training <code>SLANet_plus</code> is used as an example, and for other models, just replace the corresponding configuration file. First, you need to prepare a dataset for table structure recognition, which can be prepared with reference to the format of the table structure recognition demo data. Once ready, you can train and export the model as follows. After exporting, you can quickly integrate the model into the above API. Here, the table structure recognition demo data is used as an example. Before training the model, please make sure you have installed the dependencies required by PaddleOCR according to the installation documentation.</p>"},{"location":"en/version3.x/module_usage/table_structure_recognition.html#41-dataset-and-pretrained-model-preparation","title":"4.1 Dataset and Pretrained Model Preparation","text":""},{"location":"en/version3.x/module_usage/table_structure_recognition.html#411-prepare-dataset","title":"4.1.1 Prepare Dataset","text":"<pre><code># Download sample dataset\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/table_rec_dataset_examples.tar\ntar -xf table_rec_dataset_examples.tar\n</code></pre>"},{"location":"en/version3.x/module_usage/table_structure_recognition.html#412-download-pretrained-model","title":"4.1.2 Download Pretrained Model","text":"<pre><code># Download SLANet_plus pretrained model\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/SLANet_plus_pretrained.pdparams\n</code></pre>"},{"location":"en/version3.x/module_usage/table_structure_recognition.html#42-model-training","title":"4.2 Model Training","text":"<p>PaddleOCR is modularized. When training the <code>SLANet_plus</code> recognition model, you need to use the configuration file of <code>SLANet_plus</code>.</p> <p>The training commands are as follows:</p> <pre><code># Single card training (default training method)\npython3 tools/train.py -c configs/table/SLANet_plus.yml \\\n    -o Global.pretrained_model=./SLANet_plus_pretrained.pdparams\n    Train.dataset.data_dir=./table_rec_dataset_examples \\\n    Train.dataset.label_file_list='[./table_rec_dataset_examples/train.txt]' \\\n    Eval.dataset.data_dir=./table_rec_dataset_examples \\\n    Eval.dataset.label_file_list='[./table_rec_dataset_examples/val.txt]'\n\n# Multi-card training, specify card numbers via --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py \\\n    -c configs/table/SLANet_plus.yml \\\n    -o Global.pretrained_model=./SLANet_plus_pretrained.pdparams\n    -o Global.pretrained_model=./PP-OCRv5_server_det_pretrained.pdparams \\\n    Train.dataset.data_dir=./table_rec_dataset_examples \\\n    Train.dataset.label_file_list='[./table_rec_dataset_examples/train.txt]' \\\n    Eval.dataset.data_dir=./table_rec_dataset_examples \\\n    Eval.dataset.label_file_list='[./table_rec_dataset_examples/val.txt]'\n</code></pre>"},{"location":"en/version3.x/module_usage/table_structure_recognition.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>You can evaluate the trained weights, such as <code>output/xxx/xxx.pdparams</code>, using the following command:</p> <pre><code># Note to set the path of pretrained_model to the local path. If you use the model saved by your own training, please modify the path and file name to {path/to/weights}/{model_name}.\n # Demo test set evaluation\n python3 tools/eval.py -c configs/table/SLANet_plus.yml -o \\\n    Global.pretrained_model=output/xxx/xxx.pdparams\n    Eval.dataset.data_dir=./table_rec_dataset_examples \\\n    Eval.dataset.label_file_list='[./table_rec_dataset_examples/val.txt]'\n</code></pre>"},{"location":"en/version3.x/module_usage/table_structure_recognition.html#44-model-export","title":"4.4 Model Export","text":"<pre><code> python3 tools/export_model.py -c configs/table/SLANet_plus.yml -o \\\n    Global.pretrained_model=output/xxx/xxx.pdparams \\\n    Global.save_inference_dir=\"./SLANet_plus_infer/\"\n</code></pre> <p>After exporting the model, the static graph model will be stored in <code>./SLANet_plus_infer/</code> in the current directory. In this directory, you will see the following files:  <pre><code>./SLANet_plus_infer/\n\u251c\u2500\u2500 inference.json\n\u251c\u2500\u2500 inference.pdiparams\n\u251c\u2500\u2500 inference.yml\n</code></pre> At this point, secondary development is complete, and this static graph model can be directly integrated into the PaddleOCR API.</p>"},{"location":"en/version3.x/module_usage/table_structure_recognition.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version3.x/module_usage/text_detection.html","title":"Text Detection Module Usage Guide","text":""},{"location":"en/version3.x/module_usage/text_detection.html#1-overview","title":"1. Overview","text":"<p>The text detection module is a critical component of OCR (Optical Character Recognition) systems, responsible for locating and marking text-containing regions in images. The performance of this module directly impacts the accuracy and efficiency of the entire OCR system. The text detection module typically outputs bounding boxes for text regions, which are then passed to the text recognition module for further processing.</p>"},{"location":"en/version3.x/module_usage/text_detection.html#2-supported-models-list","title":"2. Supported Models List","text":"ModelModel Download Link Detection Hmean (%) GPU Inference Time (ms)[Standard Mode / High-Performance Mode] CPU Inference Time (ms)[Standard Mode / High-Performance Mode] Model Size (MB) Description PP-OCRv5_server_detInference Model/Training Model 83.8 89.55 / 70.19 371.65 / 371.65 84.3 PP-OCRv5 server-side text detection model with higher accuracy, suitable for deployment on high-performance servers PP-OCRv5_mobile_detInference Model/Training Model 79.0 8.79 / 3.13 51.00 / 28.58 4.7 PP-OCRv5 mobile-side text detection model with higher efficiency, suitable for deployment on edge devices PP-OCRv4_server_detInference Model/Training Model 69.2 83.34 / 80.91 442.58 / 442.58 109 PP-OCRv4 server-side text detection model with higher accuracy, suitable for deployment on high-performance servers PP-OCRv4_mobile_detInference Model/Training Model 63.8 8.79 / 3.13 51.00 / 28.58 4.7 PP-OCRv4 mobile-side text detection model with higher efficiency, suitable for deployment on edge devices <p>Testing Environment:</p> <ul> <li>Performance Testing Environment <ul> <li>Test Dataset: PaddleOCR3.0 newly constructed multilingual dataset (including Chinese, Traditional Chinese, English, Japanese), covering street scenes, web images, documents, handwriting, blur, rotation, distortion, etc., totaling 2677 images.</li> <li>Hardware Configuration: <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environments: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Description</li> </ul> Mode GPU Configuration CPU Configuration Acceleration Techniques Standard Mode FP32 precision / No TRT acceleration FP32 precision / 8 threads PaddleInference High-Performance Mode Optimal combination of precision types and acceleration strategies FP32 precision / 8 threads Optimal backend selection (Paddle/OpenVINO/TRT, etc.)"},{"location":"en/version3.x/module_usage/text_detection.html#3-quick-start","title":"3. Quick Start","text":"<p>\u2757 Before starting, please install the PaddleOCR wheel package. Refer to the Installation Guide for details.</p> <p>Use the following command for a quick experience:</p> <pre><code>paddleocr text_detection -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_001.png\n</code></pre> <p>Note: The official models would be download from HuggingFace by default. If can't access to HuggingFace, please set the environment variable <code>PADDLE_PDX_MODEL_SOURCE=\"BOS\"</code> to change the model source to BOS. In the future, more model sources will be supported.</p> <p>You can also integrate the model inference into your project. Before running the following code, download the example image locally.</p> <pre><code>from paddleocr import TextDetection\nmodel = TextDetection(model_name=\"PP-OCRv5_server_det\")\noutput = model.predict(\"general_ocr_001.png\", batch_size=1)\nfor res in output:\n    res.print()\n    res.save_to_img(save_path=\"./output/\")\n    res.save_to_json(save_path=\"./output/res.json\")\n</code></pre> <p>The output will be:</p> <pre><code>{'res': {'input_path': 'general_ocr_001.png', 'page_index': None, 'dt_polys': array([[[ 75, 549],\n        ...,\n        [ 77, 586]],\n\n       ...,\n\n       [[ 31, 406],\n        ...,\n        [ 34, 455]]], dtype=int16), 'dt_scores': [0.873949039891189, 0.8948166013613552, 0.8842595305917041, 0.876953790920377]}}\n</code></pre> <p>Output parameter meanings: - <code>input_path</code>: Path of the input image. - <code>page_index</code>: If the input is a PDF, this indicates the current page number; otherwise, it is <code>None</code>. - <code>dt_polys</code>: Predicted text detection boxes, where each box contains four vertices (x, y coordinates). - <code>dt_scores</code>: Confidence scores of the predicted text detection boxes.</p> <p>Visualization example:</p> <p></p> <p>Method and parameter descriptions:</p> <ul> <li>Instantiate the text detection model (e.g., <code>PP-OCRv5_server_det</code>):</li> </ul> Parameter Description Type Default <code>model_name</code> Model name. All supported seal text detection model names, such as <code>PP-OCRv5_mobile_det</code>. <code>str</code> <code>None</code> <code>model_dir</code> Model storage path <code>str</code> <code>None</code> <code>device</code> Device(s) to use for inference. Examples: <code>cpu</code>, <code>gpu</code>, <code>npu</code>, <code>gpu:0</code>, <code>gpu:0,1</code>. If multiple devices are specified, inference will be performed in parallel. Note that parallel inference is not always supported. By default, GPU 0 will be used if available; otherwise, the CPU will be used.  <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to use the high performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Precision for TensorRT when using the Paddle Inference TensorRT subgraph engine.Options: <code>fp32</code>, <code>fp16</code>, etc. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code>  Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads to use for inference on CPUs. <code>int</code> <code>10</code> <code>limit_side_len</code> Limit on the side length of the input image for detection. <code>int</code> specifies the value. If set to <code>None</code>, the default value from the official PaddleOCR model configuration will be used. <code>int</code> / <code>None</code> <code>None</code> <code>limit_type</code> Type of image side length limitation. <code>\"min\"</code> ensures the shortest side of the image is no less than <code>det_limit_side_len</code>; <code>\"max\"</code> ensures the longest side is no greater than <code>limit_side_len</code>. If set to <code>None</code>, the default value from the official PaddleOCR model configuration will be used. <code>str</code> / <code>None</code> <code>None</code> <code>max_side_limit</code> Limit on the max length of the input image for detection.<code>int</code> Limit the longest side of the image for input detection model. If set to <code>None</code>, the default value from the official PaddleOCR model configuration will be used. <code>int</code> / <code>None</code> <code>None</code> <code>thresh</code> Pixel score threshold. Pixels in the output probability map with scores greater than this threshold are considered text pixels. Accepts any float value greater than 0. If set to <code>None</code>, the default value from the official PaddleOCR model configuration will be used. <code>float</code> / <code>None</code> <code>None</code> <code>box_thresh</code> If the average score of all pixels inside the bounding box is greater than this threshold, the result is considered a text region. Accepts any float value greater than 0. If set to <code>None</code>, the default value from the official PaddleOCR model configuration will be used. <code>float</code> / <code>None</code> <code>None</code> <code>unclip_ratio</code> Expansion ratio for the Vatti clipping algorithm, used to expand the text region. Accepts any float value greater than 0. If set to <code>None</code>, the default value from the official PaddleOCR model configuration will be used. <code>float</code> / <code>None</code> <code>None</code> <code>input_shape</code> Input image size for the model in the format <code>(C, H, W)</code>. If set to <code>None</code>, the model's default size will be used. <code>tuple</code> / <code>None</code> <code>None</code> <ul> <li>The <code>predict()</code> method parameters:</li> </ul> Parameter Description Type Default <code>input</code>  Input data to be predicted. Required. Supports multiple input types: <ul> <li>Python variable: e.g., <code>numpy.ndarray</code> representing image data</li> <li>File path: e.g., local image file path <code>/root/data/img.jpg</code></li> <li>URL: e.g., image file URL: Example</li> <li>Directory: should contain image files for prediction (PDF files are not supported)</li> <li>List: contains elements of the above types, e.g., <code>[numpy.ndarray, \"/root/data/img.jpg\"]</code></li> </ul> <code>Python Var</code> / <code>str</code> / <code>dict</code> / <code>list</code> <code>batch_size</code> Batch size, positive integer. <code>int</code> 1 <code>limit_side_len</code> Limit on the side length of the input image for detection. <code>int</code> specifies the value. If set to <code>None</code>, the parameter value initialized by the model will be used by default. <code>int</code> / <code>None</code> <code>None</code> <code>limit_type</code> Type of image side length limitation. <code>\"min\"</code> ensures the shortest side of the image is no less than <code>det_limit_side_len</code>; <code>\"max\"</code> ensures the longest side is no greater than <code>limit_side_len</code>. If set to <code>None</code>, the parameter value initialized by the model will be used by default. <code>str</code> / <code>None</code> <code>None</code> <code>thresh</code> Pixel score threshold. Pixels in the output probability map with scores greater than this threshold are considered text pixels. Accepts any float value greater than 0. If set to <code>None</code>, the parameter value initialized by the model will be used by default. <code>float</code> / <code>None</code> <code>None</code> <code>box_thresh</code> If the average score of all pixels inside the bounding box is greater than this threshold, the result is considered a text region. Accepts any float value greater than 0. If set to <code>None</code>, the parameter value initialized by the model will be used by default. <code>float</code> / <code>None</code> <code>None</code> <code>unclip_ratio</code> Expansion ratio for the Vatti clipping algorithm, used to expand the text region. Accepts any float value greater than 0. If set to <code>None</code>, the parameter value initialized by the model will be used by default. <code>float</code> / <code>None</code> <code>None</code> <ul> <li>Result processing methods:</li> </ul> Method Description Parameters Type Description Default <code>print()</code> Print results to terminal <code>format_json</code> <code>bool</code> Format output as JSON <code>True</code> <code>indent</code> <code>int</code> JSON indentation level 4 <code>ensure_ascii</code> <code>bool</code> Escape non-ASCII characters <code>False</code> <code>save_to_json()</code> Save results as JSON file <code>save_path</code> <code>str</code> Output file path Required <code>indent</code> <code>int</code> JSON indentation level 4 <code>ensure_ascii</code> <code>bool</code> Escape non-ASCII characters <code>False</code> <code>save_to_img()</code> Save results as image <code>save_path</code> <code>str</code> Output file path Required <ul> <li>Additional attributes:</li> </ul> Attribute Description <code>json</code> Get prediction results in JSON format <code>img</code> Get visualization image as a dictionary"},{"location":"en/version3.x/module_usage/text_detection.html#4-custom-development","title":"4. Custom Development","text":"<p>If the above models do not meet your requirements, follow these steps for custom development (using <code>PP-OCRv5_server_det</code> as an example). First, prepare a text detection dataset (refer to the Demo Dataset format). After preparation, proceed with model training and export. The exported model can be integrated into the API. Ensure PaddleOCR dependencies are installed as per the Installation Guide.</p>"},{"location":"en/version3.x/module_usage/text_detection.html#41-dataset-and-pretrained-model-preparation","title":"4.1 Dataset and Pretrained Model Preparation","text":""},{"location":"en/version3.x/module_usage/text_detection.html#411-prepare-dataset","title":"4.1.1 Prepare Dataset","text":"<pre><code># Download example dataset\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/ocr_det_dataset_examples.tar\ntar -xf ocr_det_dataset_examples.tar\n</code></pre>"},{"location":"en/version3.x/module_usage/text_detection.html#412-download-pretrained-model","title":"4.1.2 Download Pretrained Model","text":"<pre><code># Download PP-OCRv5_server_det pretrained model\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-OCRv5_server_det_pretrained.pdparams \n</code></pre>"},{"location":"en/version3.x/module_usage/text_detection.html#42-model-training","title":"4.2 Model Training","text":"<p>PaddleOCR modularizes the code. To train the <code>PP-OCRv5_server_det</code> model, use its configuration file.</p> <p>Training command:</p> <pre><code># Single-GPU training (default)\npython3 tools/train.py -c configs/det/PP-OCRv5/PP-OCRv5_server_det.yml \\\n    -o Global.pretrained_model=./PP-OCRv5_server_det_pretrained.pdparams \\\n    Train.dataset.data_dir=./ocr_det_dataset_examples \\\n    Train.dataset.label_file_list='[./ocr_det_dataset_examples/train.txt]' \\\n    Eval.dataset.data_dir=./ocr_det_dataset_examples \\\n    Eval.dataset.label_file_list='[./ocr_det_dataset_examples/val.txt]'\n\n# Multi-GPU training (specify GPUs with --gpus)\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py \\\n    -c configs/det/PP-OCRv5/PP-OCRv5_server_det.yml \\\n    -o Global.pretrained_model=./PP-OCRv5_server_det_pretrained.pdparams \\\n    Train.dataset.data_dir=./ocr_det_dataset_examples \\\n    Train.dataset.label_file_list='[./ocr_det_dataset_examples/train.txt]' \\\n    Eval.dataset.data_dir=./ocr_det_dataset_examples \\\n    Eval.dataset.label_file_list='[./ocr_det_dataset_examples/val.txt]'\n</code></pre>"},{"location":"en/version3.x/module_usage/text_detection.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>You can evaluate trained weights (e.g., <code>output/PP-OCRv5_server_det/best_accuracy.pdparams</code>) using the following command:</p> <pre><code># Note: Set pretrained_model to local path. For custom-trained models, modify the path and filename as {path/to/weights}/{model_name}.\n# Demo dataset evaluation\npython3 tools/eval.py -c configs/det/PP-OCRv5/PP-OCRv5_server_det.yml \\\n    -o Global.pretrained_model=output/PP-OCRv5_server_det/best_accuracy.pdparams \\\n    Eval.dataset.data_dir=./ocr_det_dataset_examples \\\n    Eval.dataset.label_file_list='[./ocr_det_dataset_examples/val.txt]' \n</code></pre>"},{"location":"en/version3.x/module_usage/text_detection.html#44-model-export","title":"4.4 Model Export","text":"<pre><code>python3 tools/export_model.py -c configs/det/PP-OCRv5/PP-OCRv5_server_det.yml -o \\\n    Global.pretrained_model=output/PP-OCRv5_server_det/best_accuracy.pdparams \\\n    Global.save_inference_dir=\"./PP-OCRv5_server_det_infer/\"\n</code></pre> <p>After export, the static graph model will be saved in <code>./PP-OCRv5_server_det_infer/</code> with the following files: <pre><code>./PP-OCRv5_server_det_infer/\n\u251c\u2500\u2500 inference.json\n\u251c\u2500\u2500 inference.pdiparams\n\u251c\u2500\u2500 inference.yml\n</code></pre> The custom development is now complete. This static graph model can be directly integrated into PaddleOCR's API.</p>"},{"location":"en/version3.x/module_usage/text_detection.html#5-faq","title":"5. FAQ","text":"<ul> <li>Use parameters <code>limit_type</code> and <code>limit_side_len</code> to constrain image dimensions.  </li> <li><code>limit_type</code> options: [<code>max</code>, <code>min</code>]  </li> <li><code>limit_side_len</code>: Positive integer (typically multiples of 32, e.g., 960).  </li> <li>For lower-resolution images, use <code>limit_type=min</code> and <code>limit_side_len=960</code> to balance computational efficiency and detection quality.  </li> <li>For higher-resolution images requiring larger detection scales, set <code>limit_side_len</code> to desired values (e.g., 1216).</li> </ul>"},{"location":"en/version3.x/module_usage/text_image_unwarping.html","title":"Text Image Rectification Module Usage Tutorial","text":""},{"location":"en/version3.x/module_usage/text_image_unwarping.html#1-overview","title":"1. Overview","text":"<p>The primary purpose of text image rectification is to perform geometric transformations on images to correct distortions, inclinations, perspective deformations, etc., in the document images for more accurate subsequent text recognition.</p>"},{"location":"en/version3.x/module_usage/text_image_unwarping.html#2-supported-model-list","title":"2. Supported Model List","text":"ModelModel Download Link CER Model Storage Size (M) Description UVDocInference Model/Training Model 0.179 30.3 M High-accuracy text image rectification model <p>Test Environment Description:</p> <ul> <li>Performance Test Environment <ul> <li>Test Dataset: DocUNet benchmark dataset.</li> <li>Hardware Configuration: <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environment: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Explanation</li> </ul> Mode GPU Configuration CPU Configuration Acceleration Technology Combination Regular Mode FP32 Precision / No TRT Acceleration FP32 Precision / 8 Threads PaddleInference High-Performance Mode Choose the optimal combination of prior precision type and acceleration strategy FP32 Precision / 8 Threads Choose the optimal prior backend (Paddle/OpenVINO/TRT, etc.)"},{"location":"en/version3.x/module_usage/text_image_unwarping.html#3-quick-start","title":"3. Quick Start","text":"<p>\u2757 Before starting quickly, please first install the PaddleOCR wheel package. For details, please refer to the installation tutorial.</p> <p>You can quickly experience it with one command:</p> <pre><code>paddleocr text_image_unwarping -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/doc_test.jpg\n</code></pre> <p>Note: The official models would be download from HuggingFace by default. If can't access to HuggingFace, please set the environment variable <code>PADDLE_PDX_MODEL_SOURCE=\"BOS\"</code> to change the model source to BOS. In the future, more model sources will be supported.</p> <p>You can also integrate the model inference from the image rectification module into your project. Before running the following code, please download the sample image locally.</p> <pre><code>from paddleocr import TextImageUnwarping\nmodel = TextImageUnwarping(model_name=\"UVDoc\")\noutput = model.predict(\"doc_test.jpg\", batch_size=1)\nfor res in output:\n    res.print()\n    res.save_to_img(save_path=\"./output/\")\n    res.save_to_json(save_path=\"./output/res.json\")\n</code></pre> <p>After running, the result obtained is:</p> <pre><code>{'res': {'input_path': 'doc_test.jpg', 'page_index': None, 'doctr_img': '...'}}\n</code></pre> <p>The meanings of the parameters in the result are as follows: - <code>input_path</code>: Indicates the path of the image to be rectified - <code>doctr_img</code>: Indicates the rectified image result. Due to the large amount of data, it is not convenient to print directly, so it is replaced here with <code>...</code>. You can use <code>res.save_to_img()</code> to save the prediction result as an image, and <code>res.save_to_json()</code> to save the prediction result as a json file.</p> <p>The visualized image is as follows:</p> <p></p> <p>The relevant methods, parameters, etc., are described as follows:</p> <ul> <li><code>TextImageUnwarping</code> instantiates the image rectification model (taking <code>UVDoc</code> as an example here), with specific explanations as follows:</li> </ul> Parameter Description Type Default <code>model_name</code> Name of the model <code>str</code> <code>None</code> <code>model_dir</code> Model storage path <code>str</code> <code>None</code> <code>device</code> Device(s) to use for inference. Examples: <code>cpu</code>, <code>gpu</code>, <code>npu</code>, <code>gpu:0</code>, <code>gpu:0,1</code>. If multiple devices are specified, inference will be performed in parallel. Note that parallel inference is not always supported. By default, GPU 0 will be used if available; otherwise, the CPU will be used.  <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to use the high performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Precision for TensorRT when using the Paddle Inference TensorRT subgraph engine.Options: <code>fp32</code>, <code>fp16</code>, etc. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code>  Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads to use for inference on CPUs. <code>int</code> <code>10</code> <ul> <li> <p>Among them, <code>model_name</code> must be specified. After specifying <code>model_name</code>, the default model parameters built into PaddleX are used. When <code>model_dir</code> is specified, the user-defined model is used.</p> </li> <li> <p>Call the <code>predict()</code> method of the image rectification model for inference prediction. This method will return a result list. Additionally, this module also provides a <code>predict_iter()</code> method. Both methods are consistent in terms of parameter acceptance and result return. The difference is that <code>predict_iter()</code> returns a <code>generator</code>, which can process and obtain prediction results step by step, suitable for handling large datasets or scenarios where memory saving is desired. You can choose to use either of these methods according to your actual needs. The <code>predict()</code> method has parameters <code>input</code> and <code>batch_size</code>, with specific explanations as follows:</p> </li> </ul> Parameter Description Type Default <code>input</code> Input data to be predicted. Required. Supports multiple input types: <ul> <li>Python Var: e.g., <code>numpy.ndarray</code> representing image data</li> <li>str:    - Local image or PDF file path: <code>/root/data/img.jpg</code>;   - URL of image or PDF file: e.g., example;   - Local directory: directory containing images for prediction, e.g., <code>/root/data/</code> (Note: directories containing PDF files are not supported; PDFs must be specified by exact file path)</li> <li>List: Elements must be of the above types, e.g., <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code></li> </ul> <code>Python Var|str|list</code> <code>batch_size</code> Batch size, positive integer. <code>int</code> 1 <ul> <li>Process the prediction results. The prediction result for each sample is a corresponding Result object, which supports printing, saving as an image, and saving as a <code>json</code> file:</li> </ul> Method Description Parameter Type Parameter Description Default Value <code>print()</code> Print result to terminal <code>format_json</code> <code>bool</code> Whether to format the output content using <code>JSON</code> indentation <code>True</code> <code>indent</code> <code>int</code> Specifies the indentation level to beautify the output <code>JSON</code> data, making it more readable, effective only when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Controls whether to escape non-<code>ASCII</code> characters into <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> will retain the original characters, effective only when <code>format_json</code> is <code>True</code> <code>False</code> <code>save_to_json()</code> Save the result as a json format file <code>save_path</code> <code>str</code> The path to save the file. When specified as a directory, the saved file is named consistent with the input file type. None <code>indent</code> <code>int</code> Specifies the indentation level to beautify the output <code>JSON</code> data, making it more readable, effective only when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Controls whether to escape non-<code>ASCII</code> characters into <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> will retain the original characters, effective only when <code>format_json</code> is <code>True</code> <code>False</code> <code>save_to_img()</code> Save the result as an image format file <code>save_path</code> <code>str</code> The path to save the file. When specified as a directory, the saved file is named consistent with the input file type. None <ul> <li>Additionally, the result can be obtained through attributes that provide the visualized images with results and the prediction results, as follows:</li> </ul> Attribute Description <code>json</code> Get the prediction result in <code>json</code> format <code>img</code> Get the visualized image in <code>dict</code> format"},{"location":"en/version3.x/module_usage/text_image_unwarping.html#4-secondary-development","title":"4. Secondary Development","text":"<p>The current module does not support fine-tuning training and only supports inference integration. Concerning fine-tuning training for this module, there are plans to support it in the future.</p>"},{"location":"en/version3.x/module_usage/text_image_unwarping.html#5-faq","title":"5. FAQ","text":""},{"location":"en/version3.x/module_usage/text_recognition.html","title":"Text Recognition Module Tutorial","text":""},{"location":"en/version3.x/module_usage/text_recognition.html#1-overview","title":"1. Overview","text":"<p>The text recognition module is the core component of an OCR (Optical Character Recognition) system, responsible for extracting text information from text regions within images. The performance of this module directly impacts the accuracy and efficiency of the entire OCR system. Typically, the text recognition module takes the bounding boxes of text regions output by the text detection module as input and then converts the text in the images into editable and searchable electronic text through complex image processing and deep learning algorithms. The accuracy of the text recognition results is crucial for subsequent applications such as information extraction and data mining.</p>"},{"location":"en/version3.x/module_usage/text_recognition.html#2-list-of-supported-models","title":"2. List of Supported Models","text":"ModelModel Download Links Recognition Avg Accuracy(%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-OCRv5_server_recInference Model/Pretrained Model 86.38  8.45/2.36   122.69/122.69  81 M PP-OCRv5_rec is a next-generation text recognition model. It aims to efficiently and accurately support the recognition of four major languages\u2014Simplified Chinese, Traditional Chinese, English, and Japanese\u2014as well as complex text scenarios such as handwriting, vertical text, pinyin, and rare characters using a single model. While maintaining recognition performance, it balances inference speed and model robustness, providing efficient and accurate technical support for document understanding in various scenarios. PP-OCRv5_mobile_recInference Model/Pretrained Model 81.29  1.46/5.43   5.32/91.79  16 M PP-OCRv4_server_rec_docInference Model/Pretrained Model 86.58 6.65 / 2.38 32.92 / 32.92 91 M PP-OCRv4_server_rec_doc is trained on a mixed dataset of more Chinese document data and PP-OCR training data, building upon PP-OCRv4_server_rec. It enhances the recognition capabilities for some Traditional Chinese characters, Japanese characters, and special symbols, supporting over 15,000 characters. In addition to improving document-related text recognition, it also enhances general text recognition capabilities. PP-OCRv4_mobile_recInference Model/Pretrained Model 83.28 4.82 / 1.20 16.74 / 4.64 11 M A lightweight recognition model of PP-OCRv4 with high inference efficiency, suitable for deployment on various hardware devices, including edge devices. PP-OCRv4_server_rec Inference Model/Pretrained Model 85.19  6.58 / 2.43 33.17 / 33.17 87 M The server-side model of PP-OCRv4, offering high inference accuracy and deployable on various servers. en_PP-OCRv4_mobile_recInference Model/Pretrained Model 70.39 4.81 / 0.75 16.10 / 5.31 7.3 M An ultra-lightweight English recognition model trained based on the PP-OCRv4 recognition model, supporting English and numeric character recognition. <p>\u2757 The above section lists the 6 core models that are primarily supported by the text recognition module. In total, the module supports 20 comprehensive models, including multiple multilingual text recognition models. Below is the complete list of models:</p>  \ud83d\udc49Details of the Model List  * PP-OCRv5 Multi-Scenario Models ModelModel Download Links Avg Accuracy for Chinese Recognition (%) Avg Accuracy for English Recognition (%) Avg Accuracy for Traditional Chinese Recognition (%) Avg Accuracy for Japanese Recognition (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-OCRv5_server_recInference Model/Pretrained Model 86.38 64.70 93.29 60.35  8.45/2.36   122.69/122.69  81 M PP-OCRv5_rec is a next-generation text recognition model. It aims to efficiently and accurately support the recognition of four major languages\u2014Simplified Chinese, Traditional Chinese, English, and Japanese\u2014as well as complex text scenarios such as handwriting, vertical text, pinyin, and rare characters using a single model. While maintaining recognition performance, it balances inference speed and model robustness, providing efficient and accurate technical support for document understanding in various scenarios. PP-OCRv5_mobile_recInference Model/Pretrained Model 81.29 66.00 83.55 54.65  1.46/5.43   5.32/91.79  16 M   * Chinese Recognition Models ModelModel Download Links Avg Accuracy (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-OCRv4_server_rec_docInference Model/Pretrained Model 86.58 6.65 / 2.38 32.92 / 32.92 91 M PP-OCRv4_server_rec_doc is trained on a mixed dataset of more Chinese document data and PP-OCR training data, building upon PP-OCRv4_server_rec. It enhances the recognition capabilities for some Traditional Chinese characters, Japanese characters, and special symbols, supporting over 15,000 characters. In addition to improving document-related text recognition, it also enhances general text recognition capabilities. PP-OCRv4_mobile_recInference Model/Pretrained Model 83.28 4.82 / 1.20 16.74 / 4.64 11 M A lightweight recognition model of PP-OCRv4 with high inference efficiency, suitable for deployment on various hardware devices, including edge devices. PP-OCRv4_server_rec Inference Model/Pretrained Model 85.19  6.58 / 2.43 33.17 / 33.17 87 M The server-side model of PP-OCRv4, offering high inference accuracy and deployable on various servers. PP-OCRv3_mobile_recInference Model/Pretrained Model 75.43 5.87 / 1.19 9.07 / 4.28 11 M A lightweight recognition model of PP-OCRv3 with high inference efficiency, suitable for deployment on various hardware devices, including edge devices. ModelModel Download Links Avg Accuracy (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction ch_SVTRv2_recInference Model/Pretrained Model 68.81 8.08 / 2.74 50.17 / 42.50 73.9 M  SVTRv2 is a server-side text recognition model developed by the OpenOCR team of the Vision and Learning Lab (FVL) at Fudan University. It won the first prize in the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition Task, with a 6% improvement in end-to-end recognition accuracy on Leaderboard A compared to PP-OCRv4.  ModelModel Download Links Avg Accuracy (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction ch_RepSVTR_recInference Model/Pretrained Model 65.07 5.93 / 1.62 20.73 / 7.32 22.1 M     RepSVTR is a mobile-side text recognition model based on SVTRv2. It won the first prize in the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition Task, with a 2.5% improvement in end-to-end recognition accuracy on Leaderboard B compared to PP-OCRv4, while maintaining similar inference speed.   * English Recognition Models ModelModel Download Links Avg Accuracy (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction en_PP-OCRv4_mobile_recInference Model/Pretrained Model  70.39 4.81 / 0.75 16.10 / 5.31 6.8 M An ultra-lightweight English recognition model trained based on the PP-OCRv4 recognition model, supporting English and numeric character recognition. en_PP-OCRv3_mobile_recInference Model/Pretrained Model 70.69 5.44 / 0.75 8.65 / 5.57 7.8 M  An ultra-lightweight English recognition model trained based on the PP-OCRv3 recognition model, supporting English and numeric character recognition.    * Multilingual Recognition Models ModelModel Download Links Avg Accuracy (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction korean_PP-OCRv3_mobile_recInference Model/Pretrained Model 60.21 5.40 / 0.97 9.11 / 4.05 8.6 M An ultra-lightweight Korean recognition model trained based on the PP-OCRv3 recognition model, supporting Korean and numeric character recognition. japan_PP-OCRv3_mobile_recInference Model/Pretrained Model 45.69 5.70 / 1.02 8.48 / 4.07 8.8 M  An ultra-lightweight Japanese recognition model trained based on the PP-OCRv3 recognition model, supporting Japanese and numeric character recognition. chinese_cht_PP-OCRv3_mobile_recInference Model/Pretrained Model 82.06 5.90 / 1.28 9.28 / 4.34 9.7 M  An ultra-lightweight Traditional Chinese recognition model trained based on the PP-OCRv3 recognition model, supporting Traditional Chinese and numeric character recognition. te_PP-OCRv3_mobile_recInference Model/Pretrained Model 95.88 5.42 / 0.82 8.10 / 6.91 7.8 M  An ultra-lightweight Telugu recognition model trained based on the PP-OCRv3 recognition model, supporting Telugu and numeric character recognition. ka_PP-OCRv3_mobile_recInference Model/Pretrained Model 96.96 5.25 / 0.79 9.09 / 3.86 8.0 M  An ultra-lightweight Kannada recognition model trained based on the PP-OCRv3 recognition model, supporting Kannada and numeric character recognition. ta_PP-OCRv3_mobile_recInference Model/Pretrained Model 76.83 5.23 / 0.75 10.13 / 4.30 8.0 M  An ultra-lightweight Tamil recognition model trained based on the PP-OCRv3 recognition model, supporting Tamil and numeric character recognition. latin_PP-OCRv3_mobile_recInference Model/Pretrained Model 76.93 5.20 / 0.79 8.83 / 7.15 7.8 M An ultra-lightweight Latin recognition model trained based on the PP-OCRv3 recognition model, supporting Latin and numeric character recognition. arabic_PP-OCRv3_mobile_recInference Model/Pretrained Model 73.55 5.35 / 0.79 8.80 / 4.56 7.8 M An ultra-lightweight Arabic alphabet recognition model trained based on the PP-OCRv3 recognition model, supporting Arabic alphabet and numeric character recognition. cyrillic_PP-OCRv3_mobile_recInference Model/Pretrained Model 94.28 5.23 / 0.76 8.89 / 3.88 7.9 M   An ultra-lightweight Cyrillic alphabet recognition model trained based on the PP-OCRv3 recognition model, supporting Cyrillic alphabet and numeric character recognition. devanagari_PP-OCRv3_mobile_recInference Model/Pretrained Model 96.44 5.22 / 0.79 8.56 / 4.06 7.9 M An ultra-lightweight Devanagari alphabet recognition model trained based on the PP-OCRv3 recognition model, supporting Devanagari alphabet and numeric character recognition. Test Environment Description: <ul> <li>Performance Test Environment <ul> <li>Test Dataset: <ul> <li>                     Chinese Recognition Models: A self-built Chinese dataset by PaddleOCR, covering various scenarios such as street views, online images, documents, and handwriting, with 11,000 images for text recognition.                     </li> <li>                       ch_SVTRv2_rec: The evaluation set of Leaderboard A in the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition Task.                     </li> <li>                       ch_RepSVTR_rec: The evaluation set of Leaderboard B in the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition Task.                     </li> <li>                       English Recognition Models: A self-built English dataset by PaddleX.                     </li> <li>                       Multilingual Recognition Models: A self-built multilingual dataset by PaddleX.                     </li> </ul> </li> <li>Hardware Configuration: <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environment: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Explanation of Inference Modes</li> </ul> Mode GPU Configuration CPU Configuration Acceleration Technology Combination Normal Mode FP32 Precision / No TRT Acceleration FP32 Precision / 8 Threads PaddleInference High-Performance Mode Optimal combination of precision type and acceleration strategy FP32 Precision / 8 Threads Selection of the optimal backend (Paddle/OpenVINO/TRT, etc.)"},{"location":"en/version3.x/module_usage/text_recognition.html#iii-quick-start","title":"III. Quick Start","text":"<p>\u2757 Before starting, please install the wheel package of PaddleOCR. For detailed instructions, refer to the Installation Guide.</p> <p>You can quickly experience the functionality with a single command:</p> <pre><code>paddleocr text_recognition -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_rec_001.png\n</code></pre> <p>Note: The official models would be download from HuggingFace by default. If can't access to HuggingFace, please set the environment variable <code>PADDLE_PDX_MODEL_SOURCE=\"BOS\"</code> to change the model source to BOS. In the future, more model sources will be supported.</p> <p>You can also integrate the model inference of the text recognition module into your project. Before running the following code, please download the sample image to your local machine.</p> <pre><code>from paddleocr import TextRecognition\nmodel = TextRecognition(model_name=\"PP-OCRv5_server_rec\")\noutput = model.predict(input=\"general_ocr_rec_001.png\", batch_size=1)\nfor res in output:\n    res.print()\n    res.save_to_img(save_path=\"./output/\")\n    res.save_to_json(save_path=\"./output/res.json\")\n</code></pre> <p>After running, the obtained result is as follows: <pre><code>{'res': {'input_path': 'general_ocr_rec_001.png', 'page_index': None, 'rec_text': '\u200b\u7eff\u6d32\u200b\u4ed5\u683c\u7ef4\u200b\u82b1\u56ed\u200b\u516c\u5bd3\u200b', 'rec_score': 0.9823867082595825}}\n</code></pre></p> <p>The meanings of the parameters in the running result are as follows: - <code>input_path</code>: Indicates the path to the input image containing the text line to be predicted. - <code>page_index</code>: If the input is a PDF file, it indicates which page of the PDF the current text line is from; otherwise, it is <code>None</code>. - <code>rec_text</code>: Indicates the predicted text of the text line image. - <code>rec_score</code>: Indicates the confidence score of the predicted text for the text line image.</p> <p>The visualized image is as follows:</p> <p></p> <p>The descriptions of relevant methods and parameters are as follows:</p> <ul> <li>Instantiate a text recognition model using <code>TextRecognition</code> (taking <code>PP-OCRv5_server_rec</code> as an example here). The specific descriptions are as follows:</li> </ul> Parameter Description Type Default <code>model_name</code> Name of the model <code>str</code> <code>None</code> <code>model_dir</code> Model storage path <code>str</code> <code>None</code> <code>device</code> Device(s) to use for inference. Examples: <code>cpu</code>, <code>gpu</code>, <code>npu</code>, <code>gpu:0</code>, <code>gpu:0,1</code>. If multiple devices are specified, inference will be performed in parallel. Note that parallel inference is not always supported. By default, GPU 0 will be used if available; otherwise, the CPU will be used.  <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to use the high performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Precision for TensorRT when using the Paddle Inference TensorRT subgraph engine.Options: <code>fp32</code>, <code>fp16</code>, etc. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code>  Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads to use for inference on CPUs. <code>int</code> <code>10</code> <code>input_shape</code> Input image size for the model in the format <code>(C, H, W)</code>. If set to <code>None</code>, the model's default size will be used. <code>tuple</code> / <code>None</code> <code>None</code> <ul> <li> <p>Among them, <code>model_name</code> must be specified. After specifying <code>model_name</code>, the default model parameters built into PaddleX are used. On this basis, when <code>model_dir</code> is specified, the user-defined model is used.</p> </li> <li> <p>Call the <code>predict()</code> method of the text recognition model for inference prediction. This method returns a list of results. In addition, this module also provides the <code>predict_iter()</code> method. The two methods are completely consistent in terms of parameter acceptance and result return. The difference is that <code>predict_iter()</code> returns a <code>generator</code>, which can process and obtain prediction results step by step. It is suitable for scenarios where large datasets need to be processed or memory savings are desired. You can choose either of these two methods according to your actual needs. The parameters of the <code>predict()</code> method include <code>input</code> and <code>batch_size</code>, with specific descriptions as follows:</p> </li> </ul> Parameter Description Type Options Default Value <code>input</code> Data to be predicted, supporting multiple input types <code>Python Var</code>/<code>str</code>/<code>list</code> <ul> <li>Python variable, such as image data represented by <code>numpy.ndarray</code></li> <li>File path, such as the local path of an image file: <code>/root/data/img.jpg</code></li> <li>URL link, such as the network URL of an image file: Example</li> <li>Local directory, which should contain the data files to be predicted, such as the local path: <code>/root/data/</code></li> <li>List, the elements of which should be data of the above types, such as <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code></li> </ul> None <code>batch_size</code> Batch size,  positive integer. <code>int</code> Any integer 1 <ul> <li>Process the prediction results. The prediction result for each sample is a corresponding Result object, which supports operations such as printing, saving as an image, and saving as a <code>json</code> file:</li> </ul> Method Description Parameter Type Description Default Value <code>print()</code> Print the result to the terminal <code>format_json</code> <code>bool</code> Whether to format the output content using <code>JSON</code> indentation <code>True</code> <code>indent</code> <code>int</code> Specifies the indentation level to beautify the output <code>JSON</code> data, making it more readable. Only effective when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Controls whether to escape non-<code>ASCII</code> characters as <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> retains the original characters. Only effective when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_json()</code> Save the result as a file in <code>json</code> format <code>save_path</code> <code>str</code> The file path to save the result. When it is a directory, the saved file name is consistent with the naming of the input file type. None <code>indent</code> <code>int</code> Specifies the indentation level to beautify the output <code>JSON</code> data, making it more readable. Only effective when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Controls whether to escape non-<code>ASCII</code> characters as <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> retains the original characters. Only effective when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_img()</code> Save the result as a file in image format <code>save_path</code> <code>str</code> The file path to save the result. When it is a directory, the saved file name is consistent with the naming of the input file type. None <ul> <li>In addition, it also supports obtaining the visualized image with results and the prediction results through attributes, as follows:</li> </ul> Attribute Description <code>json</code> Obtain the prediction result in <code>json</code> format <code>img</code> Obtain the visualized image in <code>dict</code> format"},{"location":"en/version3.x/module_usage/text_recognition.html#v-secondary-development","title":"V. Secondary Development","text":"<p>If the performance of the above models does not meet your requirements in your specific scenario, you can follow the steps below for secondary development. Here, we use the training of <code>PP-OCRv5_server_rec</code> as an example; for other models, simply replace the corresponding configuration files. First, you need to prepare a dataset for text recognition. You can refer to the format of the Text Recognition Demo Dataset for preparation. Once prepared, you can proceed with model training and exporting as described below. After exporting, the model can be quickly integrated into the aforementioned API. This example uses the Text Recognition Demo Dataset. Before training the model, ensure that you have installed the dependencies required by PaddleOCR as per the Installation Guide.</p>"},{"location":"en/version3.x/module_usage/text_recognition.html#41-dataset-and-pre-trained-model-preparation","title":"4.1 Dataset and Pre-trained Model Preparation","text":""},{"location":"en/version3.x/module_usage/text_recognition.html#411-prepare-the-dataset","title":"4.1.1 Prepare the Dataset","text":"<pre><code># Download the example dataset\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/ocr_rec_dataset_examples.tar\ntar -xf ocr_rec_dataset_examples.tar\n</code></pre>"},{"location":"en/version3.x/module_usage/text_recognition.html#412-download-the-pre-trained-model","title":"4.1.2 Download the Pre-trained Model","text":"<pre><code># Download the PP-OCRv5_server_rec pre-trained model\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-OCRv5_server_rec_pretrained.pdparams\n</code></pre>"},{"location":"en/version3.x/module_usage/text_recognition.html#42-model-training","title":"4.2 Model Training","text":"<p>PaddleOCR modularizes its code. To train the <code>PP-OCRv5_server_rec</code> recognition model, you need to use its configuration file.</p> <p>The training commands are as follows:</p> <pre><code># Single-GPU training (default training method)\npython3 tools/train.py -c configs/rec/PP-OCRv5/PP-OCRv5_server_rec.yml \\\n   -o Global.pretrained_model=./PP-OCRv5_server_rec_pretrained.pdparams\n\n# Multi-GPU training, specify GPU IDs via the --gpus parameter\npython3 -m paddle.distributed.launch --gpus '0,1,2,3' tools/train.py -c configs/rec/PP-OCRv5/PP-OCRv5_server_rec.yml \\\n        -o Global.pretrained_model=./PP-OCRv5_server_rec_pretrained.pdparams\n</code></pre>"},{"location":"en/version3.x/module_usage/text_recognition.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>You can evaluate the weights of a trained model, such as <code>output/xxx/xxx.pdparams</code>, using the following command:</p> <pre><code># Note: Set the path of pretrained_model to a local path. If using a model you trained and saved yourself, ensure to modify the path and filename to {path/to/weights}/{model_name}.\n# Evaluation on the demo test set\npython3 tools/eval.py -c configs/rec/PP-OCRv5/PP-OCRv5_server_rec.yml -o \\\nGlobal.pretrained_model=output/xxx/xxx.pdparams\n</code></pre>"},{"location":"en/version3.x/module_usage/text_recognition.html#44-model-exporting","title":"4.4 Model Exporting","text":"<pre><code>python3 tools/export_model.py -c configs/rec/PP-OCRv5/PP-OCRv5_server_rec.yml -o \\\nGlobal.pretrained_model=output/xxx/xxx.pdparams \\\nGlobal.save_inference_dir=\"./PP-OCRv5_server_rec_infer/\"\n</code></pre> <p>After exporting the model, the static graph model will be stored in <code>./PP-OCRv5_server_rec_infer/</code> in the current directory. Under this directory, you will see the following files: <pre><code>./PP-OCRv5_server_rec_infer/\n\u251c\u2500\u2500 inference.json\n\u251c\u2500\u2500 inference.pdiparams\n\u251c\u2500\u2500 inference.yml\n</code></pre></p> <p>At this point, the secondary development is complete. This static graph model can be directly integrated into the PaddleOCR API.</p>"},{"location":"en/version3.x/module_usage/text_recognition.html#v-faq","title":"V. FAQ","text":""},{"location":"en/version3.x/module_usage/textline_orientation_classification.html","title":"Text Line Orientation Classification Module Tutorial","text":""},{"location":"en/version3.x/module_usage/textline_orientation_classification.html#1-overview","title":"1. Overview","text":"<p>The text line orientation classification module primarily distinguishes the orientation of text lines and corrects them using post-processing. In processes such as document scanning and license/certificate photography, to capture clearer images, the capture device may be rotated, resulting in text lines in various orientations. Standard OCR pipelines cannot handle such data well. By utilizing image classification technology, the orientation of text lines can be predetermined and adjusted, thereby enhancing the accuracy of OCR processing.</p>"},{"location":"en/version3.x/module_usage/textline_orientation_classification.html#2-supported-model-list","title":"2. Supported Model List","text":"ModelModel Download Link Top-1 Accuracy (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms) Model Size (M) Description PP-LCNet_x0_25_textline_oriInference Model/Training Model 98.85 - - 0.96 Text line classification model based on PP-LCNet_x0_25, with two classes: 0 degrees and 180 degrees PP-LCNet_x1_0_textline_oriInference Model/Training Model 99.42 - - 6.5 Text line classification model based on PP-LCNet_x1_0, with two classes: 0 degrees and 180 degrees <p>\u2757 Note: The text line orientation classification model was upgraded on May 26, 2025, and <code>PP-LCNet_x1_0_textline_ori</code> has been added. If you need to use the pre-upgrade model weights, please click the download link.</p> <p>Test Environment Description:</p> <ul> <li>Performance Test Environment <ul> <li>Test Dataset\uff1a PaddleX Self-built Dataset, Covering Multiple Scenarios Such as Documents and Certificates, Containing 1000 Images.</li> <li>Hardware Configuration\uff1a <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environments: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Description</li> </ul> Mode GPU Configuration  CPU Configuration  Acceleration Technology Combination Normal Mode FP32 Precision / No TRT Acceleration FP32 Precision / 8 Threads PaddleInference High-Performance Mode Optimal combination of pre-selected precision types and acceleration strategies FP32 Precision / 8 Threads Pre-selected optimal backend (Paddle/OpenVINO/TRT, etc.)"},{"location":"en/version3.x/module_usage/textline_orientation_classification.html#3-quick-integration","title":"3. Quick Integration","text":"<p>\u2757 Before starting, please install the wheel package of PaddleOCR. For detailed instructions, refer to the Installation Guide.</p> <p>You can quickly experience the functionality with a single command:</p> <pre><code>paddleocr textline_orientation_classification -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/textline_rot180_demo.jpg\n</code></pre> <p>Note: The official models would be download from HuggingFace by default. If can't access to HuggingFace, please set the environment variable <code>PADDLE_PDX_MODEL_SOURCE=\"BOS\"</code> to change the model source to BOS. In the future, more model sources will be supported.</p> <p>You can also integrate the text line orientation classification model into your project. Run the following code after downloading the example image to your local machine. </p> <pre><code>from paddleocr import TextLineOrientationClassification\nmodel = TextLineOrientationClassification(model_name=\"PP-LCNet_x0_25_textline_ori\")\noutput = model.predict(\"textline_rot180_demo.jpg\",  batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/demo.png\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> <p>After running, the result obtained is:</p> <pre><code>{'res': {'input_path': 'textline_rot180_demo.jpg', 'page_index': None, 'class_ids': array([1], dtype=int32), 'scores': array([0.99864], dtype=float32), 'label_names': ['180_degree']}}\n</code></pre> <p>The meanings of the running results parameters are as follows:</p> <ul> <li><code>input_path</code>\uff1aIndicates the path of the input image.</li> <li><code>page_index</code>\uff1aIf the input is a PDF file, it indicates the current page number of the PDF; otherwise, it is <code>None</code>.</li> <li><code>class_ids</code>\uff1aIndicates the class ID of the prediction result.</li> <li><code>scores</code>\uff1aIndicates the confidence score of the prediction result.</li> <li><code>label_names</code>\uff1aIndicates the class name of the prediction result. The visualization image is as follows:</li> </ul> <p></p> <p>The explanations for the methods, parameters, etc., are as follows:</p> <ul> <li><code>TextLineOrientationClassification</code> instantiates a textline classification model (here, <code>PP-LCNet_x0_25_textline_ori</code> is used as an example), and the specific explanations are as follows:</li> </ul> Parameter Description Type Default <code>model_name</code> Name of the model <code>str</code> <code>None</code> <code>model_dir</code> Model storage path <code>str</code> <code>None</code> <code>device</code> Device(s) to use for inference. Examples: <code>cpu</code>, <code>gpu</code>, <code>npu</code>, <code>gpu:0</code>, <code>gpu:0,1</code>. If multiple devices are specified, inference will be performed in parallel. Note that parallel inference is not always supported. By default, GPU 0 will be used if available; otherwise, the CPU will be used.  <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to use the high performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Precision for TensorRT when using the Paddle Inference TensorRT subgraph engine.Options: <code>fp32</code>, <code>fp16</code>, etc. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code>  Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads to use for inference on CPUs. <code>int</code> <code>10</code> <code>top_k</code> The top-k value for prediction results. If not specified, the default value in the official PaddleOCR model configuration is used. If the value is 5, the top 5 categories and their corresponding classification probabilities will be returned. <code>int</code> <code>None</code> <ul> <li> <p><code>model_name</code> must be specified. Once <code>model_name</code> is set, the default built-in model parameters of PaddleOCR will be used. On this basis, if <code>model_dir</code> is specified, the user-defined model will be used.</p> </li> <li> <p>Use the <code>predict()</code> method of the text line direction classification model to perform inference. This method returns a list of results. In addition, this module also provides the <code>predict_iter()</code> method. Both methods accept the same parameters and return the same result format. The difference is that <code>predict_iter()</code> returns a <code>generator</code>, which processes and retrieves prediction results step by step. It is suitable for handling large datasets or memory-efficient scenarios. You can choose either method based on your actual needs. The <code>predict()</code> method accepts the parameters <code>input</code> and <code>batch_size</code>, which are described in detail below:</p> </li> </ul> Parameter Description Type Default <code>input</code> Input data for prediction. Multiple input types are supported. This parameter is required. <ul> <li>Python Var: such as <code>numpy.ndarray</code> representing image data</li> <li>str: such as the local path of an image or PDF file: <code>/root/data/img.jpg</code>; or a URL link, such as the online URL of an image or PDF file: Example; or a local directory that contains images for prediction, such as <code>/root/data/</code> (currently, directories containing PDF files are not supported; PDF files must be specified as individual file paths)</li> <li>List: list elements must be of the above types, such as <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code></li> </ul> <code>Python Var|str|list</code> <code>batch_size</code> Batch size,  positive integer. <code>int</code> 1 <ul> <li>Call the <code>predict()</code> method of the text line orientation classification model for inference. This method will return a list of results. In addition, this module also provides a <code>predict_iter()</code> method. Both methods accept the same parameters and return the same results, but <code>predict_iter()</code> returns a <code>generator</code>, which is more suitable for processing large datasets or when you want to save memory. You can choose either method according to your needs. The parameters of the <code>predict()</code> method are <code>input</code> and <code>batch_size</code>, as described below:</li> </ul> Parameter Parameter Description Parameter Type Options Default Value <code>input</code> Data to be predicted, supporting multiple input types <code>Python Var</code>/<code>str</code>/<code>list</code> <ul> <li>Python variable, such as image data represented by <code>numpy.ndarray</code></li> <li>File path, such as the local path of an image file: <code>/root/data/img.jpg</code></li> <li>URL link, such as the network URL of an image file: Example</li> <li>Local directory, the directory should contain data files to be predicted, such as the local path: <code>/root/data/</code></li> <li>List, the elements of the list should be of the above-mentioned data types, such as <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\\\"/root/data/img1.jpg\\\", \\\"/root/data/img2.jpg\\\"]</code>, <code>[\\\"/root/data1\\\", \\\"/root/data2\\\"]</code></li> </ul> None <code>batch_size</code> Batch size <code>int</code> Any integer 1 <ul> <li>The prediction results are processed, and the prediction result for each sample is of type <code>dict</code>. It supports operations such as printing, saving as an image, and saving as a <code>json</code> file:</li> </ul> Method Method Description Parameter Parameter Type Parameter Description Default Value <code>print()</code> Print the results to the terminal <code>format_json</code> <code>bool</code> Whether to format the output content using <code>JSON</code> indentation <code>True</code> <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data, making it more readable, only effective when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. If set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> retains the original characters, only effective when <code>format_json</code> is <code>True</code> <code>False</code> <code>save_to_json()</code> Save the results as a JSON file <code>save_path</code> <code>str</code> The path to save the file. If it is a directory, the saved file name will be consistent with the input file name None <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data, making it more readable, only effective when <code>format_json</code> is <code>True</code> 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. If set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> retains the original characters, only effective when <code>format_json</code> is <code>True</code> <code>False</code> <code>save_to_img()</code> Save the results as an image file <code>save_path</code> <code>str</code> The path to save the file. If it is a directory, the saved file name will be consistent with the input file name None <ul> <li>Additionally, it supports obtaining the visualization image with results and the prediction results through attributes, as follows:</li> </ul> Attribute Attribute Description <code>json</code> Get the prediction result in <code>json</code> format <code>img</code> Get the visualization image in <code>dict</code> format"},{"location":"en/version3.x/module_usage/textline_orientation_classification.html#4-custom-development","title":"4. Custom Development","text":"<p>Since PaddleOCR does not natively support training for text line orientation classification, refer to PaddleX's Custom Development Guide for training. Trained models can seamlessly integrate into PaddleOCR's API for inference.</p>"},{"location":"en/version3.x/other_devices_support/multi_devices_use_guide.html","title":"PaddleOCR Multi-Devices Usage Guide","text":"<p>This document focuses on the usage guide of PaddleX for Huawei Ascend NPU and Kunlun XPU hardware platforms.</p>"},{"location":"en/version3.x/other_devices_support/multi_devices_use_guide.html#1installation","title":"1\u3001Installation","text":""},{"location":"en/version3.x/other_devices_support/multi_devices_use_guide.html#11-paddlepaddle-installation","title":"1.1 PaddlePaddle Installation","text":"<p>First, please complete the installation of PaddlePaddle according to your hardware platform. The installation tutorials for each hardware are as follows:</p> <p>Ascend NPU: Ascend NPU PaddlePaddle Installation Guide</p> <p>Kunlun XPU: Kunlun XPU PaddlePaddle Installation Guide</p>"},{"location":"en/version3.x/other_devices_support/multi_devices_use_guide.html#12-paddleocr-installation","title":"1.2 PaddleOCR Installation","text":"<p>Please refer to PaddleOCR Installation Guide to install PaddleOCR\u3002</p>"},{"location":"en/version3.x/other_devices_support/multi_devices_use_guide.html#2usage","title":"2\u3001Usage","text":"<p>The methods for training and inference of PaddleOCR on hardware platforms such as Ascend NPU and Kunlun XPU are the same as those on GPU. You only need to modify the configuration parameters according to the specific hardware platform. On these two hardware platforms, quick inference and model fine-tuning are supported for the three major features of PaddleOCR, including the text recognition model PP-OCRv5, the document parsing solution PP-StructureV3, and PP-ChatOCRv4.</p>"},{"location":"en/version3.x/other_devices_support/multi_devices_use_guide.html#21-quick-inference","title":"2.1 Quick Inference","text":"<p>You can quickly experience the OCR pipeline inference with a single command:</p> <ul> <li>OCR pipeline inference</li> </ul> <pre><code># The default model used is PP-OCRv5\npaddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --device npu:0\n</code></pre> <ul> <li>PP-StructureV3 pipeline inference</li> </ul> <pre><code>paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --device npu:0\n</code></pre> <p>To perform quick inference using the production line in your project, you can achieve this with just a few lines of code. Here\u2019s an example of how you might set it up:</p> <ul> <li>OCR pipeline inference</li> </ul> <pre><code>from paddleocr import PaddleOCR\n\nocr = PaddleOCR(device=\"npu:0\")\n\nresult = ocr.predict(\"./general_ocr_002.png\")\nfor res in result:\n    res.print()\n    res.save_to_img(\"output\")\n    res.save_to_json(\"output\")\n</code></pre> <ul> <li>PP-StructureV3 pipeline inference</li> </ul> <pre><code>from paddleocr import PPStructureV3\n\npipeline = PPStructureV3(device=\"npu:0\")\noutput = pipeline.predict(\"./pp_structure_v3_demo.png\")\nfor res in output:\n    res.print() \n    res.save_to_json(save_path=\"output\") \n    res.save_to_markdown(save_path=\"output\")\n</code></pre> <p>If you want to know more about OCR pipeline inference,please refer to general OCR pipeline use guide. If you want to know more about PPStructureV3 pipeline inference,please refer to PP-StructureV3 pipeline use guide.</p>"},{"location":"en/version3.x/other_devices_support/multi_devices_use_guide.html#22-model-fine-tuning","title":"2.2 Model Fine-tuning","text":"<p>If you are not satisfied with the performance of the pre-trained model, you can fine-tune it.</p> <ul> <li> <p>train on Ascend NPU <pre><code>export FLAGS_npu_storage_format=0\nexport FLAGS_npu_jit_compile=0\nexport FLAGS_use_stride_kernel=0\nexport FLAGS_allocator_strategy=auto_growth\nexport FLAGS_npu_split_aclnn=True\nexport FLAGS_npu_scale_aclnn=True\nexport CUSTOM_DEVICE_BLACK_LIST=pad3d,pad3d_grad\npython3 -m paddle.distributed.launch --devices '0,1,2,3' \\\n        tools/train.py -c configs/rec/PP-OCRv5/PP-OCRv5_mobile_rec.yml \\\n        -o Global.use_gpu=False Global.use_npu=True\n</code></pre></p> </li> <li> <p>train on Kunlun XPU <pre><code>export FLAGS_use_stride_kernel=0\nexport BKCL_FORCE_SYNC=1\nexport BKCL_TIMEOUT=1800\nexport XPU_BLACK_LIST=pad3d,pad3d_grad\npython3 -m paddle.distributed.launch --devices '0,1,2,3' \\\n        tools/train.py -c configs/rec/PP-OCRv5/PP-OCRv5_mobile_rec.yml \\\n        -o Global.use_gpu=False Global.use_xpu=True\n</code></pre></p> </li> </ul>"},{"location":"en/version3.x/other_devices_support/multi_devices_use_guide.html#23-other-inference-methods","title":"2.3 Other Inference methods","text":"<p>On the Ascend NPU, for a small number of inference samples, using the aforementioned pipeline inference method may result in abnormal outcomes (primarily with the PP-StructureV3 pipeline). To address this issue, we support using ONNX models for inference to ensure correct results.</p> <p>You can use the following command to convert a Paddle model to an ONNX model:</p> <pre><code>paddlex --install paddle2onnx\npaddlex --paddle2onnx --paddle_model_dir /paddle_model_dir --onnx_model_dir /onnx_model_dir --opset_version 7\n</code></pre> <p>Meanwhile, some models support Ascend offline OM inference, effectively optimizing inference performance and memory usage. Using models in OM + ONNX format for pipeline inference can ensure both accuracy and speed.</p> <p>Use the ATC conversion tool to convert an ONNX model to an OM model:</p> <pre><code>atc --model=inference.onnx --framework=5 --output=inference --soc_version=\"your_device_type\" --input_shape \"your_input_shape\"\n</code></pre> <p>We have deeply integrated ONNX and OM models into PaddleX for high-performance inference. By modifying the pipeline configuration file to set the model inference backend to ONNX or OM, you can use the PaddleX high-performance inference API for inference.</p> <p>For specific modification methods, inference code, and more usage instructions, please refer to Ascend NPU High-Performance Inference Tutorial\u3002</p>"},{"location":"en/version3.x/other_devices_support/multi_devices_use_guide.html#3-faq","title":"3\u3001 FAQ","text":""},{"location":"en/version3.x/other_devices_support/multi_devices_use_guide.html#1the-inference-results-using-pp-structurev3-on-the-production-line-are-incorrect","title":"1.The inference results using PP-StructureV3 on the production line are incorrect.","text":"<p>Some models on this line have precision errors in a small number of cases. You can try adjusting the model in the configuration file or use ONNX+OM models for inference.</p>"},{"location":"en/version3.x/other_devices_support/paddlepaddle_install_NPU.html","title":"Ascend NPU PaddlePaddle Installation Tutorial","text":"<p>Currently, PaddleOCR supports the Ascend 910B chip (more models are under support. If you have a related need for other models, please submit an issue to inform us). The Ascend driver version is 23.0.3. Considering the differences in environments, we recommend using the Ascend development image provided by PaddlePaddle to complete the environment preparation.</p>"},{"location":"en/version3.x/other_devices_support/paddlepaddle_install_NPU.html#1-docker-environment-preparation","title":"1. Docker Environment Preparation","text":"<ul> <li>Pull the image. This image is only for the development environment and does not contain a pre-compiled PaddlePaddle installation package. The image has CANN-8.0.0, the Ascend operator library, installed by default. <pre><code># For X86 architecture\ndocker pull ccr-2vdh3abv-pub.cnc.bj.baidubce.com/device/paddle-npu:cann800-ubuntu20-npu-910b-base-x86_64-gcc84\n# For Aarch64 architecture\ndocker pull ccr-2vdh3abv-pub.cnc.bj.baidubce.com/device/paddle-npu:cann800-ubuntu20-npu-910b-base-aarch64-gcc84\n</code></pre></li> <li>Start the container with the following command. ASCEND_RT_VISIBLE_DEVICES specifies the visible NPU card numbers. <pre><code>docker run -it --name paddle-npu-dev -v $(pwd):/work \\\n    --privileged --network=host --shm-size=128G -w=/work \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -e ASCEND_RT_VISIBLE_DEVICES=\"0,1,2,3,4,5,6,7\" \\\n    ccr-2vdh3abv-pub.cnc.bj.baidubce.com/device/paddle-npu:cann800-ubuntu20-npu-910b-base-$(uname -m)-gcc84 /bin/bash\n</code></pre></li> </ul>"},{"location":"en/version3.x/other_devices_support/paddlepaddle_install_NPU.html#2-install-paddle-package","title":"2. Install Paddle Package","text":"<ul> <li>Download and install the Python wheel installation package <pre><code># Note: You need to install the CPU version of PaddlePaddle first\npython -m pip install paddlepaddle==3.0.0.dev20250527 -i https://www.paddlepaddle.org.cn/packages/nightly/cpu\npython -m pip install paddle-custom-npu==3.0.0.dev20250527 -i https://www.paddlepaddle.org.cn/packages/nightly/npu\n</code></pre></li> <li>CANN-8.0.0 does not support some versions of numpy and opencv, it is recommended to install the specified versions. <pre><code>python -m pip install numpy==1.26.4\npython -m pip install opencv-python==3.4.18.65\n</code></pre></li> <li>Set environment variables on the arm machine (not required for x86 environment) <pre><code># Solve the error reported by libgomp on the arm machine\n# \"libgomp cannot allocate memory in static TLS block\"\nexport LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libgomp.so.1:$LD_PRELOAD\n</code></pre></li> <li>After verifying that the installation package is installed, run the following command <pre><code>python -c \"import paddle; paddle.utils.run_check()\"\n</code></pre> The expected output is as follows</li> </ul> <pre><code>Running verify PaddlePaddle program ...\nPaddlePaddle works well on 1 npu.\nPaddlePaddle works well on 8 npus.\nPaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\n</code></pre>"},{"location":"en/version3.x/other_devices_support/paddlepaddle_install_XPU.html","title":"Kunlun XPU PaddlePaddle Installation Tutorial","text":"<p>Currently, PaddleOCR supports Kunlun R200/R300 and other chips. Considering environmental differences, we recommend using the Kunlun XPU development image officially released by PaddlePaddle, which is pre-installed with the Kunlun basic runtime environment library (XRE).</p>"},{"location":"en/version3.x/other_devices_support/paddlepaddle_install_XPU.html#1-docker-environment-preparation","title":"1. Docker Environment Preparation","text":"<p>Pull the image. This image is only for the development environment and does not include a pre-compiled PaddlePaddle installation package.</p> <p><pre><code>docker pull registry.baidubce.com/device/paddle-xpu:ubuntu20-x86_64-gcc84-py310 # For X86 architecture\ndocker pull registry.baidubce.com/device/paddle-xpu:kylinv10-aarch64-gcc82-py310 # For ARM architecture\n</code></pre> Refer to the following command to start the container:</p> <pre><code>docker run -it --name=xxx -m 81920M --memory-swap=81920M \\\n    --shm-size=128G --privileged --net=host \\\n    -v $(pwd):/workspace -w /workspace \\\n    registry.baidubce.com/device/paddle-xpu:$(uname -m)-py310 bash\n</code></pre>"},{"location":"en/version3.x/other_devices_support/paddlepaddle_install_XPU.html#2-install-paddle-package","title":"2. Install Paddle Package","text":"<p>Currently, Python3.10 wheel installation packages are provided. If you have a need for other Python versions, you can refer to the PaddlePaddle official documentation to compile and install them yourself.</p> <p>Install the Python3.10 wheel installation package:</p> <pre><code>pip install https://paddle-whl.bj.bcebos.com/paddlex/xpu/paddlepaddle_xpu-2.6.1-cp310-cp310-linux_x86_64.whl # For X86 architecture\npip install https://paddle-whl.bj.bcebos.com/paddlex/xpu/paddlepaddle_xpu-2.6.1-cp310-cp310-linux_aarch64.whl # For ARM architecture\n</code></pre> <p>Verify the installation package. After installation, run the following command:</p> <pre><code>python -c \"import paddle; paddle.utils.run_check()\"\n</code></pre> <p>The expected output is:</p> <pre><code>PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\n</code></pre>"},{"location":"en/version3.x/paddlex/overview.html","title":"Overview","text":""},{"location":"en/version3.x/paddlex/overview.html#1-introduction-to-all-in-one-development","title":"1. Introduction to All-in-One Development","text":"<p>The All-in-One development tool PaddleX, based on the advanced technology of PaddleOCR, supports low-code full-process development capabilities in the OCR field. Through low-code development, simple and efficient model use, combination, and customization can be achieved. This will significantly reduce the time consumption of model development, lower its development difficulty, and greatly accelerate the application and promotion speed of models in the industry. Features include:</p> <ul> <li> <p>\ud83c\udfa8 Rich Model One-Click Call: Integrates 48 models related to text image intelligent analysis, general OCR, general layout parsing, table recognition, formula recognition, and seal recognition into 10 pipelines, which can be quickly experienced through a simple Python API one-click call. In addition, the same set of APIs also supports a total of 200+ models in image classification, object detection, image segmentation, and time series forecasting, forming 30+ single-function modules, making it convenient for developers to use model combinations.</p> </li> <li> <p>\ud83d\ude80 High Efficiency and Low barrier of entry: Provides two methods based on unified commands and GUI to achieve simple and efficient use, combination, and customization of models. Supports multiple deployment methods such as high-performance inference, service-oriented deployment, and on-device deployment. Additionally, for various mainstream hardware such as NVIDIA GPU, Kunlunxin XPU, Ascend NPU, Cambricon MLU, and Haiguang DCU, models can be developed with seamless switching.</p> </li> </ul> <p>Note: PaddleX is committed to achieving pipeline-level model training, inference, and deployment. A model pipeline refers to a series of predefined development processes for specific AI tasks, including combinations of single models (single-function modules) that can independently complete a type of task.</p>"},{"location":"en/version3.x/paddlex/overview.html#2-ocr-related-capability-support","title":"2. OCR-Related Capability Support","text":"<p>In PaddleX, all 6 OCR-related pipelines support local inference, and some pipelines support online experience. You can quickly experience the pre-trained model effects of each pipeline. If you are satisfied with the pre-trained model effects of a pipeline, you can directly proceed with high-performance inference/service-oriented deployment/on-device deployment. If not satisfied, you can also use the custom development capabilities of the pipeline to improve the effects. For the complete pipeline development process, please refer to PaddleX Pipeline Usage Overview or the tutorials for each pipeline.</p> <p>In addition, PaddleX provides developers with a full-process efficient model training and deployment tool based on a cloud-based GUI. Developers do not need code development, just need to prepare a dataset that meets the pipeline requirements to quickly start model training. For details, please refer to the tutorial \"Developing Industrial-level AI Models with Zero Barrier\".</p> Pipeline Online Experience Local Inference High-Performance Inference Service-Oriented Deployment On-Device Deployment Custom Development No-Code Development On AI Studio Document Image Preprocessing \ud83d\udea7 \u2705 \ud83d\udea7 \u2705 \ud83d\udea7 \u2705 \ud83d\udea7 OCR Link \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Table Recognition Link \u2705 \u2705 \u2705 \ud83d\udea7 \u2705 \u2705 Table Recognition V2 \ud83d\udea7 \u2705 \ud83d\udea7 \u2705 \ud83d\udea7 \u2705 \ud83d\udea7 Formula Recognition Link \u2705 \u2705 \u2705 \ud83d\udea7 \u2705 \u2705 Seal Recognition Link \u2705 \u2705 \u2705 \ud83d\udea7 \u2705 \u2705 Layout Parsing \ud83d\udea7 \u2705 \u2705 \u2705 \ud83d\udea7 \u2705 \ud83d\udea7 Layout Parsing v2 \ud83d\udea7 \u2705 \ud83d\udea7 \u2705 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 PP-ChatOCRv3-doc Link \u2705 \u2705 \u2705 \ud83d\udea7 \u2705 \u2705 PP-ChatOCRv4-doc \ud83d\udea7 \u2705 \u2705 \u2705 \ud83d\udea7 \u2705 \ud83d\udea7 <p>\u2757Note: The above capabilities are implemented based on GPU/CPU. PaddleX can also perform local inference and custom development on mainstream hardware such as Kunlunxin, Ascend, Cambricon, and Haiguang. The table below details the support status of the pipelines. For specific supported model lists, please refer to the Model List (Kunlunxin XPU)/Model List (Ascend NPU)/Model List (Cambricon MLU)/Model List (Haiguang DCU). We are continuously adapting more models and promoting the implementation of high-performance and service-oriented deployment on mainstream hardware. \ud83d\ude80 Support for Domestic Hardware Capabilities</p> Pipeline Name Ascend 910B Kunlunxin XPU Cambricon MLU Haiguang DCU General OCR \u2705 \u2705 \u2705 \ud83d\udea7 Table Recognition \u2705 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7"},{"location":"en/version3.x/paddlex/overview.html#3-list-and-tutorials-of-ocr-related-model-pipelines","title":"3. List and Tutorials of OCR-Related Model Pipelines","text":"<ul> <li>OCR Pipeline: Tutorial</li> <li>Table Recognition Pipeline: Tutorial</li> <li>PP-ChatOCRv3-doc Pipeline: Tutorial</li> <li>Layout Parsing Pipeline: Tutorial</li> <li>Formula Recognition Pipeline: Tutorial</li> <li>Seal Recognition Pipeline: Tutorial</li> </ul>"},{"location":"en/version3.x/paddlex/overview.html#4-list-and-tutorials-of-ocr-related-modules","title":"4. List and Tutorials of OCR-Related Modules","text":"<ul> <li>Text Detection Module: Tutorial</li> <li>Seal Detection Module: Tutorial</li> <li>Text Recognition Module: Tutorial</li> <li>Formula Recognition Module: Tutorial</li> <li>Table Structure Recognition Module: Tutorial</li> <li>Text Image Unwarping Module: Tutorial</li> <li>Layout Detection Module: Tutorial</li> <li>Document Image Orientation Classification Module: Tutorial</li> </ul>"},{"location":"en/version3.x/paddlex/overview.html#3-list-of-ocr-related-pipeline-models-and-tutorials","title":"3. List of OCR-related Pipeline Models and Tutorials","text":"<ul> <li>Document Image Preprocessing Pipeline: Tutorial</li> <li>OCR Pipeline: Tutorial</li> <li>Table Recognition Pipeline: Tutorial</li> <li>Table Recognition v2 Pipeline: Tutorial</li> <li>Layout Parsing Pipeline: Tutorial</li> <li>Layout Parsing v2 Pipeline: Tutorial</li> <li>Formula Recognition: Tutorial</li> <li>Seal Recognition: Tutorial</li> <li>PP-ChatOCRv3-doc Pipeline: Tutorial</li> <li>PP-ChatOCRv4-doc Pipeline: Tutorial</li> </ul>"},{"location":"en/version3.x/paddlex/overview.html#4-list-of-ocr-related-single-function-modules-and-tutorials","title":"4. List of OCR-related Single Function Modules and Tutorials","text":"<ul> <li>Text Detection Module: Tutorial</li> <li>Seal Detection Module: Tutorial</li> <li>Textline Orientation Classification Module: Tutorial</li> <li>Text Recognition Module: Tutorial</li> <li>Formula Recognition Module: Tutorial</li> <li>Table Structure Recognition Module: Tutorial</li> <li>Text Image Unwarping Module: Tutorial</li> <li>Layout Detection Module: Tutorial</li> <li>Document Image Orientation Classification Module: Tutorial</li> <li>Table Cells Detection Module: Tutorial</li> <li>Table Classification Module: Tutorial</li> </ul>"},{"location":"en/version3.x/paddlex/quick_start.html","title":"\u23ed\ufe0f Quick Start","text":""},{"location":"en/version3.x/paddlex/quick_start.html#installation","title":"\ud83d\udee0\ufe0f Installation","text":"<p>\u2757Before installing PaddleX, please ensure you have a basic Python runtime environment (Note: Currently supports running under Python 3.8 to Python 3.10, with more Python versions under adaptation). The PaddlePaddle version required by PaddleX</p> <ul> <li>Installing PaddlePaddle</li> </ul> <pre><code># CPU\npython -m pip install paddlepaddle==3.0.0rc0 -i https://www.paddlepaddle.org.cn/packages/stable/cpu/\n\n# gpu\uff0crequires GPU driver version \u2265450.80.02 (Linux) or \u2265452.39 (Windows)\npython -m pip install paddlepaddle-gpu==3.0.0rc0 -i https://www.paddlepaddle.org.cn/packages/stable/cu118/\n\n# gpu\uff0crequires GPU driver version \u2265545.23.06 (Linux) or \u2265545.84 (Windows)\npython -m pip install paddlepaddle-gpu==3.0.0rc0 -i https://www.paddlepaddle.org.cn/packages/stable/cu123/\n</code></pre> <p>\u2757No need to focus on the CUDA version on the physical machine, only the GPU driver version needs attention. For more information on PaddlePaddle Wheel versions, please refer to the PaddlePaddle Official Website.</p> <ul> <li>Installing PaddleX</li> </ul> <pre><code>pip install https://paddle-model-ecology.bj.bcebos.com/paddlex/whl/paddlex-3.0.0rc0-py3-none-any.whl\n</code></pre> <p>\u2757For more installation methods, refer to the PaddleX Installation Guide.</p>"},{"location":"en/version3.x/paddlex/quick_start.html#cli-usage","title":"\ud83d\udcbb CLI Usage","text":"<p>One command can quickly experience the pipeline effect, the unified CLI format is:</p> <pre><code>paddlex --pipeline [Pipeline Name] --input [Input Image] --device [Running Device]\n</code></pre> <p>Each Pipeline in PaddleX corresponds to specific parameters, which you can view in the respective Pipeline documentation for detailed explanations. Each Pipeline requires specifying three necessary parameters:</p> <ul> <li><code>pipeline</code>: The name of the Pipeline or the configuration file of the Pipeline</li> <li><code>input</code>: The local path, directory, or URL of the input file (e.g., an image) to be processed</li> <li><code>device</code>: The hardware device and its index to use (e.g., <code>gpu:0</code> indicates using the 0th GPU), or you can choose to use NPU (<code>npu:0</code>), XPU (<code>xpu:0</code>), CPU (<code>cpu</code>), etc.</li> </ul> <p>For example, using the  OCR pipeline: <pre><code>paddlex --pipeline OCR \\\n        --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png \\\n        --use_doc_orientation_classify False \\\n        --use_doc_unwarping False \\\n        --use_textline_orientation False \\\n        --save_path ./output \\\n        --device gpu:0\n</code></pre> \ud83d\udc49 Click to view the running result </p> <pre><code>{'res': {'input_path': 'general_ocr_002.png', 'page_index': None, 'model_settings': {'use_doc_preprocessor': False, 'use_textline_orientation': False}, 'doc_preprocessor_res': {'input_path': None, 'model_settings': {'use_doc_orientation_classify': True, 'use_doc_unwarping': False}, 'angle': 0},'dt_polys': [array([[ 3, 10],\n       [82, 10],\n       [82, 33],\n       [ 3, 33]], dtype=int16), ...], 'text_det_params': {'limit_side_len': 960, 'limit_type': 'max', 'thresh': 0.3, 'box_thresh': 0.6, 'unclip_ratio': 2.0}, 'text_type': 'general', 'textline_orientation_angles': [-1, ...], 'text_rec_score_thresh': 0.0, 'rec_texts': ['www.99*', ...], 'rec_scores': [0.8980069160461426,  ...], 'rec_polys': [array([[ 3, 10],\n       [82, 10],\n       [82, 33],\n       [ 3, 33]], dtype=int16), ...], 'rec_boxes': array([[  3,  10,  82,  33], ...], dtype=int16)}}\n</code></pre> <p>The visualization result is as follows:</p> <p></p> <p>To use the command line for other pipelines, simply adjust the <code>pipeline</code> parameter to the name of the corresponding pipeline and modify the parameters accordingly. Below are the commands for each pipeline:</p> \ud83d\udc49 More CLI usage for pipelines Pipeline Name Command OCR <code>paddlex --pipeline OCR --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False --save_path ./output --device gpu:0</code> Document Image Preprocessor <code>paddlex --pipeline doc_preprocessor --input https://paddle-model-ecology.bj.bcebos.com/paddlex/demo_image/doc_test_rotated.jpg --use_doc_orientation_classify True --use_doc_unwarping True --save_path ./output --device gpu:0</code> Table Recognition <code>paddlex --pipeline table_recognition --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/table_recognition.jpg --save_path ./output --device gpu:0</code> Table Recognition v2 <code>paddlex --pipeline table_recognition_v2 --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/table_recognition.jpg --save_path ./output --device gpu:0</code> Formula Recognition <code>paddlex --pipeline formula_recognition --input https://paddle-model-ecology.bj.bcebos.com/paddlex/demo_image/general_formula_recognition.png --use_layout_detection True --use_doc_orientation_classify False --use_doc_unwarping False --layout_threshold 0.5 --layout_nms True --layout_unclip_ratio  1.0 --layout_merge_bboxes_mode large --save_path ./output --device gpu:0</code> Seal Recognition <code>paddlex --pipeline seal_recognition --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/seal_text_det.png --use_doc_orientation_classify False --use_doc_unwarping False --device gpu:0 --save_path ./output</code> Layout Parsing <code>paddlex --pipeline layout_parsing --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/demo_paper.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False --save_path ./output --device gpu:0</code> Layout Parsing v2 <code>paddlex --pipeline layout_parsing_v2 --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/layout_parsing_v2_demo.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False --save_path ./output --device gpu:0</code>"},{"location":"en/version3.x/paddlex/quick_start.html#python-script-usage","title":"\ud83d\udcdd Python Script Usage","text":"<p>A few lines of code can complete the quick inference of the pipeline, the unified Python script format is as follows: <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=[Pipeline Name])\noutput = pipeline.predict([Input Image Name])\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/\")\n</code></pre> The following steps are executed:</p> <ul> <li><code>create_pipeline()</code> instantiates the pipeline object</li> <li>Passes the image and calls the <code>predict()</code> method of the pipeline object for inference prediction</li> <li>Processes the prediction results</li> </ul> <p>To use the Python script for other pipelines, simply adjust the <code>pipeline</code> parameter in the <code>create_pipeline()</code> method to the name of the corresponding pipeline and modify the parameters accordingly. Below are the parameter names and detailed usage explanations for each pipeline:</p> <p>\ud83d\udc49 More Python script usage for pipelines</p> pipeline Name Corresponding Parameter Detailed Explanation OCR <code>OCR</code> Instructions for Using the General OCR Pipeline Python Script Document Image Preprocessing <code>doc_preprocessor</code> Instructions for Using the Document Image Preprocessing Pipeline Python Script Table Recognition <code>table_recognition</code> Instructions for Using the General Table Recognition Pipeline Python Script Table Recognition v2 <code>table_recognition_v2</code> Instructions for Using the General Table Recognition v2 Pipeline Python Script Formula Recognition <code>formula_recognition</code> Instructions for Using the Formula Recognition Pipeline Python Script Seal Recognition <code>seal_recognition</code> Instructions for Using the Seal Text Recognition Pipeline Python Script Layout Parsing <code>layout_parsing</code> Instructions for Using the General Layout Parsing Pipeline Python Script Layout Parsing v2 <code>layout_parsing_v2</code> Instructions for Using the General Layout Parsing v2 Pipeline Python Script PP-ChatOCRv3-doc <code>PP-ChatOCRv3-doc</code> PP-ChatOCRv3-doc Pipeline Python Script Usage Instructions PP-ChatOCRv4-doc <code>PP-ChatOCRv4-doc</code> PP-ChatOCRv4-doc Pipeline Python Script Usage Instructions"},{"location":"en/version3.x/pipeline_usage/OCR.html","title":"General OCR Pipeline Usage Tutorial","text":""},{"location":"en/version3.x/pipeline_usage/OCR.html#1-ocr-pipeline-introduction","title":"1. OCR Pipeline Introduction","text":"<p>OCR is a technology that converts text from images into editable text. It is widely used in fields such as document digitization, information extraction, and data processing. OCR can recognize printed text, handwritten text, and even certain types of fonts and symbols.</p> <p>The general OCR pipeline is used to solve text recognition tasks by extracting text information from images and outputting it in text form. This pipeline supports the use of PP-OCRv3, PP-OCRv4, and PP-OCRv5 models, with the default model being the PP-OCRv5_server model released by PaddleOCR 3.0, which improves by 13 percentage points over PP-OCRv4_server in various scenarios.</p> <p></p> <p>The General OCR Pipeline consists of the following 5 modules. Each module can be independently trained and inferred, and includes multiple models. For detailed information, click the corresponding module to view its documentation.</p> <ul> <li>Document Image Orientation Classification Module (Optional)</li> <li>Text Image Unwarping Module (Optional)</li> <li>Text Line Orientation Classification Module (Optional)</li> <li>Text Detection Module</li> <li>Text Recognition Module</li> </ul> <p>In this pipeline, you can select models based on the benchmark test data provided below.</p> Document Image Orientation Classification Module (Optional): ModelModel Download Link Top-1 Acc (%) GPU Inference Time (ms)[Standard Mode / High-Performance Mode] CPU Inference Time (ms)[Standard Mode / High-Performance Mode] Model Size (MB) Description PP-LCNet_x1_0_doc_oriInference Model/Training Model 99.06 2.31 / 0.43 3.37 / 1.27 7 Document image classification model based on PP-LCNet_x1_0, with four categories: 0\u00b0, 90\u00b0, 180\u00b0, and 270\u00b0. Text Image Unwarp Module (Optional): ModelModel Download Link CER Model Size (MB) Description UVDocInference Model/Training Model 0.179 30.3 High-precision Text Image Unwarping model. Text Line Orientation Classification Module (Optional): ModelModel Download Link Top-1 Accuracy (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms) Model Size (M) Description PP-LCNet_x0_25_textline_oriInference Model/Training Model 98.85 - - 0.96 Text line classification model based on PP-LCNet_x0_25, with two classes: 0 degrees and 180 degrees PP-LCNet_x1_0_textline_oriInference Model/Training Model 99.42 - - 6.5 Text line classification model based on PP-LCNet_x1_0, with two classes: 0 degrees and 180 degrees Text Detection Module: ModelModel Download Link Detection Hmean (%) GPU Inference Time (ms)[Standard Mode / High-Performance Mode] CPU Inference Time (ms)[Standard Mode / High-Performance Mode] Model Size (MB) Description PP-OCRv5_server_detInference Model/Training Model 83.8 89.55 / 70.19 371.65 / 371.65 84.3 PP-OCRv5 server-side text detection model with higher accuracy, suitable for deployment on high-performance servers PP-OCRv5_mobile_detInference Model/Training Model 79.0 8.79 / 3.13 51.00 / 28.58 4.7 PP-OCRv5 mobile-side text detection model with higher efficiency, suitable for deployment on edge devices PP-OCRv4_server_detInference Model/Training Model 69.2 83.34 / 80.91 442.58 / 442.58 109 PP-OCRv4 server-side text detection model with higher accuracy, suitable for deployment on high-performance servers PP-OCRv4_mobile_detInference Model/Training Model 63.8 8.79 / 3.13 51.00 / 28.58 4.7 PP-OCRv4 mobile-side text detection model with higher efficiency, suitable for deployment on edge devices Text Recognition Module: ModelModel Download Links Recognition Avg Accuracy(%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-OCRv5_server_recInference Model/Pretrained Model 86.38  8.45/2.36   122.69/122.69  81 M PP-OCRv5_rec is a next-generation text recognition model. It aims to efficiently and accurately support the recognition of four major languages\u2014Simplified Chinese, Traditional Chinese, English, and Japanese\u2014as well as complex text scenarios such as handwriting, vertical text, pinyin, and rare characters using a single model. While maintaining recognition performance, it balances inference speed and model robustness, providing efficient and accurate technical support for document understanding in various scenarios. PP-OCRv5_mobile_recInference Model/Pretrained Model 81.29  1.46/5.43   5.32/91.79  16 M PP-OCRv4_server_rec_docInference Model/Pretrained Model 86.58 6.65 / 2.38 32.92 / 32.92 91 M PP-OCRv4_server_rec_doc is trained on a mixed dataset of more Chinese document data and PP-OCR training data, building upon PP-OCRv4_server_rec. It enhances the recognition capabilities for some Traditional Chinese characters, Japanese characters, and special symbols, supporting over 15,000 characters. In addition to improving document-related text recognition, it also enhances general text recognition capabilities. PP-OCRv4_mobile_recInference Model/Pretrained Model 83.28 4.82 / 1.20 16.74 / 4.64 11 M A lightweight recognition model of PP-OCRv4 with high inference efficiency, suitable for deployment on various hardware devices, including edge devices. PP-OCRv4_server_rec Inference Model/Pretrained Model 85.19  6.58 / 2.43 33.17 / 33.17 87 M The server-side model of PP-OCRv4, offering high inference accuracy and deployable on various servers. en_PP-OCRv4_mobile_recInference Model/Pretrained Model 70.39 4.81 / 0.75 16.10 / 5.31 7.3 M An ultra-lightweight English recognition model trained based on the PP-OCRv4 recognition model, supporting English and numeric character recognition.   &gt; \u2757 The above section lists the 6 core models that are primarily supported by the text recognition module. In total, the module supports 20 comprehensive models, including multiple multilingual text recognition models. Below is the complete list of models:   \ud83d\udc49Details of the Model List  * PP-OCRv5 Multi-Scenario Models ModelModel Download Links Avg Accuracy for Chinese Recognition (%) Avg Accuracy for English Recognition (%) Avg Accuracy for Traditional Chinese Recognition (%) Avg Accuracy for Japanese Recognition (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-OCRv5_server_recInference Model/Pretrained Model 86.38 64.70 93.29 60.35  8.45/2.36   122.69/122.69  81 M PP-OCRv5_rec is a next-generation text recognition model. It aims to efficiently and accurately support the recognition of four major languages\u2014Simplified Chinese, Traditional Chinese, English, and Japanese\u2014as well as complex text scenarios such as handwriting, vertical text, pinyin, and rare characters using a single model. While maintaining recognition performance, it balances inference speed and model robustness, providing efficient and accurate technical support for document understanding in various scenarios. PP-OCRv5_mobile_recInference Model/Pretrained Model 81.29 66.00 83.55 54.65  1.46/5.43   5.32/91.79  16 M   *  Chinese Recognition Models ModelDownload Link Recognition Avg Accuracy(%) GPU Inference Time (ms)[Standard Mode / High Performance Mode] CPU Inference Time (ms)[Standard Mode / High Performance Mode] Model Size (M) Description PP-OCRv4_server_rec_docInference Model/Training Model 86.58 6.65 / 2.38 32.92 / 32.92 91 M PP-OCRv4_server_rec_doc is built upon PP-OCRv4_server_rec and trained on mixed data including more Chinese document data and PP-OCR training data. It enhances recognition of traditional Chinese characters, Japanese, and special symbols, supporting 15,000+ characters. It improves both document-specific and general text recognition capabilities. PP-OCRv4_mobile_recInference Model/Training Model 83.28 4.82 / 1.20 16.74 / 4.64 11 M Lightweight recognition model of PP-OCRv4 with high inference efficiency, deployable on various hardware devices including edge devices PP-OCRv4_server_rec Inference Model/Training Model 85.19  6.58 / 2.43 33.17 / 33.17 87 M Server-side model of PP-OCRv4 with high inference accuracy, deployable on various server platforms PP-OCRv3_mobile_recInference Model/Training Model 75.43 5.87 / 1.19 9.07 / 4.28 11 M Lightweight recognition model of PP-OCRv3 with high inference efficiency, deployable on various hardware devices including edge devices ModelDownload Link Recognition Avg Accuracy(%) GPU Inference Time (ms)[Standard Mode / High Performance Mode] CPU Inference Time (ms)[Standard Mode / High Performance Mode] Model Size (M) Description ch_SVTRv2_recInference Model/Training Model 68.81 8.08 / 2.74 50.17 / 42.50 73.9 M  SVTRv2 is a server-side text recognition model developed by the OpenOCR team from Fudan University Vision and Learning Lab (FVL). It won first prize in the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition, improving end-to-end recognition accuracy by 6% compared to PP-OCRv4 on List A.  ModelDownload Link Recognition Avg Accuracy(%) GPU Inference Time (ms)[Standard Mode / High Performance Mode] CPU Inference Time (ms)[Standard Mode / High Performance Mode] Model Size (M) Description ch_RepSVTR_recInference Model/Training Model 65.07 5.93 / 1.62 20.73 / 7.32 22.1 M RepSVTR is a mobile text recognition model based on SVTRv2. It won first prize in the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition, improving end-to-end recognition accuracy by 2.5% compared to PP-OCRv4 on List B while maintaining comparable inference speed.   *  English Recognition Models ModelDownload Link Recognition Avg Accuracy(%) GPU Inference Time (ms)[Standard Mode / High Performance Mode] CPU Inference Time (ms)[Standard Mode / High Performance Mode] Model Size (M) Description en_PP-OCRv4_mobile_recInference Model/Training Model  70.39 4.81 / 0.75 16.10 / 5.31 6.8 M Ultra-lightweight English recognition model based on PP-OCRv4, supporting English and digit recognition en_PP-OCRv3_mobile_recInference Model/Training Model 70.69 5.44 / 0.75 8.65 / 5.57 7.8 M  Ultra-lightweight English recognition model based on PP-OCRv3, supporting English and digit recognition    * Multilingual Recognition Models ModelModel Download Link Recognition Avg Accuracy(%) GPU Inference Time (ms)[Standard Mode / High Performance Mode] CPU Inference Time (ms)[Standard Mode / High Performance Mode] Model Size (M) Description korean_PP-OCRv3_mobile_recInference Model/Training Model 60.21 5.40 / 0.97 9.11 / 4.05 8.6 M Ultra-lightweight Korean recognition model based on PP-OCRv3, supporting Korean and numeric recognition japan_PP-OCRv3_mobile_recInference Model/Training Model 45.69 5.70 / 1.02 8.48 / 4.07 8.8 M  Ultra-lightweight Japanese recognition model based on PP-OCRv3, supporting Japanese and numeric recognition chinese_cht_PP-OCRv3_mobile_recInference Model/Training Model 82.06 5.90 / 1.28 9.28 / 4.34 9.7 M  Ultra-lightweight Traditional Chinese recognition model based on PP-OCRv3, supporting Traditional Chinese and numeric recognition te_PP-OCRv3_mobile_recInference Model/Training Model 95.88 5.42 / 0.82 8.10 / 6.91 7.8 M  Ultra-lightweight Telugu recognition model based on PP-OCRv3, supporting Telugu and numeric recognition ka_PP-OCRv3_mobile_recInference Model/Training Model 96.96 5.25 / 0.79 9.09 / 3.86 8.0 M  Ultra-lightweight Kannada recognition model based on PP-OCRv3, supporting Kannada and numeric recognition ta_PP-OCRv3_mobile_recInference Model/Training Model 76.83 5.23 / 0.75 10.13 / 4.30 8.0 M  Ultra-lightweight Tamil recognition model based on PP-OCRv3, supporting Tamil and numeric recognition latin_PP-OCRv3_mobile_recInference Model/Training Model 76.93 5.20 / 0.79 8.83 / 7.15 7.8 M Ultra-lightweight Latin recognition model based on PP-OCRv3, supporting Latin and numeric recognition arabic_PP-OCRv3_mobile_recInference Model/Training Model 73.55 5.35 / 0.79 8.80 / 4.56 7.8 M Ultra-lightweight Arabic recognition model based on PP-OCRv3, supporting Arabic and numeric recognition cyrillic_PP-OCRv3_mobile_recInference Model/Training Model 94.28 5.23 / 0.76 8.89 / 3.88 7.9 M   Ultra-lightweight Cyrillic recognition model based on PP-OCRv3, supporting Cyrillic and numeric recognition devanagari_PP-OCRv3_mobile_recInference Model/Training Model 96.44 5.22 / 0.79 8.56 / 4.06 7.9 M Ultra-lightweight Devanagari recognition model based on PP-OCRv3, supporting Devanagari and numeric recognition Test Environment Details: <ul> <li>Performance Test Environment <ul> <li>Test Datasets:               <ul> <li>Document Image Orientation Classification Model: PaddleX in-house dataset covering ID cards and documents, with 1,000 images.</li> <li>Text Image Correction Model: DocUNet.</li> <li>Text Detection Model: PaddleOCR in-house Chinese dataset covering street views, web images, documents, and handwriting, with 500 images for detection.</li> <li>Chinese Recognition Model: PaddleOCR in-house Chinese dataset covering street views, web images, documents, and handwriting, with 11,000 images for recognition.</li> <li>ch_SVTRv2_rec: PaddleOCR Algorithm Challenge - Task 1: OCR End-to-End Recognition A-set evaluation data.</li> <li>ch_RepSVTR_rec: PaddleOCR Algorithm Challenge - Task 1: OCR End-to-End Recognition B-set evaluation data.</li> <li>English Recognition Model: PaddleX in-house English dataset.</li> <li>Multilingual Recognition Model: PaddleX in-house multilingual dataset.</li> <li>Text Line Orientation Classification Model: PaddleX in-house dataset covering ID cards and documents, with 1,000 images.</li> </ul> </li> <li>Hardware Configuration: <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environment: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Description</li> </ul> Mode GPU Configuration CPU Configuration Acceleration Techniques Standard Mode FP32 Precision / No TRT Acceleration FP32 Precision / 8 Threads PaddleInference High-Performance Mode Optimal combination of precision types and acceleration strategies FP32 Precision / 8 Threads Optimal backend selection (Paddle/OpenVINO/TRT, etc.) <p> If you prioritize model accuracy, choose models with higher accuracy; if inference speed is critical, select faster models; if model size matters, opt for smaller models.</p>"},{"location":"en/version3.x/pipeline_usage/OCR.html#2-quick-start","title":"2. Quick Start","text":"<p>Before using the general OCR pipeline locally, ensure you have installed the wheel package by following the Installation Guide. Once installed, you can experience OCR via the command line or Python integration.  </p>"},{"location":"en/version3.x/pipeline_usage/OCR.html#21-command-line","title":"2.1 Command Line","text":"<p>Run a single command to quickly test the OCR pipeline.  Before running the code below, please download the example image locally:  </p> <pre><code># Default: Uses PP-OCRv5 model  \npaddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png \\\n    --use_doc_orientation_classify False \\\n    --use_doc_unwarping False \\\n    --use_textline_orientation False \\\n    --save_path ./output \\\n    --device gpu:0 \n\n# Use PP-OCRv4 model by --ocr_version PP-OCRv4\npaddleocr ocr -i ./general_ocr_002.png --ocr_version PP-OCRv4\n</code></pre> Command line supports more parameter settings. Click to expand for detailed instructions on command line parameters. Parameter Parameter Description Parameter Type Default Value <code>input</code> Data to be predicted, required. Local path of an image file or PDF file: <code>/root/data/img.jpg</code>; URL link, such as the network URL of an image file or PDF file: Example; Local directory, which must contain images to be predicted, such as the local path: <code>/root/data/</code> (currently, predicting PDFs in a directory is not supported; PDFs need to specify the exact file path).  <code>str</code> <code>save_path</code> Path to save inference result files. If not set, inference results will not be saved locally. <code>str</code> <code>doc_orientation_classify_model_name</code> Name of the document orientation classification model. If not set, the pipeline default model will be used. <code>str</code> <code>doc_orientation_classify_model_dir</code> Directory path of the document orientation classification model. If not set, the official model will be downloaded. <code>str</code> <code>doc_unwarping_model_name</code> Name of the text image unwarping model. If not set, the pipeline default model will be used. <code>str</code> <code>doc_unwarping_model_dir</code> Directory path of the text image unwarping model. If not set, the official model will be downloaded. <code>str</code> <code>text_detection_model_name</code> Name of the text detection model. If not set, the pipeline default model will be used. <code>str</code> <code>text_detection_model_dir</code> Directory path of the text detection model. If not set, the official model will be downloaded. <code>str</code> <code>textline_orientation_model_name</code> Name of the text line orientation model. If not set, the pipeline default model will be used. <code>str</code> <code>textline_orientation_model_dir</code> Directory path of the text line orientation model. If not set, the official model will be downloaded. <code>str</code> <code>textline_orientation_batch_size</code> Batch size for the text line orientation model. If not set, the default batch size will be <code>1</code>. <code>int</code> <code>text_recognition_model_name</code> Name of the text recognition model. If not set, the pipeline default model will be used. <code>str</code> <code>text_recognition_model_dir</code> Directory path of the text recognition model. If not set, the official model will be downloaded. <code>str</code> <code>text_recognition_batch_size</code> Batch size for the text recognition model. If not set, the default batch size will be <code>1</code>. <code>int</code> <code>use_doc_orientation_classify</code> Whether to load and use the document orientation classification module. If not set, the pipeline's initialized value for this parameter (initialized to <code>True</code>) will be used. <code>bool</code> <code>use_doc_unwarping</code> Whether to load and use the text image unwarping module. If not set, the pipeline's initialized value for this parameter (initialized to <code>True</code>) will be used. <code>bool</code> <code>use_textline_orientation</code> Whether to load and use the text line orientation module. If not set, the pipeline's initialized value for this parameter (initialized to <code>True</code>) will be used. <code>bool</code> <code>text_det_limit_side_len</code> Image side length limitation for text detection. Any integer greater than <code>0</code>. If not set, the pipeline's initialized value for this parameter (initialized to <code>64</code>) will be used.  <code>int</code> <code>text_det_limit_type</code> Type of side length limit for text detection. Supports <code>min</code> and <code>max</code>. <code>min</code> means ensuring the shortest side of the image is not smaller than <code>det_limit_side_len</code>, and <code>max</code> means ensuring the longest side of the image is not larger than <code>limit_side_len</code>. If not set, the pipeline's initialized value for this parameter (initialized to <code>min</code>) will be used.  <code>str</code> <code>text_det_thresh</code> Pixel threshold for text detection. In the output probability map, pixels with scores higher than this threshold will be considered text pixels.Any floating-point number greater than <code>0</code>. If not set, the pipeline's initialized value for this parameter (<code>0.3</code>) will be used.  <code>float</code> <code>text_det_box_thresh</code> Text detection box threshold. If the average score of all pixels within the detected result boundary is higher than this threshold, the result will be considered a text region. Any floating-point number greater than <code>0</code>. If not set, the pipeline's initialized value for this parameter (<code>0.6</code>) will be used.  <code>float</code> <code>text_det_unclip_ratio</code> Text detection expansion coefficient. This method is used to expand the text region\u2014the larger the value, the larger the expanded area. Any floating-point number greater than <code>0</code>. If not set, the pipeline's initialized value for this parameter (<code>2.0</code>) will be used.  <code>float</code> <code>text_det_input_shape</code> Input shape for text detection, you can set three values to represent C, H, and W. <code>int</code> <code>text_rec_score_thresh</code> Text recognition threshold. Text results with scores higher than this threshold will be retained.Any floating-point number greater than <code>0</code> . If not set, the pipeline's initialized value for this parameter (<code>0.0</code>, i.e., no threshold) will be used.  <code>float</code> <code>text_rec_input_shape</code> Input shape for text recognition. <code>tuple</code> <code>lang</code> OCR model language to use. The table in the appendix lists all the supported languages.  <code>str</code> <code>ocr_version</code> Version of OCR models. <ul> <li>PP-OCRv5: Use PP-OCRv5 series models; <li>PP-OCRv4: Use PP-OCRv4 series models; <li>PP-OCRv3: Use PP-OCRv3 series models.</li>  Please note that not every <code>ocr_version</code> supports all <code>lang</code> options. Please refer to the correspondence table in the appendix for details.  <code>str</code> <code>det_model_dir</code> Deprecated. Please refer <code>text_detection_model_dir</code> , they cannot be specified simultaneously with the new parameters. <code>str</code> <code>det_limit_side_len</code> Deprecated. Please refer <code>text_det_limit_side_len</code> , they cannot be specified simultaneously with the new parameters. <code>int</code> <code>det_limit_type</code> Deprecated. Please refer <code>text_det_limit_type</code> , they cannot be specified simultaneously with the new parameters.  <code>str</code> <code>det_db_thresh</code> Deprecated. Please refer <code>text_det_thresh</code> , they cannot be specified simultaneously with the new parameters.  <code>float</code> <code>det_db_box_thresh</code> Deprecated. Please refer <code>text_det_box_thresh</code> , they cannot be specified simultaneously with the new parameters.  <code>float</code> <code>det_db_unclip_ratio</code> Deprecated. Please refer <code>text_det_unclip_ratio</code> , they cannot be specified simultaneously with the new parameters.  <code>float</code> <code>rec_model_dir</code> Deprecated. Please refer <code>text_recognition_model_dir</code> , they cannot be specified simultaneously with the new parameters. <code>str</code> <code>rec_batch_num</code> Deprecated. Please refer <code>text_recognition_batch_size</code> , they cannot be specified simultaneously with the new parameters. <code>int</code> <code>use_angle_cls</code> Deprecated. Please refer <code>use_textline_orientation</code> , they cannot be specified simultaneously with the new parameters. <code>bool</code> <code>cls_model_dir</code> Deprecated. Please refer <code>textline_orientation_model_dir</code> , they cannot be specified simultaneously with the new parameters. <code>str</code> <code>cls_batch_num</code> Deprecated. Please refer <code>textline_orientation_batch_size</code> , they cannot be specified simultaneously with the new parameters. <code>int</code> <code>device</code> Device for inference. Supports specifying a specific card number: <ul> <li>CPU: <code>cpu</code> indicates using CPU for inference;</li> <li>GPU: <code>gpu:0</code> indicates using the 1st GPU for inference;</li> <li>NPU: <code>npu:0</code> indicates using the 1st NPU for inference;</li> <li>XPU: <code>xpu:0</code> indicates using the 1st XPU for inference;</li> <li>MLU: <code>mlu:0</code> indicates using the 1st MLU for inference;</li> <li>DCU: <code>dcu:0</code> indicates using the 1st DCU for inference;</li> </ul>If not set, the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.  <code>str</code> <code>enable_hpi</code> Whether to enable high-performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Computational precision, such as fp32, fp16. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads used for inference on CPU. <code>int</code> <code>8</code> <code>paddlex_config</code> Path to the PaddleX pipeline configuration file. <code>str</code> <p></p> <p>Results are printed to the terminal:  </p> <pre><code>{'res': {'input_path': './general_ocr_002.png', 'page_index': None, 'model_settings': {'use_doc_preprocessor': True, 'use_textline_orientation': False}, 'doc_preprocessor_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_orientation_classify': False, 'use_doc_unwarping': False}, 'angle': -1}, 'dt_polys': array([[[  3,  10],\n        ...,\n        [  4,  30]],\n\n       ...,\n\n       [[ 99, 456],\n        ...,\n        [ 99, 479]]], dtype=int16), 'text_det_params': {'limit_side_len': 736, 'limit_type': 'min', 'thresh': 0.3, 'max_side_limit': 4000, 'box_thresh': 0.6, 'unclip_ratio': 1.5}, 'text_type': 'general', 'textline_orientation_angles': array([-1, ..., -1]), 'text_rec_score_thresh': 0.0, 'rec_texts': ['www.997700', '', 'Cm', '\u200b\u767b\u673a\u724c\u200b', 'BOARDING', 'PASS', 'CLASS', '\u200b\u5e8f\u53f7\u200bSERIAL NO.', '\u200b\u5ea7\u4f4d\u53f7\u200b', 'SEAT NO.', '\u200b\u822a\u73ed\u200bFLIGHT', '\u200b\u65e5\u671f\u200bDATE', '\u200b\u8231\u4f4d\u200b', '', 'W', '035', '12F', 'MU2379', '03DEc', '\u200b\u59cb\u53d1\u5730\u200b', 'FROM', '\u200b\u767b\u673a\u53e3\u200b', 'GATE', '\u200b\u767b\u673a\u200b\u65f6\u95f4\u200bBDT', '\u200b\u76ee\u7684\u5730\u200bTO', '\u200b\u798f\u5dde\u200b', 'TAIYUAN', 'G11', 'FUZHOU', '\u200b\u8eab\u4efd\u200b\u8bc6\u522b\u200bIDNO.', '\u200b\u59d3\u540d\u200bNAME', 'ZHANGQIWEI', '\u200b\u7968\u53f7\u200bTKT NO.', '\u200b\u5f20\u797a\u4f1f\u200b', '\u200b\u7968\u4ef7\u200bFARE', 'ETKT7813699238489/1', '\u200b\u767b\u673a\u53e3\u200b\u4e8e\u200b\u8d77\u98de\u524d\u200b10\u200b\u5206\u949f\u200b\u5173\u95ed\u200b GATESCL0SE10MINUTESBEFOREDEPARTURETIME'], 'rec_scores': array([0.67634439, ..., 0.97416091]), 'rec_polys': array([[[  3,  10],\n        ...,\n        [  4,  30]],\n\n       ...,\n\n       [[ 99, 456],\n        ...,\n        [ 99, 479]]], dtype=int16), 'rec_boxes': array([[  3, ...,  30],\n       ...,\n       [ 99, ..., 479]], dtype=int16)}}\n</code></pre> <p>If <code>save_path</code> is specified, the visualization results will be saved under <code>save_path</code>. The visualization output is shown below:</p> <p></p>"},{"location":"en/version3.x/pipeline_usage/OCR.html#22-python-script-integration","title":"2.2 Python Script Integration","text":"<p>The command-line method is for quick testing. For project integration, you can achieve OCR inference with just a few lines of code:  </p> <pre><code>from paddleocr import PaddleOCR  \n\nocr = PaddleOCR(\n    use_doc_orientation_classify=False, # Disables document orientation classification model via this parameter\n    use_doc_unwarping=False, # Disables text image rectification model via this parameter\n    use_textline_orientation=False, # Disables text line orientation classification model via this parameter\n)\n# ocr = PaddleOCR(lang=\"en\") # Uses English model by specifying language parameter\n# ocr = PaddleOCR(ocr_version=\"PP-OCRv4\") # Uses other PP-OCR versions via version parameter\n# ocr = PaddleOCR(device=\"gpu\") # Enables GPU acceleration for model inference via device parameter\n# ocr = PaddleOCR(\n#     text_detection_model_name=\"PP-OCRv5_mobile_det\",\n#     text_recognition_model_name=\"PP-OCRv5_mobile_rec\",\n#     use_doc_orientation_classify=False,\n#     use_doc_unwarping=False,\n#     use_textline_orientation=False,\n# ) # Switch to PP-OCRv5_mobile models\nresult = ocr.predict(\"./general_ocr_002.png\")  \nfor res in result:  \n    res.print()  \n    res.save_to_img(\"output\")  \n    res.save_to_json(\"output\")  \n</code></pre> <p>In the above Python script, the following steps are performed:</p> (1) Instantiate the OCR pipeline object via <code>PaddleOCR()</code>, with specific parameter descriptions as follows: Parameter Parameter Description Parameter Type Default Value <code>doc_orientation_classify_model_name</code> Name of the document orientation classification model. If set to <code>None</code>, the pipeline's default model will be used. <code>str</code> <code>None</code> <code>doc_orientation_classify_model_dir</code> Directory path of the document orientation classification model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>doc_unwarping_model_name</code> Name of the text image unwarping model. If set to <code>None</code>, the pipeline's default model will be used. <code>str</code> <code>None</code> <code>doc_unwarping_model_dir</code> Directory path of the text image unwarping model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>text_detection_model_name</code> Name of the text detection model. If set to <code>None</code>, the pipeline's default model will be used. <code>str</code> <code>None</code> <code>text_detection_model_dir</code> Directory path of the text detection model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>textline_orientation_model_name</code> Name of the text line orientation model. If set to <code>None</code>, the pipeline's default model will be used. <code>str</code> <code>None</code> <code>textline_orientation_model_dir</code> Directory path of the text line orientation model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>textline_orientation_batch_size</code> Batch size for the text line orientation model. If set to <code>None</code>, the default batch size will be <code>1</code>. <code>int</code> <code>None</code> <code>text_recognition_model_name</code> Name of the text recognition model. If set to <code>None</code>, the pipeline's default model will be used. <code>str</code> <code>None</code> <code>text_recognition_model_dir</code> Directory path of the text recognition model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>text_recognition_batch_size</code> Batch size for the text recognition model. If set to <code>None</code>, the default batch size will be <code>1</code>. <code>int</code> <code>None</code> <code>use_doc_orientation_classify</code> Whether to load and use the document orientation classification module. If set to <code>None</code>, the pipeline's initialized value for this parameter (initialized to <code>True</code>) will be used. <code>bool</code> <code>None</code> <code>use_doc_unwarping</code> Whether to load and use the text image unwarping module. If set to <code>None</code>, the pipeline's initialized value for this parameter (initialized to <code>True</code>) will be used. <code>bool</code> <code>None</code> <code>use_textline_orientation</code> Whether to load and use the text line orientation module. If set to <code>None</code>, the pipeline's initialized value for this parameter (initialized to <code>True</code>) will be used. <code>bool</code> <code>None</code> <code>text_det_limit_side_len</code> Image side length limitation for text detection.  <ul> <li>int: Any integer greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the pipeline's initialized value for this parameter (initialized to <code>64</code>) will be used.</li> </ul> <code>int</code> <code>None</code> <code>text_det_limit_type</code> Type of side length limit for text detection.  <ul> <li>str: Supports <code>min</code> and <code>max</code>, where <code>min</code> means ensuring the shortest side of the image is not smaller than <code>det_limit_side_len</code>, and <code>max</code> means ensuring the longest side of the image is not larger than <code>limit_side_len</code>;</li> <li>None: If set to <code>None</code>, the pipeline's initialized value for this parameter (initialized to <code>min</code>) will be used.</li> </ul> <code>str</code> <code>None</code> <code>text_det_thresh</code> Pixel threshold for text detection. Pixels with scores higher than this threshold in the output probability map will be considered text pixels.  <ul> <li>float: Any floating-point number greater than <code>0</code>; <li>None: If set to <code>None</code>, the pipeline's initialized value for this parameter (<code>0.3</code>) will be used.</li></li></ul> <code>float</code> <code>None</code> <code>text_det_box_thresh</code> Box threshold for text detection. A detection result will be considered a text region if the average score of all pixels within the bounding box is higher than this threshold.  <ul> <li>float: Any floating-point number greater than <code>0</code>;  <li>None: If set to <code>None</code>, the pipeline's initialized value for this parameter (<code>0.6</code>) will be used.</li></li></ul> <code>float</code> <code>None</code> <code>text_det_unclip_ratio</code> Dilation coefficient for text detection. This method is used to dilate the text region, and the larger this value, the larger the dilated area.  <ul> <li>float: Any floating-point number greater than <code>0</code>; <li>None: If set to <code>None</code>, the pipeline's initialized value for this parameter (<code>2.0</code>) will be used.</li></li></ul> <code>float</code> <code>None</code> <code>text_det_input_shape</code> Input shape for text detection. <code>tuple</code> <code>None</code> <code>text_rec_score_thresh</code> Recognition score threshold for text. Text results with scores higher than this threshold will be retained.  <ul> <li>float: Any floating-point number greater than <code>0</code>;     <li>None: If set to <code>None</code>, the pipeline's initialized value for this parameter (<code>0.0</code>, i.e., no threshold) will be used.</li></li></ul> <code>float</code> <code>None</code> <code>text_rec_input_shape</code> Input shape for text recognition. <code>tuple</code> <code>None</code> <code>lang</code> OCR model language to use. The table in the appendix lists all the supported languages.  <code>str</code> <code>None</code> <code>ocr_version</code> Version of OCR models. <ul> <li>PP-OCRv5: Use PP-OCRv5 series models;</li> <li>PP-OCRv4: Use PP-OCRv4 series models;</li> <li>PP-OCRv3: Use PP-OCRv3 series models.</li> </ul> Please note that not every <code>ocr_version</code> supports all <code>lang</code> options. Please refer to the correspondence table in the appendix for details.  <code>str</code> <code>None</code> <code>device</code> Device for inference. Supports specifying a specific card number: <ul> <li>CPU: e.g., <code>cpu</code> for CPU inference;</li> <li>GPU: e.g., <code>gpu:0</code> for inference on the 1st GPU;</li> <li>NPU: e.g., <code>npu:0</code> for inference on the 1st NPU;</li> <li>XPU: e.g., <code>xpu:0</code> for inference on the 1st XPU;</li> <li>MLU: e.g., <code>mlu:0</code> for inference on the 1st MLU;</li> <li>DCU: e.g., <code>dcu:0</code> for inference on the 1st DCU;</li> <li>None: If set to <code>None</code>, the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.</li> </ul> <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to enable high-performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Computational precision, such as fp32, fp16. <code>str</code> <code>\"fp32\"</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set. <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads used for CPU inference. <code>int</code> <code>8</code> <code>paddlex_config</code> Path to the PaddleX pipeline configuration file. <code>str</code> <code>None</code> (2) Invoke the <code>predict()</code> method of the OCR pipeline object for inference prediction, which returns a results list. Additionally, the pipeline provides the <code>predict_iter()</code> method. Both methods are completely consistent in parameter acceptance and result return, except that <code>predict_iter()</code> returns a <code>generator</code>, which can process and obtain prediction results incrementally, suitable for handling large datasets or scenarios where memory saving is desired. You can choose to use either of these two methods according to actual needs. The following are the parameters and descriptions of the <code>predict()</code> method: Parameter Parameter Description Parameter Type Default Value <code>input</code> Data to be predicted, supporting multiple input types, required. <ul> <li>Python Var: Image data represented by <code>numpy.ndarray</code>;</li> <li>str: Local path of an image file or PDF file: <code>/root/data/img.jpg</code>; URL link, such as the network URL of an image file or PDF file: example; local directory, which needs to contain images to be predicted, such as the local path: <code>/root/data/</code> (currently, predicting PDF files in the directory is not supported; PDF files need to specify the specific file path);</li> <li>List: List elements must be of the above types, such as <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code>.</li> </ul> <code>Python Var|str|list</code> <code>use_doc_orientation_classify</code> Whether to use the document orientation classification module during inference. <code>bool</code> <code>None</code> <code>use_doc_unwarping</code> Whether to use the text image unwarping module during inference. <code>bool</code> <code>None</code> <code>use_textline_orientation</code> Whether to use the text line orientation classification module during inference. <code>bool</code> <code>None</code> <code>text_det_limit_side_len</code> The same as the parameter during instantiation. <code>int</code> <code>None</code> <code>text_det_limit_type</code> The same as the parameter during instantiation. <code>str</code> <code>None</code> <code>text_det_thresh</code> The same as the parameter during instantiation. <code>float</code> <code>None</code> <code>text_det_box_thresh</code> The same as the parameter during instantiation. <code>float</code> <code>None</code> <code>text_det_unclip_ratio</code> The same as the parameter during instantiation. <code>float</code> <code>None</code> <code>text_rec_score_thresh</code> The same as the parameter during instantiation. <code>float</code> <code>None</code> (3) Process the prediction results. The prediction result of each sample is a corresponding Result object, which supports operations of printing, saving as an image, and saving as a <code>json</code> file: Method Method Description Parameter Parameter Type Parameter Description Default Value <code>print()</code> Print the results to the terminal <code>format_json</code> <code>bool</code> Whether to format the output content with <code>JSON</code> indentation. <code>True</code> <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data and make it more readable, only valid when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters as <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> retains the original characters, only valid when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_json()</code> Save the results as a json-formatted file. <code>save_path</code> <code>str</code> File path to save. When it is a directory, the saved file name will be consistent with the input file type name. No default <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data and make it more readable, only valid when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters as <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> retains the original characters, only valid when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_img()</code> Save the results as an image-formatted file <code>save_path</code> <code>str</code> File path to save, supporting directory or file path. No default <ul> <li>Calling the <code>print()</code> method will print the results to the terminal. The content printed to the terminal is explained as follows:         <ul> <li><code>input_path</code>: <code>(str)</code> Input path of the image to be predicted</li> <li><code>page_index</code>: <code>(Union[int, None])</code> If the input is a PDF file, it indicates which page of the PDF it is; otherwise, it is <code>None</code></li> <li><code>model_settings</code>: <code>(Dict[str, bool])</code> Model parameters configured for the pipeline                 <ul> <li><code>use_doc_preprocessor</code>: <code>(bool)</code> Control whether to enable the document preprocessing sub-pipeline</li> <li><code>use_textline_orientation</code>: <code>(bool)</code> Control whether to enable the text line orientation classification function</li> </ul> </li> <li><code>doc_preprocessor_res</code>: <code>(Dict[str, Union[str, Dict[str, bool], int]])</code> Output results of the document preprocessing sub-pipeline. Only exists when <code>use_doc_preprocessor=True</code> <ul> <li><code>input_path</code>: <code>(Union[str, None])</code> Image path accepted by the image preprocessing sub-pipeline. When the input is <code>numpy.ndarray</code>, it is saved as <code>None</code></li> <li><code>model_settings</code>: <code>(Dict)</code> Model configuration parameters of the preprocessing sub-pipeline                         <ul> <li><code>use_doc_orientation_classify</code>: <code>(bool)</code> Control whether to enable document orientation classification</li> <li><code>use_doc_unwarping</code>: <code>(bool)</code> Control whether to enable text image unwarping</li> </ul> </li> <li><code>angle</code>: <code>(int)</code> Prediction result of document orientation classification. When enabled, the values are [0,1,2,3], corresponding to [0\u00b0,90\u00b0,180\u00b0,270\u00b0]; when disabled, it is -1</li> </ul> </li> <li><code>dt_polys</code>: <code>(List[numpy.ndarray])</code> List of text detection polygon boxes. Each detection box is represented by a numpy array of 4 vertex coordinates, with the array shape being (4, 2) and the data type being int16</li> <li><code>dt_scores</code>: <code>(List[float])</code> List of confidence scores for text detection boxes</li> <li><code>text_det_params</code>: <code>(Dict[str, Dict[str, int, float]])</code> Configuration parameters for the text detection module                 <ul> <li><code>limit_side_len</code>: <code>(int)</code> Side length limit value during image preprocessing</li> <li><code>limit_type</code>: <code>(str)</code> Processing method for side length limits</li> <li><code>thresh</code>: <code>(float)</code> Confidence threshold for text pixel classification</li> <li><code>box_thresh</code>: <code>(float)</code> Confidence threshold for text detection boxes</li> <li><code>unclip_ratio</code>: <code>(float)</code> Dilation coefficient for text detection boxes</li> <li><code>text_type</code>: <code>(str)</code> Type of text detection, currently fixed as \"general\"</li> </ul> </li> <li><code>textline_orientation_angles</code>: <code>(List[int])</code> Prediction results of text line orientation classification. When enabled, actual angle values are returned (e.g., [0,0,1]); when disabled, [-1,-1,-1] is returned</li> <li><code>text_rec_score_thresh</code>: <code>(float)</code> Filtering threshold for text recognition results</li> <li><code>rec_texts</code>: <code>(List[str])</code> List of text recognition results, containing only texts with confidence scores exceeding <code>text_rec_score_thresh</code></li> <li><code>rec_scores</code>: <code>(List[float])</code> List of text recognition confidence scores, filtered by <code>text_rec_score_thresh</code></li> <li><code>rec_polys</code>: <code>(List[numpy.ndarray])</code> List of text detection boxes filtered by confidence, in the same format as <code>dt_polys</code></li> <li><code>rec_boxes</code>: <code>(numpy.ndarray)</code> Array of rectangular bounding boxes for detection boxes, with shape (n, 4) and dtype int16. Each row represents the [x_min, y_min, x_max, y_max] coordinates of a rectangular box, where (x_min, y_min) is the top-left coordinate and (x_max, y_max) is the bottom-right coordinate</li> </ul> </li> <li>Calling the <code>save_to_json()</code> method will save the above content to the specified <code>save_path</code>. If a directory is specified, the save path will be <code>save_path/{your_img_basename}_res.json</code>. If a file is specified, it will be saved directly to that file. Since json files do not support saving numpy arrays, <code>numpy.array</code> types will be converted to list form.</li> <li>Calling the <code>save_to_img()</code> method will save the visualization results to the specified <code>save_path</code>. If a directory is specified, the save path will be <code>save_path/{your_img_basename}_ocr_res_img.{your_img_extension}</code>. If a file is specified, it will be saved directly to that file. (The pipeline usually generates many result images, so it is not recommended to directly specify a specific file path, as multiple images will be overwritten, leaving only the last one.)</li> </ul> <p>Additionally, you can also obtain the visualized image with results and prediction results through attributes, as follows:</p> Attribute Attribute Description <code>json</code> Get the prediction results in <code>json</code> format <code>img</code> Get the visualized image in <code>dict</code> format <ul> <li>The prediction results obtained by the <code>json</code> attribute are in dict format, and the content is consistent with that saved by calling the <code>save_to_json()</code> method.</li> <li>The <code>img</code> attribute returns a dictionary-type result. The keys are <code>ocr_res_img</code> and <code>preprocessed_img</code>, with corresponding values being two <code>Image.Image</code> objects: one for displaying the visualized image of OCR results and the other for displaying the visualized image of image preprocessing. If the image preprocessing submodule is not used, only <code>ocr_res_img</code> will be included in the dictionary.</li> </ul>"},{"location":"en/version3.x/pipeline_usage/OCR.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the general OCR pipeline meets your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <p>If you need to apply the general OCR pipeline directly in your Python project, you can refer to the sample code in 2.2 Python Script Integration.</p> <p>Additionally, PaddleOCR provides two other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In real-world production environments, many applications have stringent performance requirements (especially for response speed) to ensure system efficiency and smooth user experience. To address this, PaddleOCR offers high-performance inference capabilities, which deeply optimize model inference and pre/post-processing to achieve significant end-to-end speed improvements. For detailed high-performance inference workflows, refer to the High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service Deployment: Service deployment is a common form of deployment in production environments. By encapsulating inference functionality as a service, clients can access these services via network requests to obtain inference results. For detailed pipeline service deployment workflows, refer to the Service Deployment Guide.</p> <p>Below are the API reference for basic service deployment and examples of multi-language service calls:</p> API Reference <p>For the main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>Both the request body and response body are JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body has the following attributes:</li> </ul> Name Type Description <code>logId</code> <code>string</code> UUID of the request. <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error message. Fixed as <code>\"Success\"</code>. <code>result</code> <code>object</code> Operation result. <ul> <li>When the request fails, the response body has the following attributes:</li> </ul> Name Type Description <code>logId</code> <code>string</code> UUID of the request. <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error message. <p>The main operations provided by the service are as follows:</p> <ul> <li><code>infer</code></li> </ul> <p>Obtain OCR results for an image.</p> <p><code>POST /ocr</code></p> <ul> <li>The request body has the following attributes:</li> </ul> Name Type Description Required <code>file</code> <code>string</code> A server-accessible URL to an image or PDF file, or the Base64-encoded content of such a file. By default, for PDF files with more than 10 pages, only the first 10 pages are processed. To remove the page limit, add the following configuration to the pipeline config file: <pre><code>Serving:\n  extra:\n    max_num_input_imgs: null\n</code></pre> Yes <code>fileType</code> <code>integer</code> | <code>null</code> File type. <code>0</code> for PDF, <code>1</code> for image. If omitted, the type is inferred from the URL. No <code>useDocOrientationClassify</code> <code>boolean</code> | <code>null</code> Refer to the <code>use_doc_orientation_classify</code> parameter in the pipeline object's <code>predict</code> method. No <code>useDocUnwarping</code> <code>boolean</code> | <code>null</code> Refer to the <code>use_doc_unwarping</code> parameter in the pipeline object's <code>predict</code> method. No <code>useTextlineOrientation</code> <code>boolean</code> | <code>null</code> Refer to the <code>use_textline_orientation</code> parameter in the pipeline object's <code>predict</code> method. No <code>textDetLimitSideLen</code> <code>integer</code> | <code>null</code> Refer to the <code>text_det_limit_side_len</code> parameter in the pipeline object's <code>predict</code> method. No <code>textDetLimitType</code> <code>string</code> | <code>null</code> Refer to the <code>text_det_limit_type</code> parameter in the pipeline object's <code>predict</code> method. No <code>textDetThresh</code> <code>number</code> | <code>null</code> Refer to the <code>text_det_thresh</code> parameter in the pipeline object's <code>predict</code> method. No <code>textDetBoxThresh</code> <code>number</code> | <code>null</code> Refer to the <code>text_det_box_thresh</code> parameter in the pipeline object's <code>predict</code> method. No <code>textDetUnclipRatio</code> <code>number</code> | <code>null</code> Refer to the <code>text_det_unclip_ratio</code> parameter in the pipeline object's <code>predict</code> method. No <code>textRecScoreThresh</code> <code>number</code> | <code>null</code> Refer to the <code>text_rec_score_thresh</code> parameter in the pipeline object's <code>predict</code> method. No <ul> <li>When the request is successful, the <code>result</code> in the response body has the following attributes:</li> </ul> Name Type Description <code>ocrResults</code> <code>object</code> OCR results. The array length is 1 (for image input) or the number of processed document pages (for PDF input). For PDF input, each element represents the result for a corresponding page. <code>dataInfo</code> <code>object</code> Input data information. <p>Each element in <code>ocrResults</code> is an <code>object</code> with the following attributes:</p> Name Type Description <code>prunedResult</code> <code>object</code> A simplified version of the <code>res</code> field in the JSON output of the pipeline object's <code>predict</code> method, excluding <code>input_path</code> and <code>page_index</code>. <code>ocrImage</code> <code>string</code> | <code>null</code> OCR result image with detected text regions highlighted. JPEG format, Base64-encoded. <code>docPreprocessingImage</code> <code>string</code> | <code>null</code> Visualization of preprocessing results. JPEG format, Base64-encoded. <code>inputImage</code> <code>string</code> | <code>null</code> Input image. JPEG format, Base64-encoded. Multi-Language Service Call Examples Python <pre><code>\nimport base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/ocr\"\nfile_path = \"./demo.jpg\"\n\nwith open(file_path, \"rb\") as file:\n    file_bytes = file.read()\n    file_data = base64.b64encode(file_bytes).decode(\"ascii\")\n\npayload = {\"file\": file_data, \"fileType\": 1}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nfor i, res in enumerate(result[\"ocrResults\"]):\n    print(res[\"prunedResult\"])\n    ocr_img_path = f\"ocr_{i}.jpg\"\n    with open(ocr_img_path, \"wb\") as f:\n        f.write(base64.b64decode(res[\"ocrImage\"]))\n    print(f\"Output image saved at {ocr_img_path}\")\n</code></pre>"},{"location":"en/version3.x/pipeline_usage/OCR.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the General OCR Pipeline do not meet your expectations in terms of accuracy or speed for your specific scenario, you can leverage your own domain-specific or application-specific data to further fine-tune the existing models, thereby improving the recognition performance of the General OCR Pipeline in your use case.</p>"},{"location":"en/version3.x/pipeline_usage/OCR.html#41-model-fine-tuning","title":"4.1 Model Fine-Tuning","text":"<p>The general OCR pipeline consists of multiple modules. If the pipeline's performance does not meet expectations, the issue may stem from any of these modules. You can analyze poorly recognized images to identify the problematic module and refer to the corresponding fine-tuning tutorials in the table below for adjustments.</p> Scenario Module to Fine-Tune Fine-Tuning Reference Inaccurate whole-image rotation correction Document orientation classification module Link Inaccurate image distortion correction Text image unwarping module Fine-tuning not supported Inaccurate textline rotation correction Textline orientation classification module Link Text detection misses Text detection module Link Incorrect text recognition Text recognition module Link"},{"location":"en/version3.x/pipeline_usage/OCR.html#42-model-deployment","title":"4.2 Model Deployment","text":"<p>After you complete fine-tuning training using a private dataset, you can obtain a local model weight file. You can then use the fine-tuned model weights by specifying the local model save path through parameters or by customizing the pipeline configuration file.</p>"},{"location":"en/version3.x/pipeline_usage/OCR.html#421-specify-the-local-model-path-through-parameters","title":"4.2.1 Specify the local model path through parameters","text":"<p>When initializing the pipeline object, specify the local model path through parameters. Take the usage of the weights after fine-tuning the text detection model as an example, as follows:</p> <p>Command line mode:</p> <pre><code># Specify the local model path via --text_detection_model_dir\npaddleocr ocr -i ./general_ocr_002.png --text_detection_model_dir your_det_model_path\n\n# PP-OCRv5_server_det model is used as the default text detection model. If you do not fine-tune this model, modify the model name by using --text_detection_model_name\npaddleocr ocr -i ./general_ocr_002.png --text_detection_model_name PP-OCRv5_mobile_det --text_detection_model_dir your_v5_mobile_det_model_path\n</code></pre> <p>Script mode: </p> <pre><code>from paddleocr import PaddleOCR\n\n#  Specify the local model path via text_detection_model_dir\npipeline = PaddleOCR(text_detection_model_dir=\"./your_det_model_path\")\n\n# PP-OCRv5_server_det model is used as the default text detection model. If you do not fine-tune this model, modify the model name by using text_detection_model_name\n# pipeline = PaddleOCR(text_detection_model_name=\"PP-OCRv5_mobile_det\", text_detection_model_dir=\"./your_v5_mobile_det_model_path\")\n</code></pre>"},{"location":"en/version3.x/pipeline_usage/OCR.html#422-specify-the-local-model-path-through-the-configuration-file","title":"4.2.2 Specify the local model path through the configuration file","text":"<p>1.Obtain the pipeline configuration file</p> <p>Call the <code>export_paddlex_config_to_yaml</code> method of the General OCR Pipeline object in PaddleOCR to export the current pipeline configuration as a YAML file:  </p> <pre><code>from paddleocr import PaddleOCR  \n\npipeline = PaddleOCR()  \npipeline.export_paddlex_config_to_yaml(\"PaddleOCR.yaml\")  \n</code></pre> <p>2.Modify the Configuration File  </p> <p>After obtaining the default pipeline configuration file, replace the paths of the default model weights with the local paths of your fine-tuned model weights. For example:  </p> <pre><code>......  \nSubModules:  \n  TextDetection:  \n    box_thresh: 0.6  \n    limit_side_len: 64  \n    limit_type: min\n    max_side_limit: 4000  \n    model_dir: null # Replace with the path to your fine-tuned text detection model weights  \n    model_name: PP-OCRv5_server_det  # If the name of the fine-tuned model is different from the default model name, please modify it here as well\n    module_name: text_detection  \n    thresh: 0.3  \n    unclip_ratio: 1.5  \n  TextLineOrientation:  \n    batch_size: 6  \n    model_dir: null  # Replace with the path to your fine-tuned text LineOrientation model weights  \n    model_name: PP-LCNet_x1_0_textline_ori  # If the name of the fine-tuned model is different from the default model name, please modify it here as well\n    module_name: textline_orientation  \n  TextRecognition:  \n    batch_size: 6  \n    model_dir: null # Replace with the path to your fine-tuned text recognition model weights  \n    model_name: PP-OCRv5_server_rec  # If the name of the fine-tuned model is different from the default model name, please modify it here as well\n    module_name: text_recognition  \n    score_thresh: 0.0  \n......  \n</code></pre> <p>The pipeline configuration file includes not only the parameters supported by the PaddleOCR CLI and Python API but also advanced configurations. For detailed instructions, refer to the PaddleX Pipeline Usage Overview and adjust the configurations as needed.  </p> <p>3.Load the Configuration File in CLI  </p> <p>After modifying the configuration file, specify its path using the <code>--paddlex_config</code> parameter in the command line. PaddleOCR will read the file and apply the configurations. Example:  </p> <pre><code>paddleocr ocr --paddlex_config PaddleOCR.yaml ...  \n</code></pre> <p>4.Load the Configuration File in Python API  </p> <p>When initializing the pipeline object, pass the path of the PaddleX pipeline configuration file or a configuration dictionary via the <code>paddlex_config</code> parameter. PaddleOCR will read and apply the configurations. Example:  </p> <pre><code>from paddleocr import PaddleOCR  \n\npipeline = PaddleOCR(paddlex_config=\"PaddleOCR.yaml\")  \n</code></pre>"},{"location":"en/version3.x/pipeline_usage/OCR.html#5-appendix","title":"5. Appendix","text":"Supported Languages <code>lang</code> Language Name <code>abq</code>Abaza <code>af</code>Afrikaans <code>ang</code>Old English <code>ar</code>Arabic <code>ava</code>Avaric <code>az</code>Azerbaijani <code>be</code>Belarusian <code>bg</code>Bulgarian <code>bgc</code>Haryanvi <code>bh</code>Bihari <code>bho</code>Bhojpuri <code>bs</code>Bosnian <code>ch</code>Chinese (Simplified) <code>che</code>Chechen <code>chinese_cht</code>Chinese (Traditional) <code>cs</code>Czech <code>cy</code>Welsh <code>da</code>Danish <code>dar</code>Dargwa <code>de</code> or <code>german</code>German <code>en</code>English <code>es</code>Spanish <code>et</code>Estonian <code>fa</code>Persian <code>fr</code> or <code>french</code>French <code>ga</code>Irish <code>gom</code>Konkani <code>hi</code>Hindi <code>hr</code>Croatian <code>hu</code>Hungarian <code>id</code>Indonesian <code>inh</code>Ingush <code>is</code>Icelandic <code>it</code>Italian <code>japan</code>Japanese <code>ka</code>Georgian <code>kbd</code>Kabardian <code>korean</code>Korean <code>ku</code>Kurdish <code>la</code>Latin <code>lbe</code>Lak <code>lez</code>Lezghian <code>lt</code>Lithuanian <code>lv</code>Latvian <code>mah</code>Magahi <code>mai</code>Maithili <code>mi</code>Maori <code>mn</code>Mongolian <code>mr</code>Marathi <code>ms</code>Malay <code>mt</code>Maltese <code>ne</code>Nepali <code>new</code>Newari <code>nl</code>Dutch <code>no</code>Norwegian <code>oc</code>Occitan <code>pi</code>Pali <code>pl</code>Polish <code>pt</code>Portuguese <code>ro</code>Romanian <code>rs_cyrillic</code>Serbian (Cyrillic) <code>rs_latin</code>Serbian (Latin) <code>ru</code>Russian <code>sa</code>Sanskrit <code>sck</code>Sadri <code>sk</code>Slovak <code>sl</code>Slovenian <code>sq</code>Albanian <code>sv</code>Swedish <code>sw</code>Swahili <code>tab</code>Tabassaran <code>ta</code>Tamil <code>te</code>Telugu <code>tl</code>Tagalog <code>tr</code>Turkish <code>ug</code>Uyghur <code>uk</code>Ukrainian <code>ur</code>Urdu <code>uz</code>Uzbek <code>vi</code>Vietnamese Correspondence Between OCR Model Versions and Supported Languages <code>ocr_version</code> Supported <code>lang</code> <code>PP-OCRv5</code> <code>ch</code>, <code>chinese_cht</code>, <code>en</code>, <code>japan</code> <code>PP-OCRv4</code> <code>ch</code>, <code>en</code> <code>PP-OCRv3</code> <code>abq</code>, <code>af</code>, <code>ady</code>, <code>ang</code>, <code>ar</code>, <code>ava</code>, <code>az</code>, <code>be</code>,         <code>bg</code>, <code>bgc</code>, <code>bh</code>, <code>bho</code>, <code>bs</code>, <code>ch</code>, <code>che</code>,         <code>chinese_cht</code>, <code>cs</code>, <code>cy</code>, <code>da</code>, <code>dar</code>, <code>de</code>, <code>german</code>,         <code>en</code>, <code>es</code>, <code>et</code>, <code>fa</code>, <code>fr</code>, <code>french</code>, <code>ga</code>, <code>gom</code>,         <code>hi</code>, <code>hr</code>, <code>hu</code>, <code>id</code>, <code>inh</code>, <code>is</code>, <code>it</code>, <code>japan</code>,         <code>ka</code>, <code>kbd</code>, <code>korean</code>, <code>ku</code>, <code>la</code>, <code>lbe</code>, <code>lez</code>, <code>lt</code>,         <code>lv</code>, <code>mah</code>, <code>mai</code>, <code>mi</code>, <code>mn</code>, <code>mr</code>, <code>ms</code>, <code>mt</code>,         <code>ne</code>, <code>new</code>, <code>nl</code>, <code>no</code>, <code>oc</code>, <code>pi</code>, <code>pl</code>, <code>pt</code>,         <code>ro</code>, <code>rs_cyrillic</code>, <code>rs_latin</code>, <code>ru</code>, <code>sa</code>, <code>sck</code>, <code>sk</code>,         <code>sl</code>, <code>sq</code>, <code>sv</code>, <code>sw</code>, <code>ta</code>, <code>tab</code>, <code>te</code>, <code>tl</code>,         <code>tr</code>, <code>ug</code>, <code>uk</code>, <code>ur</code>, <code>uz</code>, <code>vi</code>"},{"location":"en/version3.x/pipeline_usage/PP-ChatOCRv4.html","title":"PP-ChatOCRv4-doc Pipeline Usage Tutorial","text":""},{"location":"en/version3.x/pipeline_usage/PP-ChatOCRv4.html#1-introduction-to-pp-chatocrv4-doc-pipeline","title":"1. Introduction to PP-ChatOCRv4-doc Pipeline","text":"<p>PP-ChatOCRv4-doc is a unique document and image intelligent analysis solution from PaddlePaddle, combining LLM, MLLM, and OCR technologies to address complex document information extraction challenges such as layout analysis, rare characters, multi-page PDFs, tables, and seal recognition. Integrated with ERNIE Bot, it fuses massive data and knowledge, achieving high accuracy and wide applicability. This pipeline also provides flexible service deployment options, supporting deployment on various hardware. Furthermore, it offers custom development capabilities, allowing you to train and fine-tune models on your own datasets, with seamless integration of trained models.</p> <p></p> <p>The Document Scene Information Extraction v4 pipeline includes modules for Layout Region Detection, Table Structure Recognition, Table Classification, Table Cell Localization, Text Detection, Text Recognition, Seal Text Detection, Text Image Rectification, and Document Image Orientation Classification. </p> <p>If you prioritize model accuracy, choose a model with higher accuracy. If you prioritize inference speed, select a model with faster inference. If you prioritize model storage size, choose a model with a smaller storage size. Benchmarks for some models are as follows:</p>  \ud83d\udc49Model List Details <p>Table Structure Recognition Module Models:</p> ModelModel Download Link Accuracy (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Size (M) Description SLANetInference Model/Training Model 59.52 103.08 / 103.08 197.99 / 197.99 6.9 M SLANet is a table structure recognition model developed by Baidu PaddleX Team. The model significantly improves the accuracy and inference speed of table structure recognition by adopting a CPU-friendly lightweight backbone network PP-LCNet, a high-low-level feature fusion module CSP-PAN, and a feature decoding module SLA Head that aligns structural and positional information. SLANet_plusInference Model/Training Model 63.69 140.29 / 140.29 195.39 / 195.39 6.9 M SLANet_plus is an enhanced version of SLANet, the table structure recognition model developed by Baidu PaddleX Team. Compared to SLANet, SLANet_plus significantly improves the recognition ability for wireless and complex tables and reduces the model's sensitivity to the accuracy of table positioning, enabling more accurate recognition even with offset table positioning. <p>Layout Detection Module Models:</p> ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-DocLayout-LInference Model/Training Model 90.4 34.6244 / 10.3945 510.57 / - 123.76 M A high-precision layout area localization model trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using RT-DETR-L. PP-DocLayout-MInference Model/Training Model 75.2 13.3259 / 4.8685 44.0680 / 44.0680 22.578 A layout area localization model with balanced precision and efficiency, trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using PicoDet-L. PP-DocLayout-SInference Model/Training Model 70.9 8.3008 / 2.3794 10.0623 / 9.9296 4.834 A high-efficiency layout area localization model trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using PicoDet-S. Note: The evaluation dataset for the above precision metrics is a self-built layout area detection dataset by PaddleOCR, containing 500 common document-type images of Chinese and English papers, magazines, contracts, books, exams, and research reports. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.  &gt; \u2757 The above list includes the 3 core models that are key supported by the text recognition module. The module actually supports a total of 11 full models, including several predefined models with different categories. The complete model list is as follows:  * Table Layout Detection Model ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PicoDet_layout_1x_tableInference Model/Training Model 97.5 8.02 / 3.09 23.70 / 20.41 7.4 M A high-efficiency layout area localization model trained on a self-built dataset using PicoDet-1x, capable of detecting table regions. Note: The evaluation dataset for the above precision metrics is a self-built layout table area detection dataset by PaddleOCR, containing 7835 Chinese and English document images with tables. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.  * 3-Class Layout Detection Model, including Table, Image, and Stamp ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PicoDet-S_layout_3clsInference Model/Training Model 88.2 8.99 / 2.22 16.11 / 8.73 4.8 A high-efficiency layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-S. PicoDet-L_layout_3clsInference Model/Training Model 89.0 13.05 / 4.50 41.30 / 41.30 22.6 A balanced efficiency and precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-L. RT-DETR-H_layout_3clsInference Model/Training Model 95.8 114.93 / 27.71 947.56 / 947.56 470.1 A high-precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using RT-DETR-H. Note: The evaluation dataset for the above precision metrics is a self-built layout area detection dataset by PaddleOCR, containing 1154 common document images of Chinese and English papers, magazines, and research reports. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.  * 5-Class English Document Area Detection Model, including Text, Title, Table, Image, and List ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PicoDet_layout_1xInference Model/Training Model 97.8 9.03 / 3.10 25.82 / 20.70 7.4 A high-efficiency English document layout area localization model trained on the PubLayNet dataset using PicoDet-1x. Note: The evaluation dataset for the above precision metrics is the [PubLayNet](https://developer.ibm.com/exchanges/data/all/publaynet/) dataset, containing 11245 English document images. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.  * 17-Class Area Detection Model, including 17 common layout categories: Paragraph Title, Image, Text, Number, Abstract, Content, Figure Caption, Formula, Table, Table Caption, References, Document Title, Footnote, Header, Algorithm, Footer, and Stamp ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PicoDet-S_layout_17clsInference Model/Training Model 87.4 9.11 / 2.12 15.42 / 9.12 4.8 A high-efficiency layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-S. PicoDet-L_layout_17clsInference Model/Training Model 89.0 13.50 / 4.69 43.32 / 43.32 22.6 A balanced efficiency and precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-L. RT-DETR-H_layout_17clsInference Model/Training Model 98.3 115.29 / 104.09 995.27 / 995.27 470.2 A high-precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using RT-DETR-H. <p>Text Detection Module Models:</p> ModelModel Download Link Detection Hmean (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Size (M) Description PP-OCRv5_server_detInference Model/Training Model 83.8 89.55 / 70.19 371.65 / 371.65 84.3 PP-OCRv5 server-side text detection model with higher accuracy, suitable for deployment on high-performance servers PP-OCRv5_mobile_detInference Model/Training Model 79.0 8.79 / 3.13 51.00 / 28.58 4.7 PP-OCRv5 mobile-side text detection model with higher efficiency, suitable for deployment on edge devices PP-OCRv4_server_detInference Model/Training Model 69.2 83.34 / 80.91 442.58 / 442.58 109 PP-OCRv4 server-side text detection model with higher accuracy, suitable for deployment on high-performance servers PP-OCRv4_mobile_detInference Model/Training Model 63.8 8.79 / 3.13 51.00 / 28.58 4.7 PP-OCRv4 mobile-side text detection model with higher efficiency, suitable for deployment on edge devices <p>Text Recognition Module Models:</p> ModelModel Download Links Recognition Avg Accuracy(%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-OCRv5_server_recInference Model/Pretrained Model 86.38  8.45/2.36   122.69/122.69  81 M PP-OCRv5_rec is a next-generation text recognition model. It aims to efficiently and accurately support the recognition of four major languages\u2014Simplified Chinese, Traditional Chinese, English, and Japanese\u2014as well as complex text scenarios such as handwriting, vertical text, pinyin, and rare characters using a single model. While maintaining recognition performance, it balances inference speed and model robustness, providing efficient and accurate technical support for document understanding in various scenarios. PP-OCRv5_mobile_recInference Model/Pretrained Model 81.29  1.46/5.43   5.32/91.79  16 M PP-OCRv4_server_rec_docInference Model/Pretrained Model 86.58 6.65 / 2.38 32.92 / 32.92 91 M PP-OCRv4_server_rec_doc is trained on a mixed dataset of more Chinese document data and PP-OCR training data, building upon PP-OCRv4_server_rec. It enhances the recognition capabilities for some Traditional Chinese characters, Japanese characters, and special symbols, supporting over 15,000 characters. In addition to improving document-related text recognition, it also enhances general text recognition capabilities. PP-OCRv4_mobile_recInference Model/Pretrained Model 83.28 4.82 / 1.20 16.74 / 4.64 11 M A lightweight recognition model of PP-OCRv4 with high inference efficiency, suitable for deployment on various hardware devices, including edge devices. PP-OCRv4_server_rec Inference Model/Pretrained Model 85.19  6.58 / 2.43 33.17 / 33.17 87 M The server-side model of PP-OCRv4, offering high inference accuracy and deployable on various servers. en_PP-OCRv4_mobile_recInference Model/Pretrained Model 70.39 4.81 / 0.75 16.10 / 5.31 7.3 M An ultra-lightweight English recognition model trained based on the PP-OCRv4 recognition model, supporting English and numeric character recognition. ModelModel Download Link Recognition Avg Accuracy (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Size (M) Description ch_SVTRv2_recInference Model/Training Model 68.81 8.08 / 8.08 50.17 / 42.50 73.9 M  SVTRv2 is a server-side text recognition model developed by the OpenOCR team at the Vision and Learning Lab (FVL) of Fudan University. It won the first prize in the OCR End-to-End Recognition Task of the PaddleOCR Algorithm Model Challenge, with a 6% improvement in end-to-end recognition accuracy compared to PP-OCRv4 on the A-list.  ModelModel Download Link Recognition Avg Accuracy (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Size (M) Description ch_RepSVTR_recInference Model/Training Model 65.07 5.93 / 5.93 20.73 / 7.32 22.1 M  The RepSVTR text recognition model is a mobile-oriented text recognition model based on SVTRv2. It won the first prize in the OCR End-to-End Recognition Task of the PaddleOCR Algorithm Model Challenge, with a 2.5% improvement in end-to-end recognition accuracy compared to PP-OCRv4 on the B-list, while maintaining similar inference speed.  <p>Formula Recognition Module Models:</p> Model NameModel Download Link BLEU Score Normed Edit Distance ExpRate (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Size LaTeX_OCR_recInference Model/Training Model 0.8821 0.0823 40.01 2047.13 / 2047.13 10582.73 / 10582.73 89.7 M <p>Seal Text Detection Module Models:</p> ModelModel Download Link Detection Hmean (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Size (M) Description PP-OCRv4_server_seal_detInference Model/Training Model 98.21 74.75 / 67.72 382.55 / 382.55 109 PP-OCRv4's server-side seal text detection model, featuring higher accuracy, suitable for deployment on better-equipped servers PP-OCRv4_mobile_seal_detInference Model/Training Model 96.47 7.82 / 3.09 48.28 / 23.97 4.6 PP-OCRv4's mobile seal text detection model, offering higher efficiency, suitable for deployment on edge devices Test Environment Description: <ul> <li>Performance Test Environment <ul> <li>Test Dataset:  <ul> <li>Text Image Rectification Model: DocUNet</li> <li>Layout Region Detection Model: A self-built layout analysis dataset using PaddleOCR, containing 10,000 images of common document types such as Chinese and English papers, magazines, and research reports.</li> <li>Table Structure Recognition Model: A self-built English table recognition dataset using PaddleX.</li> <li>Text Detection Model: A self-built Chinese dataset using PaddleOCR, covering multiple scenarios such as street scenes, web images, documents, and handwriting, with 500 images for detection.</li> <li>Chinese Recognition Model: A self-built Chinese dataset using PaddleOCR, covering multiple scenarios such as street scenes, web images, documents, and handwriting, with 11,000 images for text recognition.</li> <li>ch_SVTRv2_rec: Evaluation set A for \"OCR End-to-End Recognition Task\" in the PaddleOCR Algorithm Model Challenge</li> <li>ch_RepSVTR_rec: Evaluation set B for \"OCR End-to-End Recognition Task\" in the PaddleOCR Algorithm Model Challenge</li> <li>English Recognition Model: A self-built English dataset using PaddleX.</li> <li>Multilingual Recognition Model: A self-built multilingual dataset using PaddleX.</li> <li>Text Line Orientation Classification Model: A self-built dataset using PaddleOCR, covering various scenarios such as ID cards and documents, containing 1000 images.</li> <li>Seal Text Detection Model: A self-built dataset using PaddleOCR, containing 500 images of circular seal textures.</li> </ul> </li> <li>Hardware Configuration:  <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environments: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Description</li> </ul> Mode GPU Configuration  CPU Configuration  Acceleration Technology Combination Normal Mode FP32 Precision / No TRT Acceleration FP32 Precision / 8 Threads PaddleInference High-Performance Mode Optimal combination of pre-selected precision types and acceleration strategies FP32 Precision / 8 Threads Pre-selected optimal backend (Paddle/OpenVINO/TRT, etc.)"},{"location":"en/version3.x/pipeline_usage/PP-ChatOCRv4.html#2-quick-start","title":"2. Quick Start","text":"<p>The pre-trained pipelines provided by PaddleOCR allow for quick experience of their effects. You can locally use Python to experience the effects of the PP-ChatOCRv4-doc pipeline.</p> <p>Before using the PP-ChatOCRv4-doc pipeline locally, ensure you have completed the installation of the PaddleOCR wheel package according to the PaddleOCR Local Installation Tutorial. If you wish to selectively install dependencies, please refer to the relevant instructions in the installation guide. The dependency group corresponding to this pipeline is <code>ie</code>.</p> <p>Before performing model inference, you first need to prepare the API key for the large language model. PP-ChatOCRv4 supports large model services on the Baidu Cloud Qianfan Platform or the locally deployed standard OpenAI interface. If using the Baidu Cloud Qianfan Platform, refer to Authentication and Authorization to obtain the API key. If using a locally deployed large model service, refer to the PaddleNLP Large Model Deployment Documentation for deployment of the dialogue interface and vectorization interface for large models, and fill in the corresponding <code>base_url</code> and <code>api_key</code>. If you need to use a multimodal large model for data fusion, refer to the OpenAI service deployment in the PaddleMIX Model Documentation for multimodal large model deployment, and fill in the corresponding <code>base_url</code> and <code>api_key</code>.</p> <p>Note: If local deployment of a multimodal large model is restricted due to the local environment, you can comment out the lines containing the <code>mllm</code> variable in the code and only use the large language model for information extraction.</p>"},{"location":"en/version3.x/pipeline_usage/PP-ChatOCRv4.html#21-command-line-experience","title":"2.1 Command Line Experience","text":"<p>After updating the configuration file, you can complete quick inference using just a few lines of Python code. You can use the test file for testing:</p> <pre><code>paddleocr pp_chatocrv4_doc -i vehicle_certificate-1.png -k \u200b\u9a7e\u9a76\u5ba4\u200b\u51c6\u4e58\u200b\u4eba\u6570\u200b --qianfan_api_key your_api_key\n\n# \u200b\u901a\u8fc7\u200b --invoke_mllm \u200b\u548c\u200b --pp_docbee_base_url \u200b\u4f7f\u7528\u200b\u591a\u200b\u6a21\u6001\u200b\u5927\u200b\u6a21\u578b\u200b\npaddleocr pp_chatocrv4_doc -i vehicle_certificate-1.png -k \u200b\u9a7e\u9a76\u5ba4\u200b\u51c6\u4e58\u200b\u4eba\u6570\u200b --qianfan_api_key your_api_key --invoke_mllm True --pp_docbee_base_url http://127.0.0.1:8080/\n</code></pre> The command line supports more parameter configurations. Click to expand for a detailed explanation of the command line parameters. Parameter Description Type Default <code>input</code> Data to be predicted, required. Such as the local path of an image file or PDF file: <code>/root/data/img.jpg</code>; URL link, such as the network URL of an image file or PDF file: Example; Local directory, which should contain images to be predicted, such as the local path: <code>/root/data/</code> (currently does not support prediction of PDF files in directories, PDF files need to be specified to the specific file path).  <code>str</code> <code>keys</code> Keys for information extraction. <code>str</code> <code>save_path</code>  Specify the path to save the inference results file. If not set, the inference results will not be saved locally. <code>str</code> <code>invoke_mllm</code> Whether to load and use a multimodal large model. If not set, the default is <code>False</code>. <code>bool</code> <code>layout_detection_model_name</code>  The name of the layout detection model. If not set, the default model in pipeline will be used.  <code>str</code> <code>layout_detection_model_dir</code>  The directory path of the layout detection model. If not set, the official model will be downloaded.  <code>str</code> <code>doc_orientation_classify_model_name</code>   The name of the document orientation classification model. If not set, the default model in pipeline will be used. <code>str</code> <code>doc_orientation_classify_model_dir</code> The directory path of the document orientation classification model. If not set, the official model will be downloaded. <code>str</code> <code>doc_unwarping_model_name</code>  The name of the text image unwarping model. If not set, the default model in pipeline will be used. <code>str</code> <code>doc_unwarping_model_dir</code>  The directory path of the  text image unwarping model. If not set, the official model will be downloaded.  <code>str</code> <code>text_detection_model_name</code> Name of the text detection model. If not set, the pipeline's default model will be used. <code>str</code> <code>text_detection_model_dir</code> Directory path of the text detection model. If not set, the official model will be downloaded. <code>str</code> <code>text_recognition_model_name</code> Name of the text recognition model. If not set, the pipeline's default model will be used. <code>str</code> <code>text_recognition_model_dir</code> Directory path of the text recognition model. If not set, the official model will be downloaded. <code>str</code> <code>text_recognition_batch_size</code> Batch size for the text recognition model. If not set, the default batch size will be <code>1</code>. <code>int</code> <code>table_structure_recognition_model_name</code> Name of the table structure recognition model. If not set, the official model will be downloaded. <code>str</code> <code>table_structure_recognition_model_dir</code> Directory path of the table structure recognition model. If not set, the official model will be downloaded. <code>str</code> <code>seal_text_detection_model_name</code> The name of the seal text detection model. If not set, the pipeline's default model will be used. <code>str</code> <code>seal_text_detection_model_dir</code> The directory path of the seal text detection model. If not set, the official model will be downloaded. <code>str</code> <code>seal_text_recognition_model_name</code> The name of the seal text recognition model. If not set, the default model of the pipeline will be used. <code>str</code> <code>seal_text_recognition_model_dir</code> The directory path of the seal text recognition model. If not set, the official model will be downloaded. <code>str</code> <code>seal_text_recognition_batch_size</code> The batch size for the seal text recognition model. If not set, the batch size will default to <code>1</code>. <code>int</code> <code>use_doc_orientation_classify</code> Whether to load and use the document orientation classification module. If not set, the parameter value initialized by the pipeline will be used by default, initialized as <code>True</code>. <code>bool</code> <code>use_doc_unwarping</code> Whether to load and use the text image unwarping module. If not set, the parameter value initialized by the pipeline will be used by default, initialized as <code>True</code>. <code>bool</code> <code>use_textline_orientation</code> Whether to load and use the text line orientation classification module. If not set, the parameter value initialized by the pipeline will be used by default, initialized as <code>True</code>. <code>bool</code> <code>use_seal_recognition</code> Whether to load and use the seal recognition sub-pipeline. If not set, the parameter's value initialized during pipeline setup will be used, defaulting to <code>True</code>. <code>bool</code> <code>use_table_recognition</code> Whether to load and use the table recognition sub-pipeline. If not set, the parameter's value initialized during pipeline setup will be used, defaulting to <code>True</code>. <code>bool</code> <code>layout_threshold</code> Score threshold for the layout model. Any value between <code>0-1</code>. If not set, the default value is used, which is <code>0.5</code>.  <code>float</code> <code>layout_nms</code>  Whether to use Non-Maximum Suppression (NMS) as post-processing for layout detection. If not set, the parameter will default to the value initialized in the pipeline, which is set to <code>True</code> by default.  <code>bool</code> <code>layout_unclip_ratio</code> Unclip ratio for detected boxes in layout detection model. Any float &gt; <code>0</code>. If not set, the default is <code>1.0</code>.  <code>float</code> <code>layout_merge_bboxes_mode</code> The merging mode for the detection boxes output by the model in layout region detection. <ul> <li>large: When set to \"large\", only the largest outer bounding box will be retained for overlapping bounding boxes, and the inner overlapping boxes will be removed;</li> <li>small: When set to \"small\", only the smallest inner bounding boxes will be retained for overlapping bounding boxes, and the outer overlapping boxes will be removed;</li> <li>union: No filtering of bounding boxes will be performed, and both inner and outer boxes will be retained;</li> </ul>If not set, the default is <code>large</code>.  <code>str</code> <code>text_det_limit_side_len</code> Image side length limitation for text detection. Any integer greater than <code>0</code>. If not set, the pipeline's initialized value for this parameter (initialized to <code>960</code>) will be used.  <code>int</code> <code>text_det_limit_type</code> Type of side length limit for text detection. Supports <code>min</code> and <code>max</code>. <code>min</code> means ensuring the shortest side of the image is not smaller than <code>det_limit_side_len</code>, and <code>max</code> means ensuring the longest side of the image is not larger than <code>limit_side_len</code>. If not set, the pipeline's initialized value for this parameter (initialized to <code>max</code>) will be used.  <code>str</code> <code>text_det_thresh</code> Pixel threshold for text detection. In the output probability map, pixels with scores higher than this threshold will be considered text pixels. Any floating-point number greater than <code>0</code> . If not set, the pipeline's initialized value for this parameter (<code>0.3</code>) will be used.  <code>float</code> <code>text_det_box_thresh</code> Text detection box threshold. If the average score of all pixels within the detected result boundary is higher than this threshold, the result will be considered a text region.  Any floating-point number greater than <code>0</code>. If not set, the pipeline's initialized value for this parameter (<code>0.6</code>) will be used.  <code>float</code> <code>text_det_unclip_ratio</code> Text detection expansion coefficient. This method is used to expand the text region\u2014the larger the value, the larger the expanded area. Any floating-point number greater than <code>0</code> . If not set, the pipeline's initialized value for this parameter (<code>2.0</code>) will be used.  <code>float</code> <code>text_rec_score_thresh</code> Text recognition threshold. Text results with scores higher than this threshold will be retained.  Any floating-point number greater than <code>0</code> . If not set, the pipeline's initialized value for this parameter (<code>0.0</code>, i.e., no threshold) will be used.  <code>float</code> <code>seal_det_limit_side_len</code> Image side length limit for seal text detection. Any integer &gt; <code>0</code>. If not set, the default is <code>736</code>.  <code>int</code>don\u2019t   <code>seal_det_limit_type</code> Limit type for image side in seal text detection. supports <code>min</code> and <code>max</code>; <code>min</code> ensures shortest side \u2265 <code>det_limit_side_len</code>, <code>max</code> ensures longest side \u2264 <code>limit_side_len</code>. If not set, the default is <code>min</code>.  <code>str</code> <code>seal_det_thresh</code> Pixel threshold. Pixels with scores above this value in the probability map are considered text. Any float &gt; <code>0</code> If not set, the default is <code>0.2</code>.  <code>float</code> <code>seal_det_box_thresh</code> Box threshold. Boxes with average pixel scores above this value are considered text regions.Any float &gt; <code>0</code>. If not set, the default is <code>0.6</code>.  <code>float</code> <code>seal_det_unclip_ratio</code> Expansion ratio for seal text detection. Higher value means larger expansion area. any float &gt; <code>0</code>. If not set, the default is <code>0.5</code>.  <code>float</code> <code>seal_rec_score_thresh</code> Recognition score threshold. Text results above this value will be kept. Any float &gt; <code>0</code> If not set, the default is <code>0.0</code> (no threshold).  <code>float</code> <code>qianfan_api_key</code> API key for the Qianfan Platform. <code>str</code> <code>pp_docbee_base_url</code> Configuration for the multimodal large language model. <code>str</code> <code>device</code> The device used for inference. You can specify a particular card number: <ul> <li>CPU: e.g., <code>cpu</code> indicates using CPU for inference;</li> <li>GPU: e.g., <code>gpu:0</code> indicates using the 1st GPU for inference;</li> <li>NPU: e.g., <code>npu:0</code> indicates using the 1st NPU for inference;</li> <li>XPU: e.g., <code>xpu:0</code> indicates using the 1st XPU for inference;</li> <li>MLU: e.g., <code>mlu:0</code> indicates using the 1st MLU for inference;</li> <li>DCU: e.g., <code>dcu:0</code> indicates using the 1st DCU for inference;</li> </ul>If not set, the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.  <code>str</code> <code>enable_hpi</code> Whether to enable the high-performance inference plugin. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Compute precision, such as FP32 or FP16. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code>  The number of threads to use when performing inference on the CPU. <code>int</code> <code>8</code> <code>paddlex_config</code> Path to PaddleX pipeline configuration file. <code>str</code> <p>This method will print the results to the terminal. The content printed to the terminal is explained as follows:</p> <pre><code>\u200b\u9a7e\u9a76\u5ba4\u200b\u51c6\u4e58\u200b\u4eba\u6570\u200b 2\n</code></pre>"},{"location":"en/version3.x/pipeline_usage/PP-ChatOCRv4.html#22-python-script-experience","title":"2.2 Python Script Experience","text":"<p>The command-line method is for a quick experience and to view results. Generally, in projects, integration via code is often required. You can download the Test File and use the following example code for inference:</p> <pre><code>from paddleocr import PPChatOCRv4Doc\n\nchat_bot_config = {\n    \"module_name\": \"chat_bot\",\n    \"model_name\": \"ernie-3.5-8k\",\n    \"base_url\": \"https://qianfan.baidubce.com/v2\",\n    \"api_type\": \"openai\",\n    \"api_key\": \"api_key\",  # your api_key\n}\n\nretriever_config = {\n    \"module_name\": \"retriever\",\n    \"model_name\": \"embedding-v1\",\n    \"base_url\": \"https://qianfan.baidubce.com/v2\",\n    \"api_type\": \"qianfan\",\n    \"api_key\": \"api_key\",  # your api_key\n}\n\nmllm_chat_bot_config = {\n    \"module_name\": \"chat_bot\",\n    \"model_name\": \"PP-DocBee2\",\n    \"base_url\": \"http://127.0.0.1:8080/\",  # your local mllm service url\n    \"api_type\": \"openai\",\n    \"api_key\": \"api_key\",  # your api_key\n}\n\npipeline = PPChatOCRv4Doc()\n\nvisual_predict_res = pipeline.visual_predict(\n    input=\"vehicle_certificate-1.png\",\n    use_doc_orientation_classify=False,\n    use_doc_unwarping=False,\n    use_common_ocr=True,\n    use_seal_recognition=True,\n    use_table_recognition=True,\n)\n\nvisual_info_list = []\nfor res in visual_predict_res:\n    visual_info_list.append(res[\"visual_info\"])\n    layout_parsing_result = res[\"layout_parsing_result\"]\n\nvector_info = pipeline.build_vector(\n    visual_info_list, flag_save_bytes_vector=True, retriever_config=retriever_config\n)\nmllm_predict_res = pipeline.mllm_pred(\n    input=\"vehicle_certificate-1.png\",\n    key_list=[\"Cab Seating Capacity\"], # Translated: \u200b\u9a7e\u9a76\u5ba4\u200b\u51c6\u4e58\u200b\u4eba\u6570\u200b\n    mllm_chat_bot_config=mllm_chat_bot_config,\n)\nmllm_predict_info = mllm_predict_res[\"mllm_res\"]\nchat_result = pipeline.chat(\n    key_list=[\"Cab Seating Capacity\"], # Translated: \u200b\u9a7e\u9a76\u5ba4\u200b\u51c6\u4e58\u200b\u4eba\u6570\u200b\n    visual_info=visual_info_list,\n    vector_info=vector_info,\n    mllm_predict_info=mllm_predict_info,\n    chat_bot_config=chat_bot_config,\n    retriever_config=retriever_config,\n)\nprint(chat_result)\n</code></pre> <p>After running, the output is as follows:</p> <pre><code>{'chat_res': {'\u200b\u9a7e\u9a76\u5ba4\u200b\u51c6\u4e58\u200b\u4eba\u6570\u200b': '2'}}\n</code></pre> <p>The prediction process, API description, and output description for PP-ChatOCRv4 are as follows:</p> (1) Call the <code>PPChatOCRv4Doc</code> method to instantiate the PP-ChatOCRv4 pipeline object.  The relevant parameter descriptions are as follows:   Parameter Parameter Description Parameter Type Default Value <code>layout_detection_model_name</code> The name of the model used for layout region detection. If set to<code>None</code>, the pipeline's default model will be used. <code>str</code> <code>None</code> <code>layout_detection_model_dir</code> The directory path of the layout region detection model. If set to<code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>doc_orientation_classify_model_name</code> The name of the document orientation classification model. If set to<code>None</code>, the pipeline's default model will be used. <code>str</code> <code>None</code> <code>doc_orientation_classify_model_dir</code> The directory path of the document orientation classification model. If set to<code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>doc_unwarping_model_name</code> The name of the document unwarping model. If set to<code>None</code>, the pipeline's default model will be used. <code>str</code> <code>None</code> <code>doc_unwarping_model_dir</code> The directory path of the document unwarping model. If set to<code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>text_detection_model_name</code> The name of the text detection model. If set to<code>None</code>, the pipeline's default model will be used. <code>str</code> <code>None</code> <code>text_detection_model_dir</code> The directory path of the text detection model. If set to<code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>text_recognition_model_name</code> The name of the text recognition model. If set to<code>None</code>, the pipeline's default model will be used. <code>str</code> <code>None</code> <code>text_recognition_model_dir</code> The directory path of the text recognition model. If set to<code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>text_recognition_batch_size</code> The batch size for the text recognition model. If set to<code>None</code>, the batch size will default to <code>1</code>. <code>int</code> <code>None</code> <code>table_structure_recognition_model_name</code> The name of the table structure recognition model. If set to<code>None</code>, the pipeline's default model will be used. <code>str</code> <code>None</code> <code>table_structure_recognition_model_dir</code> The directory path of the table structure recognition model. If set to<code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>seal_text_detection_model_name</code> The name of the seal text detection model. If set to<code>None</code>, the pipeline's default model will be used. <code>str</code> <code>None</code> <code>seal_text_detection_model_dir</code> The directory path of the seal text detection model. If set to<code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>seal_text_recognition_model_name</code> The name of the seal text recognition model. If set to<code>None</code>, the pipeline's default model will be used. <code>str</code> <code>None</code> <code>seal_text_recognition_model_dir</code> The directory path of the seal text recognition model. If set to<code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>seal_text_recognition_batch_size</code> The batch size for the seal text recognition model. If set to<code>None</code>, the batch size will default to <code>1</code>. <code>int</code> <code>None</code> <code>use_doc_orientation_classify</code> Whether to load and use the document orientation classification module. If set to<code>None</code>, the value initialized by the pipeline for this parameter will be used by default (initialized to <code>True</code>). <code>bool</code> <code>None</code> <code>use_doc_unwarping</code> Whether to load and use the document unwarping module. If set to<code>None</code>, the value initialized by the pipeline for this parameter will be used by default (initialized to <code>True</code>). <code>bool</code> <code>None</code> <code>use_textline_orientation</code> Whether to load and use the text line orientation classification function. If set to<code>None</code>, the value initialized by the pipeline for this parameter will be used by default (initialized to <code>True</code>). <code>bool</code> <code>None</code> <code>use_seal_recognition</code> Whether to load and use the seal recognition sub-pipeline. If set to<code>None</code>, the value initialized by the pipeline for this parameter will be used by default (initialized to <code>True</code>). <code>bool</code> <code>None</code> <code>use_table_recognition</code> Whether to load and use the table recognition sub-pipeline. If set to<code>None</code>, the value initialized by the pipeline for this parameter will be used by default (initialized to <code>True</code>). <code>bool</code> <code>None</code> <code>layout_threshold</code> Layout model score threshold. <ul> <li>float: Any float between <code>0-1</code>;</li> <li>dict: <code>{0:0.1}</code> where the key is the class ID and the value is the threshold for that class;</li> <li>None: If set to <code>None</code>, uses the pipeline default of <code>0.5</code>.</li> </ul> <code>float|dict</code> <code>None</code> <code>layout_nms</code> Whether to use Non-Maximum Suppression (NMS) as post-processing for layout detection. If set to <code>None</code>, the parameter will default to the value initialized in the pipeline, which is set to <code>True</code> by default. <code>bool</code> <code>None</code> <code>layout_unclip_ratio</code> Expansion factor for the detection boxes of the layout region detection model. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>Tuple[float,float]: Expansion ratios in horizontal and vertical directions;</li> <li>dict: A dictionary with int keys representing <code>cls_id</code>, and tuple values, e.g., <code>{0: (1.1, 2.0)}</code> means width is expanded 1.1\u00d7 and height 2.0\u00d7 for class 0 boxes;</li> <li>None: If set to <code>None</code>, uses the pipeline default of <code>1.0</code>.</li> </ul> <code>float|Tuple[float,float]|dict</code> <code>None</code> <code>layout_merge_bboxes_mode</code> Method for filtering overlapping boxes in layout region detection. <ul> <li>str: <code>large</code>,<code>small</code>, <code>union</code>, representing whether to keep the large box, small box, or both when filtering overlapping boxes;</li> <li>dict, where the key is of int type, representing <code>cls_id</code>, and the value is of str type, e.g.,<code>{0: \"large\", 2: \"small\"}</code>, meaning use \"large\" mode for class 0 detection boxes and \"small\" mode for class 2 detection boxes;</li> <li>None: If set to <code>None</code>, the value initialized by the pipeline for this parameter will be used by default (initialized to <code>large</code>).</li> </ul> <code>str|dict</code> <code>None</code> <code>text_det_limit_side_len</code> Image side length limitation for text detection. <ul> <li>int: Any integer greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the value initialized by the pipeline for this parameter will be used by default (initialized to <code>960</code>).</li> </ul> <code>int</code> <code>None</code> <code>text_det_limit_type</code> Type of side length limit for text detection. <ul> <li>str: Supports <code>min</code> and <code>max</code>. <code>min</code> ensures the shortest side of the image is not less than <code>det_limit_side_len</code>. <code>max</code> ensures the longest side of the image is not greater than <code>limit_side_len</code>;</li> <li>None: If set to <code>None</code>, the value initialized by the pipeline for this parameter will be used by default (initialized to <code>max</code>).</li> </ul> <code>str</code> <code>None</code> <code>text_det_thresh</code> Detection pixel threshold. In the output probability map, pixels with scores greater than this threshold are considered text pixels. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the value initialized by the pipeline for this parameter (<code>0.3</code>) will be used by default.</li></ul> <code>float</code> <code>None</code> <code>text_det_box_thresh</code> Detection box threshold. If the average score of all pixels within a detection result's bounding box is greater than this threshold, the result is considered a text region. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the value initialized by the pipeline for this parameter (<code>0.6</code>) will be used by default.</li></ul> <code>float</code> <code>None</code> <code>text_det_unclip_ratio</code> Text detection expansion factor. This method is used to expand text regions; the larger the value, the larger the expanded area. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the value initialized by the pipeline for this parameter (<code>2.0</code>) will be used by default.</li></ul> <code>float</code> <code>None</code> <code>text_rec_score_thresh</code> Text recognition threshold. Text results with scores greater than this threshold will be kept. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the value initialized by the pipeline for this parameter (<code>0.0</code>, i.e., no threshold) will be used by default.</li></ul> <code>float</code> <code>None</code> <code>seal_det_limit_side_len</code> Image side length limit for seal text detection. <ul> <li>int: Any integer greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the value initialized by the pipeline for this parameter will be used by default (initialized to <code>736</code>).</li> </ul> <code>int</code> <code>None</code> <code>seal_det_limit_type</code> Type of image side length limit for seal text detection. <ul> <li>str: Supports <code>min</code> and <code>max</code>. <code>min</code> ensures the shortest side of the image is not less than <code>det_limit_side_len</code>. <code>max</code> ensures the longest side of the image is not greater than <code>limit_side_len</code>;</li> <li>None: If set to <code>None</code>, the value initialized by the pipeline for this parameter will be used by default (initialized to <code>min</code>).</li> </ul> <code>str</code> <code>None</code> <code>seal_det_thresh</code> Detection pixel threshold. In the output probability map, pixels with scores greater than this threshold are considered text pixels. <ul> <li>float: Any float greater than <code>0</code>;     <li>None: If set to <code>None</code>, the value initialized by the pipeline for this parameter (<code>0.2</code>) will be used by default.</li></li></ul> <code>float</code> <code>None</code> <code>seal_det_box_thresh</code> Detection box threshold. If the average score of all pixels within a detection result's bounding box is greater than this threshold, the result is considered a text region. <ul> <li>float: Any float greater than <code>0</code>;     <li>None: If set to <code>None</code>, the value initialized by the pipeline for this parameter (<code>0.6</code>) will be used by default.</li></li></ul> <code>float</code> <code>None</code> <code>seal_det_unclip_ratio</code> Seal text detection expansion factor. This method is used to expand text regions; the larger the value, the larger the expanded area. <ul> <li>float: Any float greater than <code>0</code>;     <li>None: If set to <code>None</code>, the value initialized by the pipeline for this parameter (<code>0.5</code>) will be used by default.</li></li></ul> <code>float</code> <code>None</code> <code>seal_rec_score_thresh</code> Seal text recognition threshold. Text results with scores greater than this threshold will be kept. <ul> <li>float: Any float greater than <code>0</code>;     <li>None: If set to <code>None</code>, the value initialized by the pipeline for this parameter (<code>0.0</code>, i.e., no threshold) will be used by default.</li></li></ul> <code>float</code> <code>None</code> <code>retriever_config</code> Configuration parameters for the vector retrieval large model. The configuration content is the following dictionary: <pre><code>{\n\"module_name\": \"retriever\",\n\"model_name\": \"embedding-v1\",\n\"base_url\": \"https://qianfan.baidubce.com/v2\",\n\"api_type\": \"qianfan\",\n\"api_key\": \"api_key\"  # Please set this to your actual API key\n}</code></pre> <code>dict</code> <code>None</code> <code>mllm_chat_bot_config</code> Configuration parameters for the multimodal large model. The configuration content is the following dictionary: <pre><code>{\n\"module_name\": \"chat_bot\",\n\"model_name\": \"PP-DocBee\",\n\"base_url\": \"http://127.0.0.1:8080/\", # Please set this to the actual URL of your multimodal large model service\n\"api_type\": \"openai\",\n\"api_key\": \"api_key\"  # Please set this to your actual API key\n}</code></pre> <code>dict</code> <code>None</code> <code>chat_bot_config</code> Configuration information for the large language model. The configuration content is the following dictionary: <pre><code>{\n\"module_name\": \"chat_bot\",\n\"model_name\": \"ernie-3.5-8k\",\n\"base_url\": \"https://qianfan.baidubce.com/v2\",\n\"api_type\": \"openai\",\n\"api_key\": \"api_key\"  # Please set this to your actual API key\n}</code></pre> <code>dict</code> <code>None</code> <code>device</code> Device used for inference. Supports specifying a specific card number: <ul> <li>CPU: e.g., <code>cpu</code> indicates using CPU for inference;</li> <li>GPU: e.g., <code>gpu:0</code> indicates using the 1st GPU for inference;</li> <li>NPU: e.g., <code>npu:0</code> indicates using the 1st NPU for inference;</li> <li>XPU: e.g., <code>xpu:0</code> indicates using the 1st XPU for inference;</li> <li>MLU: e.g., <code>mlu:0</code> indicates using the 1st MLU for inference;</li> <li>DCU: e.g., <code>dcu:0</code> indicates using the 1st DCU for inference;</li> <li>None: If set to <code>None</code>, the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.</li> </ul> <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to enable high-performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Computation precision, e.g., fp32, fp16. <code>str</code> <code>\"fp32\"</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads used when performing inference on CPU. <code>int</code> <code>8</code> <code>paddlex_config</code> PaddleX pipeline configuration file path. <code>str</code> <code>None</code> (2) Call the <code>visual_predict()</code> method of the PP-ChatOCRv4 pipeline object to obtain visual prediction results. This method returns a list of results. Additionally, the pipeline also provides the <code>visual_predict_iter()</code> method. Both are identical in terms of parameter acceptance and result return, with the difference being that <code>visual_predict_iter()</code> returns a <code>generator</code>, allowing for step-by-step processing and retrieval of prediction results, suitable for handling large datasets or scenarios where memory saving is desired. You can choose either of these two methods based on your actual needs. The following are the parameters and their descriptions for the <code>visual_predict()</code> method: Parameter Parameter Description Parameter Type Default Value <code>input</code> Data to be predicted, supports multiple input types, required. <ul> <li>Python Var: e.g., image data represented by <code>numpy.ndarray</code>;</li> <li>str: e.g., local path of an image file or PDF file: <code>/root/data/img.jpg</code>; URL link, e.g., network URL of an image file or PDF file: Example; Local directory, which must contain images to be predicted, e.g., local path: <code>/root/data/</code> (Currently, prediction from directories containing PDF files is not supported; PDF files need to be specified by their full path);</li> <li>List: List elements must be of the above types, e.g.,<code>[numpy.ndarray, numpy.ndarray]</code>,<code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>,<code>[\"/root/data1\", \"/root/data2\"]</code>.</li> </ul> <code>Python Var|str|list</code> <code>use_doc_orientation_classify</code> Whether to use the document orientation classification module during inference. <code>bool</code> <code>None</code> <code>use_doc_unwarping</code> Whether to use the document image unwarping module during inference. <code>bool</code> <code>None</code> <code>use_textline_orientation</code> Whether to use the text line orientation classification module during inference. <code>bool</code> <code>None</code> <code>use_seal_recognition</code> Whether to use the seal recognition sub-pipeline during inference. <code>bool</code> <code>None</code> <code>use_table_recognition</code> Whether to use the table recognition sub-pipeline during inference. <code>bool</code> <code>None</code> <code>layout_threshold</code> Same as the parameter during instantiation. <code>float|dict</code> <code>None</code> <code>layout_nms</code> Same as the parameter during instantiation. <code>bool</code> <code>None</code> <code>layout_unclip_ratio</code> Same as the parameter during instantiation. <code>float|Tuple[float,float]|dict</code> <code>None</code> <code>layout_merge_bboxes_mode</code> Same as the parameter during instantiation. <code>str|dict</code> <code>None</code> <code>text_det_limit_side_len</code> Same as the parameter during instantiation. <code>int</code> <code>None</code> <code>text_det_limit_type</code> Same as the parameter during instantiation. <code>str</code> <code>None</code> <code>text_det_thresh</code> Same as the parameter during instantiation. <code>float</code> <code>None</code> <code>text_det_box_thresh</code> Same as the parameter during instantiation. <code>float</code> <code>None</code> <code>text_det_unclip_ratio</code> Same as the parameter during instantiation. <code>float</code> <code>None</code> <code>text_rec_score_thresh</code> Same as the parameter during instantiation. <code>float</code> <code>None</code> <code>seal_det_limit_side_len</code> Same as the parameter during instantiation. <code>int</code> <code>None</code> <code>seal_det_limit_type</code> Same as the parameter during instantiation. <code>str</code> <code>None</code> <code>seal_det_thresh</code> Same as the parameter during instantiation. <code>float</code> <code>None</code> <code>seal_det_box_thresh</code> Same as the parameter during instantiation. <code>float</code> <code>None</code> <code>seal_det_unclip_ratio</code> Same as the parameter during instantiation. <code>float</code> <code>None</code> <code>seal_rec_score_thresh</code> Same as the parameter during instantiation. <code>float</code> <code>None</code> (3) Process the visual prediction results.  The prediction result for each sample is of `dict` type, containing two fields: `visual_info` and `layout_parsing_result`. Visual information (including `normal_text_dict`, `table_text_list`, `table_html_list`, etc.) is obtained through `visual_info`, and the information for each sample is placed in the `visual_info_list` list. The content of this list will later be fed into the large language model.  Of course, you can also obtain the layout parsing results through `layout_parsing_result`. This result contains content such as tables, text, and images found in the file or image, and supports operations like printing, saving as an image, and saving as a `json` file:  <pre><code>......\nfor res in visual_predict_res:\n    visual_info_list.append(res[\"visual_info\"])\n    layout_parsing_result = res[\"layout_parsing_result\"]\n    layout_parsing_result.print()\n    layout_parsing_result.save_to_img(\"./output\")\n    layout_parsing_result.save_to_json(\"./output\")\n    layout_parsing_result.save_to_xlsx(\"./output\")\n    layout_parsing_result.save_to_html(\"./output\")\n......\n</code></pre> Method Method Description Parameter Parameter Type Parameter Description Default Value <code>print()</code> Prints the result to the terminal <code>format_json</code> <code>bool</code> Whether to format the output content using <code>JSON</code> indentation. <code>True</code> <code>indent</code> <code>int</code> Specifies the indentation level to beautify the output <code>JSON</code> data for better readability, effective only when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Controls whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. Set to <code>True</code> to escape all non-<code>ASCII</code> characters; <code>False</code> to preserve original characters, effective only when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_json()</code> Saves the result as a JSON format file <code>save_path</code> <code>str</code> Save file path. When it's a directory, the saved file name will be consistent with the input file name. None <code>indent</code> <code>int</code> Specifies the indentation level to beautify the output <code>JSON</code> data for better readability, effective only when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Controls whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. Set to <code>True</code> to escape all non-<code>ASCII</code> characters; <code>False</code> to preserve original characters, effective only when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_img()</code> Saves the visualization images of various intermediate modules as PNG format images. <code>save_path</code> <code>str</code> Save file path, supports directory or file path. None <code>save_to_html()</code> Saves the tables in the file as HTML format files. <code>save_path</code> <code>str</code> Save file path, supports directory or file path. None <code>save_to_xlsx()</code> Saves the tables in the file as XLSX format files. <code>save_path</code> <code>str</code> Save file path, supports directory or file path. None   - Calling the `print()` method will print the results to the terminal. The content printed to the terminal is explained as follows:     - `input_path`: `(str)` Input path of the image to be predicted.     - `page_index`: `(Union[int, None])` If the input is a PDF file, it indicates the current page number of the PDF; otherwise, it is `None`.     - `model_settings`: `(Dict[str, bool])` Model parameters required to configure the pipeline.         - `use_doc_preprocessor`: `(bool)` Controls whether to enable the document preprocessor sub-pipeline.         - `use_seal_recognition`: `(bool)` Controls whether to enable the seal recognition sub-pipeline.         - `use_table_recognition`: `(bool)` Controls whether to enable the table recognition sub-pipeline.         - `use_formula_recognition`: `(bool)` Controls whether to enable the formula recognition sub-pipeline.     - `parsing_res_list`: `(List[Dict])` List of parsing results, where each element is a dictionary. The list order is the reading order after parsing.         - `block_bbox`: `(np.ndarray)` Bounding box of the layout region.         - `block_label`: `(str)` Label of the layout region, e.g., `text`, `table`, etc.         - `block_content`: `(str)` Content within the layout region.     - `overall_ocr_res`: `(Dict[str, Union[List[str], List[float], numpy.ndarray]])` Dictionary of global OCR results.       -  `input_path`: `(Union[str, None])` Image path accepted by the image OCR sub-pipeline. When the input is `numpy.ndarray`, it is saved as `None`.       - `model_settings`: `(Dict)` Model configuration parameters for the OCR sub-pipeline.       - `dt_polys`: `(List[numpy.ndarray])` List of polygon boxes for text detection. Each detection box is represented by a numpy array of 4 vertex coordinates, with array shape (4, 2) and data type int16.       - `dt_scores`: `(List[float])` List of confidence scores for text detection boxes.       - `text_det_params`: `(Dict[str, Dict[str, int, float]])` Configuration parameters for the text detection module.         - `limit_side_len`: `(int)` Side length limit value for image preprocessing.         - `limit_type`: `(str)` Processing method for side length limit.         - `thresh`: `(float)` Confidence threshold for text pixel classification.         - `box_thresh`: `(float)` Confidence threshold for text detection boxes.         - `unclip_ratio`: `(float)` Expansion factor for text detection boxes.         - `text_type`: `(str)` Type of text detection, currently fixed to \"general\".       - `text_type`: `(str)` Type of text detection, currently fixed to \"general\".       - `textline_orientation_angles`: `(List[int])` Prediction results of text line orientation classification. When enabled, returns actual angle values (e.g., [0,0,1]).       - `text_rec_score_thresh`: `(float)` Filtering threshold for text recognition results.       - `rec_texts`: `(List[str])` List of text recognition results, containing only text with confidence exceeding `text_rec_score_thresh`.       - `rec_scores`: `(List[float])` List of text recognition confidence scores, filtered by `text_rec_score_thresh`.       - `rec_polys`: `(List[numpy.ndarray])` List of text detection boxes filtered by confidence, format same as `dt_polys`.     - `formula_res_list`: `(List[Dict[str, Union[numpy.ndarray, List[float], str]]])` List of formula recognition results, each element is a dictionary.         - `rec_formula`: `(str)` Formula recognition result.         - `rec_polys`: `(numpy.ndarray)` Formula detection box, shape (4, 2), dtype int16.         - `formula_region_id`: `(int)` Region number where the formula is located.     - `seal_res_list`: `(List[Dict[str, Union[numpy.ndarray, List[float], str]]])` List of seal recognition results, each element is a dictionary.         - `input_path`: `(str)` Input path of the seal image.         - `model_settings`: `(Dict)` Model configuration parameters for the seal recognition sub-pipeline.         - `dt_polys`: `(List[numpy.ndarray])` List of seal detection boxes, format same as `dt_polys`.         - `text_det_params`: `(Dict[str, Dict[str, int, float]])` Configuration parameters for the seal detection module, specific parameter meanings are the same as above.         - `text_type`: `(str)` Type of seal detection, currently fixed to \"seal\".         - `text_rec_score_thresh`: `(float)` Filtering threshold for seal recognition results.         - `rec_texts`: `(List[str])` List of seal recognition results, containing only text with confidence exceeding `text_rec_score_thresh`.         - `rec_scores`: `(List[float])` List of seal recognition confidence scores, filtered by `text_rec_score_thresh`.         - `rec_polys`: `(List[numpy.ndarray])` List of seal detection boxes filtered by confidence, format same as `dt_polys`.         - `rec_boxes`: `(numpy.ndarray)` Array of rectangular bounding boxes for detections, shape (n, 4), dtype int16. Each row represents a rectangle.     - `table_res_list`: `(List[Dict[str, Union[numpy.ndarray, List[float], str]]])` List of table recognition results, each element is a dictionary.         - `cell_box_list`: `(List[numpy.ndarray])` List of bounding boxes for table cells.         - `pred_html`: `(str)` HTML format string of the table.         - `table_ocr_pred`: `(dict)` OCR recognition result for the table.             - `rec_polys`: `(List[numpy.ndarray])` List of detection boxes for cells.             - `rec_texts`: `(List[str])` Recognition results for cells.             - `rec_scores`: `(List[float])` Recognition confidence scores for cells.             - `rec_boxes`: `(numpy.ndarray)` Array of rectangular bounding boxes for detections, shape (n, 4), dtype int16. Each row represents a rectangle.  - Calling the `save_to_json()` method will save the above content to the specified `save_path`. If a directory is specified, the save path will be `save_path/{your_img_basename}.json`. If a file is specified, it will be saved directly to that file. Since JSON files do not support saving numpy arrays, `numpy.array` types will be converted to list form. - Calling the `save_to_img()` method will save the visualization results to the specified `save_path`. If a directory is specified, the save path will be `save_path/{your_img_basename}_ocr_res_img.{your_img_extension}`. If a file is specified, it will be saved directly to that file. (The pipeline usually contains many result images, so it is not recommended to specify a specific file path directly, otherwise multiple images will be overwritten, and only the last image will be retained).  Additionally, it supports obtaining visualization images with results and prediction results through properties, as follows:  Property Property Description <code>json</code> Gets the prediction results in <code>json</code> format. <code>img</code> Gets the visualization images in <code>dict</code> format.   - The prediction result obtained by the `json` property is dict-type data, and its content is consistent with the content saved by calling the `save_to_json()` method. - The prediction result returned by the `img` property is a dictionary-type data. The keys are `layout_det_res`, `overall_ocr_res`, `text_paragraphs_ocr_res`, `formula_res_region1`, `table_cell_img`, and `seal_res_region1`, and the corresponding values are `Image.Image` objects: used to display visualization images of layout region detection, OCR, OCR text paragraphs, formulas, tables, and seal results, respectively. If optional modules are not used, the dictionary will only contain `layout_det_res`.  (4) Call the <code>build_vector()</code> method of the PP-ChatOCRv4 pipeline object to build vectors for the text content.  The following are the parameters and their descriptions for the `build_vector()` method:   Parameter Parameter Description Parameter Type Default Value <code>visual_info</code> Visual information, can be a dictionary containing visual information, or a list of such dictionaries. <code>list|dict</code> <code>None</code> <code>min_characters</code> Minimum number of characters. A positive integer greater than 0, can be determined based on the token length supported by the large language model. <code>int</code> <code>3500</code> <code>block_size</code> Block size when building a vector library for long text. A positive integer greater than 0, can be determined based on the token length supported by the large language model. <code>int</code> <code>300</code> <code>flag_save_bytes_vector</code> Whether to save text as a binary file. <code>bool</code> <code>False</code> <code>retriever_config</code> Configuration parameters for the vector retrieval large model, same as the parameter during instantiation. <code>dict</code> <code>None</code>  This method returns a dictionary containing visual text information. The content of the dictionary is as follows:  - `flag_save_bytes_vector`: `(bool)` Whether to save the result as a binary file. - `flag_too_short_text`: `(bool)` Whether the text length is less than the minimum number of characters. - `vector`: `(str|list)` Binary content of the text or the text content itself, depending on the values of `flag_save_bytes_vector` and `min_characters`. If `flag_save_bytes_vector=True` and the text length is greater than or equal to the minimum number of characters, it returns binary content; otherwise, it returns the original text.  (5) Call the <code>mllm_pred()</code> method of the PP-ChatOCRv4 pipeline object to get the extraction results from the multimodal large model.  The following are the parameters and their descriptions for the `mllm_pred()` method:   Parameter Parameter Description Parameter Type Default Value <code>input</code> Data to be predicted, supports multiple input types, required. <ul> <li>Python Var: e.g., image data represented by <code>numpy.ndarray</code>; </li> <li>str: e.g., local path of an image file or single-page PDF file: <code>/root/data/img.jpg</code>;URL link, e.g., network URL of an image file or single-page PDF file: Example.</li> </ul> <code>Python Var|str</code> <code>key_list</code> A single key or a list of keys used for extracting information. <code>Union[str, List[str]]</code> <code>None</code> <code>mllm_chat_bot_config</code> Configuration parameters for the multimodal large model, same as the parameter during instantiation. <code>dict</code> <code>None</code> (6) Call the <code>chat()</code> method of the PP-ChatOCRv4 pipeline object to extract key information.  The following are the parameters and their descriptions for the `chat()` method:   Parameter Parameter Description Parameter Type Default Value <code>key_list</code> A single key or a list of keys used for extracting information. <code>Union[str, List[str]]</code> <code>None</code> <code>visual_info</code> Visual information result. <code>List[dict]</code> <code>None</code> <code>use_vector_retrieval</code> Whether to use vector retrieval. <code>bool</code> <code>True</code> <code>vector_info</code> Vector information used for retrieval. <code>dict</code> <code>None</code> <code>min_characters</code> Required minimum number of characters. A positive integer greater than 0. <code>int</code> <code>3500</code> <code>text_task_description</code> Description of the text task. <code>str</code> <code>None</code> <code>text_output_format</code> Output format for text results. <code>str</code> <code>None</code> <code>text_rules_str</code> Rules for generating text results. <code>str</code> <code>None</code> <code>text_few_shot_demo_text_content</code> Text content for few-shot demonstration. <code>str</code> <code>None</code> <code>text_few_shot_demo_key_value_list</code> Key-value list for few-shot demonstration. <code>str</code> <code>None</code> <code>table_task_description</code> Description of the table task. <code>str</code> <code>None</code> <code>table_output_format</code> Output format for table results. <code>str</code> <code>None</code> <code>table_rules_str</code> Rules for generating table results. <code>str</code> <code>None</code> <code>table_few_shot_demo_text_content</code> Text content for table few-shot demonstration. <code>str</code> <code>None</code> <code>table_few_shot_demo_key_value_list</code> Key-value list for table few-shot demonstration. <code>str</code> <code>None</code> <code>mllm_predict_info</code> Multimodal large model result. <code>dict</code> <code>None</code> <code>mllm_integration_strategy</code> Data fusion strategy for multimodal large model and large language model, supports using one of them separately or fusing the results of both. Options: \"integration\", \"llm_only\", and \"mllm_only\". <code>str</code> <code>\"integration\"</code> <code>chat_bot_config</code> Configuration information for the large language model, same as the parameter during instantiation. <code>dict</code> <code>None</code> <code>retriever_config</code> Configuration parameters for the vector retrieval large model, same as the parameter during instantiation. <code>dict</code> <code>None</code>   This method will print the result to the terminal. The content printed to the terminal is explained as follows:   - `chat_res`: `(dict)` The result of information extraction, which is a dictionary containing the keys to be extracted and their corresponding values."},{"location":"en/version3.x/pipeline_usage/PP-ChatOCRv4.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy in production, you can proceed directly with development integration/deployment.</p> <p>If you need to apply the pipeline directly in your Python project, you can refer to the sample code in 2.2  Python Script Experience.</p> <p>Additionally, PaddleX provides two other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for the performance metrics of deployment strategies (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides a high-performance inference plugin aimed at deeply optimizing model inference and pre/post-processing to significantly speed up the end-to-end process. For detailed instructions on high-performance inference, please refer to the High-Performance Inference Guide.</p> <p>\u2601\ufe0f Serving: Serving is a common deployment form in actual production environments. By encapsulating the inference functionality as a service, clients can access these services through network requests to obtain inference results. PaddleX supports multiple serving solutions for pipelines. For detailed instructions on serving, please refer to the Service Deployment Guide.</p> <p>Below are the API references for basic serving and multi-language service invocation examples:</p> API Reference <p>For the main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>Both the request body and response body are JSON data (JSON objects).</li> <li>When the request is successfully processed, the response status code is <code>200</code>, and the response body has the following properties:</li> </ul> Name Type Meaning <code>logId</code> <code>string</code> UUID of the request. <code>errorCode</code> <code>integer</code> Error code. Fixed at <code>0</code>. <code>errorMsg</code> <code>string</code> Error description. Fixed at <code>\"Success\"</code>. <code>result</code> <code>object</code> Operation result. <ul> <li>When the request is not successfully processed, the response body has the following properties:</li> </ul> Name Type Meaning <code>logId</code> <code>string</code> UUID of the request. <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error description. <p>The main operations provided by the service are as follows:</p> <ul> <li><code>analyzeImages</code></li> </ul> <p>Uses computer vision models to analyze images, obtain OCR, table recognition results, etc., and extract key information from the images.</p> <p><code>POST /chatocr-visual</code></p> <ul> <li>Properties of the request body:</li> </ul> Name Type Meaning Required <code>file</code> <code>string</code> URL of an image file or PDF file accessible to the server, or Base64 encoded result of the content of the above file types. By default, for PDF files exceeding 10 pages, only the content of the first 10 pages will be processed. To remove the page limit, please add the following configuration to the pipeline configuration file: <pre><code>Serving:\n  extra:\n    max_num_input_imgs: null\n</code></pre> Yes <code>fileType</code> <code>integer</code> | <code>null</code> File type. <code>0</code> represents a PDF file, <code>1</code> represents an image file. If this property is not present in the request body, the file type will be inferred based on the URL. No <code>useDocOrientationClassify</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>use_doc_orientation_classify</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>useDocUnwarping</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>use_doc_unwarping</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>useSealRecognition</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>use_seal_recognition</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>useTableRecognition</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>use_table_recognition</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>layoutThreshold</code> <code>number</code> | <code>null</code> Please refer to the description of the <code>layout_threshold</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>layoutNms</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>layout_nms</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>layoutUnclipRatio</code> <code>number</code> | <code>array</code> | <code>object</code> | <code>null</code> Please refer to the description of the <code>layout_unclip_ratio</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>layoutMergeBboxesMode</code> <code>string</code> | <code>object</code> | <code>null</code> Please refer to the description of the <code>layout_merge_bboxes_mode</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>textDetLimitSideLen</code> <code>integer</code> | <code>null</code> Please refer to the description of the <code>text_det_limit_side_len</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>textDetLimitType</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>text_det_limit_type</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>textDetThresh</code> <code>number</code> | <code>null</code> Please refer to the description of the <code>text_det_thresh</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>textDetBoxThresh</code> <code>number</code> | <code>null</code> Please refer to the description of the <code>text_det_box_thresh</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>textDetUnclipRatio</code> <code>number</code> | <code>null</code> Please refer to the description of the <code>text_det_unclip_ratio</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>textRecScoreThresh</code> <code>number</code> | <code>null</code> Please refer to the description of the <code>text_rec_score_thresh</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>sealDetLimitSideLen</code> <code>integer</code> | <code>null</code> Please refer to the description of the <code>seal_det_limit_side_len</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>sealDetLimitType</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>seal_det_limit_type</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>sealDetThresh</code> <code>number</code> | <code>null</code> Please refer to the description of the <code>seal_det_thresh</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>sealDetBoxThresh</code> <code>number</code> | <code>null</code> Please refer to the description of the <code>seal_det_box_thresh</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>sealDetUnclipRatio</code> <code>number</code> | <code>null</code> Please refer to the description of the <code>seal_det_unclip_ratio</code> parameter of the pipeline object's <code>visual_predict</code> method. No <code>sealRecScoreThresh</code> <code>number</code> | <code>null</code> Please refer to the description of the <code>seal_rec_score_thresh</code> parameter of the pipeline object's <code>visual_predict</code> method. No <ul> <li>When the request is successfully processed, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Meaning <code>layoutParsingResults</code> <code>array</code> Analysis results obtained using computer vision models. The array length is 1 (for image input) or the actual number of document pages processed (for PDF input). For PDF input, each element in the array represents the result of each page actually processed in the PDF file. <code>visualInfo</code> <code>array</code> Key information in the image, which can be used as input for other operations. <code>dataInfo</code> <code>object</code> Input data information. <p>Each element in <code>layoutParsingResults</code> is an <code>object</code> with the following properties:</p> Name Type Meaning <code>prunedResult</code> <code>object</code> A simplified version of the <code>res</code> field in the JSON representation of the results generated by the pipeline's <code>visual_predict</code> method, with the <code>input_path</code> and the <code>page_index</code> fields removed. <code>outputImages</code> <code>object</code> | <code>null</code> Refer to the description of <code>img</code> attribute of the pipeline's visual prediction result. <code>inputImage</code> <code>string</code> | <code>null</code> Input image. The image is in JPEG format and encoded using Base64. <ul> <li><code>buildVectorStore</code></li> </ul> <p>Builds a vector database.</p> <p><code>POST /chatocr-vector</code></p> <ul> <li>Properties of the request body:</li> </ul> Name Type Meaning Required <code>visualInfo</code> <code>array</code> Key information in the image. Provided by the <code>analyzeImages</code> operation. Yes <code>minCharacters</code> <code>integer</code> | <code>null</code> Minimum data length to enable the vector database. No <code>blockSize</code> <code>int</code> | <code>null</code> Please refer to the description of the <code>block_size</code> parameter of the pipeline object's <code>build_vector</code> method. No <code>retrieverConfig</code> <code>object</code> | <code>null</code> Please refer to the description of the <code>retriever_config</code> parameter of the pipeline object's <code>build_vector</code> method. No <ul> <li>When the request is successfully processed, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Meaning <code>vectorInfo</code> <code>object</code> Serialized result of the vector database, which can be used as input for other operations. <li><code>invokeMLLM</code></li> <p>Invoke the MLLM.</p> <p><code>POST /chatocr-mllm</code></p> <ul> <li>Properties of the request body:</li> </ul> Name Type Meaning Required <code>image</code> <code>string</code> URL of an image file accessible by the server or the Base64-encoded content of the image file. Yes <code>keyList</code> <code>array</code> List of keys. Yes <code>mllmChatBotConfig</code> <code>object</code> | <code>null</code> Please refer to the description of the <code>mllm_chat_bot_config</code> parameter of the pipeline object's <code>mllm_pred</code> method. No <ul> <li>When the request is successfully processed, the <code>result</code> of the response body has the following property:</li> </ul> Name Type Meaning <code>mllmPredictInfo</code> <code>object</code> MLLM invocation result. <ul> <li><code>chat</code></li> </ul> <p>Interacts with large language models to extract key information using them.</p> <p><code>POST /chatocr-chat</code></p> <ul> <li>Properties of the request body:</li> </ul> Name Type Meaning Required <code>keyList</code> <code>array</code> List of keys. Yes <code>visualInfo</code> <code>object</code> Key information in the image. Provided by the <code>analyzeImages</code> operation. Yes <code>useVectorRetrieval</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>use_vector_retrieval</code> parameter of the pipeline object's <code>chat</code> method. No <code>vectorInfo</code> <code>object</code> | <code>null</code> Serialized result of the vector database. Provided by the <code>buildVectorStore</code> operation. Please note that the deserialization process involves performing an unpickle operation. To prevent malicious attacks, be sure to use data from trusted sources. No <code>minCharacters</code> <code>integer</code> Minimum data length to enable the vector database. No <code>textTaskDescription</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>text_task_description</code> parameter of the pipeline object's <code>chat</code> method. No <code>textOutputFormat</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>text_output_format</code> parameter of the pipeline object's <code>chat</code> method. No <code>textRulesStr</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>text_rules_str</code> parameter of the pipeline object's <code>chat</code> method. No <code>textFewShotDemoTextContent</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>text_few_shot_demo_text_content</code> parameter of the pipeline object's <code>chat</code> method. No <code>textFewShotDemoKeyValueList</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>text_few_shot_demo_key_value_list</code> parameter of the pipeline object's <code>chat</code> method. No <code>tableTaskDescription</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>table_task_description</code> parameter of the pipeline object's <code>chat</code> method. No <code>tableOutputFormat</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>table_output_format</code> parameter of the pipeline object's <code>chat</code> method. No <code>tableRulesStr</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>table_rules_str</code> parameter of the pipeline object's <code>chat</code> method. No <code>tableFewShotDemoTextContent</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>table_few_shot_demo_text_content</code> parameter of the pipeline object's <code>chat</code> method. No <code>tableFewShotDemoKeyValueList</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>table_few_shot_demo_key_value_list</code> parameter of the pipeline object's <code>chat</code> method. No <code>mllmPredictInfo</code> <code>object</code> | <code>null</code> MLLM invocation result. Provided by the <code>invokeMllm</code> operation. No <code>mllmIntegrationStrategy</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>mllm_integration_strategy</code> parameter of the pipeline object's <code>chat</code> method. No <code>chatBotConfig</code> <code>object</code> | <code>null</code> Please refer to the description of the <code>chat_bot_config</code> parameter of the pipeline object's <code>chat</code> method. No <code>retrieverConfig</code> <code>object</code> | <code>null</code> Please refer to the description of the <code>retriever_config</code> parameter of the pipeline object's <code>chat</code> method. No <ul> <li>When the request is successfully processed, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Meaning <code>chatResult</code> <code>object</code> Key information extraction result. <li>Note:</li> Including sensitive parameters such as API key for large model calls in the request body can be a security risk. If not necessary, set these parameters in the configuration file and do not pass them on request.  Multi-language Service Invocation Examples Python <pre><code>\n# This script only shows the use case for images. For calling with other file types, please read the API reference and make adjustments.\n\nimport base64\nimport pprint\nimport sys\nimport requests\n\n\nAPI_BASE_URL = \"http://0.0.0.0:8080\"\n\nimage_path = \"./demo.jpg\"\nkeys = [\"name\"]\n\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\n    \"file\": image_data,\n    \"fileType\": 1,\n}\n\nresp_visual = requests.post(url=f\"{API_BASE_URL}/chatocr-visual\", json=payload)\nif resp_visual.status_code != 200:\n    print(\n        f\"Request to chatocr-visual failed with status code {resp_visual.status_code}.\"\n    )\n    pprint.pp(resp_visual.json())\n    sys.exit(1)\nresult_visual = resp_visual.json()[\"result\"]\n\nfor i, res in enumerate(result_visual[\"layoutParsingResults\"]):\n    print(res[\"prunedResult\"])\n    for img_name, img in res[\"outputImages\"].items():\n        img_path = f\"{img_name}_{i}.jpg\"\n        with open(img_path, \"wb\") as f:\n            f.write(base64.b64decode(img))\n        print(f\"Output image saved at {img_path}\")\n\npayload = {\n    \"visualInfo\": result_visual[\"visualInfo\"],\n}\nresp_vector = requests.post(url=f\"{API_BASE_URL}/chatocr-vector\", json=payload)\nif resp_vector.status_code != 200:\n    print(\n        f\"Request to chatocr-vector failed with status code {resp_vector.status_code}.\"\n    )\n    pprint.pp(resp_vector.json())\n    sys.exit(1)\nresult_vector = resp_vector.json()[\"result\"]\n\npayload = {\n    \"image\": image_data,\n    \"keyList\": keys,\n}\nresp_mllm = requests.post(url=f\"{API_BASE_URL}/chatocr-mllm\", json=payload)\nif resp_mllm.status_code != 200:\n    print(\n        f\"Request to chatocr-mllm failed with status code {resp_mllm.status_code}.\"\n    )\n    pprint.pp(resp_mllm.json())\n    sys.exit(1)\nresult_mllm = resp_mllm.json()[\"result\"]\n\npayload = {\n    \"keyList\": keys,\n    \"visualInfo\": result_visual[\"visualInfo\"],\n    \"useVectorRetrieval\": True,\n    \"vectorInfo\": result_vector[\"vectorInfo\"],\n    \"mllmPredictInfo\": result_mllm[\"mllmPredictInfo\"],\n}\nresp_chat = requests.post(url=f\"{API_BASE_URL}/chatocr-chat\", json=payload)\nif resp_chat.status_code != 200:\n    print(\n        f\"Request to chatocr-chat failed with status code {resp_chat.status_code}.\"\n    )\n    pprint.pp(resp_chat.json())\n    sys.exit(1)\nresult_chat = resp_chat.json()[\"result\"]\nprint(\"Final result:\")\nprint(result_chat[\"chatResult\"])\n</code></pre> <p></p>"},{"location":"en/version3.x/pipeline_usage/PP-ChatOCRv4.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the PP-ChatOCRv4 pipeline do not meet your requirements in terms of accuracy or speed, you can try to fine-tune the existing model using your own domain-specific or application-specific data to improve the recognition performance of the PP-ChatOCRv4 pipeline in your scenario.</p>"},{"location":"en/version3.x/pipeline_usage/PP-ChatOCRv4.html#41-model-fine-tuning","title":"4.1 Model Fine-Tuning","text":"<p>Since the PP-ChatOCRv4 pipeline includes several modules, the unsatisfactory performance of the pipeline may originate from any one of these modules. You can analyze the cases with poor extraction results, identify which module is problematic through visualizing the images, and refer to the corresponding fine-tuning tutorial links in the table below to fine-tune the model.</p> Scenario Fine-tuning Module Fine-tuning Reference Link Inaccurate layout region detection, such as missed detection of seals, tables, etc. Layout Region Detection Module Link Inaccurate table structure recognition Table Structure Recognition Module Link Missed detection of seal text Seal Text Detection Module Link Missed detection of text Text Detection Module Link Inaccurate text content Text Recognition Module Link Inaccurate correction of vertical or rotated text lines Text Line Orientation Classification Module Link Inaccurate correction of whole-image rotation Document Image Orientation Classification Module Link Inaccurate correction of image distortion Text Image Correction Module Fine-tuning not supported"},{"location":"en/version3.x/pipeline_usage/PP-ChatOCRv4.html#42-model-application","title":"4.2 Model Application","text":"<p>After you complete fine-tuning with your private dataset, you will obtain a local model weight file.</p> <p>If you need to use the fine-tuned model weights, simply modify the production configuration file by replacing the local directory of the fine-tuned model weights to the corresponding position in the production configuration file:</p> <ol> <li>Exporting Pipeline Configuration Files</li> </ol> <p>You can call the <code>export_paddlex_config_to_yaml</code> method of the pipeline object to export the current pipeline configuration to a YAML file. Here is an example:</p> <pre><code>from paddleocr import PPChatOCRv4\n\npipeline = PPChatOCRv4()\npipeline.export_paddlex_config_to_yaml(\"PP-ChatOCRv4.yaml\")\n</code></pre> <ol> <li>Editing Pipeline Configuration Files</li> </ol> <p>Replace the local directory of the fine-tuned model weights to the corresponding position in the pipeline configuration file. For example:</p> <pre><code>......\nSubModules:\n    TextDetection:\n    module_name: text_detection\n    model_name: PP-OCRv5_server_det\n    model_dir: null # Replace with the fine-tuned text detection model weights directory\n    limit_side_len: 960\n    limit_type: max\n    thresh: 0.3\n    box_thresh: 0.6\n    unclip_ratio: 1.5\n\n    TextRecognition:\n    module_name: text_recognition\n    model_name: PP-OCRv5_server_rec\n    model_dir: null # Replace with the fine-tuned text recognition model weights directory\n        batch_size: 1\n    batch_size: 1\n            score_thresh: 0\n......\n</code></pre> <p>The exported PaddleX pipeline configuration file not only includes parameters supported by PaddleOCR's CLI and Python API but also allows for more advanced settings. Please refer to the corresponding pipeline usage tutorials in PaddleX Pipeline Usage Overview for detailed instructions on adjusting various configurations according to your needs.</p> <ol> <li>Loading Pipeline Configuration Files in CLI</li> </ol> <p>By specifying the path to the PaddleX pipeline configuration file using the <code>--paddlex_config</code> parameter, PaddleOCR will read its contents as the configuration for inference. Here is an example:</p> <pre><code>paddleocr pp_chatocrv4_doc --paddlex_config PP-ChatOCRv4.yaml ...\n</code></pre> <ol> <li>Loading Pipeline Configuration Files in Python API</li> </ol> <p>When initializing the pipeline object, you can pass the path to the PaddleX pipeline configuration file or a configuration dictionary through the <code>paddlex_config</code> parameter, and PaddleOCR will use it as the configuration for inference. Here is an example:</p> <pre><code>from paddleocr import PPChatOCRv4\n\npipeline = PPChatOCRv4(paddlex_config=\"PP-ChatOCRv4.yaml\")\n</code></pre>"},{"location":"en/version3.x/pipeline_usage/PP-StructureV3.html","title":"PP-StructureV3 Pipeline Usage Tutorial","text":""},{"location":"en/version3.x/pipeline_usage/PP-StructureV3.html#1-introduction-to-pp-structurev3-pipeline","title":"1. Introduction to PP-StructureV3 Pipeline","text":"<p>Layout analysis is a technique used to extract structured information from document images. It is primarily used to convert complex document layouts into machine-readable data formats. This technology has broad applications in document management, information extraction, and data digitization. Layout analysis combines Optical Character Recognition (OCR), image processing, and machine learning algorithms to identify and extract text blocks, titles, paragraphs, images, tables, and other layout elements from documents. This process generally includes three main steps: layout analysis, element analysis, and data formatting. The final result is structured document data, which enhances the efficiency and accuracy of data processing. PP-StructureV3 improves upon the general layout analysis v1 pipeline by enhancing layout region detection, table recognition, and formula recognition. It also adds capabilities such as multi-column reading order recovery, chart understanding, and result conversion to Markdown files. It performs excellently across various document types and can handle complex document data.  This pipeline also provides flexible service deployment options, supporting invocation using multiple programming languages on various hardware. In addition, it offers secondary development capabilities, allowing you to train and fine-tune models on your own dataset and integrate the trained models seamlessly.</p> <p>PP-StructureV3 includes the following six modules. Each module can be independently trained and inferred, and contains multiple models. Click the corresponding module for more documentation.</p> <ul> <li>Layout Detection Module</li> <li>General OCR Subline</li> <li>Document Image Preprocessing Subline \uff08Optional\uff09</li> <li>Table Recognition Subline  \uff08Optional\uff09</li> <li>Seal Recognition Subline \uff08Optional\uff09</li> <li>Formula Recognition Subline \uff08Optional\uff09</li> </ul> <p>In this pipeline, you can choose the model to use based on the benchmark data below.</p> Document Image Orientation Classification Module : ModelDownload Link Top-1 Acc (%) GPU Inference Time (ms)[Standard Mode / High Performance Mode] CPU Inference Time (ms)[Standard Mode / High Performance Mode] Model Size (M) Description PP-LCNet_x1_0_doc_oriInference Model/Pretrained Model 99.06 2.31 / 0.43 3.37 / 1.27 7 Document image classification model based on PP-LCNet_x1_0, supporting four categories: 0\u00b0, 90\u00b0, 180\u00b0, 270\u00b0 Text Image Rectification Module: <p>Text Image Rectification Module (Optional):</p> ModelDownload Link CER Model Size (M) Description UVDocInference Model/Pretrained Model 0.179 30.3 M High-precision text image rectification model Layout Detection Module Model: * The layout detection model includes 20 common categories: document title, paragraph title, text, page number, abstract, table, references, footnotes, header, footer, algorithm, formula, formula number, image, table, seal, figure_table title, chart, and sidebar text and lists of references ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-DocLayout_plus-LInference Model/Training Model 83.2 34.6244 / 10.3945 510.57 / -  126.01 M A higher-precision layout area localization model trained on a self-built dataset containing Chinese and English papers, PPT, multi-layout magazines, contracts, books, exams, ancient books and research reports using RT-DETR-L    * The layout detection model includes 1 category: Block: ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-DocBlockLayoutInference Model/Training Model 95.9 34.6244 / 10.3945 510.57 / -  123.92 M A layout block localization model trained on a self-built dataset containing Chinese and English papers, PPT, multi-layout magazines, contracts, books, exams, ancient books and research reports using RT-DETR-L    * The layout detection model includes 23 common categories: document title, paragraph title, text, page number, abstract, table of contents, references, footnotes, header, footer, algorithm, formula, formula number, image, figure caption, table, table caption, seal, figure title, figure, header image, footer image, and sidebar text ModelDownload Link mAP(0.5) (%) GPU Inference Time (ms)[Standard Mode / High Performance Mode] CPU Inference Time (ms)[Standard Mode / High Performance Mode] Model Size (M) Description PP-DocLayout-LInference Model/Pretrained Model 90.4 34.6244 / 10.3945 510.57 / - 123.76 M A high-precision layout area localization model trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using RT-DETR-L. PP-DocLayout-MInference Model/Pretrained Model 75.2 13.3259 / 4.8685 44.0680 / 44.0680 22.578 A layout area localization model with balanced precision and efficiency, trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using PicoDet-L. PP-DocLayout-SInference Model/Pretrained Model 70.9 8.3008 / 2.3794 10.0623 / 9.9296 4.834 A high-efficiency layout area localization model trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using PicoDet-S.   &gt; \u2757 The above list includes the 4 core models that are key supported by the text recognition module. The module actually supports a total of 12 full models, including several predefined models with different categories. The complete model list is as follows:   \ud83d\udc49 Details of Model List  * Table Layout Detection Model ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PicoDet_layout_1x_tableInference Model/Training Model 97.5 8.02 / 3.09 23.70 / 20.41 7.4 M A high-efficiency layout area localization model trained on a self-built dataset using PicoDet-1x, capable of detecting table regions.   * 3-Class Layout Detection Model, including Table, Image, and Stamp ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PicoDet-S_layout_3clsInference Model/Training Model 88.2 8.99 / 2.22 16.11 / 8.73 4.8 A high-efficiency layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-S. PicoDet-L_layout_3clsInference Model/Training Model 89.0 13.05 / 4.50 41.30 / 41.30 22.6 A balanced efficiency and precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-L. <p>Table Classification Module Models:</p> RT-DETR-H_layout_3clsInference Model/Training Model 95.8 114.93 / 27.71 947.56 / 947.56 470.1 A high-precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using RT-DETR-H.   * 5-Class English Document Area Detection Model, including Text, Title, Table, Image, and List ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PicoDet_layout_1xInference Model/Training Model 97.8 9.03 / 3.10 25.82 / 20.70 7.4 A high-efficiency English document layout area localization model trained on the PubLayNet dataset using PicoDet-1x.   * 17-Class Area Detection Model, including 17 common layout categories: Paragraph Title, Image, Text, Number, Abstract, Content, Figure Caption, Formula, Table, Table Caption, References, Document Title, Footnote, Header, Algorithm, Footer, and Stamp ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PicoDet-S_layout_17clsInference Model/Training Model 87.4 9.11 / 2.12 15.42 / 9.12 4.8 A high-efficiency layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-S. PicoDet-L_layout_17clsInference Model/Training Model 89.0 13.50 / 4.69 43.32 / 43.32 22.6 A balanced efficiency and precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-L. RT-DETR-H_layout_17clsInference Model/Training Model 98.3 115.29 / 104.09 995.27 / 995.27 470.2 A high-precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using RT-DETR-H. Table Structure Recognition Module (Optional): ModelDownload Link mAP (%) GPU Inference Time (ms)[Standard Mode / High Performance Mode] CPU Inference Time (ms)[Standard Mode / High Performance Mode] Model Size (M) Description RT-DETR-L_wired_table_cell_det Inference Model/Pretrained Model 82.7 35.00 / 10.45 495.51 / 495.51 124M RT-DETR is the first real-time end-to-end object detection model. Based on RT-DETR-L, the PaddlePaddle Vision Team pre-trained the model on a custom table cell detection dataset, achieving good performance for both wired and wireless tables. RT-DETR-L_wireless_table_cell_det Inference Model/Pretrained Model Text Detection Module (Required): ModelDownload Link Detection Hmean (%) GPU Inference Time (ms)[Standard Mode / High Performance Mode] CPU Inference Time (ms)[Standard Mode / High Performance Mode] Model Size (M) Description PP-OCRv4_server_detInference Model/Training Model 82.56 83.34 / 80.91 442.58 / 442.58 109 The server-side text detection model of PP-OCRv4, with higher accuracy, suitable for deployment on high-performance servers. PP-OCRv4_mobile_detInference Model/Training Model 77.35 8.79 / 3.13 51.00 / 28.58 4.7 The mobile text detection model of PP-OCRv4, with higher efficiency, suitable for deployment on edge devices. PP-OCRv3_mobile_detInference Model/Training Model 78.68 8.44 / 2.91 27.87 / 27.87 2.1 The mobile text detection model of PP-OCRv3, with higher efficiency, suitable for deployment on edge devices. PP-OCRv3_server_detInference Model/Training Model 80.11 65.41 / 13.67 305.07 / 305.07 102.1 The server-side text detection model of PP-OCRv3, with higher accuracy, suitable for deployment on high-performance servers. Text Recognition Module Model (Required):  \ud83d\udc49Full Model List  * PP-OCRv5 Multi-Scenario Models ModelDownload Link Chinese Avg Accuracy (%) English Avg Accuracy (%) Traditional Chinese Avg Accuracy (%) Japanese Avg Accuracy (%) GPU Inference Time (ms)[Standard Mode / High Performance Mode] CPU Inference Time (ms)[Standard Mode / High Performance Mode] Model Size (M) Description PP-OCRv5_server_recInference Model/Pretrained Model 86.38 64.70 93.29 60.35  -   -  205 M PP-OCRv5_server_rec is a new-generation text recognition model. It efficiently and accurately supports four major languages: Simplified Chinese, Traditional Chinese, English, and Japanese, as well as handwriting, vertical text, pinyin, and rare characters, offering robust and efficient support for document understanding. PP-OCRv5_mobile_recInference Model/Pretrained Model 81.29 66.00 83.55 54.65  -   -  136 M PP-OCRv5_mobile_rec is a new-generation text recognition model. It efficiently and accurately supports four major languages: Simplified Chinese, Traditional Chinese, English, and Japanese, as well as handwriting, vertical text, pinyin, and rare characters, offering robust and efficient support for document understanding.   * Chinese Recognition Models ModelDownload Link Avg Accuracy (%) GPU Inference Time (ms)[Standard Mode / High Performance Mode] CPU Inference Time (ms)[Standard Mode / High Performance Mode] Model Size (M) Description PP-OCRv4_server_rec_docInference Model/Pretrained Model 86.58 6.65 / 2.38 32.92 / 32.92 91 M Based on PP-OCRv4_server_rec, trained on additional Chinese documents and PP-OCR mixed data. It supports over 15,000 characters including Traditional Chinese, Japanese, and special symbols, enhancing both document-specific and general text recognition accuracy. PP-OCRv4_mobile_recInference Model/Pretrained Model 83.28 4.82 / 1.20 16.74 / 4.64 11 M Lightweight model of PP-OCRv4 with high inference efficiency, suitable for deployment on various edge devices. PP-OCRv4_server_recInference Model/Pretrained Model 85.19 6.58 / 2.43 33.17 / 33.17 87 M Server-side model of PP-OCRv4 with high recognition accuracy, suitable for deployment on various servers. PP-OCRv3_mobile_recInference Model/Pretrained Model 75.43 5.87 / 1.19 9.07 / 4.28 11 M Lightweight model of PP-OCRv3 with high inference efficiency, suitable for deployment on various edge devices. ModelDownload Link Avg Accuracy (%) GPU Inference Time (ms)[Standard Mode / High Performance Mode] CPU Inference Time (ms)[Standard Mode / High Performance Mode] Model Size (M) Description ch_SVTRv2_recInference Model/Pretrained Model 68.81 8.08 / 2.74 50.17 / 42.50 73.9 M SVTRv2 is a server-side recognition model developed by the OpenOCR team at Fudan University\u2019s FVL Lab. It won first place in the OCR End-to-End Recognition task of the PaddleOCR Model Challenge, improving end-to-end accuracy on Benchmark A by 6% compared to PP-OCRv4. ModelDownload Link Avg Accuracy (%) GPU Inference Time (ms)[Standard Mode / High Performance Mode] CPU Inference Time (ms)[Standard Mode / High Performance Mode] Model Size (M) Description ch_RepSVTR_recInference Model/Pretrained Model 65.07 5.93 / 1.62 20.73 / 7.32 22.1 M RepSVTR is a mobile text recognition model based on SVTRv2. It won first place in the OCR End-to-End Recognition task of the PaddleOCR Model Challenge, improving accuracy on Benchmark B by 2.5% over PP-OCRv4 with comparable inference speed.   * English Recognition Models ModelDownload Link Avg Accuracy (%) GPU Inference Time (ms)[Standard Mode / High Performance Mode] CPU Inference Time (ms)[Standard Mode / High Performance Mode] Model Size (M) Description en_PP-OCRv4_mobile_recInference Model/Pretrained Model 70.39 4.81 / 0.75 16.10 / 5.31 6.8 M Ultra-lightweight English recognition model trained on PP-OCRv4, supporting English and number recognition. en_PP-OCRv3_mobile_recInference Model/Pretrained Model 70.69 5.44 / 0.75 8.65 / 5.57 7.8 M Ultra-lightweight English recognition model trained on PP-OCRv3, supporting English and number recognition.          * Multilingual Recognition Models ModelModel Download Link Recognition Avg Accuracy(%) GPU Inference Time (ms)[Normal / High Performance] CPU Inference Time (ms)[Normal / High Performance] Model Size (M) Description korean_PP-OCRv3_mobile_recInference Model/Pretrained Model 60.21 5.40 / 0.97 9.11 / 4.05 8.6 M An ultra-lightweight Korean text recognition model trained based on PP-OCRv3, supporting Korean and digits recognition japan_PP-OCRv3_mobile_recInference Model/Pretrained Model 45.69 5.70 / 1.02 8.48 / 4.07 8.8 M  An ultra-lightweight Japanese text recognition model trained based on PP-OCRv3, supporting Japanese and digits recognition chinese_cht_PP-OCRv3_mobile_recInference Model/Pretrained Model 82.06 5.90 / 1.28 9.28 / 4.34 9.7 M  An ultra-lightweight Traditional Chinese text recognition model trained based on PP-OCRv3, supporting Traditional Chinese and digits recognition te_PP-OCRv3_mobile_recInference Model/Pretrained Model 95.88 5.42 / 0.82 8.10 / 6.91 7.8 M  An ultra-lightweight Telugu text recognition model trained based on PP-OCRv3, supporting Telugu and digits recognition ka_PP-OCRv3_mobile_recInference Model/Pretrained Model 96.96 5.25 / 0.79 9.09 / 3.86 8.0 M  An ultra-lightweight Kannada text recognition model trained based on PP-OCRv3, supporting Kannada and digits recognition ta_PP-OCRv3_mobile_recInference Model/Pretrained Model 76.83 5.23 / 0.75 10.13 / 4.30 8.0 M  An ultra-lightweight Tamil text recognition model trained based on PP-OCRv3, supporting Tamil and digits recognition latin_PP-OCRv3_mobile_recInference Model/Pretrained Model 76.93 5.20 / 0.79 8.83 / 7.15 7.8 M An ultra-lightweight Latin text recognition model trained based on PP-OCRv3, supporting Latin and digits recognition arabic_PP-OCRv3_mobile_recInference Model/Pretrained Model 73.55 5.35 / 0.79 8.80 / 4.56 7.8 M An ultra-lightweight Arabic script recognition model trained based on PP-OCRv3, supporting Arabic script and digits recognition cyrillic_PP-OCRv3_mobile_recInference Model/Pretrained Model 94.28 5.23 / 0.76 8.89 / 3.88 7.9 M   An ultra-lightweight Cyrillic script recognition model trained based on PP-OCRv3, supporting Cyrillic script and digits recognition devanagari_PP-OCRv3_mobile_recInference Model/Pretrained Model 96.44 5.22 / 0.79 8.56 / 4.06 7.9 M An ultra-lightweight Devanagari script recognition model trained based on PP-OCRv3, supporting Devanagari script and digits recognition Text Line Orientation Classification Module (Optional): Model Model Download Link Top-1 Acc (%) GPU Inference Time (ms)[Normal / High Performance] CPU Inference Time (ms)[Normal / High Performance] Model Size (M) Description PP-LCNet_x0_25_textline_oriInference Model/Pretrained Model 95.54 - - 0.32 A text line classification model based on PP-LCNet_x0_25, containing two categories: 0 degrees and 180 degrees Formula Recognition Module (Optional): ModelModel Download Link En-BLEU(%) Zh-BLEU(%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction UniMERNetInference Model/Training Model 85.91 43.50 2266.96/- -/- 1.53 G UniMERNet is a formula recognition model developed by Shanghai AI Lab. It uses Donut Swin as the encoder and MBartDecoder as the decoder. The model is trained on a dataset of one million samples, including simple formulas, complex formulas, scanned formulas, and handwritten formulas, significantly improving the recognition accuracy of real-world formulas. PP-FormulaNet-SInference Model/Training Model 87.00 45.71 202.25/- -/- 224 M PP-FormulaNet is an advanced formula recognition model developed by the Baidu PaddlePaddle Vision Team. The PP-FormulaNet-S version uses PP-HGNetV2-B4 as its backbone network. Through parallel masking and model distillation techniques, it significantly improves inference speed while maintaining high recognition accuracy, making it suitable for applications requiring fast inference. The PP-FormulaNet-L version, on the other hand, uses Vary_VIT_B as its backbone network and is trained on a large-scale formula dataset, showing significant improvements in recognizing complex formulas compared to PP-FormulaNet-S. PP-FormulaNet-LInference Model/Training Model 90.36 45.78 1976.52/- -/- 695 M PP-FormulaNet_plus-SInference Model/Training Model 88.71 53.32 191.69/- -/- 248 M PP-FormulaNet_plus is an enhanced version of the formula recognition model developed by the Baidu PaddlePaddle Vision Team, building upon the original PP-FormulaNet. Compared to the original version, PP-FormulaNet_plus utilizes a more diverse formula dataset during training, including sources such as Chinese dissertations, professional books, textbooks, exam papers, and mathematics journals. This expansion significantly improves the model\u2019s recognition capabilities. Among the models, PP-FormulaNet_plus-M and PP-FormulaNet_plus-L have added support for Chinese formulas and increased the maximum number of predicted tokens for formulas from 1,024 to 2,560, greatly enhancing the recognition performance for complex formulas. Meanwhile, the PP-FormulaNet_plus-S model focuses on improving the recognition of English formulas. With these improvements, the PP-FormulaNet_plus series models perform exceptionally well in handling complex and diverse formula recognition tasks.  PP-FormulaNet_plus-MInference Model/Training Model 91.45 89.76 1301.56/- -/- 592 M PP-FormulaNet_plus-LInference Model/Training Model 92.22 90.64 1745.25/- -/- 698 M LaTeX_OCR_recInference Model/Training Model 74.55 39.96 1244.61/- -/- 99 M LaTeX-OCR is a formula recognition algorithm based on an autoregressive large model. It uses Hybrid ViT as the backbone network and a transformer as the decoder, significantly improving the accuracy of formula recognition. Seal Text Detection Module (Optional): ModelModel Download Link Detection Hmean (%) GPU Inference Time (ms)[Normal / High Performance] CPU Inference Time (ms)[Normal / High Performance] Model Size (M) Description PP-OCRv4_server_seal_detInference Model/Pretrained Model 98.21 74.75 / 67.72 382.55 / 382.55 109 Server-side seal text detection model based on PP-OCRv4, offering higher accuracy and suitable for deployment on high-performance servers PP-OCRv4_mobile_seal_detInference Model/Pretrained Model 96.47 7.82 / 3.09 48.28 / 23.97 4.6 Mobile-side seal text detection model based on PP-OCRv4, offering higher efficiency and suitable for edge-side deployment Chart Parsing Model Module:  ModelModel Download Link Model parameter size\uff08B\uff09 Model Storage Size (GB) Model Score  Description PP-Chart2TableInference Model 0.58 1.4 75.98 PP-Chart2Table is a self-developed multimodal model by the PaddlePaddle team, focusing on chart parsing, demonstrating outstanding performance in both Chinese and English chart parsing tasks. The team adopted a carefully designed data generation strategy, constructing a high-quality multimodal dataset of nearly 700,000 entries covering common chart types like pie charts, bar charts, stacked area charts, and various application scenarios. They also designed a two-stage training method, utilizing large model distillation to fully leverage massive unlabeled OOD data. In internal business tests in both Chinese and English scenarios, PP-Chart2Table not only achieved the SOTA level among models of the same parameter scale but also reached accuracy comparable to 7B parameter scale VLM models in critical scenarios. Test Environment Description: <ul> <li>Performance Test Environment <ul> <li>Test Dataset:  <ul> <li>Document Image Orientation Classification Module: A self-built dataset using PaddleX, covering multiple scenarios such as ID cards and documents, containing 1000 images.</li> <li>Text Image Rectification Model: DocUNet</li> <li>Layout Region Detection Model: A self-built layout detection dataset using PaddleOCR, containing 10,000 images of common document types such as Chinese and English papers, magazines, and research reports.</li> <li>Table Structure Recognition Model: A self-built English table recognition dataset using PaddleX.</li> <li>Text Detection Model: A self-built Chinese dataset using PaddleOCR, covering multiple scenarios such as street scenes, web images, documents, and handwriting, with 500 images for detection.</li> <li>Chinese Recognition Model: A self-built Chinese dataset using PaddleOCR, covering multiple scenarios such as street scenes, web images, documents, and handwriting, with 11,000 images for text recognition.</li> <li>ch_SVTRv2_rec: Evaluation set A for \"OCR End-to-End Recognition Task\" in the PaddleOCR Algorithm Model Challenge</li> <li>ch_RepSVTR_rec: Evaluation set B for \"OCR End-to-End Recognition Task\" in the PaddleOCR Algorithm Model Challenge.</li> <li>English Recognition Model: A self-built English dataset using PaddleX.</li> <li>Multilingual Recognition Model: A self-built multilingual dataset using PaddleX.</li> <li>Text Line Orientation Classification Model: A self-built dataset using PaddleX, covering various scenarios such as ID cards and documents, containing 1000 images.</li> <li>Seal Text Detection Model: A self-built dataset using PaddleX, containing 500 images of circular seal textures.</li> </ul> </li> <li>Hardware Configuration:  <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environments: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Description</li> </ul> Mode GPU Configuration  CPU Configuration  Acceleration Technology Combination Normal Mode FP32 Precision / No TRT Acceleration FP32 Precision / 8 Threads PaddleInference High-Performance Mode Optimal combination of pre-selected precision types and acceleration strategies FP32 Precision / 8 Threads Pre-selected optimal backend (Paddle/OpenVINO/TRT, etc.)"},{"location":"en/version3.x/pipeline_usage/PP-StructureV3.html#2-quick-start","title":"2. Quick Start","text":"<p>All the model pipelines provided by PaddleX can be quickly experienced. You can use the command line or Python on your local machine to experience the effect of the PP-StructureV3 pipeline.</p> <p>Before using the PP-StructureV3 pipeline locally, please ensure that you have completed the installation of the PaddleX wheel package according to the PaddleOCR Local Installation Guide. If you wish to selectively install dependencies, please refer to the relevant instructions in the installation guide. The dependency group corresponding to this pipeline is <code>ocr</code>.</p> <p>When performing GPU inference, the default configuration may use more than 16\u202fGB of VRAM. Please ensure that your GPU has sufficient memory. To reduce VRAM usage, you can modify the configuration file as described below to disable unnecessary features.</p>"},{"location":"en/version3.x/pipeline_usage/PP-StructureV3.html#21-experiencing-via-command-line","title":"2.1 Experiencing via Command Line","text":"<p>You can quickly experience the PP-StructureV3 pipeline with a single command.</p> <pre><code>paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png\n\npaddleocr pp_structurev3 -i ./pp_structure_v3_demo.png --use_doc_orientation_classify True\n\npaddleocr pp_structurev3 -i ./pp_structure_v3_demo.png --use_doc_unwarping True\n\npaddleocr pp_structurev3 -i ./pp_structure_v3_demo.png --use_textline_orientation False\n\npaddleocr pp_structurev3 -i ./pp_structure_v3_demo.png --device gpu\n</code></pre> <p>The parameter description can be found in 2.2 Python Script Integration. Supports specifying multiple devices simultaneously for parallel inference. For details, please refer to Pipeline Parallel Inference.</p> <p>After running, the result will be printed to the terminal, as follows:</p> \ud83d\udc49Click to Expand <pre><code>\n\n{'res': {'input_path': 'pp_structure_v3_demo.png', 'model_settings': {'use_doc_preprocessor': False, 'use_general_ocr': True, 'use_seal_recognition': True, 'use_table_recognition': True, 'use_formula_recognition': True}, 'layout_det_res': {'input_path': None, 'page_index': None, 'boxes': [{'cls_id': 2, 'label': 'text', 'score': 0.9853514432907104, 'coordinate': [770.9531, 776.6814, 1122.6057, 1058.7322]}, {'cls_id': 1, 'label': 'image', 'score': 0.9848673939704895, 'coordinate': [775.7434, 202.27979, 1502.8113, 686.02136]}, {'cls_id': 2, 'label': 'text', 'score': 0.983731746673584, 'coordinate': [1152.3197, 1113.3275, 1503.3029, 1346.586]}, {'cls_id': 2, 'label': 'text', 'score': 0.9832221865653992, 'coordinate': [1152.5602, 801.431, 1503.8436, 986.3563]}, {'cls_id': 2, 'label': 'text', 'score': 0.9829439520835876, 'coordinate': [9.549545, 849.5713, 359.1173, 1058.7488]}, {'cls_id': 2, 'label': 'text', 'score': 0.9811657667160034, 'coordinate': [389.58298, 1137.2659, 740.66235, 1346.7488]}, {'cls_id': 2, 'label': 'text', 'score': 0.9775941371917725, 'coordinate': [9.1302185, 201.85, 359.0409, 339.05692]}, {'cls_id': 2, 'label': 'text', 'score': 0.9750366806983948, 'coordinate': [389.71454, 752.96924, 740.544, 889.92456]}, {'cls_id': 2, 'label': 'text', 'score': 0.9738152027130127, 'coordinate': [389.94565, 298.55988, 740.5585, 435.5124]}, {'cls_id': 2, 'label': 'text', 'score': 0.9737328290939331, 'coordinate': [771.50256, 1065.4697, 1122.2582, 1178.7324]}, {'cls_id': 2, 'label': 'text', 'score': 0.9728517532348633, 'coordinate': [1152.5154, 993.3312, 1503.2349, 1106.327]}, {'cls_id': 2, 'label': 'text', 'score': 0.9725610017776489, 'coordinate': [9.372787, 1185.823, 359.31738, 1298.7227]}, {'cls_id': 2, 'label': 'text', 'score': 0.9724331498146057, 'coordinate': [389.62848, 610.7389, 740.83234, 746.2377]}, {'cls_id': 2, 'label': 'text', 'score': 0.9720287322998047, 'coordinate': [389.29898, 897.0936, 741.41516, 1034.6616]}, {'cls_id': 2, 'label': 'text', 'score': 0.9713053703308105, 'coordinate': [10.323685, 1065.4663, 359.6786, 1178.8872]}, {'cls_id': 2, 'label': 'text', 'score': 0.9689728021621704, 'coordinate': [9.336395, 537.6609, 359.2901, 652.1881]}, {'cls_id': 2, 'label': 'text', 'score': 0.9684857130050659, 'coordinate': [10.7608185, 345.95068, 358.93616, 434.64087]}, {'cls_id': 2, 'label': 'text', 'score': 0.9681928753852844, 'coordinate': [9.674866, 658.89075, 359.56528, 770.4319]}, {'cls_id': 2, 'label': 'text', 'score': 0.9634978175163269, 'coordinate': [770.9464, 1281.1785, 1122.6522, 1346.7156]}, {'cls_id': 2, 'label': 'text', 'score': 0.96304851770401, 'coordinate': [390.0113, 201.28055, 740.1684, 291.53073]}, {'cls_id': 2, 'label': 'text', 'score': 0.962053120136261, 'coordinate': [391.21393, 1040.952, 740.5046, 1130.32]}, {'cls_id': 2, 'label': 'text', 'score': 0.9565253853797913, 'coordinate': [10.113251, 777.1482, 359.439, 842.437]}, {'cls_id': 2, 'label': 'text', 'score': 0.9497362375259399, 'coordinate': [390.31357, 537.86285, 740.47595, 603.9285]}, {'cls_id': 2, 'label': 'text', 'score': 0.9371236562728882, 'coordinate': [10.2034, 1305.9753, 359.5958, 1346.7295]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.9338151216506958, 'coordinate': [791.6062, 1200.8479, 1103.3257, 1259.9324]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.9326773285865784, 'coordinate': [408.0737, 457.37024, 718.9509, 516.63464]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.9274250864982605, 'coordinate': [29.448685, 456.6762, 340.99194, 515.6999]}, {'cls_id': 2, 'label': 'text', 'score': 0.8742568492889404, 'coordinate': [1154.7095, 777.3624, 1330.3086, 794.5853]}, {'cls_id': 2, 'label': 'text', 'score': 0.8442489504814148, 'coordinate': [586.49316, 160.15454, 927.468, 179.64203]}, {'cls_id': 11, 'label': 'doc_title', 'score': 0.8332607746124268, 'coordinate': [133.80017, 37.41908, 1380.8601, 124.1429]}, {'cls_id': 6, 'label': 'figure_title', 'score': 0.6770150661468506, 'coordinate': [812.1718, 705.1199, 1484.6973, 747.1692]}]}, 'overall_ocr_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_preprocessor': False, 'use_textline_orientation': False}, 'dt_polys': array([[[ 133,   35],\n        ...,\n        [ 133,  131]],\n\n       ...,\n\n       [[1154, 1323],\n        ...,\n        [1152, 1355]]], dtype=int16), 'text_det_params': {'limit_side_len': 960, 'limit_type': 'max', 'thresh': 0.3, 'box_thresh': 0.6, 'unclip_ratio': 2.0}, 'text_type': 'general', 'textline_orientation_angles': array([-1, ..., -1]), 'text_rec_score_thresh': 0.0, 'rec_texts': ['\u200b\u52a9\u529b\u200b\u53cc\u65b9\u200b\u4ea4\u5f80\u200b', '\u200b\u642d\u5efa\u200b\u53cb\u8c0a\u200b\u6865\u6881\u200b', '\u200b\u672c\u62a5\u8bb0\u8005\u200b', '\u200b\u6c88\u5c0f\u6653\u200b', '\u200b\u4efb\u200b', '\u200b\u5f66\u200b', '\u200b\u9ec4\u57f9\u662d\u200b', '\u200b\u8eab\u7740\u200b\u4e2d\u56fd\u200b\u4f20\u7edf\u200b\u6c11\u65cf\u670d\u88c5\u200b\u7684\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u9752\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u9ad8\u7b49\u6559\u80b2\u200b\u4e0e\u200b\u7814\u7a76\u9662\u200b\u5408\u4f5c\u200b\u5efa\u7acb\u200b\uff0c\u200b\u5f00\u200b', '\u200b\u5e74\u200b\u4f9d\u6b21\u200b\u767b\u53f0\u200b\u8868\u6f14\u200b\u4e2d\u56fd\u200b\u6c11\u65cf\u821e\u200b\u3001\u200b\u73b0\u4ee3\u821e\u200b\u3001\u200b\u6247\u5b50\u821e\u200b', '\u200b\u8bbe\u200b\u4e86\u200b\u4e2d\u56fd\u200b\u8bed\u8a00\u200b\u8bfe\u7a0b\u200b\u548c\u200b\u4e2d\u56fd\u200b\u6587\u5316\u200b\u8bfe\u7a0b\u200b\uff0c\u200b\u6ce8\u518c\u200b\u5b66\u200b', '\u200b\u7b49\u200b,\u200b\u66fc\u5999\u200b\u7684\u200b\u821e\u59ff\u200b\u8d62\u5f97\u200b\u73b0\u573a\u200b\u89c2\u4f17\u200b\u9635\u9635\u200b\u638c\u58f0\u200b\u3002\u200b\u8fd9\u200b', '\u200b\u751f\u200b2\u200b\u4e07\u4f59\u4eba\u6b21\u200b\u300210\u200b\u4f59\u5e74\u200b\u6765\u200b\uff0c\u200b\u5384\u7279\u200b\u5b54\u9662\u200b\u5df2\u200b\u6210\u4e3a\u200b', '\u200b\u662f\u200b\u65e5\u524d\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u9ad8\u7b49\u6559\u80b2\u200b\u4e0e\u200b\u7814\u7a76\u9662\u200b\u5b54\u5b50\u200b\u5b66\u200b', '\u200b\u5f53\u5730\u200b\u6c11\u4f17\u200b\u4e86\u89e3\u200b\u4e2d\u56fd\u200b\u7684\u200b\u4e00\u6247\u200b\u7a97\u53e3\u200b\u3002', '\u200b\u9662\u200b(\u200b\u4ee5\u4e0b\u200b\u7b80\u79f0\u200b\"\u200b\u5384\u7279\u200b\u5b54\u9662\u200b\")\u200b\u4e3e\u529e\u200b\"\u200b\u559c\u8fce\u200b\u65b0\u5e74\u200b\"\u200b\u4e2d\u56fd\u200b', '\u200b\u9ec4\u9e23\u98de\u200b\u8868\u793a\u200b\uff0c\u200b\u968f\u7740\u200b\u6765\u200b\u5b66\u4e60\u200b\u4e2d\u6587\u200b\u7684\u200b\u4eba\u200b\u65e5\u76ca\u200b', '\u200b\u6b4c\u821e\u200b\u6bd4\u8d5b\u200b\u7684\u200b\u573a\u666f\u200b\u3002', '\u200b\u589e\u591a\u200b\uff0c\u200b\u963f\u65af\u9a6c\u62c9\u200b\u5927\u5b66\u200b\u6559\u5b66\u70b9\u200b\u5df2\u200b\u96be\u4ee5\u200b\u6ee1\u8db3\u200b\u6559\u5b66\u200b', '\u200b\u4e2d\u56fd\u200b\u548c\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u4f20\u7edf\u53cb\u8c0a\u200b\u6df1\u539a\u200b\u3002\u200b\u8fd1\u5e74\u200b', '\u200b\u9700\u8981\u200b\u30022024\u200b\u5e74\u200b4\u200b\u6708\u200b\uff0c\u200b\u7531\u200b\u4e2d\u4f01\u200b\u8700\u9053\u200b\u96c6\u56e2\u200b\u6240\u5c5e\u200b\u56db\u200b', '\u200b\u6765\u200b,\u200b\u5728\u200b\u9ad8\u8d28\u91cf\u200b\u5171\u5efa\u200b\"\u200b\u4e00\u5e26\u200b\u4e00\u8def\u200b\"\u200b\u6846\u67b6\u200b\u4e0b\u200b\uff0c\u200b\u4e2d\u200b\u5384\u200b\u4e24\u200b', '\u200b\u5ddd\u200b\u8def\u6865\u200b\u627f\u5efa\u200b\u7684\u200b\u5b54\u9662\u200b\u6559\u5b66\u697c\u200b\u9879\u76ee\u200b\u5728\u200b\u963f\u65af\u9a6c\u62c9\u200b\u5f00\u200b', '\u200b\u56fd\u200b\u4eba\u6587\u200b\u4ea4\u6d41\u200b\u4e0d\u65ad\u200b\u6df1\u5316\u200b\uff0c\u200b\u4e92\u5229\u200b\u5408\u4f5c\u200b\u7684\u200b\u6c11\u610f\u57fa\u7840\u200b', '\u200b\u5de5\u200b\u5efa\u8bbe\u200b\uff0c\u200b\u9884\u8ba1\u200b\u4eca\u5e74\u200b\u4e0a\u534a\u5e74\u200b\u5cfb\u5de5\u200b\uff0c\u200b\u5efa\u6210\u200b\u540e\u200b\u5c06\u200b\u4e3a\u200b\u5384\u200b', '\u200b\u65e5\u76ca\u200b\u6df1\u539a\u200b\u3002', '\u200b\u7279\u5b54\u9662\u200b\u63d0\u4f9b\u200b\u5168\u65b0\u200b\u7684\u200b\u529e\u5b66\u200b\u573a\u5730\u200b\u3002', '\u201c\u200b\u5b66\u597d\u200b\u4e2d\u6587\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b', '\u201c\u200b\u5728\u200b\u4e2d\u56fd\u200b\u5b66\u4e60\u200b\u7684\u200b\u7ecf\u5386\u200b', '\u200b\u672a\u6765\u200b\u4e0d\u662f\u200b\u68a6\u200b\u201d', '\u200b\u8ba9\u200b\u6211\u200b\u770b\u5230\u200b\u66f4\u200b\u5e7f\u9614\u200b\u7684\u200b\u4e16\u754c\u200b\u201d', '\u201c\u200b\u9c9c\u82b1\u200b\u66fe\u200b\u544a\u8bc9\u200b\u6211\u200b\u4f60\u200b\u600e\u6837\u200b\u8d70\u8fc7\u200b\uff0c\u200b\u5927\u5730\u200b\u77e5\u9053\u200b\u4f60\u200b', '\u200b\u591a\u5e74\u200b\u6765\u200b\uff0c\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5e7f\u5927\u200b\u8d74\u534e\u200b\u7559\u5b66\u751f\u200b\u548c\u200b', '\u200b\u5fc3\u4e2d\u200b\u7684\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u89d2\u843d\u200b\u2026\"\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u963f\u65af\u9a6c\u62c9\u200b', '\u200b\u57f9\u8bad\u200b\u4eba\u5458\u200b\u79ef\u6781\u200b\u6295\u8eab\u200b\u56fd\u5bb6\u200b\u5efa\u8bbe\u200b\uff0c\u200b\u6210\u4e3a\u200b\u52a9\u529b\u200b\u8be5\u56fd\u200b', '\u200b\u5927\u5b66\u200b\u7efc\u5408\u697c\u200b\u4e8c\u5c42\u200b\uff0c\u200b\u4e00\u9635\u200b\u4f18\u7f8e\u200b\u7684\u200b\u6b4c\u58f0\u200b\u5728\u200b\u8d70\u5eca\u200b\u91cc\u200b\u56de\u200b', '\u200b\u53d1\u5c55\u200b\u7684\u200b\u4eba\u624d\u200b\u548c\u200b\u5384\u200b\u4e2d\u200b\u53cb\u597d\u200b\u7684\u200b\u89c1\u8bc1\u8005\u200b\u548c\u200b\u63a8\u52a8\u8005\u200b\u3002', '\u200b\u54cd\u200b\u3002\u200b\u5faa\u7740\u200b\u719f\u6089\u200b\u7684\u200b\u65cb\u5f8b\u200b\u8f7b\u8f7b\u200b\u63a8\u5f00\u200b\u4e00\u95f4\u200b\u6559\u5ba4\u200b\u7684\u200b\u95e8\u200b\uff0c', '\u200b\u5728\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5168\u56fd\u200b\u5987\u5973\u200b\u8054\u76df\u200b\u5de5\u4f5c\u200b\u7684\u200b\u7ea6\u7ff0\u200b', '\u200b\u5b66\u751f\u200b\u4eec\u200b\u6b63\u200b\u8ddf\u7740\u200b\u8001\u5e08\u200b\u5b66\u5531\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\u300a\u200b\u540c\u4e00\u9996\u6b4c\u200b\u300b\u3002', '\u200b\u5a1c\u200b\u00b7\u200b\u7279\u200b\u97e6\u5c14\u5fb7\u200b\u00b7\u200b\u51ef\u83b1\u200b\u5854\u200b\u5c31\u662f\u200b\u5176\u4e2d\u200b\u4e00\u4f4d\u200b\u3002\u200b\u5979\u200b\u66fe\u200b\u5728\u200b', '\u200b\u8fd9\u200b\u662f\u200b\u5384\u200b\u7279\u5b54\u9662\u200b\u963f\u65af\u9a6c\u62c9\u200b\u5927\u5b66\u200b\u6559\u5b66\u70b9\u200b\u7684\u200b\u4e00\u200b', '\u200b\u4e2d\u534e\u200b\u5973\u5b50\u200b\u5b66\u9662\u200b\u653b\u8bfb\u200b\u7855\u58eb\u5b66\u4f4d\u200b\uff0c\u200b\u7814\u7a76\u200b\u65b9\u5411\u200b\u662f\u200b\u5973\u200b', '\u200b\u8282\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\u8bfe\u200b\u3002\u200b\u4e3a\u4e86\u200b\u8ba9\u200b\u5b66\u751f\u200b\u4eec\u200b\u66f4\u597d\u200b\u5730\u200b\u7406\u89e3\u200b\u6b4c\u200b', '\u200b\u6027\u200b\u9886\u5bfc\u529b\u200b\u4e0e\u200b\u793e\u4f1a\u200b\u53d1\u5c55\u200b\u3002\u200b\u5176\u95f4\u200b\uff0c\u200b\u5979\u200b\u5b9e\u5730\u200b\u8d70\u8bbf\u200b\u4e2d\u56fd\u200b', '\u200b\u8bcd\u200b\u5927\u610f\u200b\uff0c\u200b\u8001\u5e08\u200b\u5c24\u200b\u65af\u62c9\u200b\u00b7\u200b\u7a46\u7f55\u9ed8\u5fb7\u200b\u8428\u5c14\u200b\u00b7\u200b\u4faf\u8d5b\u56e0\u200b\u9010\u200b', '\u200b\u591a\u4e2a\u200b\u5730\u533a\u200b\uff0c\u200b\u83b7\u5f97\u200b\u4e86\u200b\u89c2\u5bdf\u200b\u4e2d\u56fd\u200b\u793e\u4f1a\u200b\u53d1\u5c55\u200b\u7684\u200b\u7b2c\u4e00\u200b', '\u200b\u5728\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u4e0d\u4e45\u524d\u200b\u4e3e\u529e\u200b\u7684\u200b\u7b2c\u516d\u5c4a\u200b\u4e2d\u56fd\u200b\u98ce\u7b5d\u200b\u6587\u5316\u8282\u200b\u4e0a\u200b\uff0c\u200b\u5f53\u5730\u200b\u5c0f\u5b66\u751f\u200b\u4f53\u9a8c\u200b\u98ce\u7b5d\u200b\u5236\u4f5c\u200b\u3002', '\u200b\u5b57\u200b\u7ffb\u8bd1\u200b\u548c\u200b\u89e3\u91ca\u200b\u6b4c\u8bcd\u200b\u3002\u200b\u968f\u7740\u200b\u4f34\u594f\u200b\u58f0\u54cd\u200b\u8d77\u200b\uff0c\u200b\u5b66\u751f\u200b\u4eec\u200b', '\u200b\u624b\u200b\u8d44\u6599\u200b\u3002', '\u200b\u4e2d\u56fd\u200b\u9a7b\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5927\u4f7f\u9986\u200b\u4f9b\u56fe\u200b', '\u200b\u8fb9\u5531\u8fb9\u200b\u968f\u7740\u200b\u8282\u62cd\u200b\u6447\u52a8\u200b\u8eab\u4f53\u200b\uff0c\u200b\u73b0\u573a\u200b\u6c14\u6c1b\u200b\u70ed\u70c8\u200b\u3002', '\u200b\u8c08\u8d77\u200b\u5728\u200b\u4e2d\u56fd\u200b\u6c42\u5b66\u200b\u7684\u200b\u7ecf\u5386\u200b\uff0c\u200b\u7ea6\u7ff0\u200b\u5a1c\u200b\u8bb0\u5fc6\u200b\u72b9\u200b', '\u201c\u200b\u8fd9\u662f\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\u521d\u7ea7\u73ed\u200b\uff0c\u200b\u5171\u6709\u200b32\u200b\u4eba\u200b\u3002\u200b\u5b66\u200b', '\u200b\u65b0\u200b\uff1a\"\u200b\u4e2d\u56fd\u200b\u7684\u200b\u53d1\u5c55\u200b\u5728\u200b\u5f53\u4eca\u4e16\u754c\u200b\u662f\u200b\u72ec\u4e00\u65e0\u4e8c\u200b\u7684\u200b\u3002', '\u201c\u200b\u4e0d\u7ba1\u200b\u8fdc\u8fd1\u200b\u90fd\u200b\u662f\u200b\u5ba2\u4eba\u200b\uff0c\u200b\u8bf7\u200b\u4e0d\u7528\u200b\u5ba2\u6c14\u200b\uff1b\u200b\u76f8\u7ea6\u200b', '\u200b\u74e6\u200b\u7684\u200b\u5317\u200b\u7ea2\u6d77\u200b\u7701\u200b\u535a\u7269\u9986\u200b\u3002', '\u200b\u751f\u200b\u5927\u90e8\u5206\u200b\u6765\u81ea\u200b\u9996\u90fd\u200b\u963f\u65af\u9a6c\u62c9\u200b\u7684\u200b\u4e2d\u5c0f\u5b66\u200b\uff0c\u200b\u5e74\u9f84\u200b', '\u200b\u6cbf\u7740\u200b\u4e2d\u56fd\u200b\u7279\u8272\u200b\u793e\u4f1a\u4e3b\u4e49\u200b\u9053\u8def\u200b\u575a\u5b9a\u200b\u524d\u884c\u200b\uff0c\u200b\u4e2d\u56fd\u200b', '\u200b\u597d\u200b\u4e86\u200b\u5728\u200b\u4e00\u8d77\u200b\u6211\u4eec\u200b\u6b22\u8fce\u200b\u4f60\u200b\"\u200b\u5728\u200b\u4e00\u573a\u200b\u4e2d\u5384\u9752\u200b', '\u200b\u535a\u7269\u9986\u200b\u4e8c\u5c42\u200b\u9648\u5217\u200b\u7740\u200b\u4e00\u4e2a\u200b\u53d1\u6398\u200b\u81ea\u963f\u675c\u5229\u200b', '\u200b\u6700\u5c0f\u200b\u7684\u200b\u4ec5\u200b\u6709\u200b6\u200b\u5c81\u200b\u3002\u201d\u200b\u5c24\u200b\u65af\u62c9\u200b\u544a\u8bc9\u200b\u8bb0\u8005\u200b\u3002', '\u200b\u521b\u9020\u200b\u4e86\u200b\u53d1\u5c55\u200b\u5947\u8ff9\u200b\uff0c\u200b\u8fd9\u200b\u4e00\u5207\u200b\u90fd\u200b\u79bb\u4e0d\u5f00\u200b\u4e2d\u56fd\u5171\u4ea7\u515a\u200b', '\u200b\u5e74\u200b\u8054\u8c0a\u200b\u6d3b\u52a8\u200b\u4e0a\u200b\uff0c\u200b\u56db\u5ddd\u200b\u8def\u6865\u200b\u4e2d\u65b9\u200b\u5458\u5de5\u200b\u540c\u200b\u5f53\u5730\u200b\u5927\u200b', '\u200b\u65af\u200b\u53e4\u57ce\u200b\u7684\u200b\u4e2d\u56fd\u200b\u53e4\u4ee3\u200b\u9676\u5236\u200b\u9152\u5668\u200b\uff0c\u200b\u7f50\u200b\u8eab\u4e0a\u200b\u5199\u200b\u7740\u200b', '\u200b\u5c24\u200b\u65af\u62c9\u200b\u4eca\u5e74\u200b23\u200b\u5c81\u200b\uff0c\u200b\u662f\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u4e00\u6240\u200b\u516c\u7acb\u200b', '\u200b\u7684\u200b\u9886\u5bfc\u200b\u3002\u200b\u4e2d\u56fd\u200b\u7684\u200b\u53d1\u5c55\u200b\u7ecf\u9a8c\u200b\u503c\u5f97\u200b\u8bb8\u591a\u200b\u56fd\u5bb6\u200b\u5b66\u4e60\u200b', '\u200b\u5b66\u751f\u200b\u5408\u5531\u200b\u300a\u200b\u5317\u4eac\u200b\u6b22\u8fce\u200b\u4f60\u200b\u300b\u3002\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u6280\u672f\u200b\u5b66\u200b', '\u201c\u200b\u4e07\u200b\"\"\u200b\u548c\u200b\"\"\u200b\u7985\u200b\"\u201c\u200b\u5c71\u200b\"\u200b\u7b49\u200b\u6c49\u5b57\u200b\u3002\u201c\u200b\u8fd9\u4ef6\u200b\u6587\u7269\u200b\u8bc1\u200b', '\u200b\u5b66\u6821\u200b\u7684\u200b\u827a\u672f\u200b\u8001\u5e08\u200b\u3002\u200b\u5979\u200b12\u200b\u5c81\u200b\u5f00\u59cb\u200b\u5728\u200b\u5384\u200b\u7279\u5b54\u200b\u9662\u5b66\u200b', '\u200b\u501f\u9274\u200b\u3002\u201d', '\u200b\u9662\u200b\u8ba1\u7b97\u673a\u79d1\u5b66\u200b\u4e0e\u200b\u5de5\u7a0b\u200b\u4e13\u4e1a\u200b\u5b66\u751f\u200b\u9c81\u592b\u5854\u200b\u00b7\u200b\u8c22\u62c9\u200b', '\u200b\u660e\u200b,\u200b\u5f88\u65e9\u4ee5\u524d\u200b\u6211\u4eec\u200b\u5c31\u200b\u901a\u8fc7\u200b\u6d77\u4e0a\u200b\u4e1d\u7ef8\u4e4b\u8def\u200b\u8fdb\u884c\u200b', '\u200b\u4e60\u200b\u4e2d\u6587\u200b,\u200b\u5728\u200b2017\u200b\u5e74\u200b\u7b2c\u5341\u5c4a\u200b\"\u200b\u6c49\u8bed\u200b\u6865\u200b\"\u200b\u4e16\u754c\u200b\u4e2d\u5b66\u751f\u200b', '\u200b\u6b63\u5728\u200b\u897f\u5357\u200b\u5927\u5b66\u200b\u5b66\u4e60\u200b\u7684\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u535a\u58eb\u751f\u200b', '\u200b\u662f\u200b\u5176\u4e2d\u200b\u4e00\u540d\u200b\u6f14\u5531\u8005\u200b\uff0c\u200b\u5979\u200b\u5f88\u65e9\u200b\u4fbf\u200b\u5728\u200b\u5b54\u9662\u200b\u5b66\u4e60\u200b\u4e2d\u200b', '\u200b\u8d38\u6613\u5f80\u6765\u200b\u4e0e\u200b\u6587\u5316\u4ea4\u6d41\u200b\u3002\u200b\u8fd9\u200b\u4e5f\u200b\u662f\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b', '\u200b\u4e2d\u6587\u200b\u6bd4\u8d5b\u200b\u4e2d\u200b\u83b7\u5f97\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u8d5b\u533a\u200b\u7b2c\u4e00\u540d\u200b\uff0c\u200b\u5e76\u200b\u548c\u200b', '\u200b\u7a46\u5362\u76d6\u200b\u5854\u200b\u00b7\u200b\u6cfd\u7a46\u200b\u4f0a\u5bf9\u200b\u4e2d\u56fd\u200b\u6000\u6709\u200b\u6df1\u539a\u611f\u60c5\u200b\u30028', '\u200b\u6587\u200b\uff0c\u200b\u4e00\u76f4\u200b\u5728\u200b\u4e3a\u200b\u53bb\u200b\u4e2d\u56fd\u200b\u7559\u5b66\u200b\u4f5c\u200b\u51c6\u5907\u200b\u3002\"\u200b\u8fd9\u53e5\u200b\u6b4c\u8bcd\u200b', '\u200b\u4e0e\u200b\u4e2d\u56fd\u200b\u53cb\u597d\u200b\u4ea4\u5f80\u200b\u5386\u53f2\u200b\u7684\u200b\u6709\u529b\u200b\u8bc1\u660e\u200b\u3002\"\u200b\u5317\u200b\u7ea2\u6d77\u200b', '\u200b\u540c\u4f34\u200b\u4ee3\u8868\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u524d\u5f80\u200b\u4e2d\u56fd\u200b\u53c2\u52a0\u200b\u51b3\u8d5b\u200b\uff0c\u200b\u83b7\u5f97\u200b', '\u200b\u5e74\u524d\u200b\uff0c\u200b\u5728\u200b\u5317\u4eac\u5e08\u8303\u5927\u5b66\u200b\u83b7\u5f97\u200b\u7855\u58eb\u5b66\u4f4d\u200b\u540e\u200b\uff0c\u200b\u7a46\u5362\u200b', '\u200b\u662f\u200b\u6211\u4eec\u200b\u4e24\u56fd\u4eba\u6c11\u200b\u53cb\u8c0a\u200b\u7684\u200b\u751f\u52a8\u200b\u5199\u7167\u200b\u3002\u200b\u65e0\u8bba\u662f\u200b\u6295\u200b', '\u200b\u7701\u200b\u535a\u7269\u9986\u200b\u7814\u7a76\u200b\u4e0e\u200b\u6587\u732e\u200b\u90e8\u200b\u8d1f\u8d23\u4eba\u200b\u4f0a\u8428\u200b\u4e9a\u65af\u200b\u00b7\u200b\u7279\u200b', '\u200b\u56e2\u4f53\u200b\u4f18\u80dc\u5956\u200b\u30022022\u200b\u5e74\u200b\u8d77\u200b\uff0c\u200b\u5c24\u200b\u65af\u62c9\u200b\u5f00\u59cb\u200b\u5728\u200b\u5384\u200b\u7279\u5b54\u200b', '\u200b\u76d6\u5854\u200b\u5728\u200b\u793e\u4ea4\u200b\u5a92\u4f53\u200b\u4e0a\u200b\u5199\u4e0b\u200b\u8fd9\u6837\u200b\u4e00\u6bb5\u8bdd\u200b\uff1a\"\u200b\u8fd9\u200b\u662f\u200b\u6211\u200b', '\u200b\u8eab\u4e8e\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u57fa\u7840\u8bbe\u65bd\u200b\u5efa\u8bbe\u200b\u7684\u200b\u4e2d\u4f01\u200b\u5458\u5de5\u200b\uff0c', '\u200b\u65af\u6cd5\u5179\u5409\u8bf4\u200b\u3002', '\u200b\u9662\u200b\u517c\u804c\u200b\u6559\u6388\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\uff0c\u200b\u6bcf\u200b\u5468\u672b\u200b\u4e24\u4e2a\u200b\u8bfe\u65f6\u200b\u3002\u201c\u200b\u4e2d\u56fd\u200b', '\u200b\u4eba\u751f\u200b\u7684\u200b\u91cd\u8981\u200b\u4e00\u6b65\u200b\uff0c\u200b\u81ea\u6b64\u200b\u6211\u200b\u62e5\u6709\u200b\u4e86\u200b\u4e00\u53cc\u200b\u575a\u56fa\u200b\u7684\u200b', '\u200b\u8fd8\u662f\u200b\u5728\u200b\u4e2d\u56fd\u200b\u7559\u5b66\u200b\u7684\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5b66\u5b50\u200b\uff0c\u200b\u4e24\u200b\u56fd\u4eba\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u56fd\u5bb6\u535a\u7269\u9986\u200b\u8003\u53e4\u5b66\u200b\u548c\u200b\u4eba\u7c7b\u5b66\u200b', '\u200b\u6587\u5316\u200b\u535a\u5927\u7cbe\u6df1\u200b\uff0c\u200b\u6211\u200b\u5e0c\u671b\u200b\u6211\u200b\u7684\u200b\u5b66\u751f\u200b\u4eec\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u4e2d\u200b', '\u200b\u978b\u5b50\u200b\uff0c\u200b\u8d4b\u4e88\u200b\u6211\u200b\u7a7f\u8d8a\u200b\u8346\u68d8\u200b\u7684\u200b\u529b\u91cf\u200b\u3002\u201d', '\u200b\u6c11\u200b\u643a\u624b\u200b\u52aa\u529b\u200b\uff0c\u200b\u5fc5\u5c06\u200b\u63a8\u52a8\u200b\u4e24\u56fd\u5173\u7cfb\u200b\u4e0d\u65ad\u200b\u5411\u524d\u200b\u53d1\u200b', '\u200b\u7814\u7a76\u5458\u200b\u83f2\u5c14\u200b\u8499\u200b\u00b7\u200b\u7279\u200b\u97e6\u5c14\u5fb7\u200b\u5341\u5206\u200b\u559c\u7231\u200b\u4e2d\u56fd\u200b\u6587\u200b', '\u200b\u6587\u200b\u6b4c\u66f2\u200b\u66f4\u597d\u200b\u5730\u200b\u7406\u89e3\u200b\u4e2d\u56fd\u200b\u6587\u5316\u200b\u3002\"\u200b\u5979\u200b\u8bf4\u200b\u3002', '\u200b\u7a46\u5362\u76d6\u200b\u5854\u200b\u5bc6\u5207\u200b\u5173\u6ce8\u200b\u4e2d\u56fd\u200b\u5728\u200b\u7ecf\u6d4e\u200b\u3001\u200b\u79d1\u6280\u200b\u3001\u200b\u6559\u200b', '\u200b\u5c55\u200b\u3002\"\u200b\u9c81\u592b\u5854\u200b\u8bf4\u200b\u3002', '\u200b\u5316\u200b\u3002\u200b\u4ed6\u200b\u8868\u793a\u200b\uff1a\u201c\u200b\u5b66\u4e60\u200b\u5f7c\u6b64\u200b\u7684\u200b\u8bed\u8a00\u200b\u548c\u200b\u6587\u5316\u200b\uff0c\u200b\u5c06\u200b\u5e2e\u200b', '\u201c\u200b\u59d0\u59d0\u200b\uff0c\u200b\u4f60\u200b\u60f3\u200b\u53bb\u200b\u4e2d\u56fd\u200b\u5417\u200b\uff1f\"\"\u200b\u975e\u5e38\u200b\u60f3\u200b\uff01\u200b\u6211\u200b\u60f3\u200b', '\u200b\u80b2\u200b\u7b49\u200b\u9886\u57df\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0c\u200b\u4e2d\u56fd\u200b\u5728\u200b\u79d1\u7814\u200b\u7b49\u200b\u65b9\u9762\u200b\u7684\u200b\u5b9e\u529b\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u9ad8\u7b49\u6559\u80b2\u200b\u59d4\u5458\u4f1a\u200b\u4e3b\u4efb\u200b\u52a9\u7406\u200b\u8428\u200b', '\u200b\u52a9\u5384\u4e2d\u200b\u4e24\u56fd\u4eba\u6c11\u200b\u66f4\u597d\u200b\u5730\u200b\u7406\u89e3\u200b\u5f7c\u6b64\u200b\uff0c\u200b\u52a9\u529b\u200b\u53cc\u65b9\u200b', '\u200b\u53bb\u200b\u770b\u200b\u6545\u5bab\u200b\u3001\u200b\u722c\u200b\u957f\u57ce\u200b\u3002\"\u200b\u5c24\u200b\u65af\u62c9\u200b\u7684\u200b\u5b66\u751f\u200b\u4e2d\u6709\u200b\u4e00\u5bf9\u200b', '\u200b\u4e0e\u65e5\u4ff1\u589e\u200b\u3002\u200b\u5728\u200b\u4e2d\u56fd\u200b\u5b66\u4e60\u200b\u7684\u200b\u7ecf\u5386\u200b\u8ba9\u200b\u6211\u200b\u770b\u5230\u200b\u66f4\u5e7f\u200b', '\u200b\u9a6c\u745e\u200b\u8868\u793a\u200b\uff1a\"\u200b\u6bcf\u5e74\u200b\u6211\u4eec\u200b\u90fd\u200b\u4f1a\u200b\u7ec4\u7ec7\u200b\u5b66\u751f\u200b\u5230\u200b\u4e2d\u56fd\u200b\u8bbf\u200b', '\u200b\u4ea4\u5f80\u200b\uff0c\u200b\u642d\u5efa\u200b\u53cb\u8c0a\u200b\u6865\u6881\u200b\u3002\"', '\u200b\u80fd\u6b4c\u5584\u821e\u200b\u7684\u200b\u59d0\u59b9\u200b\uff0c\u200b\u59d0\u59d0\u200b\u9732\u5a05\u200b\u4eca\u5e74\u200b15\u200b\u5c81\u200b\uff0c\u200b\u59b9\u59b9\u200b', '\u200b\u9614\u200b\u7684\u200b\u4e16\u754c\u200b\uff0c\u200b\u4ece\u4e2d\u200b\u53d7\u76ca\u532a\u6d45\u200b\u3002', '\u200b\u95ee\u200b\u5b66\u4e60\u200b\uff0c\u200b\u76ee\u524d\u200b\u6709\u200b\u8d85\u8fc7\u200b5000\u200b\u540d\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5b66\u751f\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u56fd\u5bb6\u535a\u7269\u9986\u200b\u9986\u957f\u200b\u5854\u5409\u200b\u4e01\u200b\u00b7\u200b\u52aa\u200b', '\u200b\u8389\u5a05\u200b14\u200b\u5c81\u200b\uff0c\u200b\u4e24\u4eba\u200b\u90fd\u200b\u5df2\u200b\u5728\u200b\u5384\u200b\u7279\u5b54\u9662\u200b\u5b66\u4e60\u200b\u591a\u5e74\u200b\uff0c', '23\u200b\u5c81\u200b\u7684\u200b\u8389\u8fea\u4e9a\u200b\u00b7\u200b\u57c3\u200b\u65af\u8482\u6cd5\u200b\u8bfa\u65af\u200b\u5df2\u200b\u5728\u200b\u5384\u7279\u200b', '\u200b\u5728\u200b\u4e2d\u56fd\u200b\u7559\u5b66\u200b\u3002\u200b\u5b66\u4e60\u200b\u4e2d\u56fd\u200b\u7684\u200b\u6559\u80b2\u200b\u7ecf\u9a8c\u200b\uff0c\u200b\u6709\u52a9\u4e8e\u200b', '\u200b\u91cc\u200b\u8fbe\u59c6\u200b\u00b7\u200b\u4f18\u7d20\u798f\u200b\u66fe\u591a\u6b21\u200b\u8bbf\u95ee\u200b\u4e2d\u56fd\u200b\uff0c\u200b\u5bf9\u200b\u4e2d\u534e\u6587\u660e\u200b', '\u200b\u4e2d\u6587\u200b\u8bf4\u200b\u5f97\u200b\u683c\u5916\u200b\u6d41\u5229\u200b\u3002', '\u200b\u5b54\u9662\u200b\u5b66\u4e60\u200b3\u200b\u5e74\u200b\uff0c\u200b\u5728\u200b\u4e2d\u56fd\u200b\u4e66\u6cd5\u200b\u3001\u200b\u4e2d\u56fd\u753b\u200b\u7b49\u200b\u65b9\u9762\u200b\u8868\u200b', '\u200b\u63d0\u5347\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u7684\u200b\u6559\u80b2\u200b\u6c34\u5e73\u200b\u3002\u201d', '\u200b\u7684\u200b\u4f20\u627f\u200b\u4e0e\u200b\u521b\u65b0\u200b\u3001\u200b\u73b0\u4ee3\u5316\u200b\u535a\u7269\u9986\u200b\u7684\u200b\u5efa\u8bbe\u200b\u4e0e\u200b\u53d1\u5c55\u200b', '\u200b\u9732\u5a05\u200b\u5bf9\u200b\u8bb0\u8005\u200b\u8bf4\u200b\uff1a\"\u200b\u8fd9\u4e9b\u5e74\u6765\u200b\uff0c\u200b\u6000\u7740\u200b\u5bf9\u200b\u4e2d\u6587\u200b', '\u200b\u73b0\u200b\u5341\u5206\u200b\u4f18\u79c0\u200b\uff0c\u200b\u5728\u200b2024\u200b\u5e74\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u8d5b\u533a\u200b\u7684\u200b', '\u201c\u200b\u5171\u540c\u200b\u5411\u200b\u4e16\u754c\u200b\u5c55\u793a\u200b\u975e\u200b', '\u200b\u5370\u8c61\u200b\u6df1\u523b\u200b\u3002\u201c\u200b\u4e2d\u56fd\u200b\u535a\u7269\u9986\u200b\u4e0d\u4ec5\u200b\u6709\u200b\u8bb8\u591a\u200b\u4fdd\u5b58\u200b\u5b8c\u597d\u200b', '\u200b\u548c\u200b\u4e2d\u56fd\u200b\u6587\u5316\u200b\u7684\u200b\u70ed\u7231\u200b\uff0c\u200b\u6211\u4eec\u200b\u59d0\u59b9\u4fe9\u200b\u59cb\u7ec8\u200b\u76f8\u4e92\u200b\u9f13\u200b', '\u201c\u200b\u6c49\u8bed\u200b\u6865\u200b\"\u200b\u6bd4\u8d5b\u200b\u4e2d\u200b\u83b7\u5f97\u200b\u4e00\u7b49\u5956\u200b\u3002\u200b\u8389\u8fea\u4e9a\u200b\u8bf4\u200b\uff1a\"\u200b\u5b66\u200b', '\u200b\u7684\u200b\u6587\u7269\u200b\uff0c\u200b\u8fd8\u200b\u5145\u5206\u8fd0\u7528\u200b\u5148\u8fdb\u200b\u79d1\u6280\u200b\u624b\u6bb5\u200b\u8fdb\u884c\u200b\u5c55\u793a\u200b\uff0c', '\u200b\u52b1\u200b\uff0c\u200b\u4e00\u8d77\u200b\u5b66\u4e60\u200b\u3002\u200b\u6211\u4eec\u200b\u7684\u200b\u4e2d\u6587\u200b\u4e00\u5929\u200b\u6bd4\u200b\u4e00\u5929\u200b\u597d\u200b\uff0c\u200b\u8fd8\u200b', '\u200b\u4e60\u200b\u4e2d\u56fd\u200b\u4e66\u6cd5\u200b\u8ba9\u200b\u6211\u200b\u7684\u200b\u5185\u5fc3\u200b\u53d8\u5f97\u200b\u5b89\u5b81\u200b\u548c\u200b\u7eaf\u7cb9\u200b\u3002\u200b\u6211\u200b', '\u200b\u6d32\u200b\u548c\u200b\u4e9a\u6d32\u200b\u7684\u200b\u707f\u70c2\u200b\u6587\u660e\u200b\u201d', '\u200b\u5e2e\u52a9\u200b\u4eba\u4eec\u200b\u66f4\u597d\u200b\u7406\u89e3\u200b\u4e2d\u534e\u6587\u660e\u200b\u3002\"\u200b\u5854\u5409\u200b\u4e01\u8bf4\u200b\uff0c\"\u200b\u5384\u200b', '\u200b\u5b66\u4f1a\u200b\u4e86\u200b\u4e2d\u6587\u200b\u6b4c\u200b\u548c\u200b\u4e2d\u56fd\u200b\u821e\u200b\u3002\u200b\u6211\u4eec\u200b\u4e00\u5b9a\u200b\u8981\u200b\u5230\u200b\u4e2d\u56fd\u200b', '\u200b\u4e5f\u200b\u559c\u6b22\u200b\u4e2d\u56fd\u200b\u7684\u200b\u670d\u9970\u200b,\u200b\u5e0c\u671b\u200b\u672a\u6765\u200b\u80fd\u200b\u53bb\u200b\u4e2d\u56fd\u200b\u5b66\u4e60\u200b\uff0c', '\u200b\u7acb\u200b\u7279\u91cc\u4e9a\u200b\u4e0e\u200b\u4e2d\u56fd\u200b\u90fd\u200b\u62e5\u6709\u200b\u60a0\u4e45\u200b\u7684\u200b\u6587\u660e\u200b\uff0c\u200b\u59cb\u7ec8\u200b\u76f8\u200b', '\u200b\u53bb\u200b\u3002\u200b\u5b66\u597d\u200b\u4e2d\u6587\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u672a\u6765\u200b\u4e0d\u662f\u200b\u68a6\u200b\uff01\"', '\u200b\u628a\u200b\u4e2d\u56fd\u200b\u4e0d\u540c\u200b\u6c11\u65cf\u200b\u5143\u7d20\u200b\u878d\u5165\u200b\u670d\u88c5\u8bbe\u8ba1\u200b\u4e2d\u200b\uff0c\u200b\u521b\u4f5c\u200b', '\u200b\u4ece\u200b\u963f\u65af\u9a6c\u62c9\u200b\u51fa\u53d1\u200b\uff0c\u200b\u6cbf\u7740\u200b\u873f\u8713\u200b\u66f2\u6298\u200b\u7684\u200b\u76d8\u5c71\u200b', '\u200b\u4e92\u200b\u7406\u89e3\u200b\u3001\u200b\u76f8\u4e92\u5c0a\u91cd\u200b\u3002\u200b\u6211\u200b\u5e0c\u671b\u200b\u672a\u6765\u200b\u4e0e\u200b\u4e2d\u56fd\u200b\u540c\u884c\u200b', '\u200b\u636e\u200b\u5384\u200b\u7279\u5b54\u9662\u200b\u4e2d\u65b9\u200b\u9662\u957f\u200b\u9ec4\u9e23\u98de\u200b\u4ecb\u7ecd\u200b\uff0c\u200b\u8fd9\u6240\u200b', '\u200b\u51fa\u200b\u66f4\u200b\u591a\u200b\u7cbe\u7f8e\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u4e5f\u200b\u628a\u200b\u5384\u7279\u200b\u6587\u5316\u200b\u5206\u4eab\u200b\u7ed9\u200b\u66f4\u200b\u591a\u200b', '\u200b\u516c\u8def\u200b\u4e00\u8def\u200b\u5411\u4e1c\u200b\u5bfb\u627e\u200b\u4e1d\u8def\u200b\u5370\u8ff9\u200b\u3002\u200b\u9a71\u8f66\u200b\u4e24\u4e2a\u200b\u5c0f\u200b', '\u200b\u52a0\u5f3a\u200b\u5408\u4f5c\u200b\uff0c\u200b\u5171\u540c\u200b\u5411\u200b\u4e16\u754c\u200b\u5c55\u793a\u200b\u975e\u6d32\u200b\u548c\u200b\u4e9a\u6d32\u200b\u7684\u200b\u707f\u200b', '\u200b\u5b54\u9662\u200b\u6210\u7acb\u200b\u4e8e\u200b2013\u200b\u5e74\u200b3\u200b\u6708\u200b\uff0c\u200b\u7531\u200b\u8d35\u5dde\u200b\u8d22\u7ecf\u5927\u5b66\u200b\u548c\u200b', '\u200b\u7684\u200b\u4e2d\u56fd\u200b\u670b\u53cb\u200b\u3002\u201d', '\u200b\u65f6\u200b\uff0c\u200b\u8bb0\u8005\u200b\u6765\u5230\u200b\u4f4d\u4e8e\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u6e2f\u53e3\u57ce\u5e02\u200b\u9a6c\u8428\u200b', '\u200b\u70c2\u200b\u6587\u660e\u200b\u3002\u201d'], 'rec_scores': array([0.99943757, ..., 0.98181838]), 'rec_polys': array([[[ 133,   35],\n        ...,\n        [ 133,  131]],\n\n       ...,\n\n       [[1154, 1323],\n        ...,\n        [1152, 1355]]], dtype=int16), 'rec_boxes': array([[ 133, ...,  131],\n       ...,\n       [1152, ..., 1359]], dtype=int16)}, 'text_paragraphs_ocr_res': {'rec_polys': array([[[ 133,   35],\n        ...,\n        [ 133,  131]],\n\n       ...,\n\n       [[1154, 1323],\n        ...,\n        [1152, 1355]]], dtype=int16), 'rec_texts': ['\u200b\u52a9\u529b\u200b\u53cc\u65b9\u200b\u4ea4\u5f80\u200b', '\u200b\u642d\u5efa\u200b\u53cb\u8c0a\u200b\u6865\u6881\u200b', '\u200b\u672c\u62a5\u8bb0\u8005\u200b', '\u200b\u6c88\u5c0f\u6653\u200b', '\u200b\u4efb\u200b', '\u200b\u5f66\u200b', '\u200b\u9ec4\u57f9\u662d\u200b', '\u200b\u8eab\u7740\u200b\u4e2d\u56fd\u200b\u4f20\u7edf\u200b\u6c11\u65cf\u670d\u88c5\u200b\u7684\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u9752\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u9ad8\u7b49\u6559\u80b2\u200b\u4e0e\u200b\u7814\u7a76\u9662\u200b\u5408\u4f5c\u200b\u5efa\u7acb\u200b\uff0c\u200b\u5f00\u200b', '\u200b\u5e74\u200b\u4f9d\u6b21\u200b\u767b\u53f0\u200b\u8868\u6f14\u200b\u4e2d\u56fd\u200b\u6c11\u65cf\u821e\u200b\u3001\u200b\u73b0\u4ee3\u821e\u200b\u3001\u200b\u6247\u5b50\u821e\u200b', '\u200b\u8bbe\u200b\u4e86\u200b\u4e2d\u56fd\u200b\u8bed\u8a00\u200b\u8bfe\u7a0b\u200b\u548c\u200b\u4e2d\u56fd\u200b\u6587\u5316\u200b\u8bfe\u7a0b\u200b\uff0c\u200b\u6ce8\u518c\u200b\u5b66\u200b', '\u200b\u7b49\u200b,\u200b\u66fc\u5999\u200b\u7684\u200b\u821e\u59ff\u200b\u8d62\u5f97\u200b\u73b0\u573a\u200b\u89c2\u4f17\u200b\u9635\u9635\u200b\u638c\u58f0\u200b\u3002\u200b\u8fd9\u200b', '\u200b\u751f\u200b2\u200b\u4e07\u4f59\u4eba\u6b21\u200b\u300210\u200b\u4f59\u5e74\u200b\u6765\u200b\uff0c\u200b\u5384\u7279\u200b\u5b54\u9662\u200b\u5df2\u200b\u6210\u4e3a\u200b', '\u200b\u662f\u200b\u65e5\u524d\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u9ad8\u7b49\u6559\u80b2\u200b\u4e0e\u200b\u7814\u7a76\u9662\u200b\u5b54\u5b50\u200b\u5b66\u200b', '\u200b\u5f53\u5730\u200b\u6c11\u4f17\u200b\u4e86\u89e3\u200b\u4e2d\u56fd\u200b\u7684\u200b\u4e00\u6247\u200b\u7a97\u53e3\u200b\u3002', '\u200b\u9662\u200b(\u200b\u4ee5\u4e0b\u200b\u7b80\u79f0\u200b\"\u200b\u5384\u7279\u200b\u5b54\u9662\u200b\")\u200b\u4e3e\u529e\u200b\"\u200b\u559c\u8fce\u200b\u65b0\u5e74\u200b\"\u200b\u4e2d\u56fd\u200b', '\u200b\u9ec4\u9e23\u98de\u200b\u8868\u793a\u200b\uff0c\u200b\u968f\u7740\u200b\u6765\u200b\u5b66\u4e60\u200b\u4e2d\u6587\u200b\u7684\u200b\u4eba\u200b\u65e5\u76ca\u200b', '\u200b\u6b4c\u821e\u200b\u6bd4\u8d5b\u200b\u7684\u200b\u573a\u666f\u200b\u3002', '\u200b\u589e\u591a\u200b\uff0c\u200b\u963f\u65af\u9a6c\u62c9\u200b\u5927\u5b66\u200b\u6559\u5b66\u70b9\u200b\u5df2\u200b\u96be\u4ee5\u200b\u6ee1\u8db3\u200b\u6559\u5b66\u200b', '\u200b\u4e2d\u56fd\u200b\u548c\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u4f20\u7edf\u53cb\u8c0a\u200b\u6df1\u539a\u200b\u3002\u200b\u8fd1\u5e74\u200b', '\u200b\u9700\u8981\u200b\u30022024\u200b\u5e74\u200b4\u200b\u6708\u200b\uff0c\u200b\u7531\u200b\u4e2d\u4f01\u200b\u8700\u9053\u200b\u96c6\u56e2\u200b\u6240\u5c5e\u200b\u56db\u200b', '\u200b\u6765\u200b,\u200b\u5728\u200b\u9ad8\u8d28\u91cf\u200b\u5171\u5efa\u200b\"\u200b\u4e00\u5e26\u200b\u4e00\u8def\u200b\"\u200b\u6846\u67b6\u200b\u4e0b\u200b\uff0c\u200b\u4e2d\u200b\u5384\u200b\u4e24\u200b', '\u200b\u5ddd\u200b\u8def\u6865\u200b\u627f\u5efa\u200b\u7684\u200b\u5b54\u9662\u200b\u6559\u5b66\u697c\u200b\u9879\u76ee\u200b\u5728\u200b\u963f\u65af\u9a6c\u62c9\u200b\u5f00\u200b', '\u200b\u56fd\u200b\u4eba\u6587\u200b\u4ea4\u6d41\u200b\u4e0d\u65ad\u200b\u6df1\u5316\u200b\uff0c\u200b\u4e92\u5229\u200b\u5408\u4f5c\u200b\u7684\u200b\u6c11\u610f\u57fa\u7840\u200b', '\u200b\u5de5\u200b\u5efa\u8bbe\u200b\uff0c\u200b\u9884\u8ba1\u200b\u4eca\u5e74\u200b\u4e0a\u534a\u5e74\u200b\u5cfb\u5de5\u200b\uff0c\u200b\u5efa\u6210\u200b\u540e\u200b\u5c06\u200b\u4e3a\u200b\u5384\u200b', '\u200b\u65e5\u76ca\u200b\u6df1\u539a\u200b\u3002', '\u200b\u7279\u5b54\u9662\u200b\u63d0\u4f9b\u200b\u5168\u65b0\u200b\u7684\u200b\u529e\u5b66\u200b\u573a\u5730\u200b\u3002', '\u201c\u200b\u5b66\u597d\u200b\u4e2d\u6587\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b', '\u201c\u200b\u5728\u200b\u4e2d\u56fd\u200b\u5b66\u4e60\u200b\u7684\u200b\u7ecf\u5386\u200b', '\u200b\u672a\u6765\u200b\u4e0d\u662f\u200b\u68a6\u200b\u201d', '\u200b\u8ba9\u200b\u6211\u200b\u770b\u5230\u200b\u66f4\u200b\u5e7f\u9614\u200b\u7684\u200b\u4e16\u754c\u200b\u201d', '\u201c\u200b\u9c9c\u82b1\u200b\u66fe\u200b\u544a\u8bc9\u200b\u6211\u200b\u4f60\u200b\u600e\u6837\u200b\u8d70\u8fc7\u200b\uff0c\u200b\u5927\u5730\u200b\u77e5\u9053\u200b\u4f60\u200b', '\u200b\u591a\u5e74\u200b\u6765\u200b\uff0c\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5e7f\u5927\u200b\u8d74\u534e\u200b\u7559\u5b66\u751f\u200b\u548c\u200b', '\u200b\u5fc3\u4e2d\u200b\u7684\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u89d2\u843d\u200b\u2026\"\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u963f\u65af\u9a6c\u62c9\u200b', '\u200b\u57f9\u8bad\u200b\u4eba\u5458\u200b\u79ef\u6781\u200b\u6295\u8eab\u200b\u56fd\u5bb6\u200b\u5efa\u8bbe\u200b\uff0c\u200b\u6210\u4e3a\u200b\u52a9\u529b\u200b\u8be5\u56fd\u200b', '\u200b\u5927\u5b66\u200b\u7efc\u5408\u697c\u200b\u4e8c\u5c42\u200b\uff0c\u200b\u4e00\u9635\u200b\u4f18\u7f8e\u200b\u7684\u200b\u6b4c\u58f0\u200b\u5728\u200b\u8d70\u5eca\u200b\u91cc\u200b\u56de\u200b', '\u200b\u53d1\u5c55\u200b\u7684\u200b\u4eba\u624d\u200b\u548c\u200b\u5384\u200b\u4e2d\u200b\u53cb\u597d\u200b\u7684\u200b\u89c1\u8bc1\u8005\u200b\u548c\u200b\u63a8\u52a8\u8005\u200b\u3002', '\u200b\u54cd\u200b\u3002\u200b\u5faa\u7740\u200b\u719f\u6089\u200b\u7684\u200b\u65cb\u5f8b\u200b\u8f7b\u8f7b\u200b\u63a8\u5f00\u200b\u4e00\u95f4\u200b\u6559\u5ba4\u200b\u7684\u200b\u95e8\u200b\uff0c', '\u200b\u5728\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5168\u56fd\u200b\u5987\u5973\u200b\u8054\u76df\u200b\u5de5\u4f5c\u200b\u7684\u200b\u7ea6\u7ff0\u200b', '\u200b\u5b66\u751f\u200b\u4eec\u200b\u6b63\u200b\u8ddf\u7740\u200b\u8001\u5e08\u200b\u5b66\u5531\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\u300a\u200b\u540c\u4e00\u9996\u6b4c\u200b\u300b\u3002', '\u200b\u5a1c\u200b\u00b7\u200b\u7279\u200b\u97e6\u5c14\u5fb7\u200b\u00b7\u200b\u51ef\u83b1\u200b\u5854\u200b\u5c31\u662f\u200b\u5176\u4e2d\u200b\u4e00\u4f4d\u200b\u3002\u200b\u5979\u200b\u66fe\u200b\u5728\u200b', '\u200b\u8fd9\u200b\u662f\u200b\u5384\u200b\u7279\u5b54\u9662\u200b\u963f\u65af\u9a6c\u62c9\u200b\u5927\u5b66\u200b\u6559\u5b66\u70b9\u200b\u7684\u200b\u4e00\u200b', '\u200b\u4e2d\u534e\u200b\u5973\u5b50\u200b\u5b66\u9662\u200b\u653b\u8bfb\u200b\u7855\u58eb\u5b66\u4f4d\u200b\uff0c\u200b\u7814\u7a76\u200b\u65b9\u5411\u200b\u662f\u200b\u5973\u200b', '\u200b\u8282\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\u8bfe\u200b\u3002\u200b\u4e3a\u4e86\u200b\u8ba9\u200b\u5b66\u751f\u200b\u4eec\u200b\u66f4\u597d\u200b\u5730\u200b\u7406\u89e3\u200b\u6b4c\u200b', '\u200b\u6027\u200b\u9886\u5bfc\u529b\u200b\u4e0e\u200b\u793e\u4f1a\u200b\u53d1\u5c55\u200b\u3002\u200b\u5176\u95f4\u200b\uff0c\u200b\u5979\u200b\u5b9e\u5730\u200b\u8d70\u8bbf\u200b\u4e2d\u56fd\u200b', '\u200b\u8bcd\u200b\u5927\u610f\u200b\uff0c\u200b\u8001\u5e08\u200b\u5c24\u200b\u65af\u62c9\u200b\u00b7\u200b\u7a46\u7f55\u9ed8\u5fb7\u200b\u8428\u5c14\u200b\u00b7\u200b\u4faf\u8d5b\u56e0\u200b\u9010\u200b', '\u200b\u591a\u4e2a\u200b\u5730\u533a\u200b\uff0c\u200b\u83b7\u5f97\u200b\u4e86\u200b\u89c2\u5bdf\u200b\u4e2d\u56fd\u200b\u793e\u4f1a\u200b\u53d1\u5c55\u200b\u7684\u200b\u7b2c\u4e00\u200b', '\u200b\u5728\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u4e0d\u4e45\u524d\u200b\u4e3e\u529e\u200b\u7684\u200b\u7b2c\u516d\u5c4a\u200b\u4e2d\u56fd\u200b\u98ce\u7b5d\u200b\u6587\u5316\u8282\u200b\u4e0a\u200b\uff0c\u200b\u5f53\u5730\u200b\u5c0f\u5b66\u751f\u200b\u4f53\u9a8c\u200b\u98ce\u7b5d\u200b\u5236\u4f5c\u200b\u3002', '\u200b\u5b57\u200b\u7ffb\u8bd1\u200b\u548c\u200b\u89e3\u91ca\u200b\u6b4c\u8bcd\u200b\u3002\u200b\u968f\u7740\u200b\u4f34\u594f\u200b\u58f0\u54cd\u200b\u8d77\u200b\uff0c\u200b\u5b66\u751f\u200b\u4eec\u200b', '\u200b\u624b\u200b\u8d44\u6599\u200b\u3002', '\u200b\u4e2d\u56fd\u200b\u9a7b\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5927\u4f7f\u9986\u200b\u4f9b\u56fe\u200b', '\u200b\u8fb9\u5531\u8fb9\u200b\u968f\u7740\u200b\u8282\u62cd\u200b\u6447\u52a8\u200b\u8eab\u4f53\u200b\uff0c\u200b\u73b0\u573a\u200b\u6c14\u6c1b\u200b\u70ed\u70c8\u200b\u3002', '\u200b\u8c08\u8d77\u200b\u5728\u200b\u4e2d\u56fd\u200b\u6c42\u5b66\u200b\u7684\u200b\u7ecf\u5386\u200b\uff0c\u200b\u7ea6\u7ff0\u200b\u5a1c\u200b\u8bb0\u5fc6\u200b\u72b9\u200b', '\u201c\u200b\u8fd9\u662f\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\u521d\u7ea7\u73ed\u200b\uff0c\u200b\u5171\u6709\u200b32\u200b\u4eba\u200b\u3002\u200b\u5b66\u200b', '\u200b\u65b0\u200b\uff1a\"\u200b\u4e2d\u56fd\u200b\u7684\u200b\u53d1\u5c55\u200b\u5728\u200b\u5f53\u4eca\u4e16\u754c\u200b\u662f\u200b\u72ec\u4e00\u65e0\u4e8c\u200b\u7684\u200b\u3002', '\u201c\u200b\u4e0d\u7ba1\u200b\u8fdc\u8fd1\u200b\u90fd\u200b\u662f\u200b\u5ba2\u4eba\u200b\uff0c\u200b\u8bf7\u200b\u4e0d\u7528\u200b\u5ba2\u6c14\u200b\uff1b\u200b\u76f8\u7ea6\u200b', '\u200b\u74e6\u200b\u7684\u200b\u5317\u200b\u7ea2\u6d77\u200b\u7701\u200b\u535a\u7269\u9986\u200b\u3002', '\u200b\u751f\u200b\u5927\u90e8\u5206\u200b\u6765\u81ea\u200b\u9996\u90fd\u200b\u963f\u65af\u9a6c\u62c9\u200b\u7684\u200b\u4e2d\u5c0f\u5b66\u200b\uff0c\u200b\u5e74\u9f84\u200b', '\u200b\u6cbf\u7740\u200b\u4e2d\u56fd\u200b\u7279\u8272\u200b\u793e\u4f1a\u4e3b\u4e49\u200b\u9053\u8def\u200b\u575a\u5b9a\u200b\u524d\u884c\u200b\uff0c\u200b\u4e2d\u56fd\u200b', '\u200b\u597d\u200b\u4e86\u200b\u5728\u200b\u4e00\u8d77\u200b\u6211\u4eec\u200b\u6b22\u8fce\u200b\u4f60\u200b\"\u200b\u5728\u200b\u4e00\u573a\u200b\u4e2d\u5384\u9752\u200b', '\u200b\u535a\u7269\u9986\u200b\u4e8c\u5c42\u200b\u9648\u5217\u200b\u7740\u200b\u4e00\u4e2a\u200b\u53d1\u6398\u200b\u81ea\u963f\u675c\u5229\u200b', '\u200b\u6700\u5c0f\u200b\u7684\u200b\u4ec5\u200b\u6709\u200b6\u200b\u5c81\u200b\u3002\u201d\u200b\u5c24\u200b\u65af\u62c9\u200b\u544a\u8bc9\u200b\u8bb0\u8005\u200b\u3002', '\u200b\u521b\u9020\u200b\u4e86\u200b\u53d1\u5c55\u200b\u5947\u8ff9\u200b\uff0c\u200b\u8fd9\u200b\u4e00\u5207\u200b\u90fd\u200b\u79bb\u4e0d\u5f00\u200b\u4e2d\u56fd\u5171\u4ea7\u515a\u200b', '\u200b\u5e74\u200b\u8054\u8c0a\u200b\u6d3b\u52a8\u200b\u4e0a\u200b\uff0c\u200b\u56db\u5ddd\u200b\u8def\u6865\u200b\u4e2d\u65b9\u200b\u5458\u5de5\u200b\u540c\u200b\u5f53\u5730\u200b\u5927\u200b', '\u200b\u65af\u200b\u53e4\u57ce\u200b\u7684\u200b\u4e2d\u56fd\u200b\u53e4\u4ee3\u200b\u9676\u5236\u200b\u9152\u5668\u200b\uff0c\u200b\u7f50\u200b\u8eab\u4e0a\u200b\u5199\u200b\u7740\u200b', '\u200b\u5c24\u200b\u65af\u62c9\u200b\u4eca\u5e74\u200b23\u200b\u5c81\u200b\uff0c\u200b\u662f\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u4e00\u6240\u200b\u516c\u7acb\u200b', '\u200b\u7684\u200b\u9886\u5bfc\u200b\u3002\u200b\u4e2d\u56fd\u200b\u7684\u200b\u53d1\u5c55\u200b\u7ecf\u9a8c\u200b\u503c\u5f97\u200b\u8bb8\u591a\u200b\u56fd\u5bb6\u200b\u5b66\u4e60\u200b', '\u200b\u5b66\u751f\u200b\u5408\u5531\u200b\u300a\u200b\u5317\u4eac\u200b\u6b22\u8fce\u200b\u4f60\u200b\u300b\u3002\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u6280\u672f\u200b\u5b66\u200b', '\u201c\u200b\u4e07\u200b\"\"\u200b\u548c\u200b\"\"\u200b\u7985\u200b\"\u201c\u200b\u5c71\u200b\"\u200b\u7b49\u200b\u6c49\u5b57\u200b\u3002\u201c\u200b\u8fd9\u4ef6\u200b\u6587\u7269\u200b\u8bc1\u200b', '\u200b\u5b66\u6821\u200b\u7684\u200b\u827a\u672f\u200b\u8001\u5e08\u200b\u3002\u200b\u5979\u200b12\u200b\u5c81\u200b\u5f00\u59cb\u200b\u5728\u200b\u5384\u200b\u7279\u5b54\u200b\u9662\u5b66\u200b', '\u200b\u501f\u9274\u200b\u3002\u201d', '\u200b\u9662\u200b\u8ba1\u7b97\u673a\u79d1\u5b66\u200b\u4e0e\u200b\u5de5\u7a0b\u200b\u4e13\u4e1a\u200b\u5b66\u751f\u200b\u9c81\u592b\u5854\u200b\u00b7\u200b\u8c22\u62c9\u200b', '\u200b\u660e\u200b,\u200b\u5f88\u65e9\u4ee5\u524d\u200b\u6211\u4eec\u200b\u5c31\u200b\u901a\u8fc7\u200b\u6d77\u4e0a\u200b\u4e1d\u7ef8\u4e4b\u8def\u200b\u8fdb\u884c\u200b', '\u200b\u4e60\u200b\u4e2d\u6587\u200b,\u200b\u5728\u200b2017\u200b\u5e74\u200b\u7b2c\u5341\u5c4a\u200b\"\u200b\u6c49\u8bed\u200b\u6865\u200b\"\u200b\u4e16\u754c\u200b\u4e2d\u5b66\u751f\u200b', '\u200b\u6b63\u5728\u200b\u897f\u5357\u200b\u5927\u5b66\u200b\u5b66\u4e60\u200b\u7684\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u535a\u58eb\u751f\u200b', '\u200b\u662f\u200b\u5176\u4e2d\u200b\u4e00\u540d\u200b\u6f14\u5531\u8005\u200b\uff0c\u200b\u5979\u200b\u5f88\u65e9\u200b\u4fbf\u200b\u5728\u200b\u5b54\u9662\u200b\u5b66\u4e60\u200b\u4e2d\u200b', '\u200b\u8d38\u6613\u5f80\u6765\u200b\u4e0e\u200b\u6587\u5316\u4ea4\u6d41\u200b\u3002\u200b\u8fd9\u200b\u4e5f\u200b\u662f\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b', '\u200b\u4e2d\u6587\u200b\u6bd4\u8d5b\u200b\u4e2d\u200b\u83b7\u5f97\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u8d5b\u533a\u200b\u7b2c\u4e00\u540d\u200b\uff0c\u200b\u5e76\u200b\u548c\u200b', '\u200b\u7a46\u5362\u76d6\u200b\u5854\u200b\u00b7\u200b\u6cfd\u7a46\u200b\u4f0a\u5bf9\u200b\u4e2d\u56fd\u200b\u6000\u6709\u200b\u6df1\u539a\u611f\u60c5\u200b\u30028', '\u200b\u6587\u200b\uff0c\u200b\u4e00\u76f4\u200b\u5728\u200b\u4e3a\u200b\u53bb\u200b\u4e2d\u56fd\u200b\u7559\u5b66\u200b\u4f5c\u200b\u51c6\u5907\u200b\u3002\"\u200b\u8fd9\u53e5\u200b\u6b4c\u8bcd\u200b', '\u200b\u4e0e\u200b\u4e2d\u56fd\u200b\u53cb\u597d\u200b\u4ea4\u5f80\u200b\u5386\u53f2\u200b\u7684\u200b\u6709\u529b\u200b\u8bc1\u660e\u200b\u3002\"\u200b\u5317\u200b\u7ea2\u6d77\u200b', '\u200b\u540c\u4f34\u200b\u4ee3\u8868\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u524d\u5f80\u200b\u4e2d\u56fd\u200b\u53c2\u52a0\u200b\u51b3\u8d5b\u200b\uff0c\u200b\u83b7\u5f97\u200b', '\u200b\u5e74\u524d\u200b\uff0c\u200b\u5728\u200b\u5317\u4eac\u5e08\u8303\u5927\u5b66\u200b\u83b7\u5f97\u200b\u7855\u58eb\u5b66\u4f4d\u200b\u540e\u200b\uff0c\u200b\u7a46\u5362\u200b', '\u200b\u662f\u200b\u6211\u4eec\u200b\u4e24\u56fd\u4eba\u6c11\u200b\u53cb\u8c0a\u200b\u7684\u200b\u751f\u52a8\u200b\u5199\u7167\u200b\u3002\u200b\u65e0\u8bba\u662f\u200b\u6295\u200b', '\u200b\u7701\u200b\u535a\u7269\u9986\u200b\u7814\u7a76\u200b\u4e0e\u200b\u6587\u732e\u200b\u90e8\u200b\u8d1f\u8d23\u4eba\u200b\u4f0a\u8428\u200b\u4e9a\u65af\u200b\u00b7\u200b\u7279\u200b', '\u200b\u56e2\u4f53\u200b\u4f18\u80dc\u5956\u200b\u30022022\u200b\u5e74\u200b\u8d77\u200b\uff0c\u200b\u5c24\u200b\u65af\u62c9\u200b\u5f00\u59cb\u200b\u5728\u200b\u5384\u200b\u7279\u5b54\u200b', '\u200b\u76d6\u5854\u200b\u5728\u200b\u793e\u4ea4\u200b\u5a92\u4f53\u200b\u4e0a\u200b\u5199\u4e0b\u200b\u8fd9\u6837\u200b\u4e00\u6bb5\u8bdd\u200b\uff1a\"\u200b\u8fd9\u200b\u662f\u200b\u6211\u200b', '\u200b\u8eab\u4e8e\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u57fa\u7840\u8bbe\u65bd\u200b\u5efa\u8bbe\u200b\u7684\u200b\u4e2d\u4f01\u200b\u5458\u5de5\u200b\uff0c', '\u200b\u65af\u6cd5\u5179\u5409\u8bf4\u200b\u3002', '\u200b\u9662\u200b\u517c\u804c\u200b\u6559\u6388\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\uff0c\u200b\u6bcf\u200b\u5468\u672b\u200b\u4e24\u4e2a\u200b\u8bfe\u65f6\u200b\u3002\u201c\u200b\u4e2d\u56fd\u200b', '\u200b\u4eba\u751f\u200b\u7684\u200b\u91cd\u8981\u200b\u4e00\u6b65\u200b\uff0c\u200b\u81ea\u6b64\u200b\u6211\u200b\u62e5\u6709\u200b\u4e86\u200b\u4e00\u53cc\u200b\u575a\u56fa\u200b\u7684\u200b', '\u200b\u8fd8\u662f\u200b\u5728\u200b\u4e2d\u56fd\u200b\u7559\u5b66\u200b\u7684\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5b66\u5b50\u200b\uff0c\u200b\u4e24\u200b\u56fd\u4eba\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u56fd\u5bb6\u535a\u7269\u9986\u200b\u8003\u53e4\u5b66\u200b\u548c\u200b\u4eba\u7c7b\u5b66\u200b', '\u200b\u6587\u5316\u200b\u535a\u5927\u7cbe\u6df1\u200b\uff0c\u200b\u6211\u200b\u5e0c\u671b\u200b\u6211\u200b\u7684\u200b\u5b66\u751f\u200b\u4eec\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u4e2d\u200b', '\u200b\u978b\u5b50\u200b\uff0c\u200b\u8d4b\u4e88\u200b\u6211\u200b\u7a7f\u8d8a\u200b\u8346\u68d8\u200b\u7684\u200b\u529b\u91cf\u200b\u3002\u201d', '\u200b\u6c11\u200b\u643a\u624b\u200b\u52aa\u529b\u200b\uff0c\u200b\u5fc5\u5c06\u200b\u63a8\u52a8\u200b\u4e24\u56fd\u5173\u7cfb\u200b\u4e0d\u65ad\u200b\u5411\u524d\u200b\u53d1\u200b', '\u200b\u7814\u7a76\u5458\u200b\u83f2\u5c14\u200b\u8499\u200b\u00b7\u200b\u7279\u200b\u97e6\u5c14\u5fb7\u200b\u5341\u5206\u200b\u559c\u7231\u200b\u4e2d\u56fd\u200b\u6587\u200b', '\u200b\u6587\u200b\u6b4c\u66f2\u200b\u66f4\u597d\u200b\u5730\u200b\u7406\u89e3\u200b\u4e2d\u56fd\u200b\u6587\u5316\u200b\u3002\"\u200b\u5979\u200b\u8bf4\u200b\u3002', '\u200b\u7a46\u5362\u76d6\u200b\u5854\u200b\u5bc6\u5207\u200b\u5173\u6ce8\u200b\u4e2d\u56fd\u200b\u5728\u200b\u7ecf\u6d4e\u200b\u3001\u200b\u79d1\u6280\u200b\u3001\u200b\u6559\u200b', '\u200b\u5c55\u200b\u3002\"\u200b\u9c81\u592b\u5854\u200b\u8bf4\u200b\u3002', '\u200b\u5316\u200b\u3002\u200b\u4ed6\u200b\u8868\u793a\u200b\uff1a\u201c\u200b\u5b66\u4e60\u200b\u5f7c\u6b64\u200b\u7684\u200b\u8bed\u8a00\u200b\u548c\u200b\u6587\u5316\u200b\uff0c\u200b\u5c06\u200b\u5e2e\u200b', '\u201c\u200b\u59d0\u59d0\u200b\uff0c\u200b\u4f60\u200b\u60f3\u200b\u53bb\u200b\u4e2d\u56fd\u200b\u5417\u200b\uff1f\"\"\u200b\u975e\u5e38\u200b\u60f3\u200b\uff01\u200b\u6211\u200b\u60f3\u200b', '\u200b\u80b2\u200b\u7b49\u200b\u9886\u57df\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0c\u200b\u4e2d\u56fd\u200b\u5728\u200b\u79d1\u7814\u200b\u7b49\u200b\u65b9\u9762\u200b\u7684\u200b\u5b9e\u529b\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u9ad8\u7b49\u6559\u80b2\u200b\u59d4\u5458\u4f1a\u200b\u4e3b\u4efb\u200b\u52a9\u7406\u200b\u8428\u200b', '\u200b\u52a9\u5384\u4e2d\u200b\u4e24\u56fd\u4eba\u6c11\u200b\u66f4\u597d\u200b\u5730\u200b\u7406\u89e3\u200b\u5f7c\u6b64\u200b\uff0c\u200b\u52a9\u529b\u200b\u53cc\u65b9\u200b', '\u200b\u53bb\u200b\u770b\u200b\u6545\u5bab\u200b\u3001\u200b\u722c\u200b\u957f\u57ce\u200b\u3002\"\u200b\u5c24\u200b\u65af\u62c9\u200b\u7684\u200b\u5b66\u751f\u200b\u4e2d\u6709\u200b\u4e00\u5bf9\u200b', '\u200b\u4e0e\u65e5\u4ff1\u589e\u200b\u3002\u200b\u5728\u200b\u4e2d\u56fd\u200b\u5b66\u4e60\u200b\u7684\u200b\u7ecf\u5386\u200b\u8ba9\u200b\u6211\u200b\u770b\u5230\u200b\u66f4\u5e7f\u200b', '\u200b\u9a6c\u745e\u200b\u8868\u793a\u200b\uff1a\"\u200b\u6bcf\u5e74\u200b\u6211\u4eec\u200b\u90fd\u200b\u4f1a\u200b\u7ec4\u7ec7\u200b\u5b66\u751f\u200b\u5230\u200b\u4e2d\u56fd\u200b\u8bbf\u200b', '\u200b\u4ea4\u5f80\u200b\uff0c\u200b\u642d\u5efa\u200b\u53cb\u8c0a\u200b\u6865\u6881\u200b\u3002\"', '\u200b\u80fd\u6b4c\u5584\u821e\u200b\u7684\u200b\u59d0\u59b9\u200b\uff0c\u200b\u59d0\u59d0\u200b\u9732\u5a05\u200b\u4eca\u5e74\u200b15\u200b\u5c81\u200b\uff0c\u200b\u59b9\u59b9\u200b', '\u200b\u9614\u200b\u7684\u200b\u4e16\u754c\u200b\uff0c\u200b\u4ece\u4e2d\u200b\u53d7\u76ca\u532a\u6d45\u200b\u3002', '\u200b\u95ee\u200b\u5b66\u4e60\u200b\uff0c\u200b\u76ee\u524d\u200b\u6709\u200b\u8d85\u8fc7\u200b5000\u200b\u540d\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5b66\u751f\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u56fd\u5bb6\u535a\u7269\u9986\u200b\u9986\u957f\u200b\u5854\u5409\u200b\u4e01\u200b\u00b7\u200b\u52aa\u200b', '\u200b\u8389\u5a05\u200b14\u200b\u5c81\u200b\uff0c\u200b\u4e24\u4eba\u200b\u90fd\u200b\u5df2\u200b\u5728\u200b\u5384\u200b\u7279\u5b54\u9662\u200b\u5b66\u4e60\u200b\u591a\u5e74\u200b\uff0c', '23\u200b\u5c81\u200b\u7684\u200b\u8389\u8fea\u4e9a\u200b\u00b7\u200b\u57c3\u200b\u65af\u8482\u6cd5\u200b\u8bfa\u65af\u200b\u5df2\u200b\u5728\u200b\u5384\u7279\u200b', '\u200b\u5728\u200b\u4e2d\u56fd\u200b\u7559\u5b66\u200b\u3002\u200b\u5b66\u4e60\u200b\u4e2d\u56fd\u200b\u7684\u200b\u6559\u80b2\u200b\u7ecf\u9a8c\u200b\uff0c\u200b\u6709\u52a9\u4e8e\u200b', '\u200b\u91cc\u200b\u8fbe\u59c6\u200b\u00b7\u200b\u4f18\u7d20\u798f\u200b\u66fe\u591a\u6b21\u200b\u8bbf\u95ee\u200b\u4e2d\u56fd\u200b\uff0c\u200b\u5bf9\u200b\u4e2d\u534e\u6587\u660e\u200b', '\u200b\u4e2d\u6587\u200b\u8bf4\u200b\u5f97\u200b\u683c\u5916\u200b\u6d41\u5229\u200b\u3002', '\u200b\u5b54\u9662\u200b\u5b66\u4e60\u200b3\u200b\u5e74\u200b\uff0c\u200b\u5728\u200b\u4e2d\u56fd\u200b\u4e66\u6cd5\u200b\u3001\u200b\u4e2d\u56fd\u753b\u200b\u7b49\u200b\u65b9\u9762\u200b\u8868\u200b', '\u200b\u63d0\u5347\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u7684\u200b\u6559\u80b2\u200b\u6c34\u5e73\u200b\u3002\u201d', '\u200b\u7684\u200b\u4f20\u627f\u200b\u4e0e\u200b\u521b\u65b0\u200b\u3001\u200b\u73b0\u4ee3\u5316\u200b\u535a\u7269\u9986\u200b\u7684\u200b\u5efa\u8bbe\u200b\u4e0e\u200b\u53d1\u5c55\u200b', '\u200b\u9732\u5a05\u200b\u5bf9\u200b\u8bb0\u8005\u200b\u8bf4\u200b\uff1a\"\u200b\u8fd9\u4e9b\u5e74\u6765\u200b\uff0c\u200b\u6000\u7740\u200b\u5bf9\u200b\u4e2d\u6587\u200b', '\u200b\u73b0\u200b\u5341\u5206\u200b\u4f18\u79c0\u200b\uff0c\u200b\u5728\u200b2024\u200b\u5e74\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u8d5b\u533a\u200b\u7684\u200b', '\u201c\u200b\u5171\u540c\u200b\u5411\u200b\u4e16\u754c\u200b\u5c55\u793a\u200b\u975e\u200b', '\u200b\u5370\u8c61\u200b\u6df1\u523b\u200b\u3002\u201c\u200b\u4e2d\u56fd\u200b\u535a\u7269\u9986\u200b\u4e0d\u4ec5\u200b\u6709\u200b\u8bb8\u591a\u200b\u4fdd\u5b58\u200b\u5b8c\u597d\u200b', '\u200b\u548c\u200b\u4e2d\u56fd\u200b\u6587\u5316\u200b\u7684\u200b\u70ed\u7231\u200b\uff0c\u200b\u6211\u4eec\u200b\u59d0\u59b9\u4fe9\u200b\u59cb\u7ec8\u200b\u76f8\u4e92\u200b\u9f13\u200b', '\u201c\u200b\u6c49\u8bed\u200b\u6865\u200b\"\u200b\u6bd4\u8d5b\u200b\u4e2d\u200b\u83b7\u5f97\u200b\u4e00\u7b49\u5956\u200b\u3002\u200b\u8389\u8fea\u4e9a\u200b\u8bf4\u200b\uff1a\"\u200b\u5b66\u200b', '\u200b\u7684\u200b\u6587\u7269\u200b\uff0c\u200b\u8fd8\u200b\u5145\u5206\u8fd0\u7528\u200b\u5148\u8fdb\u200b\u79d1\u6280\u200b\u624b\u6bb5\u200b\u8fdb\u884c\u200b\u5c55\u793a\u200b\uff0c', '\u200b\u52b1\u200b\uff0c\u200b\u4e00\u8d77\u200b\u5b66\u4e60\u200b\u3002\u200b\u6211\u4eec\u200b\u7684\u200b\u4e2d\u6587\u200b\u4e00\u5929\u200b\u6bd4\u200b\u4e00\u5929\u200b\u597d\u200b\uff0c\u200b\u8fd8\u200b', '\u200b\u4e60\u200b\u4e2d\u56fd\u200b\u4e66\u6cd5\u200b\u8ba9\u200b\u6211\u200b\u7684\u200b\u5185\u5fc3\u200b\u53d8\u5f97\u200b\u5b89\u5b81\u200b\u548c\u200b\u7eaf\u7cb9\u200b\u3002\u200b\u6211\u200b', '\u200b\u6d32\u200b\u548c\u200b\u4e9a\u6d32\u200b\u7684\u200b\u707f\u70c2\u200b\u6587\u660e\u200b\u201d', '\u200b\u5e2e\u52a9\u200b\u4eba\u4eec\u200b\u66f4\u597d\u200b\u7406\u89e3\u200b\u4e2d\u534e\u6587\u660e\u200b\u3002\"\u200b\u5854\u5409\u200b\u4e01\u8bf4\u200b\uff0c\"\u200b\u5384\u200b', '\u200b\u5b66\u4f1a\u200b\u4e86\u200b\u4e2d\u6587\u200b\u6b4c\u200b\u548c\u200b\u4e2d\u56fd\u200b\u821e\u200b\u3002\u200b\u6211\u4eec\u200b\u4e00\u5b9a\u200b\u8981\u200b\u5230\u200b\u4e2d\u56fd\u200b', '\u200b\u4e5f\u200b\u559c\u6b22\u200b\u4e2d\u56fd\u200b\u7684\u200b\u670d\u9970\u200b,\u200b\u5e0c\u671b\u200b\u672a\u6765\u200b\u80fd\u200b\u53bb\u200b\u4e2d\u56fd\u200b\u5b66\u4e60\u200b\uff0c', '\u200b\u7acb\u200b\u7279\u91cc\u4e9a\u200b\u4e0e\u200b\u4e2d\u56fd\u200b\u90fd\u200b\u62e5\u6709\u200b\u60a0\u4e45\u200b\u7684\u200b\u6587\u660e\u200b\uff0c\u200b\u59cb\u7ec8\u200b\u76f8\u200b', '\u200b\u53bb\u200b\u3002\u200b\u5b66\u597d\u200b\u4e2d\u6587\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u672a\u6765\u200b\u4e0d\u662f\u200b\u68a6\u200b\uff01\"', '\u200b\u628a\u200b\u4e2d\u56fd\u200b\u4e0d\u540c\u200b\u6c11\u65cf\u200b\u5143\u7d20\u200b\u878d\u5165\u200b\u670d\u88c5\u8bbe\u8ba1\u200b\u4e2d\u200b\uff0c\u200b\u521b\u4f5c\u200b', '\u200b\u4ece\u200b\u963f\u65af\u9a6c\u62c9\u200b\u51fa\u53d1\u200b\uff0c\u200b\u6cbf\u7740\u200b\u873f\u8713\u200b\u66f2\u6298\u200b\u7684\u200b\u76d8\u5c71\u200b', '\u200b\u4e92\u200b\u7406\u89e3\u200b\u3001\u200b\u76f8\u4e92\u5c0a\u91cd\u200b\u3002\u200b\u6211\u200b\u5e0c\u671b\u200b\u672a\u6765\u200b\u4e0e\u200b\u4e2d\u56fd\u200b\u540c\u884c\u200b', '\u200b\u636e\u200b\u5384\u200b\u7279\u5b54\u9662\u200b\u4e2d\u65b9\u200b\u9662\u957f\u200b\u9ec4\u9e23\u98de\u200b\u4ecb\u7ecd\u200b\uff0c\u200b\u8fd9\u6240\u200b', '\u200b\u51fa\u200b\u66f4\u200b\u591a\u200b\u7cbe\u7f8e\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u4e5f\u200b\u628a\u200b\u5384\u7279\u200b\u6587\u5316\u200b\u5206\u4eab\u200b\u7ed9\u200b\u66f4\u200b\u591a\u200b', '\u200b\u516c\u8def\u200b\u4e00\u8def\u200b\u5411\u4e1c\u200b\u5bfb\u627e\u200b\u4e1d\u8def\u200b\u5370\u8ff9\u200b\u3002\u200b\u9a71\u8f66\u200b\u4e24\u4e2a\u200b\u5c0f\u200b', '\u200b\u52a0\u5f3a\u200b\u5408\u4f5c\u200b\uff0c\u200b\u5171\u540c\u200b\u5411\u200b\u4e16\u754c\u200b\u5c55\u793a\u200b\u975e\u6d32\u200b\u548c\u200b\u4e9a\u6d32\u200b\u7684\u200b\u707f\u200b', '\u200b\u5b54\u9662\u200b\u6210\u7acb\u200b\u4e8e\u200b2013\u200b\u5e74\u200b3\u200b\u6708\u200b\uff0c\u200b\u7531\u200b\u8d35\u5dde\u200b\u8d22\u7ecf\u5927\u5b66\u200b\u548c\u200b', '\u200b\u7684\u200b\u4e2d\u56fd\u200b\u670b\u53cb\u200b\u3002\u201d', '\u200b\u65f6\u200b\uff0c\u200b\u8bb0\u8005\u200b\u6765\u5230\u200b\u4f4d\u4e8e\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u6e2f\u53e3\u57ce\u5e02\u200b\u9a6c\u8428\u200b', '\u200b\u70c2\u200b\u6587\u660e\u200b\u3002\u201d'], 'rec_scores': array([0.99943757, ..., 0.98181838]), 'rec_boxes': array([[ 133, ...,  131],\n       ...,\n       [1152, ..., 1359]], dtype=int16)}}}\n\n</code></pre> <p>The result parameter description can be found in the result interpretation in 2.2.2 Python Script Integration.</p> <p>Note: Since the default model of the pipeline is relatively large, the inference speed may be slow. You can refer to the model list in Section 1 and replace it with a model that has faster inference speed.</p>"},{"location":"en/version3.x/pipeline_usage/PP-StructureV3.html#22-python-script-integration","title":"2.2 Python Script Integration","text":"<p>Just a few lines of code can complete the quick inference of the pipeline. Taking the PP-StructureV3 pipeline as an example:</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"PP-StructureV3\")\n</code></pre>"},{"location":"en/version3.x/pipeline_usage/PP-StructureV3.html#2-quick-start_1","title":"2. Quick Start","text":"<p>Before using the PP-StructureV3 pipeline locally, please make sure you have completed the installation of the wheel package according to the installation guide. After installation, you can use it via command line or Python integration.</p>"},{"location":"en/version3.x/pipeline_usage/PP-StructureV3.html#21-command-line-usage","title":"2.1 Command Line Usage","text":"<p>Use a single command to quickly experience the PP-StructureV3 pipeline:</p> <pre><code>paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png\n\n# Use --use_doc_orientation_classify to enable document orientation classification\npaddleocr pp_structurev3 -i ./pp_structure_v3_demo.png --use_doc_orientation_classify True\n\n# Use --use_doc_unwarping to enable document unwarping module\npaddleocr pp_structurev3 -i ./pp_structure_v3_demo.png --use_doc_unwarping True\n\n# Use --use_textline_orientation to enable text line orientation classification\npaddleocr pp_structurev3 -i ./pp_structure_v3_demo.png --use_textline_orientation False\n\n# Use --device to specify GPU for inference\npaddleocr pp_structurev3 -i ./pp_structure_v3_demo.png --device gpu\n</code></pre> Command line supports more parameters. Click to expand for detailed parameter descriptions Parameter Description Type Default <code>input</code> Data to be predicted. Required. .e.g., local path to image or PDF file: <code>/root/data/img.jpg</code>; URL, e.g., online image or PDF: example; local directory: directory containing images to predict, e.g., <code>/root/data/</code> (currently, directories with PDFs are not supported; PDFs must be specified by file path).  <code>str</code> <code>save_path</code> Path to save inference results. If not set, results will not be saved locally. <code>str</code> <code>layout_detection_model_name</code> Name of the layout detection model. If not set, the default model will be used. <code>str</code> <code>layout_detection_model_dir</code> Directory path of the layout detection model. If not set, the official model will be downloaded. <code>str</code> <code>layout_threshold</code> Score threshold for the layout model. Any value between <code>0-1</code>. If not set, the default value is used, which is <code>0.5</code>.  <code>float</code> <code>layout_nms</code> Whether to use Non-Maximum Suppression (NMS) as post-processing for layout detection. If not set, the parameter will default to the value initialized in the pipeline, which is set to <code>True</code> by default. <code>bool</code> <code>layout_unclip_ratio</code> Unclip ratio for detected boxes in layout detection model. Any float &gt; <code>0</code>. If not set, the default is <code>1.0</code>. <code>float</code> <code>layout_merge_bboxes_mode</code> The merging mode for the detection boxes output by the model in layout region detection. <ul> <li>large: When set to \"large\", only the largest outer bounding box will be retained for overlapping bounding boxes, and the inner overlapping boxes will be removed;</li> <li>small: When set to \"small\", only the smallest inner bounding boxes will be retained for overlapping bounding boxes, and the outer overlapping boxes will be removed;</li> <li>union: No filtering of bounding boxes will be performed, and both inner and outer boxes will be retained;</li> </ul>If not set, the default is <code>large</code>.  <code>str</code> <code>chart_recognition_model_name</code> Name of the chart recognition model. If not set, the default model will be used. <code>str</code> <code>chart_recognition_model_dir</code> Directory path of the chart recognition model. If not set, the official model will be downloaded. <code>str</code> <code>chart_recognition_batch_size</code> Batch size for the chart recognition model. If not set, the default batch size is <code>1</code>. <code>int</code> <code>region_detection_model_name</code> Name of the region detection model. If not set, the default model will be used. <code>str</code> <code>region_detection_model_dir</code> Directory path of the region detection model. If not set, the official model will be downloaded. <code>str</code> <code>doc_orientation_classify_model_name</code> Name of the document orientation classification model. If not set, the default model will be used. <code>str</code> <code>doc_orientation_classify_model_dir</code> Directory path of the document orientation classification model. If not set, the official model will be downloaded. <code>str</code> <code>doc_unwarping_model_name</code> Name of the document unwarping model. If not set, the default model will be used. <code>str</code> <code>doc_unwarping_model_dir</code> Directory path of the document unwarping model. If not set, the official model will be downloaded. <code>str</code> <code>text_detection_model_name</code> Name of the text detection model. If not set, the default model will be used. <code>str</code> <code>text_detection_model_dir</code> Directory path of the text detection model. If not set, the official model will be downloaded. <code>str</code> <code>text_det_limit_side_len</code> Image side length limitation for text detection. Any integer &gt; <code>0</code>. If not set, the default value will be <code>960</code>.  <code>int</code> <code>text_det_limit_type</code> Type of the image side length limit for text detection. supports <code>min</code> and <code>max</code>; <code>min</code> means ensuring the shortest side of the image is not less than <code>det_limit_side_len</code>, <code>max</code> means the longest side does not exceed <code>limit_side_len</code>. If not set, the default value will be <code>max</code>.  <code>str</code> <code>text_det_thresh</code> Pixel threshold for detection. Pixels with scores above this value in the probability map are considered text.Any float &gt; <code>0</code> . If not set, the default is <code>0.3</code>.  <code>float</code> <code>text_det_box_thresh</code> Box threshold. A bounding box is considered text if the average score of pixels inside is greater than this value. Any float &gt; <code>0</code>. If not set, the default is <code>0.6</code>.  <code>float</code> <code>text_det_unclip_ratio</code> Expansion ratio for text detection. The higher the value, the larger the expansion area. any float &gt; <code>0</code>. If not set, the default is <code>2.0</code>.  <code>float</code> <code>textline_orientation_model_name</code> Name of the text line orientation model. If not set, the default model will be used. <code>str</code> <code>textline_orientation_model_dir</code> Directory of the text line orientation model. If not set, the official model will be downloaded. <code>str</code> <code>textline_orientation_batch_size</code> Batch size for the text line orientation model. If not set, the default is <code>1</code>. <code>int</code> <code>text_recognition_model_name</code> Name of the text recognition model. If not set, the default model will be used. <code>str</code> <code>text_recognition_model_dir</code> Directory of the text recognition model. If not set, the official model will be downloaded. <code>str</code> <code>text_recognition_batch_size</code> Batch size for text recognition. If not set, the default is <code>1</code>. <code>int</code> <code>text_rec_score_thresh</code> Score threshold for text recognition. Only results above this value will be kept. Any float &gt; <code>0</code>. If not set, the default is <code>0.0</code> (no threshold).  <code>float</code> <code>table_classification_model_name</code> Name of the table classification model. If not set, the default model will be used. <code>str</code> <code>table_classification_model_dir</code> Directory of the table classification model. If not set, the official model will be downloaded. <code>str</code> <code>wired_table_structure_recognition_model_name</code> Name of the wired table structure recognition model. If not set, the default model will be used. <code>str</code> <code>wired_table_structure_recognition_model_dir</code> Directory of the wired table structure recognition model. If not set, the official model will be downloaded. <code>str</code> <code>wireless_table_structure_recognition_model_name</code> Name of the wireless table structure recognition model. If not set, the default model will be used. <code>str</code> <code>wireless_table_structure_recognition_model_dir</code> Directory of the wireless table structure recognition model. If not set, the official model will be downloaded. <code>str</code> <code>wired_table_cells_detection_model_name</code> Name of the wired table cell detection model. If not set, the default model will be used. <code>str</code> <code>wired_table_cells_detection_model_dir</code> Directory of the wired table cell detection model. If not set, the official model will be downloaded. <code>str</code> <code>wireless_table_cells_detection_model_name</code> Name of the wireless table cell detection model. If not set, the default model will be used. <code>str</code> <code>wireless_table_cells_detection_model_dir</code> Directory of the wireless table cell detection model. If not set, the official model will be downloaded. <code>str</code> <code>table_orientation_classify_model_name</code> Name of the wireless table orientation classification model. If not set, the default model will be used. <code>str</code> <code>table_orientation_classify_model_dir</code> Directory of the table orientation classification model. If not set, the official model will be downloaded. <code>str</code> <code>seal_text_detection_model_name</code> Name of the seal text detection model. If not set, the default model will be used. <code>str</code> <code>seal_text_detection_model_dir</code> Directory of the seal text detection model. If not set, the official model will be downloaded. <code>str</code> <code>seal_det_limit_side_len</code> Image side length limit for seal text detection. Any integer &gt; <code>0</code>. If not set, the default is <code>736</code>.  <code>int</code> <code>seal_det_limit_type</code> Limit type for image side in seal text detection. supports <code>min</code> and <code>max</code>; <code>min</code> ensures shortest side \u2265 <code>det_limit_side_len</code>, <code>max</code> ensures longest side \u2264 <code>limit_side_len</code>. If not set, the default is <code>min</code>.  <code>str</code> <code>seal_det_thresh</code> Pixel threshold. Pixels with scores above this value in the probability map are considered text. any float &gt; <code>0</code>. If not set, the default is <code>0.2</code>.  <code>float</code> <code>seal_det_box_thresh</code> Box threshold. Boxes with average pixel scores above this value are considered text regions. any float &gt; <code>0</code>. If not set, the default is <code>0.6</code>.  <code>float</code> <code>seal_det_unclip_ratio</code> Expansion ratio for seal text detection. Higher value means larger expansion area.Any float &gt; <code>0</code>. If not set, the default is <code>0.5</code>.  <code>float</code> <code>seal_text_recognition_model_name</code> Name of the seal text recognition model. If not set, the default model will be used. <code>str</code> <code>seal_text_recognition_model_dir</code> Directory of the seal text recognition model. If not set, the official model will be downloaded. <code>str</code> <code>seal_text_recognition_batch_size</code> Batch size for seal text recognition. If not set, the default is <code>1</code>. <code>int</code> <code>seal_rec_score_thresh</code> Recognition score threshold. Text results above this value will be kept. Any float &gt; <code>0</code>. If not set, the default is <code>0.0</code> (no threshold).  <code>float</code> <code>formula_recognition_model_name</code> Name of the formula recognition model. If not set, the default model will be used. <code>str</code> <code>formula_recognition_model_dir</code> Directory of the formula recognition model. If not set, the official model will be downloaded. <code>str</code> <code>formula_recognition_batch_size</code> Batch size of the formula recognition model. If not set, the default is <code>1</code>. <code>int</code> <code>use_doc_orientation_classify</code> Whether to load and use document orientation classification module. If not set, the default is <code>True</code>. <code>bool</code> <code>use_doc_unwarping</code> Whether to load and use document unwarping module. If not set, the default is <code>True</code>. <code>bool</code> <code>use_textline_orientation</code> Whether to load and use the text line orientation classification module. If not set, the default is <code>True</code>. <code>bool</code> <code>use_seal_recognition</code> Whether to load and use seal recognition subpipeline. If not set, the default is <code>True</code>. <code>bool</code> <code>use_table_recognition</code> Whether to load and use table recognition subpipeline. If not set, the default is <code>True</code>. <code>bool</code> <code>use_formula_recognition</code> Whether to load and use formula recognition subpipeline. If not set, the default is <code>True</code>. <code>bool</code> <code>use_chart_recognition</code> Whether to load and use the chart recognition sub-pipeline. If not set, the default is <code>True</code>. <code>bool</code> <code>use_region_detection</code> Whether to load and use the document region detection pipeline. If not set, the default is <code>True</code>. <code>bool</code> <code>device</code> Device for inference. You can specify a device ID: <ul> <li>CPU: e.g., <code>cpu</code> means using CPU for inference;</li> <li>GPU: e.g., <code>gpu:0</code> means GPU 0</li> <li>NPU: e.g., <code>npu:0</code> means NPU 0</li> <li>XPU: e.g., <code>xpu:0</code> means XPU 0</li> <li>MLU: e.g., <code>mlu:0</code> means MLU 0</li> <li>DCU: e.g., <code>dcu:0</code> means DCU 0</li> </ul>If not set, the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.  <code>str</code> <code>enable_hpi</code> Whether to enable high performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Computation precision, e.g., fp32, fp16. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set. <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads to use when inferring on CPU. <code>int</code> <code>8</code> <code>paddlex_config</code> Path to the PaddleX pipeline configuration file. <code>str</code> <p></p> <p>The inference result will be printed in the terminal. The default output of the PP-StructureV3 pipeline is as follows:</p>  \ud83d\udc49Click to expand <pre>\n<code>\n{'res': {'input_path': '/root/.paddlex/predict_input/pp_structure_v3_demo.png', 'page_index': None, 'model_settings': {'use_doc_preprocessor': True, 'use_seal_recognition': True, 'use_table_recognition': True, 'use_formula_recognition': True}, 'doc_preprocessor_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_orientation_classify': True, 'use_doc_unwarping': True}, 'angle': 0}, 'layout_det_res': {'input_path': None, 'page_index': None, 'boxes': [{'cls_id': 2, 'label': 'text', 'score': 0.9848763942718506, 'coordinate': [743.2788696289062, 777.3158569335938, 1115.24755859375, 1067.84228515625]}, {'cls_id': 2, 'label': 'text', 'score': 0.9827454686164856, 'coordinate': [1137.95556640625, 1127.66943359375, 1524, 1367.6356201171875]}, {'cls_id': 1, 'label': 'image', 'score': 0.9813530445098877, 'coordinate': [755.2349243164062, 184.64149475097656, 1523.7294921875, 684.6146392822266]}, {'cls_id': 2, 'label': 'text', 'score': 0.980336606502533, 'coordinate': [350.7603759765625, 1148.5648193359375, 706.8020629882812, 1367.00341796875]}, {'cls_id': 2, 'label': 'text', 'score': 0.9798877239227295, 'coordinate': [1147.3890380859375, 802.6549072265625, 1523.9051513671875, 994.9046630859375]}, {'cls_id': 2, 'label': 'text', 'score': 0.9724758863449097, 'coordinate': [741.2205810546875, 1074.2657470703125, 1110.120849609375, 1191.2010498046875]}, {'cls_id': 2, 'label': 'text', 'score': 0.9724437594413757, 'coordinate': [355.6563415527344, 899.6616821289062, 710.9073486328125, 1042.1270751953125]}, {'cls_id': 2, 'label': 'text', 'score': 0.9723313450813293, 'coordinate': [0, 181.92404174804688, 334.43384313583374, 330.294677734375]}, {'cls_id': 2, 'label': 'text', 'score': 0.9720360636711121, 'coordinate': [356.7376403808594, 753.35302734375, 714.37841796875, 892.6129760742188]}, {'cls_id': 2, 'label': 'text', 'score': 0.9711183905601501, 'coordinate': [1144.5242919921875, 1001.2548217773438, 1524, 1120.6578369140625]}, {'cls_id': 2, 'label': 'text', 'score': 0.9707457423210144, 'coordinate': [0, 849.873291015625, 325.0664693713188, 1067.2911376953125]}, {'cls_id': 2, 'label': 'text', 'score': 0.9700680375099182, 'coordinate': [363.04437255859375, 289.2635498046875, 719.1571655273438, 427.5818786621094]}, {'cls_id': 2, 'label': 'text', 'score': 0.9693533182144165, 'coordinate': [359.4466857910156, 606.0006103515625, 717.9885864257812, 746.55126953125]}, {'cls_id': 2, 'label': 'text', 'score': 0.9682930111885071, 'coordinate': [0.050221771001815796, 1073.1942138671875, 323.85799154639244, 1191.3121337890625]}, {'cls_id': 2, 'label': 'text', 'score': 0.9649553894996643, 'coordinate': [0.7939082384109497, 1198.5465087890625, 321.2581721544266, 1317.218017578125]}, {'cls_id': 2, 'label': 'text', 'score': 0.9644040465354919, 'coordinate': [0, 337.225830078125, 332.2462143301964, 428.298583984375]}, {'cls_id': 2, 'label': 'text', 'score': 0.9637495279312134, 'coordinate': [365.5925598144531, 188.2151336669922, 718.556640625, 283.7483215332031]}, {'cls_id': 2, 'label': 'text', 'score': 0.9603620767593384, 'coordinate': [355.30633544921875, 1048.5457763671875, 708.771484375, 1141.828369140625]}, {'cls_id': 2, 'label': 'text', 'score': 0.9508902430534363, 'coordinate': [361.0450744628906, 530.7780151367188, 719.6325073242188, 599.1027221679688]}, {'cls_id': 2, 'label': 'text', 'score': 0.9459834694862366, 'coordinate': [0.035085976123809814, 532.7417602539062, 330.5401824116707, 772.7175903320312]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.9400503635406494, 'coordinate': [760.1524658203125, 1214.560791015625, 1085.24853515625, 1274.7890625]}, {'cls_id': 2, 'label': 'text', 'score': 0.9341079592704773, 'coordinate': [1.025873064994812, 777.8804931640625, 326.99016749858856, 844.8532104492188]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.9259933233261108, 'coordinate': [0.11050379276275635, 450.3547058105469, 311.77746546268463, 510.5243835449219]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.9208691716194153, 'coordinate': [380.79510498046875, 447.859130859375, 698.1744384765625, 509.0489807128906]}, {'cls_id': 2, 'label': 'text', 'score': 0.8683002591133118, 'coordinate': [1149.1656494140625, 778.3809814453125, 1339.960205078125, 796.5060424804688]}, {'cls_id': 2, 'label': 'text', 'score': 0.8455104231834412, 'coordinate': [561.3448486328125, 140.87547302246094, 915.4432983398438, 162.76724243164062]}, {'cls_id': 11, 'label': 'doc_title', 'score': 0.735536515712738, 'coordinate': [76.71978759765625, 0, 1400.4561157226562, 98.32131713628769]}, {'cls_id': 6, 'label': 'figure_title', 'score': 0.7187536954879761, 'coordinate': [790.4249267578125, 704.4551391601562, 1509.9013671875, 747.6876831054688]}, {'cls_id': 2, 'label': 'text', 'score': 0.6218013167381287, 'coordinate': [737.427001953125, 1296.2047119140625, 1104.2994384765625, 1368]}]}, 'overall_ocr_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_preprocessor': False, 'use_textline_orientation': True}, 'dt_polys': array([[[  77,    0],\n        ...,\n        [  76,   98]],\n\n       ...,\n\n       [[1142, 1350],\n        ...,\n        [1142, 1367]]], dtype=int16), 'text_det_params': {'limit_side_len': 736, 'limit_type': 'min', 'thresh': 0.3, 'box_thresh': 0.6, 'unclip_ratio': 1.5}, 'text_type': 'general', 'textline_orientation_angles': array([0, ..., 0]), 'text_rec_score_thresh': 0.0, 'rec_texts': ['\u200b\u52a9\u529b\u200b\u53cc\u65b9\u200b\u4ea4\u5f80\u200b', '\u200b\u642d\u5efa\u200b\u53cb\u8c0a\u200b\u6865\u6881\u200b', '\u200b\u672c\u62a5\u8bb0\u8005\u200b\u6c88\u5c0f\u6653\u4efb\u200b\u5f66\u200b', '\u200b\u9ec4\u57f9\u7167\u200b', '\u200b\u8eab\u7740\u200b\u4e2d\u56fd\u200b\u4f20\u7edf\u200b\u6c11\u65cf\u670d\u88c5\u200b\u7684\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u9752\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u9ad8\u7b49\u6559\u80b2\u200b\u4e0e\u200b\u7814\u7a76\u9662\u200b\u5408\u4f5c\u200b\u5efa\u7acb\u200b\uff0c\u200b\u5f00\u200b', '\u200b\u6b21\u200b\u767b\u53f0\u200b\u8868\u6f14\u200b\u4e2d\u56fd\u200b\u6c11\u65cf\u821e\u200b\u3001\u200b\u73b0\u4ee3\u821e\u200b\u3001\u200b\u6247\u5b50\u821e\u200b', '\u200b\u8bbe\u200b\u4e86\u200b\u4e2d\u56fd\u200b\u8bed\u8a00\u200b\u8bfe\u7a0b\u200b\u548c\u200b\u4e2d\u56fd\u200b\u6587\u5316\u200b\u8bfe\u7a0b\u200b\uff0c\u200b\u6ce8\u518c\u200b\u5b66\u200b', '\u200b\u66fc\u5999\u200b\u7684\u200b\u821e\u59ff\u200b\u8d62\u5f97\u200b\u73b0\u573a\u200b\u89c2\u4f17\u200b\u9635\u9635\u200b\u638c\u58f0\u200b\u3002\u200b\u8fd9\u200b', '\u200b\u751f\u200b2\u200b\u4e07\u4f59\u4eba\u6b21\u200b\u300210\u200b\u4f59\u5e74\u200b\u6765\u200b\uff0c\u200b\u5384\u7279\u200b\u5b54\u9662\u200b\u5df2\u200b\u6210\u4e3a\u200b', '\u200b\u65e5\u524d\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u9ad8\u7b49\u6559\u80b2\u200b\u4e0e\u200b\u7814\u7a76\u9662\u200b\u5b54\u5b50\u200b\u5b66\u200b', '\u200b\u5f53\u5730\u200b\u6c11\u4f17\u200b\u4e86\u89e3\u200b\u4e2d\u56fd\u200b\u7684\u200b\u4e00\u6247\u200b\u7a97\u53e3\u200b\u3002', '\u200b\u4ee5\u4e0b\u200b\u7b80\u79f0\u200b\"\u200b\u5384\u7279\u200b\u5b54\u9662\u200b\")\u200b\u4e3e\u529e\u200b\u201c\u200b\u559c\u8fce\u200b\u65b0\u5e74\u200b\"\u200b\u4e2d\u56fd\u200b', '\u200b\u9ec4\u9e23\u98de\u200b\u8868\u793a\u200b\uff0c\u200b\u968f\u7740\u200b\u6765\u200b\u5b66\u4e60\u200b\u4e2d\u6587\u200b\u7684\u200b\u4eba\u200b\u65e5\u76ca\u200b', '\u200b\u821e\u200b\u6bd4\u8d5b\u200b\u7684\u200b\u573a\u666f\u200b\u3002', '\u200b\u589e\u591a\u200b\uff0c\u200b\u963f\u65af\u9a6c\u62c9\u200b\u5927\u5b66\u200b\u6559\u5b66\u70b9\u200b\u5df2\u200b\u96be\u4ee5\u200b\u6ee1\u8db3\u200b\u6559\u5b66\u200b', '\u200b\u4e2d\u56fd\u200b\u548c\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u4f20\u7edf\u53cb\u8c0a\u200b\u6df1\u539a\u200b\u3002\u200b\u8fd1\u5e74\u200b', '\u200b\u9700\u8981\u200b\u30022024\u200b\u5e74\u200b4\u200b\u6708\u200b\uff0c\u200b\u7531\u200b\u4e2d\u4f01\u200b\u8700\u9053\u200b\u96c6\u56e2\u200b\u6240\u5c5e\u200b\u56db\u200b', '\u200b\u5728\u200b\u9ad8\u8d28\u91cf\u200b\u5171\u5efa\u200b\"\u200b\u4e00\u5e26\u200b\u4e00\u8def\u200b\"\u200b\u6846\u67b6\u200b\u4e0b\u200b\uff0c\u200b\u4e2d\u200b\u5384\u200b\u4e24\u200b', '\u200b\u5ddd\u200b\u8def\u6865\u200b\u627f\u5efa\u200b\u7684\u200b\u5b54\u9662\u200b\u6559\u5b66\u697c\u200b\u9879\u76ee\u200b\u5728\u200b\u963f\u65af\u9a6c\u62c9\u200b\u5f00\u200b', '\u200b\u4eba\u6587\u200b\u4ea4\u6d41\u200b\u4e0d\u65ad\u200b\u6df1\u5316\u200b\uff0c\u200b\u4e92\u5229\u200b\u5408\u4f5c\u200b\u7684\u200b\u6c11\u610f\u57fa\u7840\u200b', '\u200b\u5de5\u200b\u5efa\u8bbe\u200b\uff0c\u200b\u9884\u8ba1\u200b\u4eca\u5e74\u200b\u4e0a\u534a\u5e74\u200b\u7ae3\u5de5\u200b\uff0c\u200b\u5efa\u6210\u200b\u540e\u200b\u5c06\u200b\u4e3a\u200b\u5384\u200b', '\u200b\u76ca\u200b\u6df1\u539a\u200b\u3002', '\u200b\u7279\u5b54\u9662\u200b\u63d0\u4f9b\u200b\u5168\u65b0\u200b\u7684\u200b\u529e\u5b66\u200b\u573a\u5730\u200b\u3002', '\u200b\u5b66\u597d\u200b\u4e2d\u6587\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b', '\u25a1', '\u200b\u5728\u200b\u4e2d\u56fd\u200b\u5b66\u4e60\u200b\u7684\u200b\u7ecf\u5386\u200b', '\u200b\u672a\u6765\u200b\u4e0d\u662f\u200b\u68a6\u200b\u201d', '\u200b\u8ba9\u200b\u6211\u200b\u770b\u5230\u200b\u66f4\u200b\u5e7f\u9614\u200b\u7684\u200b\u4e16\u754c\u200b\u201d', '\u201c\u200b\u9c9c\u82b1\u200b\u66fe\u200b\u544a\u8bc9\u200b\u6211\u200b\u4f60\u200b\u600e\u6837\u200b\u8d70\u8fc7\u200b\uff0c\u200b\u5927\u5730\u200b\u77e5\u9053\u200b\u4f60\u200b', '\u200b\u591a\u5e74\u200b\u6765\u200b\uff0c\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5e7f\u5927\u200b\u8d74\u534e\u200b\u7559\u5b66\u751f\u200b\u548c\u200b', '\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u89d2\u843d\u200b\"\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u963f\u65af\u9a6c\u62c9\u200b', '\u200b\u57f9\u8bad\u200b\u4eba\u5458\u200b\u79ef\u6781\u200b\u6295\u8eab\u200b\u56fd\u5bb6\u200b\u5efa\u8bbe\u200b\uff0c\u200b\u6210\u4e3a\u200b\u52a9\u529b\u200b\u8be5\u56fd\u200b', '\u200b\u7efc\u5408\u697c\u200b\u4e8c\u5c42\u200b\uff0c\u200b\u4e00\u9635\u200b\u4f18\u7f8e\u200b\u7684\u200b\u6b4c\u58f0\u200b\u5728\u200b\u8d70\u5eca\u200b\u91cc\u200b\u56de\u200b', '\u200b\u53d1\u5c55\u200b\u7684\u200b\u4eba\u624d\u200b\u548c\u200b\u5384\u200b\u4e2d\u200b\u53cb\u597d\u200b\u7684\u200b\u89c1\u8bc1\u8005\u200b\u548c\u200b\u63a8\u52a8\u8005\u200b\u3002', '\u200b\u5faa\u7740\u200b\u719f\u6089\u200b\u7684\u200b\u65cb\u5f8b\u200b\u8f7b\u8f7b\u200b\u63a8\u5f00\u200b\u4e00\u95f4\u200b\u6559\u5ba4\u200b\u7684\u200b\u95e8\u200b\uff0c', '\u200b\u5728\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5168\u56fd\u200b\u5987\u5973\u200b\u8054\u76df\u200b\u5de5\u4f5c\u200b\u7684\u200b\u7ea6\u7ff0\u200b', '\u200b\u4eec\u200b\u6b63\u200b\u8ddf\u7740\u200b\u8001\u5e08\u200b\u5b66\u5531\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\u300a\u200b\u540c\u4e00\u9996\u6b4c\u200b\u300b\u3002', '\u200b\u5a1c\u200b\u00b7\u200b\u7279\u200b\u97e6\u5c14\u5fb7\u200b\u00b7\u200b\u51ef\u83b1\u200b\u5854\u200b\u5c31\u662f\u200b\u5176\u4e2d\u200b\u4e00\u4f4d\u200b\u3002\u200b\u5979\u200b\u66fe\u200b\u5728\u200b', '\u200b\u8fd9\u200b\u662f\u200b\u5384\u200b\u7279\u5b54\u9662\u200b\u963f\u65af\u9a6c\u62c9\u200b\u5927\u5b66\u200b\u6559\u5b66\u70b9\u200b\u7684\u200b\u4e00\u200b', '\u200b\u4e2d\u534e\u200b\u5973\u5b50\u200b\u5b66\u9662\u200b\u653b\u8bfb\u200b\u7855\u58eb\u5b66\u4f4d\u200b\uff0c\u200b\u7814\u7a76\u200b\u65b9\u5411\u200b\u662f\u200b\u5973\u200b', '\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\u8bfe\u200b\u3002\u200b\u4e3a\u4e86\u200b\u8ba9\u200b\u5b66\u751f\u200b\u4eec\u200b\u66f4\u597d\u200b\u5730\u200b\u7406\u89e3\u200b\u6b4c\u200b', '\u200b\u6027\u200b\u9886\u5bfc\u529b\u200b\u4e0e\u200b\u793e\u4f1a\u200b\u53d1\u5c55\u200b\u3002\u200b\u5176\u95f4\u200b\uff0c\u200b\u5979\u200b\u5b9e\u5730\u200b\u8d70\u8bbf\u200b\u4e2d\u56fd\u200b', '\u200b\u5927\u610f\u200b\uff0c\u200b\u8001\u5e08\u200b\u5c24\u200b\u65af\u62c9\u200b\u00b7\u200b\u7a46\u7f55\u9ed8\u5fb7\u200b\u8428\u5c14\u200b\u00b7\u200b\u4faf\u8d5b\u56e0\u200b\u9010\u200b', '\u200b\u591a\u4e2a\u200b\u5730\u533a\u200b\uff0c\u200b\u83b7\u5f97\u200b\u4e86\u200b\u89c2\u5bdf\u200b\u4e2d\u56fd\u200b\u793e\u4f1a\u200b\u53d1\u5c55\u200b\u7684\u200b\u7b2c\u4e00\u200b', '\u200b\u5728\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u4e0d\u4e45\u524d\u200b\u4e3e\u529e\u200b\u7684\u200b\u7b2c\u516d\u5c4a\u200b\u4e2d\u56fd\u200b\u98ce\u7b5d\u200b\u6587\u5316\u8282\u200b\u4e0a\u200b\uff0c\u200b\u5f53\u5730\u200b\u5c0f\u5b66\u751f\u200b\u4f53\u9a8c\u200b\u98ce\u7b5d\u200b\u5236\u4f5c\u200b\u3002', '\u200b\u8bd1\u200b\u548c\u200b\u89e3\u91ca\u200b\u6b4c\u8bcd\u200b\u3002\u200b\u968f\u7740\u200b\u4f34\u594f\u200b\u58f0\u54cd\u200b\u8d77\u200b\uff0c\u200b\u5b66\u751f\u200b\u4eec\u200b', '\u200b\u624b\u200b\u8d44\u6599\u200b\u3002', '\u200b\u4e2d\u56fd\u200b\u9a7b\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5927\u4f7f\u9986\u200b\u4f9b\u56fe\u200b', '\u200b\u660c\u8fb9\u200b\u968f\u7740\u200b\u8282\u62cd\u200b\u6447\u52a8\u200b\u8eab\u4f53\u200b\uff0c\u200b\u73b0\u573a\u200b\u6c14\u6c1b\u200b\u70ed\u70c8\u200b\u3002', '\u200b\u8c08\u8d77\u200b\u5728\u200b\u4e2d\u56fd\u200b\u6c42\u5b66\u200b\u7684\u200b\u7ecf\u5386\u200b\uff0c\u200b\u7ea6\u7ff0\u200b\u5a1c\u200b\u8bb0\u5fc6\u200b\u72b9\u200b', '\u201c\u200b\u8fd9\u662f\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\u521d\u7ea7\u73ed\u200b\uff0c\u200b\u5171\u6709\u200b32\u200b\u4eba\u200b\u3002\u200b\u5b66\u200b', '\u200b\u65b0\u200b\uff1a\u201c\u200b\u4e2d\u56fd\u200b\u7684\u200b\u53d1\u5c55\u200b\u5728\u200b\u5f53\u4eca\u4e16\u754c\u200b\u662f\u200b\u72ec\u4e00\u65e0\u4e8c\u200b\u7684\u200b\u3002', '\u201c\u200b\u4e0d\u7ba1\u200b\u8fdc\u8fd1\u200b\u90fd\u200b\u662f\u200b\u5ba2\u4eba\u200b\uff0c\u200b\u8bf7\u200b\u4e0d\u7528\u200b\u5ba2\u6c14\u200b\uff1b\u200b\u76f8\u7ea6\u200b', '\u200b\u74e6\u200b\u7684\u200b\u5317\u200b\u7ea2\u6d77\u200b\u7701\u200b\u535a\u7269\u9986\u200b\u3002', '\u200b\u5927\u90e8\u5206\u200b\u6765\u81ea\u200b\u9996\u90fd\u200b\u963f\u65af\u9a6c\u62c9\u200b\u7684\u200b\u4e2d\u5c0f\u5b66\u200b\uff0c\u200b\u5e74\u9f84\u200b', '\u200b\u6cbf\u7740\u200b\u4e2d\u56fd\u200b\u7279\u8272\u200b\u793e\u4f1a\u4e3b\u4e49\u200b\u9053\u8def\u200b\u575a\u5b9a\u200b\u524d\u884c\u200b\uff0c\u200b\u4e2d\u56fd\u200b', '\u200b\u597d\u200b\u4e86\u200b\u5728\u200b\u4e00\u8d77\u200b\uff0c\u200b\u6211\u4eec\u200b\u6b22\u8fce\u200b\u4f60\u200b\u2026\"\u200b\u5728\u200b\u4e00\u573a\u200b\u4e2d\u5384\u9752\u200b', '\u200b\u535a\u7269\u9986\u200b\u4e8c\u5c42\u200b\u9648\u5217\u200b\u7740\u200b\u4e00\u4e2a\u200b\u53d1\u6398\u200b\u81ea\u963f\u675c\u79be\u200b', '\u200b\u5c0f\u200b\u7684\u200b\u4ec5\u200b\u6709\u200b6\u200b\u5c81\u200b\u3002\"\u200b\u5c24\u200b\u65af\u62c9\u200b\u544a\u8bc9\u200b\u8bb0\u8005\u200b\u3002', '\u200b\u521b\u9020\u200b\u4e86\u200b\u53d1\u5c55\u200b\u5947\u8ff9\u200b\uff0c\u200b\u8fd9\u200b\u4e00\u5207\u200b\u90fd\u200b\u79bb\u4e0d\u5f00\u200b\u4e2d\u56fd\u5171\u4ea7\u515a\u200b', '\u200b\u5e74\u200b\u8054\u8c0a\u200b\u6d3b\u52a8\u200b\u4e0a\u200b\uff0c\u200b\u56db\u5ddd\u200b\u8def\u6865\u200b\u4e2d\u65b9\u200b\u5458\u5de5\u200b\u540c\u200b\u5f53\u5730\u200b\u5927\u200b', '\u200b\u65af\u200b\u53e4\u57ce\u200b\u7684\u200b\u4e2d\u56fd\u200b\u53e4\u4ee3\u200b\u9676\u5236\u200b\u9152\u5668\u200b\uff0c\u200b\u7f50\u200b\u8eab\u4e0a\u200b\u5199\u200b', '\u200b\u5c24\u200b\u65af\u62c9\u200b\u4eca\u5e74\u200b23\u200b\u5c81\u200b\uff0c\u200b\u662f\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u4e00\u6240\u200b\u516c\u7acb\u200b', '\u200b\u7684\u200b\u9886\u5bfc\u200b\u3002\u200b\u4e2d\u56fd\u200b\u7684\u200b\u53d1\u5c55\u200b\u7ecf\u9a8c\u200b\u503c\u5f97\u200b\u8bb8\u591a\u200b\u56fd\u5bb6\u200b\u5b66\u4e60\u200b', '\u200b\u5b66\u751f\u200b\u5408\u5531\u200b\u300a\u200b\u5317\u4eac\u200b\u6b22\u8fce\u200b\u4f60\u200b\u300b\u3002\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u6280\u672f\u200b\u5b66\u200b', '\u201c\u200b\u4e07\u200b\u201d\u201c\u200b\u548c\u200b\u201d\u201c\u200b\u7985\u200b\u201d\u201c\u200b\u5c71\u200b\"\u200b\u7b49\u200b\u6c49\u5b57\u200b\u3002\u201c\u200b\u8fd9\u4ef6\u200b\u6587\u7269\u200b\u8bc1\u200b', '\u200b\u4ea4\u200b\u7684\u200b\u827a\u672f\u200b\u8001\u5e08\u200b\u3002\u200b\u5979\u200b12\u200b\u5c81\u200b\u5f00\u59cb\u200b\u5728\u200b\u5384\u200b\u7279\u5b54\u200b\u9662\u5b66\u200b', '\u200b\u501f\u9274\u200b\u3002\u201d', '\u200b\u9662\u200b\u8ba1\u7b97\u673a\u79d1\u5b66\u200b\u4e0e\u200b\u5de5\u7a0b\u200b\u4e13\u4e1a\u200b\u5b66\u751f\u200b\u9c81\u592b\u5854\u200b\u00b7\u200b\u8c22\u62c9\u200b', '\u200b\u660e\u200b\uff0c\u200b\u5f88\u65e9\u4ee5\u524d\u200b\u6211\u4eec\u200b\u5c31\u200b\u901a\u8fc7\u200b\u6d77\u4e0a\u200b\u4e1d\u7ef8\u4e4b\u8def\u200b\u8fdb\u884c\u200b', '\u200b\u4e2d\u6587\u200b\uff0c\u200b\u5728\u200b2017\u200b\u5e74\u200b\u7b2c\u5341\u5c4a\u200b\u201c\u200b\u6c49\u8bed\u200b\u6865\u200b\"\u200b\u4e16\u754c\u200b\u4e2d\u5b66\u751f\u200b', '\u200b\u6b63\u5728\u200b\u897f\u5357\u200b\u5927\u5b66\u200b\u5b66\u4e60\u200b\u7684\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u535a\u58eb\u751f\u200b', '\u200b\u662f\u200b\u5176\u4e2d\u200b\u4e00\u540d\u200b\u6f14\u5531\u8005\u200b\uff0c\u200b\u5979\u200b\u5f88\u65e9\u200b\u4fbf\u200b\u5728\u200b\u5b54\u9662\u200b\u5b66\u4e60\u200b\u4e2d\u200b', '\u200b\u8d38\u6613\u5f80\u6765\u200b\u4e0e\u200b\u6587\u5316\u4ea4\u6d41\u200b\u3002\u200b\u8fd9\u200b\u4e5f\u200b\u662f\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b', '\u200b\u6587\u200b\u6bd4\u8d5b\u200b\u4e2d\u200b\u83b7\u5f97\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u8d5b\u533a\u200b\u7b2c\u4e00\u540d\u200b\uff0c\u200b\u5e76\u200b\u548c\u200b', '\u200b\u7a46\u5362\u76d6\u200b\u5854\u200b\u00b7\u200b\u6cfd\u7a46\u200b\u4f0a\u5bf9\u200b\u4e2d\u56fd\u200b\u6000\u6709\u200b\u6df1\u539a\u611f\u60c5\u200b\u30028', '\u200b\u6587\u200b\uff0c\u200b\u4e00\u76f4\u200b\u5728\u200b\u4e3a\u200b\u53bb\u200b\u4e2d\u56fd\u200b\u7559\u5b66\u200b\u4f5c\u200b\u51c6\u5907\u200b\u3002\u201c\u200b\u8fd9\u53e5\u200b\u6b4c\u8bcd\u200b', '\u200b\u4e0e\u200b\u4e2d\u56fd\u200b\u53cb\u597d\u200b\u4ea4\u5f80\u200b\u5386\u53f2\u200b\u7684\u200b\u6709\u529b\u200b\u8bc1\u660e\u200b\u3002\u201d\u200b\u5317\u200b\u7ea2\u6d77\u200b', '\u200b\u534a\u200b\u4ee3\u8868\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u524d\u5f80\u200b\u4e2d\u56fd\u200b\u53c2\u52a0\u200b\u51b3\u8d5b\u200b\uff0c\u200b\u83b7\u5f97\u200b', '\u200b\u5e74\u524d\u200b\uff0c\u200b\u5728\u200b\u5317\u4eac\u5e08\u8303\u5927\u5b66\u200b\u83b7\u5f97\u200b\u7855\u58eb\u5b66\u4f4d\u200b\u540e\u200b\uff0c\u200b\u7a46\u5362\u200b', '\u200b\u662f\u200b\u6211\u4eec\u200b\u4e24\u56fd\u4eba\u6c11\u200b\u53cb\u8c0a\u200b\u7684\u200b\u751f\u52a8\u200b\u5199\u7167\u200b\u3002\u200b\u65e0\u8bba\u662f\u200b\u6295\u200b', '\u200b\u7701\u200b\u535a\u7269\u9986\u200b\u7814\u7a76\u200b\u4e0e\u200b\u6587\u732e\u200b\u90e8\u200b\u8d1f\u8d23\u4eba\u200b\u4f0a\u8428\u200b\u4e9a\u65af\u200b\u00b7\u200b\u7279\u200b', '\u200b\u672c\u200b\u4f18\u80dc\u5956\u200b\u30022022\u200b\u5e74\u200b\u8d77\u200b\uff0c\u200b\u5c24\u200b\u65af\u62c9\u200b\u5f00\u59cb\u200b\u5728\u200b\u5384\u200b\u7279\u5b54\u200b', '\u200b\u76d6\u5854\u200b\u5728\u200b\u793e\u4ea4\u200b\u5a92\u4f53\u200b\u4e0a\u200b\u5199\u4e0b\u200b\u8fd9\u6837\u200b\u4e00\u6bb5\u8bdd\u200b\uff1a\u201c\u200b\u8fd9\u200b\u662f\u200b\u6211\u200b', '\u200b\u8eab\u4e8e\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u57fa\u7840\u8bbe\u65bd\u200b\u5efa\u8bbe\u200b\u7684\u200b\u4e2d\u4f01\u200b\u5458\u5de5\u200b\uff0c', '\u200b\u65af\u6cd5\u5179\u5409\u8bf4\u200b\u3002', '\u200b\u517c\u804c\u200b\u6559\u6388\u200b\u4e2d\u6587\u6b4c\u66f2\u200b\uff0c\u200b\u6bcf\u200b\u5468\u672b\u200b\u4e24\u4e2a\u200b\u8bfe\u65f6\u200b\u3002\u201c\u200b\u4e2d\u56fd\u200b', '\u200b\u4eba\u751f\u200b\u7684\u200b\u91cd\u8981\u200b\u4e00\u6b65\u200b\uff0c\u200b\u81ea\u6b64\u200b\u6211\u200b\u62e5\u6709\u200b\u4e86\u200b\u4e00\u53cc\u200b\u575a\u56fa\u200b\u7684\u200b', '\u200b\u8fd8\u662f\u200b\u5728\u200b\u4e2d\u56fd\u200b\u7559\u5b66\u200b\u7684\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5b66\u5b50\u200b\uff0c\u200b\u4e24\u200b\u56fd\u4eba\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u56fd\u5bb6\u535a\u7269\u9986\u200b\u8003\u53e4\u5b66\u200b\u548c\u200b\u4eba\u7c7b\u5b66\u200b', '\u200b\u5316\u200b\u535a\u5927\u7cbe\u6df1\u200b\uff0c\u200b\u6211\u200b\u5e0c\u671b\u200b\u6211\u200b\u7684\u200b\u5b66\u751f\u200b\u4eec\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u4e2d\u200b', '\u200b\u978b\u5b50\u200b\uff0c\u200b\u8d4b\u4e88\u200b\u6211\u200b\u7a7f\u8d8a\u200b\u8346\u68d8\u200b\u7684\u200b\u529b\u91cf\u200b\u3002\u201d', '\u200b\u6c11\u200b\u643a\u624b\u200b\u52aa\u529b\u200b\uff0c\u200b\u5fc5\u5c06\u200b\u63a8\u52a8\u200b\u4e24\u56fd\u5173\u7cfb\u200b\u4e0d\u65ad\u200b\u5411\u524d\u200b\u53d1\u200b', '\u200b\u7814\u7a76\u5458\u200b\u83f2\u5c14\u200b\u8499\u200b\u00b7\u200b\u7279\u200b\u97e6\u5c14\u5fb7\u200b\u5341\u5206\u200b\u559c\u7231\u200b\u4e2d\u56fd\u200b\u6587\u200b', '\u200b\u8f6f\u66f2\u200b\u66f4\u597d\u200b\u5730\u200b\u7406\u89e3\u200b\u4e2d\u56fd\u200b\u6587\u5316\u200b\u3002\"\u200b\u5979\u200b\u8bf4\u200b\u3002', '\u200b\u7a46\u5362\u76d6\u200b\u5854\u200b\u5bc6\u5207\u200b\u5173\u6ce8\u200b\u4e2d\u56fd\u200b\u5728\u200b\u7ecf\u6d4e\u200b\u3001\u200b\u79d1\u6280\u200b\u3001\u200b\u6559\u200b', '\u200b\u5c55\u200b\u3002\"\u200b\u9c81\u592b\u5854\u200b\u8bf4\u200b\u3002', '\u200b\u5316\u200b\u3002\u200b\u4ed6\u200b\u8868\u793a\u200b\uff1a\u201c\u200b\u5b66\u4e60\u200b\u5f7c\u6b64\u200b\u7684\u200b\u8bed\u8a00\u200b\u548c\u200b\u6587\u5316\u200b\uff0c\u200b\u5c06\u200b\u5e2e\u200b', '\u201c\u200b\u59d0\u59d0\u200b\uff0c\u200b\u4f60\u200b\u60f3\u200b\u53bb\u200b\u4e2d\u56fd\u200b\u5417\u200b\uff1f\"\u201c\u200b\u975e\u5e38\u200b\u60f3\u200b\uff01\u200b\u6211\u200b\u60f3\u200b', '\u200b\u80b2\u200b\u7b49\u200b\u9886\u57df\u200b\u7684\u200b\u53d1\u5c55\u200b\uff0c\u201c\u200b\u4e2d\u56fd\u200b\u5728\u200b\u79d1\u7814\u200b\u7b49\u200b\u65b9\u9762\u200b\u7684\u200b\u5b9e\u529b\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u9ad8\u7b49\u6559\u80b2\u200b\u59d4\u5458\u4f1a\u200b\u4e3b\u4efb\u200b\u52a9\u7406\u200b\u8428\u200b', '\u200b\u52a9\u5384\u4e2d\u200b\u4e24\u56fd\u4eba\u6c11\u200b\u66f4\u597d\u200b\u5730\u200b\u7406\u89e3\u200b\u5f7c\u6b64\u200b\uff0c\u200b\u52a9\u529b\u200b\u53cc\u65b9\u200b', '\u200b\u770b\u200b\u6545\u5bab\u200b\u3001\u200b\u722c\u200b\u957f\u57ce\u200b\u3002\"\u200b\u5c24\u200b\u65af\u62c9\u200b\u7684\u200b\u5b66\u751f\u200b\u4e2d\u6709\u200b\u4e00\u5bf9\u200b', '\u200b\u4e0e\u65e5\u4ff1\u589e\u200b\u3002\u200b\u5728\u200b\u4e2d\u56fd\u200b\u5b66\u4e60\u200b\u7684\u200b\u7ecf\u5386\u200b\u8ba9\u200b\u6211\u200b\u770b\u5230\u200b\u66f4\u5e7f\u200b', '\u200b\u9a6c\u745e\u200b\u8868\u793a\u200b\uff1a\u201c\u200b\u6bcf\u5e74\u200b\u6211\u4eec\u200b\u90fd\u200b\u4f1a\u200b\u7ec4\u7ec7\u200b\u5b66\u751f\u200b\u5230\u200b\u4e2d\u56fd\u200b\u8bbf\u200b', '\u200b\u4ea4\u5f80\u200b\uff0c\u200b\u642d\u5efa\u200b\u53cb\u8c0a\u200b\u6865\u6881\u200b\u3002\u201d', '\u200b\u8f6f\u5584\u200b\u821e\u200b\u7684\u200b\u59d0\u59b9\u200b\uff0c\u200b\u59d0\u59d0\u200b\u9732\u5a05\u200b\u4eca\u5e74\u200b15\u200b\u5c81\u200b\uff0c\u200b\u59b9\u59b9\u200b', '\u200b\u9614\u200b\u7684\u200b\u4e16\u754c\u200b\uff0c\u200b\u4ece\u4e2d\u200b\u53d7\u76ca\u532a\u6d45\u200b\u3002\u201d', '\u200b\u95ee\u200b\u5b66\u4e60\u200b\uff0c\u200b\u76ee\u524d\u200b\u6709\u200b\u8d85\u8fc7\u200b5000\u200b\u540d\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u5b66\u751f\u200b', '\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u56fd\u5bb6\u535a\u7269\u9986\u200b\u9986\u957f\u200b\u5854\u5409\u200b\u4e01\u200b\u00b7', '\u200b\u4e9a\u200b14\u200b\u5c81\u200b\uff0c\u200b\u4e24\u4eba\u200b\u90fd\u200b\u5df2\u200b\u5728\u200b\u5384\u200b\u7279\u5b54\u9662\u200b\u5b66\u4e60\u200b\u591a\u5e74\u200b\uff0c', '23\u200b\u5c81\u200b\u7684\u200b\u8389\u8fea\u4e9a\u200b\u00b7\u200b\u57c3\u200b\u65af\u8482\u6cd5\u200b\u8bfa\u65af\u200b\u5df2\u200b\u5728\u200b\u5384\u7279\u200b', '\u200b\u5728\u200b\u4e2d\u56fd\u200b\u7559\u5b66\u200b\u3002\u200b\u5b66\u4e60\u200b\u4e2d\u56fd\u200b\u7684\u200b\u6559\u80b2\u200b\u7ecf\u9a8c\u200b\uff0c\u200b\u6709\u52a9\u4e8e\u200b', '\u200b\u91cc\u200b\u8fbe\u59c6\u200b\u00b7\u200b\u4f18\u7d20\u798f\u200b\u66fe\u591a\u6b21\u200b\u8bbf\u95ee\u200b\u4e2d\u56fd\u200b\uff0c\u200b\u5bf9\u200b\u4e2d\u534e\u6587\u660e\u200b', '\u200b\u6587\u8bf4\u200b\u5f97\u200b\u683c\u5916\u200b\u6d41\u5229\u200b\u3002', '\u200b\u5b54\u9662\u200b\u5b66\u4e60\u200b3\u200b\u5e74\u200b\uff0c\u200b\u5728\u200b\u4e2d\u56fd\u200b\u4e66\u6cd5\u200b\u3001\u200b\u4e2d\u56fd\u753b\u200b\u7b49\u200b\u65b9\u9762\u200b\u8868\u200b', '\u200b\u63d0\u5347\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u7684\u200b\u6559\u80b2\u200b\u6c34\u5e73\u200b\u3002\u201d', '\u200b\u7684\u200b\u4f20\u627f\u200b\u4e0e\u200b\u521b\u65b0\u200b\u3001\u200b\u73b0\u4ee3\u5316\u200b\u535a\u7269\u9986\u200b\u7684\u200b\u5efa\u8bbe\u200b\u4e0e\u200b\u53d1\u7528\u200b', '\u200b\u9732\u5a05\u200b\u5bf9\u200b\u8bb0\u8005\u200b\u8bf4\u200b\uff1a\u201c\u200b\u8fd9\u4e9b\u5e74\u6765\u200b\uff0c\u200b\u6000\u7740\u200b\u5bf9\u200b\u4e2d\u6587\u200b', '\u200b\u73b0\u200b\u5341\u5206\u200b\u4f18\u79c0\u200b\uff0c\u200b\u5728\u200b2024\u200b\u5e74\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u8d5b\u533a\u200b\u7684\u200b', '\u200b\u5370\u8c61\u200b\u6df1\u523b\u200b\u3002\u201c\u200b\u4e2d\u56fd\u200b\u535a\u7269\u9986\u200b\u4e0d\u4ec5\u200b\u6709\u200b\u8bb8\u591a\u200b\u4fdd\u5b58\u200b\u5b8c\u597d\u200b', '\u201c\u200b\u5171\u540c\u200b\u5411\u200b\u4e16\u754c\u200b\u5c55\u793a\u200b\u975e\u200b', '\u200b\u4e2d\u56fd\u200b\u6587\u5316\u200b\u7684\u200b\u70ed\u7231\u200b\uff0c\u200b\u6211\u4eec\u200b\u59d0\u59b9\u4fe9\u200b\u59cb\u7ec8\u200b\u76f8\u4e92\u200b\u9f13\u200b', '\u201c\u200b\u6c49\u8bed\u200b\u6865\u200b\u201d\u200b\u6bd4\u8d5b\u200b\u4e2d\u200b\u83b7\u5f97\u200b\u4e00\u7b49\u5956\u200b\u3002\u200b\u8389\u8fea\u4e9a\u200b\u8bf4\u200b\uff1a\u201c\u200b\u5b66\u200b', '\u200b\u7684\u200b\u6587\u7269\u200b\uff0c\u200b\u8fd8\u200b\u5145\u5206\u8fd0\u7528\u200b\u5148\u8fdb\u200b\u79d1\u6280\u200b\u624b\u6bb5\u200b\u8fdb\u884c\u200b\u5c55\u793a\u200b', '\u200b\u4e00\u8d77\u200b\u5b66\u4e60\u200b\u3002\u200b\u6211\u4eec\u200b\u7684\u200b\u4e2d\u6587\u200b\u4e00\u5929\u200b\u6bd4\u200b\u4e00\u5929\u200b\u597d\u200b\uff0c\u200b\u8fd8\u200b', '\u200b\u4e60\u200b\u4e2d\u56fd\u200b\u4e66\u6cd5\u200b\u8ba9\u200b\u6211\u200b\u7684\u200b\u5185\u5fc3\u200b\u53d8\u5f97\u200b\u5b89\u5b81\u200b\u548c\u200b\u7eaf\u7cb9\u200b\u3002\u200b\u6211\u200b', '\u200b\u6d32\u200b\u548c\u200b\u4e9a\u6d32\u200b\u7684\u200b\u707f\u70c2\u200b\u6587\u660e\u200b\u201d', '\u200b\u5e2e\u52a9\u200b\u4eba\u4eec\u200b\u66f4\u597d\u200b\u7406\u89e3\u200b\u4e2d\u534e\u6587\u660e\u200b\u3002\"\u200b\u5854\u5409\u200b\u4e01\u8bf4\u200b\uff0c\u201c', '\u200b\u4e86\u200b\u4e2d\u6587\u200b\u6b4c\u200b\u548c\u200b\u4e2d\u56fd\u200b\u821e\u200b\u3002\u200b\u6211\u4eec\u200b\u4e00\u5b9a\u200b\u8981\u200b\u5230\u200b\u4e2d\u56fd\u200b', '\u200b\u4e5f\u200b\u559c\u6b22\u200b\u4e2d\u56fd\u200b\u7684\u200b\u670d\u9970\u200b\uff0c\u200b\u5e0c\u671b\u200b\u672a\u6765\u200b\u80fd\u200b\u53bb\u200b\u4e2d\u56fd\u200b\u5b66\u4e60\u200b\uff0c', '\u200b\u7acb\u200b\u7279\u91cc\u4e9a\u200b\u4e0e\u200b\u4e2d\u56fd\u200b\u90fd\u200b\u62e5\u6709\u200b\u60a0\u4e45\u200b\u7684\u200b\u6587\u660e\u200b\uff0c\u200b\u59cb\u7ec8\u200b\u6728\u200b', '\u200b\u5b66\u597d\u200b\u4e2d\u6587\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u672a\u6765\u200b\u4e0d\u662f\u200b\u68a6\u200b\uff01\u201d', '\u200b\u628a\u200b\u4e2d\u56fd\u200b\u4e0d\u540c\u200b\u6c11\u65cf\u200b\u5143\u7d20\u200b\u878d\u5165\u200b\u670d\u88c5\u8bbe\u8ba1\u200b\u4e2d\u200b\uff0c\u200b\u521b\u4f5c\u200b', '\u200b\u4ece\u200b\u963f\u65af\u9a6c\u62c9\u200b\u51fa\u53d1\u200b\uff0c\u200b\u6cbf\u7740\u200b\u873f\u8712\u200b\u66f2\u6298\u200b\u7684\u200b\u76d8\u5c71\u200b', '\u200b\u4e92\u200b\u7406\u89e3\u200b\u3001\u200b\u76f8\u4e92\u5c0a\u91cd\u200b\u3002\u200b\u6211\u200b\u5e0c\u671b\u200b\u672a\u6765\u200b\u4e0e\u200b\u4e2d\u56fd\u200b\u540c\u884c\u200b', '\u200b\u636e\u200b\u5384\u200b\u7279\u5b54\u9662\u200b\u4e2d\u65b9\u200b\u9662\u957f\u200b\u9ec4\u9e23\u98de\u200b\u4ecb\u7ecd\u200b\uff0c\u200b\u8fd9\u6240\u200b', '\u200b\u51fa\u200b\u66f4\u200b\u591a\u200b\u7cbe\u7f8e\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u4e5f\u200b\u628a\u200b\u5384\u7279\u200b\u6587\u5316\u200b\u5206\u4eab\u200b\u7ed9\u200b\u66f4\u200b\u591a\u200b', '\u200b\u516c\u8def\u200b\u4e00\u8def\u200b\u5411\u4e1c\u200b\u5bfb\u627e\u200b\u4e1d\u8def\u200b\u5370\u8ff9\u200b\u3002\u200b\u9a71\u8f66\u200b\u4e24\u4e2a\u200b\u5c0f\u200b', '\u200b\u52a0\u5f3a\u200b\u5408\u4f5c\u200b\uff0c\u200b\u5171\u540c\u200b\u5411\u200b\u4e16\u754c\u200b\u5c55\u793a\u200b\u975e\u6d32\u200b\u548c\u200b\u4e9a\u6d32\u200b\u7684\u200b\u707f\u200b', '\u200b\u4e2d\u200b\u8d35\u5dde\u200b\u8d22\u7ecf\u5927\u5b66\u200b\u548c\u200b', '\u200b\u7684\u200b\u4e2d\u56fd\u200b\u670b\u53cb\u200b\u3002\u201d', '\u200b\u65f6\u200b\uff0c\u200b\u8bb0\u8005\u200b\u6765\u5230\u200b\u4f4d\u4e8e\u200b\u5384\u7acb\u7279\u91cc\u4e9a\u200b\u6e2f\u53e3\u57ce\u5e02\u200b\u9a6c\u8428\u200b', '\u200b\u70c2\u200b\u6587\u660e\u200b\u3002\u201d'], 'rec_scores': array([0.99875408, ..., 0.98324996]), 'rec_polys': array([[[  77,    0],\n        ...,\n        [  76,   98]],\n\n       ...,\n\n       [[1142, 1350],\n        ...,\n        [1142, 1367]]], dtype=int16), 'rec_boxes': array([[  76, ...,  103],\n       ...,\n       [1142, ..., 1367]], dtype=int16)}}}\n</code></pre> <p>For explanation of the result parameters, refer to 2.2 Python Script Integration.</p> <p>Note: Due to the large size of the default model in the pipeline, the inference speed may be slow. You can refer to the model list in Section 1 to replace it with a faster model.</p>"},{"location":"en/version3.x/pipeline_usage/PP-StructureV3.html#22-python-script-integration_1","title":"2.2 Python Script Integration","text":"<p>The command line method is for quick testing and visualization. In actual projects, you usually need to integrate the model via code. You can perform pipeline inference with just a few lines of code as shown below:</p> <pre><code>from paddleocr import PPStructureV3\n\npipeline = PPStructureV3()\n# ocr = PPStructureV3(use_doc_orientation_classify=True) # Use use_doc_orientation_classify to enable/disable document orientation classification model\n# ocr = PPStructureV3(use_doc_unwarping=True) # Use use_doc_unwarping to enable/disable document unwarping module\n# ocr = PPStructureV3(use_textline_orientation=True) # Use use_textline_orientation to enable/disable textline orientation classification model\n# ocr = PPStructureV3(device=\"gpu\") # Use device to specify GPU for model inference\noutput = pipeline.predict(\"./pp_structure_v3_demo.png\")\nfor res in output:\n    res.print() ## Print the structured prediction output\n    res.save_to_json(save_path=\"output\") ## Save the current image's structured result in JSON format\n    res.save_to_markdown(save_path=\"output\") ## Save the current image's result in Markdown format\n</code></pre> <p>For PDF files, each page will be processed individually and generate a separate Markdown file. If you want to convert the entire PDF to a single Markdown file, use the following method:</p> <pre><code>from pathlib import Path\nfrom paddleocr import PPStructureV3\n\ninput_file = \"./your_pdf_file.pdf\"\noutput_path = Path(\"./output\")\n\npipeline = PPStructureV3()\noutput = pipeline.predict(\"./pp_structure_v3_demo.png\")\n\nmarkdown_list = []\nmarkdown_images = []\n\nfor res in output:\n    md_info = res.markdown\n    markdown_list.append(md_info)\n    markdown_images.append(md_info.get(\"markdown_images\", {}))\n\nmarkdown_texts = pipeline.concatenate_markdown_pages(markdown_list)\n\nmkd_file_path = output_path / f\"{Path(input_file).stem}.md\"\nmkd_file_path.parent.mkdir(parents=True, exist_ok=True)\n\nwith open(mkd_file_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(markdown_texts)\n\nfor item in markdown_images:\n    if item:\n        for path, image in item.items():\n            file_path = output_path / path\n            file_path.parent.mkdir(parents=True, exist_ok=True)\n            image.save(file_path)\n</code></pre> <p>Note:</p> <ul> <li> <p>The default text recognition model used by PP-StructureV3 is a Chinese-English recognition model, which has limited accuracy for purely English texts. For English-only scenarios, you can set the <code>text_recognition_model_name</code> parameter to an English model such as <code>en_PP-OCRv4_mobile_rec</code> to achieve better recognition performance. For other languages, refer to the model list above and select the appropriate language recognition model for replacement.</p> </li> <li> <p>In the example code, the parameters <code>use_doc_orientation_classify</code>, <code>use_doc_unwarping</code>, and <code>use_textline_orientation</code> are all set to <code>False</code> by default. These indicate that document orientation classification, document image unwarping, and textline orientation classification are disabled. You can manually set them to <code>True</code> if needed.</p> </li> </ul> <p>The above Python script performs the following steps:</p> (1) Instantiate <code>PPStructureV3</code> to create the pipeline object. The parameter descriptions are as follows: Parameter Description Type Default <code>layout_detection_model_name</code> Name of the layout detection model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>layout_detection_model_dir</code> Directory path of the layout detection model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>layout_threshold</code> Score threshold for the layout model. <ul> <li>float: Any float between <code>0-1</code>;</li> <li>dict: <code>{0:0.1}</code> where the key is the class ID and the value is the threshold for that class;</li> <li>None: If set to <code>None</code>, uses the pipeline default of <code>0.5</code>.</li> </ul> <code>float|dict</code> <code>None</code> <code>layout_nms</code> Whether to use Non-Maximum Suppression (NMS) as post-processing for layout detection. If set to <code>None</code>, the parameter will default to the value initialized in the pipeline, which is set to <code>True</code> by default. <code>bool</code> <code>None</code> <code>layout_unclip_ratio</code> Expansion ratio for the bounding boxes from the layout detection model. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>Tuple[float,float]: Expansion ratios in horizontal and vertical directions;</li> <li>dict: A dictionary with int keys representing <code>cls_id</code>, and tuple values, e.g., <code>{0: (1.1, 2.0)}</code> means width is expanded 1.1\u00d7 and height 2.0\u00d7 for class 0 boxes;</li> <li>None: If set to <code>None</code>, uses the pipeline default of <code>1.0</code>.</li> </ul> <code>float|Tuple[float,float]|dict</code> <code>None</code> <code>layout_merge_bboxes_mode</code> Filtering method for overlapping boxes in layout detection. <ul> <li>str: Options include <code>large</code>, <code>small</code>, and <code>union</code> to retain the larger box, smaller box, or both;</li> <li>dict: A dictionary with int keys representing <code>cls_id</code>, and str values, e.g., <code>{0: \"large\", 2: \"small\"}</code> means using different modes for different classes;</li> <li>None: If set to <code>None</code>, uses the pipeline default value <code>large</code>.</li> </ul> <code>str|dict</code> <code>None</code> <code>chart_recognition_model_name</code> Name of the chart recognition model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>chart_recognition_model_dir</code> Directory path of the chart recognition model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>chart_recognition_batch_size</code> Batch size for the chart recognition model. If set to <code>None</code>, the default is <code>1</code>. <code>int</code> <code>None</code> <code>region_detection_model_name</code> Name of the region detection model for sub-modules in document layout. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>region_detection_model_dir</code> Directory path of the region detection model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>doc_orientation_classify_model_name</code> Name of the document orientation classification model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>doc_orientation_classify_model_dir</code> Directory path of the document orientation classification model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>doc_unwarping_model_name</code> Name of the document unwarping model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>doc_unwarping_model_dir</code> Directory path of the document unwarping model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>text_detection_model_name</code> Name of the text detection model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>text_detection_model_dir</code> Directory path of the text detection model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>text_det_limit_side_len</code> Image side length limitation for text detection. <ul> <li>int: Any integer greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, uses the pipeline default of <code>960</code>.</li> </ul> <code>int</code> <code>None</code> <code>text_det_limit_type</code> <ul> <li>str: Supports <code>min</code> and <code>max</code>. <code>min</code> ensures the shortest side is no less than <code>det_limit_side_len</code>, while <code>max</code> ensures the longest side is no greater than <code>limit_side_len</code>;</li> <li>None: If set to <code>None</code>, uses the pipeline default of <code>max</code>.</li> </ul> <code>str</code> <code>None</code> <code>text_det_thresh</code> Pixel threshold for detection. Pixels in the output probability map with scores above this value are considered as text pixels. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, uses the pipeline default value of <code>0.3</code>.</li> </ul> <code>float</code> <code>None</code> <code>text_det_box_thresh</code> Bounding box threshold. If the average score of all pixels inside the box exceeds this threshold, it is considered a text region. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, uses the pipeline default value of <code>0.6</code>.</li> </ul> <code>float</code> <code>None</code> <code>text_det_unclip_ratio</code> Expansion ratio for text detection. The larger the value, the more the text region is expanded. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, uses the pipeline default value of <code>2.0</code>.</li> </ul> <code>float</code> <code>None</code> <code>textline_orientation_model_name</code> Name of the textline orientation model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>textline_orientation_model_dir</code> Directory path of the textline orientation model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>textline_orientation_batch_size</code> Batch size for the textline orientation model. If set to <code>None</code>, the default batch size is <code>1</code>. <code>int</code> <code>None</code> <code>text_recognition_model_name</code> Name of the text recognition model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>text_recognition_model_dir</code> Directory path of the text recognition model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>text_recognition_batch_size</code> Batch size for the text recognition model. If set to <code>None</code>, the default batch size is <code>1</code>. <code>int</code> <code>None</code> <code>text_rec_score_thresh</code> Score threshold for text recognition. Only results with scores above this threshold will be retained. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, uses the pipeline default of <code>0.0</code> (no threshold).</li> </ul> <code>float</code> <code>None</code> <code>table_classification_model_name</code> Name of the table classification model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>table_classification_model_dir</code> Directory path of the table classification model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>wired_table_structure_recognition_model_name</code> Name of the wired table structure recognition model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>wired_table_structure_recognition_model_dir</code> Directory path of the wired table structure recognition model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>wireless_table_structure_recognition_model_name</code> Name of the wireless table structure recognition model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>wireless_table_structure_recognition_model_dir</code> Directory path of the wireless table structure recognition model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>wired_table_cells_detection_model_name</code> Name of the wired table cell detection model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>wired_table_cells_detection_model_dir</code> Directory path of the wired table cell detection model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>wireless_table_cells_detection_model_name</code> Name of the wireless table cell detection model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>wireless_table_cells_detection_model_dir</code> Directory path of the wireless table cell detection model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>table_orientation_classify_model_name</code> Name of the wireless table orientation classification model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>table_orientation_classify_model_dir</code> Directory of the table orientation classification model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>seal_text_detection_model_name</code> Name of the seal text detection model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>seal_text_detection_model_dir</code> Directory path of the seal text detection model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>seal_det_limit_side_len</code> Image side length limit for seal text detection. <ul> <li>int: Any integer greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the default value is <code>736</code>.</li> </ul> <code>int</code> <code>None</code> <code>seal_det_limit_type</code> Limit type for seal text detection image side length. <ul> <li>str: Supports <code>min</code> and <code>max</code>. <code>min</code> ensures the shortest side is no less than <code>det_limit_side_len</code>, while <code>max</code> ensures the longest side is no greater than <code>limit_side_len</code>;</li> <li>None: If set to <code>None</code>, the default value is <code>min</code>.</li> </ul> <code>str</code> <code>None</code> <code>seal_det_thresh</code> Pixel threshold for detection. Pixels with scores greater than this value in the probability map are considered text pixels. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the default value is <code>0.2</code>.</li> </ul> <code>float</code> <code>None</code> <code>seal_det_box_thresh</code> Bounding box threshold. If the average score of all pixels inside a detection box exceeds this threshold, it is considered a text region. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the default value is <code>0.6</code>.</li> </ul> <code>float</code> <code>None</code> <code>seal_det_unclip_ratio</code> Expansion ratio for seal text detection. The larger the value, the larger the expanded area. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the default value is <code>0.5</code>.</li> </ul> <code>float</code> <code>None</code> <code>seal_text_recognition_model_name</code> Name of the seal text recognition model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>seal_text_recognition_model_dir</code> Directory path of the seal text recognition model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>seal_text_recognition_batch_size</code> Batch size for the seal text recognition model. If set to <code>None</code>, the default value is <code>1</code>. <code>int</code> <code>None</code> <code>seal_rec_score_thresh</code> Score threshold for seal text recognition. Text results with scores above this threshold will be retained. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the default value is <code>0.0</code> (no threshold).</li> </ul> <code>float</code> <code>None</code> <code>formula_recognition_model_name</code> Name of the formula recognition model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>formula_recognition_model_dir</code> Directory path of the formula recognition model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>formula_recognition_batch_size</code> Batch size for the formula recognition model. If set to <code>None</code>, the default value is <code>1</code>. <code>int</code> <code>None</code> <code>use_doc_orientation_classify</code> Whether to enable the document orientation classification module. If set to <code>None</code>, the default value is <code>True</code>. <code>bool</code> <code>None</code> <code>use_doc_unwarping</code> Whether to enable the document image unwarping module. If set to <code>None</code>, the default value is <code>True</code>. <code>bool</code> <code>None</code> <code>use_textline_orientation</code> Whether to use the text line orientation classification. If not set, the default is <code>True</code>. <code>bool</code> <code>use_seal_recognition</code> Whether to enable seal recognition subpipeline. If not set, the default is <code>True</code>. <code>bool</code> <code>use_table_recognition</code> Whether to enable table recognition subpipeline. If not set, the default is <code>True</code>. <code>bool</code> <code>use_formula_recognition</code> Whether to enable formula recognition subpipeline. If not set, the default is <code>True</code>. <code>bool</code> <code>use_chart_recognition</code> Whether to use the chart recognition sub-pipeline. If set to <code>None</code>, the default value is <code>True</code>. <code>bool</code> <code>None</code> <code>use_region_detection</code> Whether to use the document region detection pipeline. If set to <code>None</code>, the default value is <code>True</code>. <code>bool</code> <code>None</code> <code>device</code> Device used for inference. Supports specifying device ID: <ul> <li>CPU: e.g., <code>cpu</code> means using CPU for inference;</li> <li>GPU: e.g., <code>gpu:0</code> means using GPU 0;</li> <li>NPU: e.g., <code>npu:0</code> means using NPU 0;</li> <li>XPU: e.g., <code>xpu:0</code> means using XPU 0;</li> <li>MLU: e.g., <code>mlu:0</code> means using MLU 0;</li> <li>DCU: e.g., <code>dcu:0</code> means using DCU 0;</li> <li>None: If set to <code>None</code>, the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.</li> </ul> <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to enable high-performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Computation precision, e.g., fp32, fp16. <code>str</code> <code>\"fp32\"</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set. <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads used for inference on CPU. <code>int</code> <code>8</code> <code>paddlex_config</code> Path to the PaddleX pipeline configuration file. <code>str</code> <code>None</code> (2) Call the <code>predict()</code> method of the PP-StructureV3 pipeline object for inference. This method returns a result list. The pipeline also provides a <code>predict_iter()</code> method. Both methods accept the same parameters and return the same type of results. The only difference is that <code>predict_iter()</code> returns a <code>generator</code> that allows incremental processing and retrieval of prediction results, which is useful for handling large datasets or saving memory. Choose the method that fits your needs. Below are the parameters of the <code>predict()</code> method: Parameter Description Type Default <code>input</code> Input data to be predicted. Required. Supports multiple types: <ul> <li>Python Var: Image data represented as <code>numpy.ndarray</code>;</li> <li>str: Local path to image or PDF file, e.g., <code>/root/data/img.jpg</code>; URL to image or PDF, e.g., example; directory containing image files, e.g., <code>/root/data/</code> (directories with PDFs are not supported, use full file path for PDFs);</li> <li>List: Elements can be any of the above types, e.g., <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"].</code></li> </ul> <code>Python Var|str|list</code> <code>use_doc_orientation_classify</code> Whether to use document orientation classification during inference. <code>bool</code> <code>None</code> <code>use_doc_unwarping</code> Whether to use document image unwarping during inference. <code>bool</code> <code>None</code> <code>use_textline_orientation</code> Whether to use textline orientation classification during inference. <code>bool</code> <code>None</code> <code>use_seal_recognition</code> Whether to use the seal recognition sub-pipeline during inference. <code>bool</code> <code>None</code> <code>use_table_recognition</code> Whether to use the table recognition sub-pipeline during inference. <code>bool</code> <code>None</code> <code>use_formula_recognition</code> Whether to use the formula recognition sub-pipeline during inference. <code>bool</code> <code>None</code> <code>use_chart_recognition</code> Whether to use the chart recognition sub-pipeline. If set to <code>None</code>, the default value is <code>True</code>. <code>bool</code> <code>None</code> <code>use_region_detection</code> Whether to use the document region detection pipeline. If set to <code>None</code>, the default value is <code>True</code>. <code>bool</code> <code>None</code> <code>layout_threshold</code> Same as the parameter used during initialization. <code>float|dict</code> <code>None</code> <code>layout_nms</code> Same as the parameter used during initialization. <code>bool</code> <code>None</code> <code>layout_unclip_ratio</code> Same as the parameter used during initialization. <code>float|Tuple[float,float]|dict</code> <code>None</code> <code>layout_merge_bboxes_mode</code> Same as the parameter used during initialization. <code>str|dict</code> <code>None</code> <code>text_det_limit_side_len</code> Same as the parameter used during initialization. <code>int</code> <code>None</code> <code>text_det_limit_type</code> Same as the parameter used during initialization. <code>str</code> <code>None</code> <code>text_det_thresh</code> Same as the parameter used during initialization. <code>float</code> <code>None</code> <code>text_det_box_thresh</code> Same as the parameter used during initialization. <code>float</code> <code>None</code> <code>text_det_unclip_ratio</code> Same as the parameter used during initialization. <code>float</code> <code>None</code> <code>text_rec_score_thresh</code> Same as the parameter used during initialization. <code>float</code> <code>None</code> <code>seal_det_limit_side_len</code> Same as the parameter used during initialization. <code>int</code> <code>None</code> <code>seal_det_limit_type</code> Same as the parameter used during initialization. <code>str</code> <code>None</code> <code>seal_det_thresh</code> Same as the parameter used during initialization. <code>float</code> <code>None</code> <code>seal_det_box_thresh</code> Same as the parameter used during initialization. <code>float</code> <code>None</code> <code>seal_det_unclip_ratio</code> Same as the parameter used during initialization. <code>float</code> <code>None</code> <code>seal_rec_score_thresh</code> Same as the parameter used during initialization. <code>float</code> <code>None</code> <code>use_wired_table_cells_trans_to_html</code> Whether to enable direct conversion of wired table cell detection results to HTML. Default is False. If enabled, HTML will be constructed directly based on the geometric relationship of wired table cell detection results. <ul> <li>bool: <code>True</code> or <code>False</code>;</li> <li>None: If set to <code>None</code>, it will default to the initialized parameter value, initialized as <code>False</code>;</li> </ul> <code>bool|None</code> <code>False</code> <code>use_wireless_table_cells_trans_to_html</code> Whether to enable direct conversion of wireless table cell detection results to HTML. Default is False. If enabled, HTML will be constructed directly based on the geometric relationship of wireless table cell detection results. <ul> <li>bool: <code>True</code> or <code>False</code>;</li> <li>None: If set to <code>None</code>, it will default to the initialized parameter value, initialized as <code>False</code>;</li> </ul> <code>bool|None</code> <code>False</code> <code>use_table_orientation_classify</code> Whether to enable table orientation classification. When enabled, it can correct the orientation and correctly complete table recognition if the table in the image is rotated by 90/180/270 degrees. <ul> <li>bool: <code>True</code> or <code>False</code>;</li> <li>None: If set to <code>None</code>, it will default to the initialized parameter value, initialized as <code>True</code>;</li> </ul> <code>bool|None</code> <code>True</code> <code>use_ocr_results_with_table_cells</code> Whether to enable OCR within cell segmentation. When enabled, OCR detection results will be segmented and re-recognized based on cell prediction results to avoid text loss. <ul> <li>bool: <code>True</code> or <code>False</code>;</li> <li>None: If set to <code>None</code>, it will default to the initialized parameter value, initialized as <code>True</code>;</li> </ul> <code>bool|None</code> <code>True</code> <code>use_e2e_wired_table_rec_model</code> Whether to enable end-to-end wired table recognition mode. If enabled, the cell detection model will not be used, and only the table structure recognition model will be used. <ul> <li>bool: <code>True</code> or <code>False</code>;</li> <li>None: If set to <code>None</code>, it will default to the initialized parameter value, initialized as <code>False</code>;</li> </ul> <code>bool|None</code> <code>False</code> <code>use_e2e_wireless_table_rec_model</code> Whether to enable end-to-end wireless table recognition mode. If enabled, the cell detection model will not be used, and only the table structure recognition model will be used. <ul> <li>bool: <code>True</code> or <code>False</code>;</li> <li>None: If set to <code>None</code>, it will default to the initialized parameter value, initialized as <code>False</code>;</li> </ul> <code>bool|None</code> <code>True</code> (3) Process the prediction results: each prediction result corresponds to a Result object, which supports printing, saving as image, or saving as a <code>json</code> file: Method Description Parameter Type Parameter Description Default <code>print()</code> Print result to terminal <code>format_json</code> <code>bool</code> Whether to format output as indented <code>JSON</code>. <code>True</code> <code>indent</code> <code>int</code> Indentation level to beautify the <code>JSON</code> output. Only effective when <code>format_json=True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. When <code>True</code>, all non-ASCII characters are escaped. When <code>False</code>, original characters are retained. Only effective when <code>format_json=True</code>. <code>False</code> <code>save_to_json()</code> Save result as a JSON file <code>save_path</code> <code>str</code> Path to save the file. If a directory, the filename will be based on the input type. None <code>indent</code> <code>int</code> Indentation level for beautified <code>JSON</code> output. Only effective when <code>format_json=True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. Only effective when <code>format_json=True</code>. <code>False</code> <code>save_to_img()</code> Save intermediate visualization results as PNG image files <code>save_path</code> <code>str</code> Path to save the file, supports directory or file path. None <code>save_to_markdown()</code> Save each page of an image or PDF file as a markdown file <code>save_path</code> <code>str</code> Path to save the file, supports directory or file path. None <code>save_to_html()</code> Save tables in the file as HTML format <code>save_path</code> <code>str</code> Path to save the file, supports directory or file path. None <code>save_to_xlsx()</code> Save tables in the file as XLSX format <code>save_path</code> <code>str</code> Path to save the file, supports directory or file path. None <code>concatenate_markdown_pages()</code> Concatenate multiple markdown pages into a single document <code>markdown_list</code> <code>list</code> List of markdown data for each page. Returns the merged markdown text and image list.   - Calling `print()` will print the result to the terminal. Explanation of the printed content:     - `input_path`: `(str)` Input path of the image or PDF to be predicted      - `page_index`: `(Union[int, None])` If input is a PDF, indicates the page number; otherwise `None`      - `model_settings`: `(Dict[str, bool])` Model parameters configured for the pipeline          - `use_doc_preprocessor`: `(bool)` Whether to enable document preprocessor sub-pipeline         - `use_seal_recognition`: `(bool)` Whether to enable seal recognition sub-pipeline         - `use_table_recognition`: `(bool)` Whether to enable table recognition sub-pipeline         - `use_formula_recognition`: `(bool)` Whether to enable formula recognition sub-pipeline      - `doc_preprocessor_res`: `(Dict[str, Union[List[float], str]])` Document preprocessing result dictionary, only exists if `use_doc_preprocessor=True`         - `input_path`: `(str)` Image path accepted by document preprocessor, `None` if input is `numpy.ndarray`         - `page_index`: `None` since input is `numpy.ndarray`         - `model_settings`: `(Dict[str, bool])` Model configuration for the document preprocessor           - `use_doc_orientation_classify`: `(bool)` Whether to enable document orientation classification           - `use_doc_unwarping`: `(bool)` Whether to enable image unwarping         - `angle`: `(int)` Predicted angle result if orientation classification is enabled      - `parsing_res_list`: `(List[Dict])` List of parsed results, each item is a dictionary in reading order         - `block_bbox`: `(np.ndarray)` Bounding box of the layout block         - `block_label`: `(str)` Block label such as `text`, `table`         - `block_content`: `(str)` Content within the layout block         - `seg_start_flag`: `(bool)` Whether the block starts a paragraph         - `seg_end_flag`: `(bool)` Whether the block ends a paragraph         - `sub_label`: `(str)` Sub-label of the block, e.g., `title_text`         - `sub_index`: `(int)` Sub-index of the block, used for markdown reconstruction         - `index`: `(int)` Index of the block, used for layout sorting           - `overall_ocr_res`: `(Dict[str, Union[List[str], List[float], numpy.ndarray]])` Dictionary of global OCR results       - `input_path`: `(Union[str, None])` OCR sub-pipeline input path; `None` if input is `numpy.ndarray`       - `page_index`: `None` since input is `numpy.ndarray`       - `model_settings`: `(Dict)` OCR model configuration       - `dt_polys`: `(List[numpy.ndarray])` List of polygons for text detection. Each box is a numpy array with shape (4, 2), dtype int16       - `dt_scores`: `(List[float])` Confidence scores for detection boxes       - `text_det_params`: `(Dict[str, Dict[str, int, float]])` Text detection module parameters         - `limit_side_len`: `(int)` Side length limit for image preprocessing         - `limit_type`: `(str)` Limit processing method         - `thresh`: `(float)` Threshold for text pixel classification         - `box_thresh`: `(float)` Threshold for text detection boxes         - `unclip_ratio`: `(float)` Unclip ratio for expanding boxes         - `text_type`: `(str)` Text detection type, currently fixed as \"general\"        - `text_type`: `(str)` Text detection type, currently fixed as \"general\"       - `textline_orientation_angles`: `(List[int])` Orientation classification results for text lines       - `text_rec_score_thresh`: `(float)` Threshold for text recognition filtering       - `rec_texts`: `(List[str])` Recognized texts filtered by score threshold       - `rec_scores`: `(List[float])` Recognition scores filtered by threshold       - `rec_polys`: `(List[numpy.ndarray])` Filtered detection boxes, same format as `dt_polys`      - `formula_res_list`: `(List[Dict[str, Union[numpy.ndarray, List[float], str]]])` List of formula recognition results         - `rec_formula`: `(str)` Recognized formula string         - `rec_polys`: `(numpy.ndarray)` Bounding box for the formula, shape (4, 2), dtype int16         - `formula_region_id`: `(int)` Region ID of the formula      - `seal_res_list`: `(List[Dict[str, Union[numpy.ndarray, List[float], str]]])` List of seal recognition results         - `input_path`: `(str)` Input path for the seal image         - `page_index`: `None` since input is `numpy.ndarray`         - `model_settings`: `(Dict)` Model configuration for seal recognition         - `dt_polys`: `(List[numpy.ndarray])` Seal detection boxes, same format as `dt_polys`         - `text_det_params`: `(Dict[str, Dict[str, int, float]])` Detection parameters, same as above         - `text_type`: `(str)` Detection type, currently fixed as \"seal\"         - `text_rec_score_thresh`: `(float)` Score threshold for recognition         - `rec_texts`: `(List[str])` Recognized texts filtered by score         - `rec_scores`: `(List[float])` Recognition scores filtered by threshold         - `rec_polys`: `(List[numpy.ndarray])` Filtered seal boxes, same format as `dt_polys`         - `rec_boxes`: `(numpy.ndarray)` Rectangle boxes, shape (n, 4), dtype int16      - `table_res_list`: `(List[Dict[str, Union[numpy.ndarray, List[float], str]]])` List of table recognition results         - `cell_box_list`: `(List[numpy.ndarray])` Bounding boxes of table cells         - `pred_html`: `(str)` Table as an HTML string         - `table_ocr_pred`: `(dict)` OCR results for the table             - `rec_polys`: `(List[numpy.ndarray])` Detected cell boxes             - `rec_texts`: `(List[str])` Recognized texts for cells             - `rec_scores`: `(List[float])` Confidence scores for cell recognition             - `rec_boxes`: `(numpy.ndarray)` Rectangle boxes for detection, shape (n, 4), dtype int16  - Calling `save_to_json()` saves the above content to the specified `save_path`. If it\u2019s a directory, the saved path will be `save_path/{your_img_basename}_res.json`. If it\u2019s a file, it saves directly. Numpy arrays are converted to lists since JSON doesn't support them. - Calling `save_to_img()` saves visual results to the specified `save_path`. If a directory, various visualizations such as layout detection, OCR, and reading order are saved. If a file, only the last image is saved and others are overwritten. - Calling `save_to_markdown()` saves converted markdown files to `save_path/{your_img_basename}.md`. For PDF input, it's recommended to specify a directory to avoid file overwriting. - Calling `concatenate_markdown_pages()` merges multi-page markdown results from the `PP-StructureV3 pipeline` into a single document and returns the merged content.  Additionally, you can access the prediction results and visual images through the following attributes:   Attribute Description <code>json</code> Get the prediction result in <code>json</code> format <code>img</code> Get visualized image results as a <code>dict</code> <code>markdown</code> Get markdown results as a <code>dict</code>  - The `json` attribute returns the prediction result as a dictionary, which is consistent with the content saved using the `save_to_json()` method. - The `img` attribute returns the prediction result as a dictionary. The keys include `layout_det_res`, `overall_ocr_res`, `text_paragraphs_ocr_res`, `formula_res_region1`, `table_cell_img`, and `seal_res_region1`, each corresponding to a visualized `Image.Image` object for layout detection, OCR, text paragraph, formula, table, and seal results. If optional modules are not used, the dictionary only contains `layout_det_res`. - The `markdown` attribute returns the prediction result as a dictionary. The keys include `markdown_texts`, `markdown_images`, and `page_continuation_flags`, where the values represent the markdown text, displayed images (`Image.Image` objects), and a boolean tuple indicating whether the first and last elements of the current page are paragraph boundaries."},{"location":"en/version3.x/pipeline_usage/PP-StructureV3.html#3-development-integration-deployment","title":"3. Development Integration / Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy, you can proceed with development integration or deployment.</p> <p>If you want to directly use the pipeline in your Python project, refer to the example code in 2.2 Python script mode.</p> <p>In addition, PaddleOCR provides two other deployment options described in detail below:</p> <p>\ud83d\ude80 High-Performance Inference: In production environments, many applications have strict performance requirements (especially response speed) to ensure system efficiency and smooth user experience. PaddleOCR offers a high-performance inference option that deeply optimizes model inference and pre/post-processing for significant end-to-end acceleration. For detailed high-performance inference workflow, refer to High Performance Inference.</p> <p>\u2601\ufe0f Service Deployment: Service-based deployment is common in production. It encapsulates the inference logic as a service, allowing clients to access it via network requests to obtain results. For detailed instructions on service deployment, refer to Service Deployment.</p> <p>Below is the API reference and multi-language service invocation examples for basic service deployment:</p> API Reference <p>Main operations provided by the service:</p> <ul> <li>HTTP method: POST</li> <li>Request and response bodies are both JSON objects.</li> <li>When the request is successful, the response status code is <code>200</code>, and the response body contains:</li> </ul> Name Type Description <code>logId</code> <code>string</code> UUID of the request <code>errorCode</code> <code>integer</code> Error code, fixed to <code>0</code> <code>errorMsg</code> <code>string</code> Error message, fixed to <code>\"Success\"</code> <code>result</code> <code>object</code> Operation result <ul> <li>When the request fails, the response body includes:</li> </ul> Name Type Description <code>logId</code> <code>string</code> UUID of the request <code>errorCode</code> <code>integer</code> Error code, same as HTTP status code <code>errorMsg</code> <code>string</code> Error message <p>Main operation provided:</p> <ul> <li><code>infer</code></li> </ul> <p>Perform layout parsing.</p> <p><code>POST /layout-parsing</code></p> <ul> <li>Request body parameters:</li> </ul> Name Type Description Required <code>file</code> <code>string</code> URL of image or PDF file accessible to the server, or base64-encoded file content. By default, only the first 10 pages of a PDF are processed.To remove this limit, add the following to the pipeline config: <pre><code>Serving:\n  extra:\n    max_num_input_imgs: null\n</code></pre> Yes <code>fileType</code> <code>integer</code>\uff5c<code>null</code> File type. <code>0</code> for PDF, <code>1</code> for image. If omitted, the type is inferred from the URL. No <code>useDocOrientationClassify</code> <code>boolean</code> | <code>null</code> Refer to the <code>use_doc_orientation_classify</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>useDocUnwarping</code> <code>boolean</code> | <code>null</code> Refer to the <code>use_doc_unwarping</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>useTextlineOrientation</code> <code>boolean</code> | <code>null</code> Refer to the <code>use_textline_orientation</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>useSealRecognition</code> <code>boolean</code> | <code>null</code> Refer to the <code>use_seal_recognition</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>useTableRecognition</code> <code>boolean</code> | <code>null</code> Refer to the <code>use_table_recognition</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>useFormulaRecognition</code> <code>boolean</code> | <code>null</code> Refer to the <code>use_formula_recognition</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>layoutThreshold</code> <code>number</code> | <code>null</code> Refer to the <code>layout_threshold</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>layoutNms</code> <code>boolean</code> | <code>null</code> Refer to the <code>layout_nms</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>layoutUnclipRatio</code> <code>number</code> | <code>array</code> | <code>object</code> | <code>null</code> Refer to the <code>layout_unclip_ratio</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>layoutMergeBboxesMode</code> <code>string</code> | <code>object</code> | <code>null</code> Refer to the <code>layout_merge_bboxes_mode</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>textDetLimitSideLen</code> <code>integer</code> | <code>null</code> Refer to the <code>text_det_limit_side_len</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>textDetLimitType</code> <code>string</code> | <code>null</code> Refer to the <code>text_det_limit_type</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>textDetThresh</code> <code>number</code> | <code>null</code> Refer to the <code>text_det_thresh</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>textDetBoxThresh</code> <code>number</code> | <code>null</code> Refer to the <code>text_det_box_thresh</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>textDetUnclipRatio</code> <code>number</code> | <code>null</code> Refer to the <code>text_det_unclip_ratio</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>textRecScoreThresh</code> <code>number</code> | <code>null</code> Refer to the <code>text_rec_score_thresh</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>sealDetLimitSideLen</code> <code>integer</code> | <code>null</code> Refer to the <code>seal_det_limit_side_len</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>sealDetLimitType</code> <code>string</code> | <code>null</code> Refer to the <code>seal_det_limit_type</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>sealDetThresh</code> <code>number</code> | <code>null</code> Refer to the <code>seal_det_thresh</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>sealDetBoxThresh</code> <code>number</code> | <code>null</code> Refer to the <code>seal_det_box_thresh</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>sealDetUnclipRatio</code> <code>number</code> | <code>null</code> Refer to the <code>seal_det_unclip_ratio</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>sealRecScoreThresh</code> <code>number</code> | <code>null</code> Refer to the <code>seal_rec_score_thresh</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>useTableCellsOcrResults</code> <code>boolean</code> Refer to the <code>use_table_cells_ocr_results</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>useE2eWiredTableRecModel</code> <code>boolean</code> Refer to the <code>use_e2e_wired_table_rec_model</code> parameter in the pipeline\u2019s <code>predict</code> method. No <code>useE2eWirelessTableRecModel</code> <code>boolean</code> Refer to the <code>use_e2e_wireless_table_rec_model</code> parameter in the pipeline\u2019s <code>predict</code> method. No <ul> <li>When the request is successful, the <code>result</code> field of the response contains the following attributes:</li> </ul> Name Type Description <code>layoutParsingResults</code> <code>array</code> Layout parsing results. The array length is 1 (for image input) or the number of processed pages (for PDF input). For PDF input, each element corresponds to one processed page. <code>dataInfo</code> <code>object</code> Information about the input data. <p>Each element in <code>layoutParsingResults</code> is an <code>object</code> with the following attributes:</p> Name Type Description <code>prunedResult</code> <code>object</code> A simplified version of the <code>res</code> field from the JSON output of the pipeline\u2019s <code>predict</code> method, with <code>input_path</code> and <code>page_index</code> removed. <code>markdown</code> <code>object</code> Markdown result. <code>outputImages</code> <code>object</code> | <code>null</code> Refer to the pipeline\u2019s <code>img</code> attribute. Images are JPEG encoded in Base64. <code>inputImage</code> <code>string</code> | <code>null</code> Input image. JPEG encoded in Base64. <p>The <code>markdown</code> object has the following attributes:</p> Name Type Description <code>text</code> <code>string</code> Markdown text. <code>images</code> <code>object</code> Key-value pairs of image relative paths and base64-encoded image content. <code>isStart</code> <code>boolean</code> Whether the first element on the current page is the start of a paragraph. <code>isEnd</code> <code>boolean</code> Whether the last element on the current page is the end of a paragraph. Multi-language Service Call Examples Python <pre><code>\nimport base64\nimport requests\nimport pathlib\n\nAPI_URL = \"http://localhost:8080/layout-parsing\" # Service URL\n\nimage_path = \"./demo.jpg\"\n\n# Encode the local image to Base64\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\n    \"file\": image_data, # Base64-encoded file content or file URL\n    \"fileType\": 1, # File type, 1 indicates image file\n}\n\n# Call the API\nresponse = requests.post(API_URL, json=payload)\n\n# Handle the response data\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nfor i, res in enumerate(result[\"layoutParsingResults\"]):\n    print(res[\"prunedResult\"])\n    md_dir = pathlib.Path(f\"markdown_{i}\")\n    md_dir.mkdir(exist_ok=True)\n    (md_dir / \"doc.md\").write_text(res[\"markdown\"][\"text\"])\n    for img_path, img in res[\"markdown\"][\"images\"].items():\n        img_path = md_dir / img_path\n        img_path.parent.mkdir(parents=True, exist_ok=True)\n        img_path.write_bytes(base64.b64decode(img))\n    print(f\"Markdown document saved at {md_dir / 'doc.md'}\")\n    for img_name, img in res[\"outputImages\"].items():\n        img_path = f\"{img_name}_{i}.jpg\"\n        with open(img_path, \"wb\") as f:\n            f.write(base64.b64decode(img))\n        print(f\"Output image saved at {img_path}\")\n</code></pre> <p></p>"},{"location":"en/version3.x/pipeline_usage/PP-StructureV3.html#4-secondary-development","title":"4. Secondary Development","text":"<p>If the default model weights provided by the PP-StructureV3 pipeline do not meet your accuracy or speed requirements in your scenario, you can try fine-tuning the existing model using your own domain-specific or application-specific data to improve the performance of the PP-StructureV3 pipeline for your use case.</p>"},{"location":"en/version3.x/pipeline_usage/PP-StructureV3.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the PP-StructureV3 pipeline contains multiple modules, unsatisfactory results may originate from any individual module. You can analyze the problematic cases with poor extraction performance, visualize the images, identify the specific module causing the issue, and then refer to the fine-tuning tutorials linked in the table below to perform model fine-tuning.</p> Scenario Fine-tuning Module Fine-tuning Reference Link Inaccurate layout detection, such as missing seals or tables Layout Detection Module Link Inaccurate table structure recognition Table Structure Recognition Module Link Inaccurate formula recognition Formula Recognition Module Link Missing seal text detection Seal Text Detection Module Link Missing text detection Text Detection Module Link Incorrect text recognition results Text Recognition Module Link Incorrect correction of vertical or rotated text lines Text Line Orientation Classification Module Link Incorrect correction of full image orientation Document Image Orientation Classification Module Link Inaccurate image distortion correction Text Image Correction Module Fine-tuning not supported yet"},{"location":"en/version3.x/pipeline_usage/PP-StructureV3.html#42-model-deployment","title":"4.2 Model Deployment","text":"<p>Once you have completed fine-tuning with your private dataset, you will obtain the local model weights. You can then use these fine-tuned weights by customizing the pipeline configuration file.</p> <ol> <li>Export the pipeline configuration file</li> </ol> <p>You can call the <code>export_paddlex_config_to_yaml</code> method of the PPStructureV3 object in PaddleOCR to export the current pipeline configuration as a YAML file:</p> <pre><code>from paddleocr import PPStructureV3\n\npipeline = PPStructureV3()\npipeline.export_paddlex_config_to_yaml(\"PP-StructureV3.yaml\")\n</code></pre> <ol> <li>Modify the configuration file After obtaining the default pipeline configuration file, replace the corresponding path in the configuration with the local path of your fine-tuned model weights. For example: <pre><code>......\nSubModules:\n  LayoutDetection:\n    module_name: layout_detection\n    model_name: PP-DocLayout_plus-L\n    model_dir: null # Replace with the path to the fine-tuned layout detection model weights\n......\nSubPipelines:\n  GeneralOCR:\n    pipeline_name: OCR\n    text_type: general\n    use_doc_preprocessor: False\n    use_textline_orientation: False\n    SubModules:\n      TextDetection:\n        module_name: text_detection\n        model_name: PP-OCRv5_server_det\n        model_dir: null # Replace with the path to the fine-tuned text detection model weights\n        limit_side_len: 960\n        limit_type: max\n        max_side_limit: 4000\n        thresh: 0.3\n        box_thresh: 0.6\n        unclip_ratio: 1.5\n\n      TextRecognition:\n        module_name: text_recognition\n        model_name: PP-OCRv5_server_rec\n        model_dir: null # Replace with the path to the fine-tuned text recognition model weights\n        batch_size: 1\n        score_thresh: 0\n......\n</code></pre></li> </ol> <p>The pipeline configuration file not only includes parameters supported by the PaddleOCR CLI and Python API but also allows for more advanced configurations. For more details, refer to the corresponding pipeline usage tutorial in the PaddleX Pipeline Usage Overview, and adjust the configurations as needed based on your requirements.</p> <ol> <li>Load the pipeline configuration file via CLI</li> </ol> <p>After modifying the configuration file, specify the updated pipeline configuration path using the <code>--paddlex_config</code> parameter in the command line. PaddleOCR will load its content as the pipeline configuration. Example:</p> <pre><code>paddleocr pp_structurev3 --paddlex_config PP-StructureV3.yaml ...\n</code></pre> <ol> <li>Load the pipeline configuration file via Python API When initializing the pipeline object, you can pass the PaddleX pipeline configuration file path or a configuration dictionary using the paddlex_config parameter. PaddleOCR will load its content as the pipeline configuration. Example:</li> </ol> <pre><code>from paddleocr import PPStructureV3\n\npipeline = PPStructureV3(paddlex_config=\"PP-StructureV3.yaml\")\n</code></pre>"},{"location":"en/version3.x/pipeline_usage/doc_preprocessor.html","title":"Document Image Preprocessing Pipeline Tutorial","text":""},{"location":"en/version3.x/pipeline_usage/doc_preprocessor.html#1-introduction-to-document-image-preprocessing-pipeline","title":"1. Introduction to Document Image Preprocessing Pipeline","text":"<p>The Document Image Preprocessing Pipeline integrates two key functions: document orientation classification and geometric distortion correction. The document orientation classification module automatically identifies the four possible orientations of a document (0\u00b0, 90\u00b0, 180\u00b0, 270\u00b0), ensuring that the document is processed in the correct direction. The text image unwarping model is designed to correct geometric distortions that occur during document photography or scanning, restoring the document's original shape and proportions. This pipeline is suitable for digital document management, preprocessing tasks for OCR, and any scenario requiring improved document image quality. By automating orientation correction and geometric distortion correction, this module significantly enhances the accuracy and efficiency of document processing, providing a more reliable foundation for image analysis. The pipeline also offers flexible service-oriented deployment options, supporting calls from various programming languages on multiple hardware platforms. Additionally, the pipeline supports secondary development, allowing you to fine-tune the models on your own datasets and seamlessly integrate the trained models.</p> <p></p> <p>The General Document Image Preprocessing Pipeline includes the following two modules. Each module can be trained and inferred independently and contains multiple models. For detailed information, please click on the corresponding module to view the documentation.</p> <ul> <li>Document Image Orientation Classification Module (Optional)</li> <li>Text Image Unwarping Module (Optional)</li> </ul> <p>In this pipeline, you can select the models to use based on the benchmark data provided below.</p> Document Image Orientation Classification Module (Optional): ModelModel Download Links Top-1 Acc (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (MB) Description PP-LCNet_x1_0_doc_oriInference Model/Training Model 99.06 2.31 / 0.43 3.37 / 1.27 7 A document image classification model based on PP-LCNet_x1_0, which includes four categories: 0\u00b0, 90\u00b0, 180\u00b0, and 270\u00b0. Text Image Unwarping Module (Optional): ModelModel Download Links CER  Model Storage Size (MB) Description UVDocInference Model/Training Model 0.179 30.3 MB A high-precision text image unwarping model. Test Environment Description: <ul> <li>Performance Test Environment <ul> <li>Test Datasets:               <ul> <li>Document Image Orientation Classification Model: A self-built dataset by PaddleX, covering various scenarios including ID cards and documents, containing 1000 images.</li> <li>Text Image Unwarping Model: DocUNet.</li> </ul> </li> <li>Hardware Configuration: <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environment: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Description</li> </ul> Mode GPU Configuration CPU Configuration Acceleration Technology Combination Normal Mode FP32 Precision / No TRT Acceleration FP32 Precision / 8 Threads PaddleInference High-Performance Mode Optimal combination of precision type and acceleration strategy selected in advance FP32 Precision / 8 Threads Optimal backend (Paddle/OpenVINO/TRT, etc.) selected in advance"},{"location":"en/version3.x/pipeline_usage/doc_preprocessor.html#2-quick-start","title":"2. Quick Start","text":"<p>Before using the General Document Image Preprocessing Pipeline locally, ensure that you have completed the wheel package installation according to the Installation Guide. After installation, you can experience it via the command line or integrate it into Python locally.</p>"},{"location":"en/version3.x/pipeline_usage/doc_preprocessor.html#21-command-line-experience","title":"2.1 Command Line Experience","text":"<p>You can quickly experience the <code>doc_preprocessor</code> pipeline with a single command:</p> <pre><code>paddleocr doc_preprocessor -i https://paddle-model-ecology.bj.bcebos.com/paddlex/demo_image/doc_test_rotated.jpg\n\n# Specify whether to use the document orientation classification model via --use_doc_orientation_classify\npaddleocr doc_preprocessor -i ./doc_test_rotated.jpg --use_doc_orientation_classify True\n\n# Specify whether to use the text image unwarping module via --use_doc_unwarping\npaddleocr doc_preprocessor -i ./doc_test_rotated.jpg --use_doc_unwarping True\n\n# Specify the use of GPU for model inference via --device\npaddleocr doc_preprocessor -i ./doc_test_rotated.jpg --device gpu\n</code></pre> The command line supports more parameter settings. Click to expand for detailed explanations of command line parameters. Parameter Description Parameter Type Default Value <code>input</code> The data to be predicted. This parameter is required. For example, the local path of an image file or PDF file: <code>/root/data/img.jpg</code>; or a URL link, such as the network URL of an image file or PDF file: example; or a local directory, which should contain the images to be predicted, such as the local path: <code>/root/data/</code> (currently does not support prediction of PDF files in directories; PDF files need to be specified to a specific file path).  <code>str</code> <code>save_path</code> Specify the path to save the inference result file. If not set, the inference result will not be saved locally. <code>str</code> <code>doc_orientation_classify_model_name</code> The name of the document orientation classification model. If not set, the pipeline's default model will be used. <code>str</code> <code>doc_orientation_classify_model_dir</code> The directory path of the document orientation classification model. If not set, the official model will be downloaded. <code>str</code> <code>doc_unwarping_model_name</code> The name of the text image unwarping model. If not set, the pipeline's default model will be used. <code>str</code> <code>doc_unwarping_model_dir</code> The directory path of the text image unwarping model. If not set, the official model will be downloaded. <code>str</code> <code>use_doc_orientation_classify</code> Whether to load and use  the document orientation classification module. If not set, the parameter value initialized by the pipeline will be used by default, initialized as <code>True</code>. <code>bool</code> <code>use_doc_unwarping</code> Whether to load and use  the text image unwarping module. If not set, the parameter value initialized by the pipeline will be used by default, initialized as <code>True</code>. <code>bool</code> <code>device</code> The device used for inference. Support for specifying specific card numbers: <ul> <li>CPU: For example, <code>cpu</code> indicates using the CPU for inference;</li> <li>GPU: For example, <code>gpu:0</code> indicates using the first GPU for inference;</li> <li>NPU: For example, <code>npu:0</code> indicates using the first NPU for inference;</li> <li>XPU: For example, <code>xpu:0</code> indicates using the first XPU for inference;</li> <li>MLU: For example, <code>mlu:0</code> indicates using the first MLU for inference;</li> <li>DCU: For example, <code>dcu:0</code> indicates using the first DCU for inference;</li> </ul> If not set, the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.  <code>str</code> <code>enable_hpi</code> Whether to enable high-performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> The computational precision, such as fp32, fp16. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set. <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> The number of threads used for inference on the CPU. <code>int</code> <code>8</code> <code>paddlex_config</code> Path to PaddleX pipeline configuration file. <code>str</code> <p></p> <p>The running results will be printed to the terminal. The running results of the <code>doc_preprocessor</code> pipeline with default configuration are as follows: <pre><code>{'res': {'input_path': '/root/.paddlex/predict_input/doc_test_rotated.jpg', 'page_index': None, 'model_settings': {'use_doc_orientation_classify': True, 'use_doc_unwarping': True}, 'angle': 180}}\n</code></pre></p> <p>The visualization results are saved under the <code>save_path</code>. The visualization results are as follows:</p> <p></p>"},{"location":"en/version3.x/pipeline_usage/doc_preprocessor.html#22-integration-via-python-script","title":"2.2 Integration via Python Script","text":"<p>The command-line approach is for quick experience and viewing results. Generally, in projects, integration through code is often required. You can achieve rapid inference in pipelines with just a few lines of code. The inference code is as follows:</p> <pre><code>from paddleocr import DocPreprocessor\n\npipeline = DocPreprocessor()\n# docpp = DocPreprocessor(use_doc_orientation_classify=True) # Specify whether to use the document orientation classification model via use_doc_orientation_classify\n# docpp = DocPreprocessor(use_doc_unwarping=True) # Specify whether to use the text image unwarping module via use_doc_unwarping\n# docpp = DocPreprocessor(device=\"gpu\") # Specify whether to use GPU for model inference via device\noutput = pipeline.predict(\"./doc_test_rotated.jpg\")\nfor res in output:\n    res.print()  ## Print the structured output of the prediction\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/\")\n</code></pre> <p>In the above Python script, the following steps are executed:</p> <p>(1) Instantiate the <code>doc_preprocessor</code> pipeline object via <code>DocPreprocessor()</code>. The specific parameter descriptions are as follows:</p> Parameter Description Parameter Type Default Value <code>doc_orientation_classify_model_name</code> The name of the document orientation classification model. If set to <code>None</code>, the pipeline's default model will be used. <code>str</code> <code>None</code> <code>doc_orientation_classify_model_dir</code> The directory path of the document orientation classification model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>doc_unwarping_model_name</code> The name of the text image unwarping model. If set to <code>None</code>, the pipeline's default model will be used. <code>str</code> <code>None</code> <code>doc_unwarping_model_dir</code> The directory path of the text image unwarping model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>use_doc_orientation_classify</code> Whether to load and use the document orientation classification module. If set to <code>None</code>, the parameter value initialized by the pipeline will be used by default, initialized as <code>True</code>. <code>bool</code> <code>None</code> <code>use_doc_unwarping</code> Whether to load and use the text image unwarping module. If set to <code>None</code>, the parameter value initialized by the pipeline will be used by default, initialized as <code>True</code>. <code>bool</code> <code>None</code> <code>device</code> The device used for inference. Support for specifying specific card numbers: <ul> <li>CPU: For example, <code>cpu</code> indicates using the CPU for inference;</li> <li>GPU: For example, <code>gpu:0</code> indicates using the first GPU for inference;</li> <li>NPU: For example, <code>npu:0</code> indicates using the first NPU for inference;</li> <li>XPU: For example, <code>xpu:0</code> indicates using the first XPU for inference;</li> <li>MLU: For example, <code>mlu:0</code> indicates using the first MLU for inference;</li> <li>DCU: For example, <code>dcu:0</code> indicates using the first DCU for inference;</li> <li>None: If set to <code>None</code>,  the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.</li> </ul> <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to enable high-performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> The computational precision, such as fp32, fp16. <code>str</code> <code>\"fp32\"</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set. <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> The number of threads used for inference on the CPU. <code>int</code> <code>8</code> <code>paddlex_config</code> Path to PaddleX pipeline configuration file. <code>str</code> <code>None</code> <p>(2) Call the <code>predict()</code> method of the <code>doc_preprocessor</code> pipeline object for inference prediction. This method will return a list of results.</p> <p>In addition, the pipeline also provides the <code>predict_iter()</code> method. The two methods are completely consistent in terms of parameter acceptance and result return. The difference is that <code>predict_iter()</code> returns a <code>generator</code>, which can process and obtain prediction results step by step, suitable for scenarios with large datasets or where memory savings are desired. You can choose either of the two methods according to your actual needs.</p> <p>The following are the parameters and their descriptions of the <code>predict()</code> method:</p> Parameter Description Parameter Type Default Value <code>input</code> The data to be predicted, supporting multiple input types. This parameter is required. <ul> <li>Python Var: For example, image data represented as <code>numpy.ndarray</code>;</li> <li>str: For example, the local path of an image file or PDF file: <code>/root/data/img.jpg</code>; or a URL link, such as the network URL of an image file or PDF file: example; or a local directory, which should contain the images to be predicted, such as the local path: <code>/root/data/</code> (currently does not support prediction of PDF files in directories; PDF files need to be specified to a specific file path);</li> <li>List: The list elements should be of the above types, such as <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code>.</li> </ul> <code>Python Var|str|list</code> <code>use_doc_orientation_classify</code> Whether to use the document orientation classification module during inference. <code>bool</code> <code>None</code> <code>use_doc_unwarping</code> Whether to use the text image unwarping module during inference. <code>bool</code> <code>None</code> <p>(3) Process the prediction results. The prediction result for each sample is a corresponding Result object, which supports operations such as printing, saving as an image, and saving as a <code>json</code> file:</p> Method Description Parameter Parameter Type Description Default Value <code>print()</code> Print the result to the terminal <code>format_json</code> <code>bool</code> Whether to format the output content using <code>JSON</code> indentation <code>True</code> <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data for better readability. Only valid when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> retains the original characters. Only valid when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_json()</code> Save the result as a JSON file <code>save_path</code> <code>str</code> The file path for saving. When it is a directory, the saved file name will be consistent with the input file type name. None <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data for better readability. Only valid when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> retains the original characters. Only valid when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_img()</code> Save the result as an image file <code>save_path</code> <code>str</code> The file path for saving. Supports directory or file paths. None <p>Here's the continuation of the translation:</p> <ul> <li> <p>Calling the <code>print()</code> method will output the results to the terminal. The content printed to the terminal is explained as follows:</p> <ul> <li> <p><code>input_path</code>: <code>(str)</code> The input path of the image to be predicted</p> </li> <li> <p><code>page_index</code>: <code>(Union[int, None])</code> If the input is a PDF file, it indicates the current page number of the PDF; otherwise, it is <code>None</code></p> </li> <li> <p><code>model_settings</code>: <code>(Dict[str, bool])</code> Model parameters configured for the pipeline</p> <ul> <li><code>use_doc_orientation_classify</code>: <code>(bool)</code> Controls whether to enable the document orientation classification module</li> <li><code>use_doc_unwarping</code>: <code>(bool)</code> Controls whether to enable the text image rectification module</li> </ul> </li> <li> <p><code>angle</code>: <code>(int)</code> The prediction result of the document orientation classification. When enabled, the value is one of [0,90,180,270]; when disabled, it is -1</p> </li> </ul> </li> <li> <p>Calling the <code>save_to_json()</code> method will save the above content to the specified <code>save_path</code>. If a directory is specified, the saved path will be <code>save_path/{your_img_basename}.json</code>. If a file is specified, it will be saved directly to that file. Since JSON files do not support saving numpy arrays, <code>numpy.array</code> types will be converted to list form.</p> </li> <li> <p>Calling the <code>save_to_img()</code> method will save the visualization results to the specified <code>save_path</code>. If a directory is specified, the saved path will be <code>save_path/{your_img_basename}_doc_preprocessor_res_img.{your_img_extension}</code>. If a file is specified, it will be saved directly to that file. (Production lines usually contain many result images, so it is not recommended to specify a specific file path directly, as multiple images will be overwritten, and only the last image will be retained)</p> </li> <li> <p>In addition, it also supports obtaining visualization images and prediction results with results through attributes, as follows:</p> </li> </ul> Attribute Attribute Description <code>json</code> Obtain the prediction result in JSON format <code>img</code> Obtain visualization images in dictionary format <ul> <li>The prediction result obtained by the <code>json</code> attribute is data of type dict, and the content is consistent with that saved by calling the <code>save_to_json()</code> method.</li> <li>The prediction result returned by the <code>img</code> attribute is a dictionary-type data. The key is <code>preprocessed_img</code>, and the corresponding value is an <code>Image.Image</code> object: a visualization image for displaying the doc_preprocessor result.</li> </ul>"},{"location":"en/version3.x/pipeline_usage/doc_preprocessor.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy, you can proceed directly to development integration/deployment.</p> <p>If you need to apply the pipeline directly to your Python project, you can refer to the example code in 2.2 Python Script Integration.</p> <p>In addition, PaddleOCR also provides two other deployment methods, which are detailed as follows:</p> <p>\ud83d\ude80 High-performance inference: In actual production environments, many applications have strict performance requirements (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleOCR provides high-performance inference functionality, aiming to deeply optimize model inference and pre/post-processing to achieve significant end-to-end process acceleration. For detailed high-performance inference procedures, please refer to the High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-oriented deployment: Service-oriented deployment is a common form of deployment in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. For detailed pipeline service-oriented deployment procedures, please refer to the Service-Oriented Deployment Guide.</p> <p>Below are the API references for basic service-oriented deployment and examples of multi-language service calls:</p> API Reference <p>Main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the properties of the response body are as follows:</li> </ul> Name Type Description <code>logId</code> <code>string</code> The UUID of the request. <code>errorCode</code> <code>integer</code> Error code. Fixed to <code>0</code>. <code>errorMsg</code> <code>string</code> Error description. Fixed to <code>\"Success\"</code>. <code>result</code> <code>object</code> Operation result. <ul> <li>When the request is not processed successfully, the properties of the response body are as follows:</li> </ul> Name Type Description <code>logId</code> <code>string</code> The UUID of the request. <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error description. <p>Main operations provided by the service:</p> <ul> <li><code>infer</code></li> </ul> <p>Obtain the preprocessing result of the image document image.</p> <p><code>POST /document-preprocessing</code></p> <ul> <li>Properties of the request body:</li> </ul> Name Type Description Required <code>file</code> <code>string</code> The URL of an image file or PDF file accessible to the server, or the Base64 encoding result of the content of the above types of files. By default, for PDF files with more than 10 pages, only the first 10 pages will be processed. To remove the page limit, please add the following configuration to the pipeline configuration file: <pre><code>Serving:\n  extra:\n    max_num_input_imgs: null\n</code></pre> Yes <code>fileType</code> <code>integer</code> | <code>null</code> File type. <code>0</code> indicates a PDF file, and <code>1</code> indicates an image file. If this property is not present in the request body, the file type will be inferred based on the URL. No <code>useDocOrientationClassify</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>use_doc_orientation_classify</code> parameter in the <code>predict</code> method of the pipeline object. No <code>useDocUnwarping</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>use_doc_unwarping</code> parameter in the <code>predict</code> method of the pipeline object. No <ul> <li>When the request is processed successfully, the <code>result</code> in the response body has the following properties:</li> </ul> Name Type Description <code>docPreprocessingResults</code> <code>object</code> Document image preprocessing results. The array length is 1 (for image input) or the actual number of processed document pages (for PDF input). For PDF input, each element in the array represents the result of each page actually processed in the PDF file. <code>dataInfo</code> <code>object</code> Input data information. <p>Each element in <code>docPreprocessingResults</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>outputImage</code> <code>string</code> The preprocessed image. The image is in PNG format and uses Base64 encoding. <code>prunedResult</code> <code>object</code> A simplified version of the <code>res</code> field in the JSON representation of the result generated by the <code>predict</code> method of the pipeline object, with the <code>input_path</code> and <code>page_index</code> fields removed. <code>docPreprocessingImage</code> <code>string</code> \uff5c <code>null</code> Visualization result image. The image is in JPEG format and uses Base64 encoding. <code>inputImage</code> <code>string</code> \uff5c <code>null</code> Input image. The image is in JPEG format and uses Base64 encoding. Multi-language Service Call Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/document-preprocessing\"\nfile_path = \"./demo.jpg\"\n\nwith open(file_path, \"rb\") as file:\n    file_bytes = file.read()\n    file_data = base64.b64encode(file_bytes).decode(\"ascii\")\n\npayload = {\"file\": file_data, \"fileType\": 1}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nfor i, res in enumerate(result[\"docPreprocessingResults\"]):\n    print(res[\"prunedResult\"])\n    output_img_path = f\"out_{i}.png\"\n    with open(output_img_path, \"wb\") as f:\n        f.write(base64.b64decode(res[\"outputImage\"]))\n    print(f\"Output image saved at {output_img_path}\")\n</code></pre> <p></p>"},{"location":"en/version3.x/pipeline_usage/doc_preprocessor.html#4-secondary-development","title":"4. Secondary Development","text":"<p>If the default model weights provided by the document image preprocessing pipeline do not meet your accuracy or speed requirements in your specific scenario, you can attempt to further fine-tune the existing model using your own domain-specific or application-specific data to enhance the recognition performance of the document image preprocessing pipeline in your context.</p>"},{"location":"en/version3.x/pipeline_usage/doc_preprocessor.html#41-model-fine-tuning","title":"4.1 Model Fine-Tuning","text":"<p>Since the document image preprocessing pipeline comprises multiple modules, any module could potentially contribute to suboptimal performance if the overall pipeline does not meet expectations. You can analyze images with poor recognition results to identify which module is causing the issue and then refer to the corresponding fine-tuning tutorial links in the table below to perform model fine-tuning.</p> Scenario Module to Fine-Tune Fine-Tuning Reference Link Inaccurate rotation correction of the entire image Document Image Orientation Classification Module Link Inaccurate distortion correction of the image Text Image Rectification Module Fine-tuning is currently not supported"},{"location":"en/version3.x/pipeline_usage/doc_understanding.html","title":"Document Understanding Pipeline Usage Tutorial","text":""},{"location":"en/version3.x/pipeline_usage/doc_understanding.html#1-introduction-to-the-document-understanding-pipeline","title":"1. Introduction to the Document Understanding Pipeline","text":"<p>The Document Understanding Pipeline is an advanced document processing technology based on Visual-Language Models (VLM), designed to overcome the limitations of traditional document processing. Traditional methods rely on fixed templates or predefined rules to parse documents, whereas this pipeline leverages the multimodal capabilities of VLM to accurately answer user queries by inputting document images and user questions, integrating visual and language information. This technology does not require pre-training for specific document formats, allowing it to flexibly handle diverse document content, significantly enhancing the generalization and practicality of document processing. It has broad application prospects in intelligent Q&amp;A, information extraction, and other scenarios. Currently, the pipeline does not support secondary development of VLM models, but plans to support it in the future.</p> <p></p> <p>The document understanding pipeline includes the following module. Each module can be trained and inferred independently and contains multiple models. For more details, click the corresponding module to view the documentation.</p> <ul> <li>Document-like Vision Language Model Module</li> </ul> <p>In this pipeline, you can choose the model to use based on the benchmark data below.</p> Document-like Vision Language Model Module: ModelModel Download Link Model Storage Size (GB) Total Score Description PP-DocBee-2BInference Model 4.2 765 PP-DocBee is a multimodal large model independently developed by the PaddlePaddle team, focusing on document understanding, with excellent performance in Chinese document understanding tasks. The model is fine-tuned and optimized using nearly 5 million multimodal datasets related to document understanding, including general VQA, OCR, chart, text-rich documents, math and complex reasoning, synthetic data, pure text data, etc., with different training data ratios set. In several authoritative English document understanding evaluation lists in academia, PP-DocBee has generally achieved SOTA for models of the same parameter scale. In internal business Chinese scenarios, PP-DocBee also outperforms current popular open and closed-source models. PP-DocBee-7BInference Model 15.8 - PP-DocBee2-3BInference Model 7.6 852 PP-DocBee2 is a multimodal large model independently developed by the PaddlePaddle team, focusing on document understanding. It further optimizes the basic model based on PP-DocBee and introduces new data optimization schemes to improve data quality. With only 470,000 data generated using self-developed data synthesis strategy, PP-DocBee2 performs better in Chinese document understanding tasks. In internal business Chinese scenarios, PP-DocBee2 improves by about 11.4% compared to PP-DocBee and also outperforms current popular open and closed-source models of the same scale. Note: The above total scores are the model test results of the internal evaluation set. All images in the internal evaluation set have a resolution (height, width) of (1680,1204), with a total of 1196 data, including scenarios such as financial reports, laws and regulations, science and engineering papers, manuals, humanities papers, contracts, and research reports, with no plans to make it public for now. <p> If you focus more on model accuracy, choose a model with higher accuracy; if you care more about inference speed, choose a model with faster inference speed; if you are concerned about storage size, choose a model with a smaller storage volume.</p>"},{"location":"en/version3.x/pipeline_usage/doc_understanding.html#2-quick-start","title":"2. Quick Start","text":"<p>Before using the document understanding pipeline locally, ensure that you have completed the installation of the wheel package according to the installation tutorial. After installation, you can experience it locally using the command line or Python integration.</p>"},{"location":"en/version3.x/pipeline_usage/doc_understanding.html#21-command-line-experience","title":"2.1 Command Line Experience","text":"<p>Experience the doc_understanding pipeline with just one command line:</p> <pre><code>paddleocr doc_understanding -i \"{'image': 'https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/medal_table.png', 'query': '\u200b\u8bc6\u522b\u200b\u8fd9\u4efd\u200b\u8868\u683c\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u4ee5\u200bmarkdown\u200b\u683c\u5f0f\u200b\u8f93\u51fa\u200b'}\"\n</code></pre> The command line supports more parameter settings, click to expand for a detailed explanation of the command line parameters Parameter Description Type Default Value <code>input</code> Data to be predicted,  required. \"{'image': 'https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/medal_table.png', 'query': 'Recognize the content of this table and output it in markdown format'}\".  <code>str</code> <code>save_path</code> Specify the path for saving the inference result file. If not set, the inference result will not be saved locally. <code>str</code> <code>doc_understanding_model_name</code> The name of the document understanding model. If not set, the default model of the pipeline will be used. <code>str</code> <code>doc_understanding_model_dir</code> The directory path of the document understanding model. If not set, the official model will be downloaded. <code>str</code> <code>doc_understanding_batch_size</code> The batch size of the document understanding model. If not set, the default batch size will be set to <code>1</code>. <code>int</code> <code>device</code> The device used for inference. Supports specifying a specific card number: <ul> <li>CPU: For example, <code>cpu</code> indicates using the CPU for inference;</li> <li>GPU: For example, <code>gpu:0</code> indicates using the first GPU for inference;</li> <li>NPU: For example, <code>npu:0</code> indicates using the first NPU for inference;</li> <li>XPU: For example, <code>xpu:0</code> indicates using the first XPU for inference;</li> <li>MLU: For example, <code>mlu:0</code> indicates using the first MLU for inference;</li> <li>DCU: For example, <code>dcu:0</code> indicates using the first DCU for inference;</li> </ul>If not set,  the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.  <code>str</code> <code>enable_hpi</code> Whether to enable high-performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Calculation precision, such as fp32, fp16. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set. <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> The number of threads used for inference on the CPU. <code>int</code> <code>8</code> <code>paddlex_config</code> Path to PaddleX pipeline configuration file. <code>str</code> <p></p> <p>The results will be printed to the terminal, and the default configuration of the doc_understanding pipeline will produce the following output:</p> <pre><code>{'res': {'image': 'https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/medal_table.png', 'query': '\u200b\u8bc6\u522b\u200b\u8fd9\u4efd\u200b\u8868\u683c\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u4ee5\u200bmarkdown\u200b\u683c\u5f0f\u200b\u8f93\u51fa\u200b', 'result': '| \u200b\u540d\u6b21\u200b | \u200b\u56fd\u5bb6\u200b/\u200b\u5730\u533a\u200b | \u200b\u91d1\u724c\u200b | \u200b\u94f6\u724c\u200b | \u200b\u94dc\u724c\u200b | \u200b\u5956\u724c\u200b\u603b\u6570\u200b |\\n| --- | --- | --- | --- | --- | --- |\\n| 1 | \u200b\u4e2d\u56fd\u200b\uff08CHN\uff09 | 48 | 22 | 30 | 100 |\\n| 2 | \u200b\u7f8e\u56fd\u200b\uff08USA\uff09 | 36 | 39 | 37 | 112 |\\n| 3 | \u200b\u4fc4\u7f57\u65af\u200b\uff08RUS\uff09 | 24 | 13 | 23 | 60 |\\n| 4 | \u200b\u82f1\u56fd\u200b\uff08GBR\uff09 | 19 | 13 | 19 | 51 |\\n| 5 | \u200b\u5fb7\u56fd\u200b\uff08GER\uff09 | 16 | 11 | 14 | 41 |\\n| 6 | \u200b\u6fb3\u5927\u5229\u4e9a\u200b\uff08AUS\uff09 | 14 | 15 | 17 | 46 |\\n| 7 | \u200b\u97e9\u56fd\u200b\uff08KOR\uff09 | 13 | 11 | 8 | 32 |\\n| 8 | \u200b\u65e5\u672c\u200b\uff08JPN\uff09 | 9 | 8 | 8 | 25 |\\n| 9 | \u200b\u610f\u5927\u5229\u200b\uff08ITA\uff09 | 8 | 9 | 10 | 27 |\\n| 10 | \u200b\u6cd5\u56fd\u200b\uff08FRA\uff09 | 7 | 16 | 20 | 43 |\\n| 11 | \u200b\u8377\u5170\u200b\uff08NED\uff09 | 7 | 5 | 4 | 16 |\\n| 12 | \u200b\u4e4c\u514b\u5170\u200b\uff08UKR\uff09 | 7 | 4 | 11 | 22 |\\n| 13 | \u200b\u80af\u5c3c\u4e9a\u200b\uff08KEN\uff09 | 6 | 4 | 6 | 16 |\\n| 14 | \u200b\u897f\u73ed\u7259\u200b\uff08ESP\uff09 | 5 | 11 | 3 | 19 |\\n| 15 | \u200b\u7259\u4e70\u52a0\u200b\uff08JAM\uff09 | 5 | 4 | 2 | 11 |\\n'}}\n</code></pre>"},{"location":"en/version3.x/pipeline_usage/doc_understanding.html#22-python-script-integration","title":"2.2 Python Script Integration","text":"<p>The command line method is for quickly experiencing the effect. Generally, in projects, code integration is often required. You can complete quick inference of the pipeline with just a few lines of code. The inference code is as follows:</p> <pre><code>from paddleocr import DocUnderstanding\n\npipeline = DocUnderstanding()\noutput = pipeline.predict(\n    {\n        \"image\": \"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/medal_table.png\",\n        \"query\": \"\u200b\u8bc6\u522b\u200b\u8fd9\u4efd\u200b\u8868\u683c\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u4ee5\u200bmarkdown\u200b\u683c\u5f0f\u200b\u8f93\u51fa\u200b\"\n    }\n)\nfor res in output:\n    res.print() ## Print the structured output of the prediction\n    res.save_to_json(\"./output/\")\n</code></pre> <p>In the above Python script, the following steps are performed:</p> <p>(1) Instantiate a Document Understanding Pipeline object through <code>DocUnderstanding()</code>. The specific parameter descriptions are as follows:</p> Parameter Description Type Default Value <code>doc_understanding_model_name</code> The name of the document understanding model. If set to <code>None</code>, the default model of the pipeline will be used. <code>str</code> <code>None</code> <code>doc_understanding_model_dir</code> The directory path of the document understanding model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>doc_understanding_batch_size</code> The batch size of the document understanding model. If set to <code>None</code>, the default batch size will be set to <code>1</code>. <code>int</code> <code>None</code> <code>device</code> The device used for inference. Supports specifying a specific card number: <ul> <li>CPU: For example, <code>cpu</code> indicates using the CPU for inference;</li> <li>GPU: For example, <code>gpu:0</code> indicates using the first GPU for inference;</li> <li>NPU: For example, <code>npu:0</code> indicates using the first NPU for inference;</li> <li>XPU: For example, <code>xpu:0</code> indicates using the first XPU for inference;</li> <li>MLU: For example, <code>mlu:0</code> indicates using the first MLU for inference;</li> <li>DCU: For example, <code>dcu:0</code> indicates using the first DCU for inference;</li> <li>None: If set to <code>None</code>,  the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.</li> </ul> <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to enable high-performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Calculation precision, such as fp32, fp16. <code>str</code> <code>\"fp32\"</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set. <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> The number of threads used for inference on the CPU. <code>int</code> <code>8</code> <code>paddlex_config</code> Path to PaddleX pipeline configuration file. <code>str</code> <code>None</code> <p>(2) Call the <code>predict()</code> method of the Document Understanding Pipeline object for inference prediction, which will return a result list.</p> <p>Additionally, the pipeline also provides a <code>predict_iter()</code> method. Both methods are consistent in terms of parameter acceptance and result return. The difference is that <code>predict_iter()</code> returns a <code>generator</code> that can process and obtain prediction results step by step, suitable for handling large datasets or scenarios where memory saving is desired. You can choose to use either method according to your actual needs.</p> <p>Below are the parameters and their descriptions for the <code>predict()</code> method:</p> Parameter Description Type Default Value <code>input</code> Data to be predicted, currently only supports dictionary type input <ul> <li>Python Dict: The input format for PP-DocBee is: <code>{\"image\":/path/to/image, \"query\": user question}</code>, representing the input image and corresponding user question.</li> </ul> <code>Python Dict</code> <p>(3) Process the prediction results. The prediction result for each sample is a corresponding Result object, which supports printing and saving as a <code>json</code> file:</p> Method Description Parameter Type Parameter Description Default Value <code>print()</code> Print the result to the terminal <code>format_json</code> <code>bool</code> Whether to format the output content using <code>JSON</code> indentation. <code>True</code> <code>indent</code> <code>int</code> Specifies the indentation level to beautify the output <code>JSON</code> data, making it more readable, effective only when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Controls whether to escape non-<code>ASCII</code> characters into <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> will retain the original characters, effective only when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_json()</code> Save the result as a JSON format file <code>save_path</code> <code>str</code> The path to save the file. When specified as a directory, the saved file is named consistent with the input file type. None <code>indent</code> <code>int</code> Specifies the indentation level to beautify the output <code>JSON</code> data, making it more readable, effective only when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Controls whether to escape non-<code>ASCII</code> characters into <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> will retain the original characters, effective only when <code>format_json</code> is <code>True</code>. <code>False</code> <ul> <li> <p>Calling the <code>print()</code> method will print the result to the terminal. The content printed to the terminal is explained as follows:</p> <ul> <li> <p><code>image</code>: <code>(str)</code> Input path of the image</p> </li> <li> <p><code>query</code>: <code>(str)</code> Question regarding the input image</p> </li> <li> <p><code>result</code>: <code>(str)</code> Output result of the model</p> </li> </ul> </li> <li> <p>Calling the <code>save_to_json()</code> method will save the above content to the specified <code>save_path</code>. If specified as a directory, the path saved will be <code>save_path/{your_img_basename}_res.json</code>, and if specified as a file, it will be saved directly to that file.</p> </li> <li> <p>Additionally, the result can be obtained through attributes that provide the visualized images with results and the prediction results, as follows:</p> </li> </ul> Attribute Description <code>json</code> Get the prediction result in <code>json</code> format <code>img</code> Get the visualized image in <code>dict</code> format <ul> <li>The prediction result obtained through the <code>json</code> attribute is data of the dict type, consistent with the content saved by calling the <code>save_to_json()</code> method.</li> </ul>"},{"location":"en/version3.x/pipeline_usage/doc_understanding.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for pipeline inference speed and accuracy, you can proceed with development integration/deployment directly.</p> <p>If you need to apply the pipeline directly to your Python project, you can refer to the example code in 2.2 Python Script Integration.</p> <p>In addition, PaddleOCR also provides two other deployment methods, detailed descriptions are as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In real production environments, many applications have strict standards for the performance indicators of deployment strategies (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleOCR provides high-performance inference capabilities, aiming to deeply optimize the performance of model inference and pre-and post-processing, achieving significant acceleration of the end-to-end process. For detailed high-performance inference processes, refer to High-Performance Inference.</p> <p>\u2601\ufe0f Service Deployment: Service deployment is a common form of deployment in real production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. For detailed pipeline service deployment processes, refer to Serving.</p> <p>Below is the API reference for basic service deployment and examples of service invocation in multiple languages:</p> API Reference <p>For the main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>Both the request body and response body are JSON data (JSON object).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body has the following attributes:</li> </ul> Name Type Meaning <code>logId</code> <code>string</code> UUID of the request. <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error description. Fixed as <code>\"Success\"</code>. <code>result</code> <code>object</code> Operation result. <ul> <li>When the request is not processed successfully, the response body has the following attributes:</li> </ul> Name Type Meaning <code>logId</code> <code>string</code> UUID of the request. <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error description. <p>The main operations provided by the service are as follows:</p> <ul> <li><code>infer</code></li> </ul> <p>Perform inference on the input message to generate a response.</p> <p><code>POST /document-understanding</code></p> <p>Note: The above interface is also known as /chat/completion, compatible with OpenAI interfaces.</p> <ul> <li>The attributes of the request body are as follows:</li> </ul> Name Type Meaning Required Default Value <code>model</code> <code>string</code> The name of the model to use Yes - <code>messages</code> <code>array</code> List of dialogue messages Yes - <code>max_tokens</code> <code>integer</code> Maximum number of tokens to generate No 1024 <code>temperature</code> <code>float</code> Sampling temperature No 0.1 <code>top_p</code> <code>float</code> Core sampling probability No 0.95 <code>stream</code> <code>boolean</code> Whether to output in streaming mode No false <code>max_image_tokens</code> <code>int</code> Maximum number of input tokens for images No None <p>Each element in <code>messages</code> is an <code>object</code> with the following attributes:</p> Name Type Meaning Required <code>role</code> <code>string</code> Message role (user/assistant/system) Yes <code>content</code> <code>string</code> or <code>array</code> Message content (text or mixed media) Yes <p>When <code>content</code> is an array, each element is an <code>object</code> with the following attributes:</p> Name Type Meaning Required Default Value <code>type</code> <code>string</code> Content type (text/image_url) Yes - <code>text</code> <code>string</code> Text content (when type is text) Conditionally required - <code>image_url</code> <code>string</code> or <code>object</code> Image URL or object (when type is image_url) Conditionally required - <p>When <code>image_url</code> is an object, it has the following attributes:</p> Name Type Meaning Required Default Value <code>url</code> <code>string</code> Image URL Yes - <code>detail</code> <code>string</code> Image detail processing method (low/high/auto) No auto <p>When the request is processed successfully, the <code>result</code> in the response body has the following attributes:</p> Name Type Meaning <code>id</code> <code>string</code> Request ID <code>object</code> <code>string</code> Object type (chat.completion) <code>created</code> <code>integer</code> Creation timestamp <code>choices</code> <code>array</code> Generated result options <code>usage</code> <code>object</code> Token usage <p>Each element in <code>choices</code> is a <code>Choice</code> object with the following attributes:</p> Name Type Meaning Optional Values <code>finish_reason</code> <code>string</code> Reason for the model to stop generating tokens <code>stop</code> (natural stop)<code>length</code> (reached max token count)<code>tool_calls</code> (called a tool)<code>content_filter</code> (content filtering)<code>function_call</code> (called a function, deprecated) <code>index</code> <code>integer</code> Index of the option in the list - <code>logprobs</code> <code>object</code> | <code>null</code> Log probability information of the option - <code>message</code> <code>ChatCompletionMessage</code> Chat message generated by the model - <p>The <code>message</code> object has the following attributes:</p> Name Type Meaning Remarks <code>content</code> <code>string</code> | <code>null</code> Message content May be empty <code>refusal</code> <code>string</code> | <code>null</code> Refusal message generated by the model Provided when content is refused <code>role</code> <code>string</code> Role of the message author Fixed as <code>\"assistant\"</code> <code>audio</code> <code>object</code> | <code>null</code> Audio output data Provided when audio output is requestedLearn more <code>function_call</code> <code>object</code> | <code>null</code> Name and parameters of the function to be called Deprecated, recommended to use <code>tool_calls</code> <code>tool_calls</code> <code>array</code> | <code>null</code> Tool calls generated by the model Such as function calls <p>The <code>usage</code> object has the following attributes:</p> Name Type Meaning <code>prompt_tokens</code> <code>integer</code> Number of prompt tokens <code>completion_tokens</code> <code>integer</code> Number of generated tokens <code>total_tokens</code> <code>integer</code> Total number of tokens <p>An example of a <code>result</code> is as follows:</p> <pre><code>{\n    \"id\": \"ed960013-eb19-43fa-b826-3c1b59657e35\",\n    \"choices\": [\n        {\n            \"finish_reason\": \"stop\",\n            \"index\": 0,\n            \"message\": {\n                \"content\": \"| \u200b\u540d\u6b21\u200b | \u200b\u56fd\u5bb6\u200b/\u200b\u5730\u533a\u200b | \u200b\u91d1\u724c\u200b | \u200b\u94f6\u724c\u200b | \u200b\u94dc\u724c\u200b | \u200b\u5956\u724c\u200b\u603b\u6570\u200b |\\n| --- | --- | --- | --- | --- | --- |\\n| 1 | \u200b\u4e2d\u56fd\u200b\uff08CHN\uff09 | 48 | 22 | 30 | 100 |\\n| 2 | \u200b\u7f8e\u56fd\u200b\uff08USA\uff09 | 36 | 39 | 37 | 112 |\\n| 3 | \u200b\u4fc4\u7f57\u65af\u200b\uff08RUS\uff09 | 24 | 13 | 23 | 60 |\\n| 4 | \u200b\u82f1\u56fd\u200b\uff08GBR\uff09 | 19 | 13 | 19 | 51 |\\n| 5 | \u200b\u5fb7\u56fd\u200b\uff08GER\uff09 | 16 | 11 | 14 | 41 |\\n| 6 | \u200b\u6fb3\u5927\u5229\u4e9a\u200b\uff08AUS\uff09 | 14 | 15 | 17 | 46 |\\n| 7 | \u200b\u97e9\u56fd\u200b\uff08KOR\uff09 | 13 | 11 | 8 | 32 |\\n| 8 | \u200b\u65e5\u672c\u200b\uff08JPN\uff09 | 9 | 8 | 8 | 25 |\\n| 9 | \u200b\u610f\u5927\u5229\u200b\uff08ITA\uff09 | 8 | 9 | 10 | 27 |\\n| 10 | \u200b\u6cd5\u56fd\u200b\uff08FRA\uff09 | 7 | 16 | 20 | 43 |\\n| 11 | \u200b\u8377\u5170\u200b\uff08NED\uff09 | 7 | 5 | 4 | 16 |\\n| 12 | \u200b\u4e4c\u514b\u5170\u200b\uff08UKR\uff09 | 7 | 4 | 11 | 22 |\\n| 13 | \u200b\u80af\u5c3c\u4e9a\u200b\uff08KEN\uff09 | 6 | 4 | 6 | 16 |\\n| 14 | \u200b\u897f\u73ed\u7259\u200b\uff08ESP\uff09 | 5 | 11 | 3 | 19 |\\n| 15 | \u200b\u7259\u4e70\u52a0\u200b\uff08JAM\uff09 | 5 | 4 | 2 | 11 |\\n\",\n                \"role\": \"assistant\"\n            }\n        }\n    ],\n    \"created\": 1745218041,\n    \"model\": \"pp-docbee\",\n    \"object\": \"chat.completion\"\n}\n</code></pre> Multi-language Service Invocation Examples Python OpenAI interface invocation example  <pre><code>import base64\nfrom openai import OpenAI\n\nAPI_BASE_URL = \"http://0.0.0.0:8080\"\n\n# Initialize OpenAI client\nclient = OpenAI(\n    api_key='xxxxxxxxx',\n    base_url=f'{API_BASE_URL}'\n)\n\n# Function to convert image to base64\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\n# Input image path\nimage_path = \"medal_table.png\"\n\n# Convert original image to base64\nbase64_image = encode_image(image_path)\n\n# Submit information to PP-DocBee model\nresponse = client.chat.completions.create(\n    model=\"pp-docbee\",# Choose Model\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\":[\n                {\n                    \"type\": \"text\",\n                    \"text\": \"\u200b\u8bc6\u522b\u200b\u8fd9\u4efd\u200b\u8868\u683c\u200b\u7684\u200b\u5185\u5bb9\u200b,\u200b\u8f93\u51fa\u200bhtml\u200b\u683c\u5f0f\u200b\u7684\u200b\u5185\u5bb9\u200b\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n                },\n            ]\n        },\n    ],\n)\ncontent = response.choices[0].message.content\nprint('Reply:', content)\n</code></pre> <p></p>"},{"location":"en/version3.x/pipeline_usage/doc_understanding.html#4-secondary-development","title":"4. Secondary Development","text":"<p>The current pipeline does not support fine-tuning training and only supports inference integration. Concerning fine-tuning training for this pipeline, there are plans to support it in the future.</p>"},{"location":"en/version3.x/pipeline_usage/formula_recognition.html","title":"Formula Recognition Pipeline Usage Tutorial","text":""},{"location":"en/version3.x/pipeline_usage/formula_recognition.html#1-introduction-to-formula-recognition-pipeline","title":"1. Introduction to Formula Recognition Pipeline","text":"<p>Formula recognition is a technology that automatically identifies and extracts LaTeX formula content and structure from documents or images. It is widely used in fields such as mathematics, physics, and computer science for document editing and data analysis. By using computer vision and machine learning algorithms, formula recognition can convert complex mathematical formula information into editable LaTeX format, facilitating further processing and analysis of data.</p> <p>The formula recognition pipeline is designed to solve formula recognition tasks by extracting formula information from images and outputting it in LaTeX source code format. This pipeline integrates the advanced formula recognition model PP-FormulaNet developed by the PaddlePaddle Vision Team and the well-known formula recognition model UniMERNet. It is an end-to-end formula recognition system that supports the recognition of simple printed formulas, complex printed formulas, and handwritten formulas. Additionally, it includes functions for image orientation correction and distortion correction. Based on this pipeline, precise formula content prediction can be achieved, covering various application scenarios in education, research, finance, manufacturing, and other fields. The pipeline also provides flexible deployment options, supporting multiple hardware devices and programming languages. Moreover, it offers the capability for custom development. You can train and optimize the pipeline on your own dataset, and the trained model can be seamlessly integrated.</p> <p></p> <p> The formula recognition pipeline includes the following four modules. Each module can be trained and inferred independently and contains multiple models. For more details, please click on the respective module to view the documentation.</p> <ul> <li>Formula Recognition Module</li> <li>Layout Detection Module\uff08Optional\uff09</li> <li>Document Image Orientation Classification Module \uff08Optional\uff09</li> <li>Text Image Correction Module \uff08Optional\uff09</li> </ul> <p>In this pipeline, you can choose the model you want to use based on the benchmark data provided below.</p> Document Image Orientation Classification Module (Optional): ModelModel Download Link Top-1 Acc (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-LCNet_x1_0_doc_oriInference Model/Training Model 99.06 2.31 / 0.43 3.37 / 1.27 7 A document image classification model based on PP-LCNet_x1_0, with four categories: 0 degrees, 90 degrees, 180 degrees, and 270 degrees. Text Image Correction Module (Optional): ModelModel Download Link CER Model Storage Size (M) Introduction UVDocInference Model/Training Model 0.179 30.3 M High-precision text image correction model Layout Detection Module (Optional):  * The layout detection model includes 20 common categories: document title, paragraph title, text, page number, abstract, table, references, footnotes, header, footer, algorithm, formula, formula number, image, table, seal, figure_table title, chart, and sidebar text and lists of references ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-DocLayout_plus-LInference Model/Training Model 83.2 34.6244 / 10.3945 510.57 / -  126.01 M A higher-precision layout area localization model trained on a self-built dataset containing Chinese and English papers, PPT, multi-layout magazines, contracts, books, exams, ancient books and research reports using RT-DETR-L   * The layout detection model includes 23 common categories: document title, paragraph title, text, page number, abstract, table of contents, references, footnotes, header, footer, algorithm, formula, formula number, image, figure caption, table, table caption, seal, figure title, figure, header image, footer image, and sidebar text ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-DocLayout-LInference Model/Training Model 90.4 34.6244 / 10.3945 510.57 / - 123.76 M A high-precision layout area localization model trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using RT-DETR-L. PP-DocLayout-MInference Model/Training Model 75.2 13.3259 / 4.8685 44.0680 / 44.0680 22.578 A layout area localization model with balanced precision and efficiency, trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using PicoDet-L. PP-DocLayout-SInference Model/Training Model 70.9 8.3008 / 2.3794 10.0623 / 9.9296 4.834 A high-efficiency layout area localization model trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using PicoDet-S.   &gt;\u2757 The above list includes the 4 core models that are key supported by the layout detection module. The module actually supports a total of 7 full models, including several predefined models with different categories. The complete model list is as follows:   \ud83d\udc49Details of Model List  * Layout Detection Model, including 17 common layout categories: Paragraph Title, Image, Text, Number, Abstract, Content, Figure Caption, Formula, Table, Table Caption, References, Document Title, Footnote, Header, Algorithm, Footer, and Stamp ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PicoDet-S_layout_17clsInference Model/Training Model 87.4 9.11 / 2.12 15.42 / 9.12 4.8 A high-efficiency layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-S. PicoDet-L_layout_17clsInference Model/Training Model 89.0 13.50 / 4.69 43.32 / 43.32 22.6 A balanced efficiency and precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-L. RT-DETR-H_layout_17clsInference Model/Training Model 98.3 115.29 / 104.09 995.27 / 995.27 470.2 A high-precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using RT-DETR-H.   * Layout Detection Model, including 23 common layout categories: Document Title, Section Title, Text, Page Number, Abstract, Table of Contents, References, Footnotes, Header, Footer, Algorithm, Formula, Formula Number, Image, Figure Caption, Table, Table Caption, Seal, Chart Caption, Chart, Header Image, Footer Image, Sidebar Text ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-DocLayout-LInference Model/Training Model 90.4 34.6244 / 10.3945 510.57 / - 123.76 M A high-precision layout area localization model trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using RT-DETR-L. PP-DocLayout-MInference Model/Training Model 75.2 13.3259 / 4.8685 44.0680 / 44.0680 22.578 A layout area localization model with balanced precision and efficiency, trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using PicoDet-L. PP-DocLayout-SInference Model/Training Model 70.9 8.3008 / 2.3794 10.0623 / 9.9296 4.834 A high-efficiency layout area localization model trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using PicoDet-S.   * The layout detection model includes 20 common categories: document title, paragraph title, text, page number, abstract, table, references, footnotes, header, footer, algorithm, formula, formula number, image, table, seal, figure_table title, chart, and sidebar text and lists of references ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-DocLayout_plus-LInference Model/Training Model 83.2 34.6244 / 10.3945 510.57 / -  126.01 M A higher-precision layout area localization model trained on a self-built dataset containing Chinese and English papers, PPT, multi-layout magazines, contracts, books, exams, ancient books and research reports using RT-DETR-L Formula Recognition Module :  ModelModel Download Link En-BLEU(%) Zh-BLEU(%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction UniMERNetInference Model/Training Model 85.91 43.50 2266.96/- -/- 1.53 G UniMERNet is a formula recognition model developed by Shanghai AI Lab. It uses Donut Swin as the encoder and MBartDecoder as the decoder. The model is trained on a dataset of one million samples, including simple formulas, complex formulas, scanned formulas, and handwritten formulas, significantly improving the recognition accuracy of real-world formulas. PP-FormulaNet-SInference Model/Training Model 87.00 45.71 202.25/- -/- 224 M PP-FormulaNet is an advanced formula recognition model developed by the Baidu PaddlePaddle Vision Team. The PP-FormulaNet-S version uses PP-HGNetV2-B4 as its backbone network. Through parallel masking and model distillation techniques, it significantly improves inference speed while maintaining high recognition accuracy, making it suitable for applications requiring fast inference. The PP-FormulaNet-L version, on the other hand, uses Vary_VIT_B as its backbone network and is trained on a large-scale formula dataset, showing significant improvements in recognizing complex formulas compared to PP-FormulaNet-S. PP-FormulaNet-LInference Model/Training Model 90.36 45.78 1976.52/- -/- 695 M PP-FormulaNet_plus-SInference Model/Training Model 88.71 53.32 191.69/- -/- 248 M PP-FormulaNet_plus is an enhanced version of the formula recognition model developed by the Baidu PaddlePaddle Vision Team, building upon the original PP-FormulaNet. Compared to the original version, PP-FormulaNet_plus utilizes a more diverse formula dataset during training, including sources such as Chinese dissertations, professional books, textbooks, exam papers, and mathematics journals. This expansion significantly improves the model\u2019s recognition capabilities. Among the models, PP-FormulaNet_plus-M and PP-FormulaNet_plus-L have added support for Chinese formulas and increased the maximum number of predicted tokens for formulas from 1,024 to 2,560, greatly enhancing the recognition performance for complex formulas. Meanwhile, the PP-FormulaNet_plus-S model focuses on improving the recognition of English formulas. With these improvements, the PP-FormulaNet_plus series models perform exceptionally well in handling complex and diverse formula recognition tasks.  PP-FormulaNet_plus-MInference Model/Training Model 91.45 89.76 1301.56/- -/- 592 M PP-FormulaNet_plus-LInference Model/Training Model 92.22 90.64 1745.25/- -/- 698 M LaTeX_OCR_recInference Model/Training Model 74.55 39.96 1244.61/- -/- 99 M LaTeX-OCR is a formula recognition algorithm based on an autoregressive large model. It uses Hybrid ViT as the backbone network and a transformer as the decoder, significantly improving the accuracy of formula recognition. Test Environment Description:  <ul> <li>Performance Test Environment <ul> <li>Test Dataset:                <ul> <li>Document Image Orientation Classification Module: A self-built dataset using PaddleOCR, covering multiple scenarios such as ID cards and documents, containing 1000 images.</li> <li> Text Image Rectification Module: DocUNet\u3002</li> <li>Layout Region Detection Module: A self-built layout region detection dataset using PaddleOCR, including 500 images of common document types such as Chinese and English papers, magazines, contracts, books, exam papers, and research reports.</li> <li>17-Class Region Detection Model: A self-built layout region detection dataset using PaddleOCR, including 892 images of common document types such as Chinese and English papers, magazines, and research reports.</li> <li>Formula Recognition Module: A self-built formula recognition test set using PaddleX.</li> </ul> </li> <li>Hardware Configuration:  <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environments: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Description</li> </ul> Mode GPU Configuration  CPU Configuration  Acceleration Technology Combination Normal Mode FP32 Precision / No TRT Acceleration FP32 Precision / 8 Threads PaddleInference High-Performance Mode Optimal combination of pre-selected precision types and acceleration strategies FP32 Precision / 8 Threads Pre-selected optimal backend (Paddle/OpenVINO/TRT, etc.) <p> If you prioritize model accuracy, choose a model with higher precision; if you care more about inference speed, choose a faster model; if you are concerned about model storage size, choose a smaller model.</p>"},{"location":"en/version3.x/pipeline_usage/formula_recognition.html#2-quick-start","title":"2. Quick Start","text":"<p>Before using the formula recognition pipeline locally, please ensure that you have completed the wheel package installation according to the installation guide. Once installed, you can experience it locally via the command line or integrate it with Python.</p>"},{"location":"en/version3.x/pipeline_usage/formula_recognition.html#21-command-line-experience","title":"2.1 Command Line Experience","text":"<p>You can quickly experience the effect of the formula recognition pipeline with one command. Before running the code below, please download the example image locally\uff1a</p> <pre><code>paddleocr formula_recognition_pipeline -i https://paddle-model-ecology.bj.bcebos.com/paddlex/demo_image/pipelines/general_formula_recognition_001.png\n\n# Specify whether to use the document orientation classification model with --use_doc_orientation_classify.\npaddleocr formula_recognition_pipeline -i ./general_formula_recognition_001.png --use_doc_orientation_classify True\n\n# Specify whether to use the text image unwarping module with --use_doc_unwarping.\npaddleocr formula_recognition_pipeline -i ./general_formula_recognition_001.png --use_doc_unwarping True\n\n# Specify the use of GPU for model inference with --device.\npaddleocr formula_recognition_pipeline -i ./general_formula_recognition_001.png --device gpu\n</code></pre> The command line supports more parameter settings. Click to expand for detailed descriptions of the command line parameters. Parameter Description Type Default <code>input</code> Data to be predicted, required. Local path of image or PDF file, e.g., <code>/root/data/img.jpg</code>; URL link, e.g., network URL of image or PDF file: Example; Local directory, the directory should contain images to be predicted, e.g., local path: <code>/root/data/</code> (currently does not support prediction of PDF files in directories; PDF files must be specified with a specific file path).  <code>str</code> <code>save_path</code>  Specify the path to save the inference results file. If not set, the inference results will not be saved locally. <code>str</code> <code>doc_orientation_classify_model_name</code>   The name of the document orientation classification model. If not set, the default model in pipeline will be used. <code>str</code> <code>doc_orientation_classify_model_dir</code> The directory path of the document orientation classification model. If not set, the official model will be downloaded. <code>str</code> <code>doc_orientation_classify_batch_size</code> The batch size of the document orientation classification model. If not set, the default batch size will be set to <code>1</code>.  <code>int</code> <code>doc_unwarping_model_name</code>  The name of the text image unwarping model. If not set, the default model in pipeline will be used. <code>str</code> <code>doc_unwarping_model_dir</code>  The directory path of the  text image unwarping model. If not set, the official model will be downloaded.  <code>str</code> <code>doc_unwarping_batch_size</code> The batch size of the text image unwarping model. If not set, the default batch size will be set to <code>1</code>. <code>int</code> <code>use_doc_orientation_classify</code> Whether to load and use the document orientation classification module. If not set, the parameter will default to the value initialized in the pipeline, which is <code>True</code>. <code>bool</code> <code>use_doc_unwarping</code>  Whether to load and use the text image unwarping module. If not set, the parameter will default to the value initialized in the pipeline, which is <code>True</code>. <code>bool</code> <code>layout_detection_model_name</code>  The name of the layout detection model. If not set, the default model in pipeline will be used.  <code>str</code> <code>layout_detection_model_dir</code>  The directory path of the  layout detection model. If not set, the official model will be downloaded.  <code>str</code> <code>layout_threshold</code> Score threshold for the layout model. Any value between <code>0-1</code>. If not set, the default value is used, which is <code>0.5</code>.  <code>float</code> <code>layout_nms</code>  Whether to use Non-Maximum Suppression (NMS) as post-processing for layout detection. If not set, the parameter will default to the value initialized in the pipeline, which is set to <code>True</code> by default.  <code>bool</code> <code>layout_unclip_ratio</code> Unclip ratio for detected boxes in layout detection model. Any float &gt; <code>0</code>. If not set, the default is <code>1.0</code>.  <code>float</code> <code>layout_merge_bboxes_mode</code> The merging mode for the detection boxes output by the model in layout region detection. <ul> <li>large: When set to \"large\", only the largest outer bounding box will be retained for overlapping bounding boxes, and the inner overlapping boxes will be removed;</li> <li>small: When set to \"small\", only the smallest inner bounding boxes will be retained for overlapping bounding boxes, and the outer overlapping boxes will be removed;</li> <li>union: No filtering of bounding boxes will be performed, and both inner and outer boxes will be retained;</li> </ul>If not set, the default is <code>large</code>.  <code>str</code> <code>layout_detection_batch_size</code> The batch size for the layout region detection model. If not set, the default batch size will be set to <code>1</code>. <code>int</code> <code>use_layout_detection</code>  Whether to load and use the layout detection module. If not set, the parameter will default to the value initialized in the pipeline, which is <code>True</code>. <code>bool</code> <code>formula_recognition_model_name</code>  The name of the formula recognition model. If not set, the default model from the pipeline will be used.  <code>str</code> <code>formula_recognition_model_dir</code> The directory path of the formula recognition model. If not set, the official model will be downloaded.  <code>str</code> <code>formula_recognition_batch_size</code> The batch size for the formula recognition model. If not set, the batch size will default to <code>1</code>. <code>int</code> <code>device</code> The device used for inference. You can specify a particular card number: <ul> <li>CPU: e.g., <code>cpu</code> indicates using CPU for inference;</li> <li>GPU: e.g., <code>gpu:0</code> indicates using the 1st GPU for inference;</li> <li>NPU: e.g., <code>npu:0</code> indicates using the 1st NPU for inference;</li> <li>XPU: e.g., <code>xpu:0</code> indicates using the 1st XPU for inference;</li> <li>MLU: e.g., <code>mlu:0</code> indicates using the 1st MLU for inference;</li> <li>DCU: e.g., <code>dcu:0</code> indicates using the 1st DCU for inference;</li> </ul>If not set, the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.  <code>str</code> <code>enable_hpi</code> Whether to enable the high-performance inference plugin. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Compute precision, such as FP32 or FP16. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code>  The number of threads to use when performing inference on the CPU. <code>int</code> <code>8</code> <code>paddlex_config</code> Path to PaddleX pipeline configuration file. <code>str</code> <p></p> <p>The results of running the default configuration of the formula recognition pipeline will be printed to the terminal as follows:</p> <pre><code>{'res': {'input_path': './general_formula_recognition_001.png', 'page_index': None, 'model_settings': {'use_doc_preprocessor': True, 'use_layout_detection': True}, 'doc_preprocessor_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_orientation_classify': True, 'use_doc_unwarping': True}, 'angle': 0}, 'layout_det_res': {'input_path': None, 'page_index': None, 'boxes': [{'cls_id': 2, 'label': 'text', 'score': 0.9855189323425293, 'coordinate': [90.56131, 1086.7773, 658.8992, 1553.2681]}, {'cls_id': 2, 'label': 'text', 'score': 0.9814704060554504, 'coordinate': [93.04651, 127.988556, 664.8587, 396.60892]}, {'cls_id': 2, 'label': 'text', 'score': 0.9767388105392456, 'coordinate': [698.4391, 591.0454, 1293.3676, 748.28345]}, {'cls_id': 2, 'label': 'text', 'score': 0.9712911248207092, 'coordinate': [701.4946, 286.61566, 1299.0099, 391.87457]}, {'cls_id': 2, 'label': 'text', 'score': 0.9709068536758423, 'coordinate': [697.0126, 751.93604, 1290.2236, 883.64453]}, {'cls_id': 2, 'label': 'text', 'score': 0.9689271450042725, 'coordinate': [704.01196, 79.645935, 1304.7493, 187.96674]}, {'cls_id': 2, 'label': 'text', 'score': 0.9683637619018555, 'coordinate': [93.063385, 799.3567, 660.6935, 902.0344]}, {'cls_id': 7, 'label': 'formula', 'score': 0.9660536646842957, 'coordinate': [728.5045, 440.9215, 1224.0634, 570.8518]}, {'cls_id': 7, 'label': 'formula', 'score': 0.9616329669952393, 'coordinate': [722.9789, 1333.5085, 1257.1136, 1468.0432]}, {'cls_id': 7, 'label': 'formula', 'score': 0.9610316753387451, 'coordinate': [756.4525, 1211.323, 1188.0428, 1268.2336]}, {'cls_id': 7, 'label': 'formula', 'score': 0.960993230342865, 'coordinate': [777.51355, 207.87927, 1222.8966, 267.33014]}, {'cls_id': 2, 'label': 'text', 'score': 0.9594196677207947, 'coordinate': [697.5154, 957.6764, 1288.6238, 1033.5211]}, {'cls_id': 2, 'label': 'text', 'score': 0.9593432545661926, 'coordinate': [691.333, 1511.8015, 1282.0968, 1642.5906]}, {'cls_id': 7, 'label': 'formula', 'score': 0.9589930176734924, 'coordinate': [153.89856, 924.2046, 601.0946, 1036.9038]}, {'cls_id': 2, 'label': 'text', 'score': 0.9582098722457886, 'coordinate': [87.02347, 1557.2971, 655.9584, 1632.6912]}, {'cls_id': 7, 'label': 'formula', 'score': 0.9579620957374573, 'coordinate': [810.86975, 1057.0771, 1175.101, 1117.6631]}, {'cls_id': 7, 'label': 'formula', 'score': 0.9557801485061646, 'coordinate': [165.26271, 557.8495, 598.1803, 614.35]}, {'cls_id': 7, 'label': 'formula', 'score': 0.953873872756958, 'coordinate': [116.48187, 713.88416, 614.2181, 774.02576]}, {'cls_id': 2, 'label': 'text', 'score': 0.9521227478981018, 'coordinate': [96.6882, 478.32745, 662.573, 536.5877]}, {'cls_id': 2, 'label': 'text', 'score': 0.944242000579834, 'coordinate': [96.12866, 639.1591, 661.7959, 692.4849]}, {'cls_id': 2, 'label': 'text', 'score': 0.9403323531150818, 'coordinate': [695.9436, 1138.6748, 1286.7242, 1188.0049]}, {'cls_id': 7, 'label': 'formula', 'score': 0.9249663949012756, 'coordinate': [852.90137, 908.64386, 1131.1882, 933.81793]}, {'cls_id': 7, 'label': 'formula', 'score': 0.9249223470687866, 'coordinate': [195.28397, 424.81024, 567.697, 451.1291]}, {'cls_id': 17, 'label': 'formula_number', 'score': 0.9173304438591003, 'coordinate': [1246.2393, 1079.0535, 1286.3281, 1104.3323]}, {'cls_id': 17, 'label': 'formula_number', 'score': 0.9169507026672363, 'coordinate': [1246.9003, 908.6482, 1288.2013, 934.61426]}, {'cls_id': 17, 'label': 'formula_number', 'score': 0.915979266166687, 'coordinate': [1247.0374, 1229.1572, 1287.094, 1254.9805]}, {'cls_id': 17, 'label': 'formula_number', 'score': 0.9085646867752075, 'coordinate': [1252.864, 492.1079, 1294.6238, 518.47095]}, {'cls_id': 17, 'label': 'formula_number', 'score': 0.9017605781555176, 'coordinate': [1242.1719, 1473.6951, 1283.02, 1498.6316]}, {'cls_id': 17, 'label': 'formula_number', 'score': 0.8999755382537842, 'coordinate': [1269.8164, 220.34933, 1299.8589, 247.01102]}, {'cls_id': 7, 'label': 'formula', 'score': 0.8965252041816711, 'coordinate': [96.00711, 235.49493, 295.43823, 265.60016]}, {'cls_id': 2, 'label': 'text', 'score': 0.8954343199729919, 'coordinate': [696.85693, 1286.2236, 1083.3921, 1310.8643]}, {'cls_id': 7, 'label': 'formula', 'score': 0.8952110409736633, 'coordinate': [166.60979, 129.20242, 511.65692, 156.29672]}, {'cls_id': 2, 'label': 'text', 'score': 0.893648624420166, 'coordinate': [725.64575, 396.18964, 1263.0391, 422.76813]}, {'cls_id': 17, 'label': 'formula_number', 'score': 0.8922948837280273, 'coordinate': [634.14124, 427.77087, 661.1686, 454.10022]}, {'cls_id': 2, 'label': 'text', 'score': 0.8892256617546082, 'coordinate': [94.483246, 1058.7595, 441.92313, 1082.4875]}, {'cls_id': 17, 'label': 'formula_number', 'score': 0.8878197073936462, 'coordinate': [630.4175, 939.3015, 657.7135, 965.36426]}, {'cls_id': 17, 'label': 'formula_number', 'score': 0.8831961154937744, 'coordinate': [630.5835, 1000.95715, 657.4309, 1026.2128]}, {'cls_id': 17, 'label': 'formula_number', 'score': 0.8767948150634766, 'coordinate': [634.1024, 575.3833, 660.59094, 601.1677]}, {'cls_id': 7, 'label': 'formula', 'score': 0.873543918132782, 'coordinate': [95.29655, 1320.3627, 264.93008, 1345.8473]}, {'cls_id': 17, 'label': 'formula_number', 'score': 0.8702306151390076, 'coordinate': [633.82825, 730.31525, 659.83215, 755.5485]}, {'cls_id': 7, 'label': 'formula', 'score': 0.8387619853019714, 'coordinate': [365.19897, 268.29675, 515.7938, 296.07013]}, {'cls_id': 7, 'label': 'formula', 'score': 0.8314349055290222, 'coordinate': [1090.509, 1599.1382, 1276.6736, 1622.156]}, {'cls_id': 7, 'label': 'formula', 'score': 0.817135751247406, 'coordinate': [246.175, 161.22958, 314.3764, 186.40591]}, {'cls_id': 3, 'label': 'number', 'score': 0.8042846322059631, 'coordinate': [1297.4036, 7.1497707, 1310.5969, 27.737753]}, {'cls_id': 7, 'label': 'formula', 'score': 0.7970448136329651, 'coordinate': [538.45593, 478.09354, 661.8812, 508.50778]}, {'cls_id': 7, 'label': 'formula', 'score': 0.7644855976104736, 'coordinate': [916.51746, 1618.5188, 1009.62537, 1640.8206]}, {'cls_id': 7, 'label': 'formula', 'score': 0.7423419952392578, 'coordinate': [694.8439, 1612.2507, 861.05334, 1635.9768]}, {'cls_id': 7, 'label': 'formula', 'score': 0.7072376608848572, 'coordinate': [99.72007, 508.21167, 254.91953, 535.74744]}, {'cls_id': 7, 'label': 'formula', 'score': 0.6976271867752075, 'coordinate': [696.8011, 1561.4375, 899.79584, 1586.7349]}, {'cls_id': 7, 'label': 'formula', 'score': 0.6707713007926941, 'coordinate': [1117.0862, 1571.9763, 1191.502, 1594.742]}, {'cls_id': 7, 'label': 'formula', 'score': 0.6338322162628174, 'coordinate': [577.33484, 1274.4131, 602.5636, 1296.7021]}, {'cls_id': 7, 'label': 'formula', 'score': 0.6199935674667358, 'coordinate': [175.28284, 349.82376, 241.24683, 376.6708]}, {'cls_id': 7, 'label': 'formula', 'score': 0.612853467464447, 'coordinate': [773.06287, 595.202, 800.43884, 617.3812]}, {'cls_id': 7, 'label': 'formula', 'score': 0.6107096672058105, 'coordinate': [706.6776, 316.87082, 736.69714, 339.9352]}, {'cls_id': 7, 'label': 'formula', 'score': 0.5520269870758057, 'coordinate': [1263.9711, 314.65167, 1292.7728, 337.3896]}, {'cls_id': 7, 'label': 'formula', 'score': 0.5346108675003052, 'coordinate': [1219.2955, 316.599, 1243.9181, 339.71802]}, {'cls_id': 7, 'label': 'formula', 'score': 0.5195119380950928, 'coordinate': [254.65729, 323.6553, 326.57758, 349.53494]}, {'cls_id': 7, 'label': 'formula', 'score': 0.501812219619751, 'coordinate': [255.8518, 1350.6472, 301.74304, 1375.5286]}]}, 'formula_res_list': [{'rec_formula': '\\\\begin{aligned}{\\\\psi_{0}(M)-\\\\psi_{}(M,z)=}&amp;{{}\\\\frac{(1-\\\\epsilon_{r})}{\\\\epsilon_{r}}\\\\frac{\\\\lambda^{2}c^{2}}{t_{\\\\operatorname{E}}^{2}\\\\operatorname{l n}(10)}\\\\times}\\\\\\\\ {}&amp;{{}\\\\int_{0}^{z}d z^{\\\\prime}\\\\frac{d t}{d z^{\\\\prime}}\\\\left.\\\\frac{\\\\partial\\\\phi}{\\\\partial L}\\\\right|_{L=\\\\lambda M c^{2}/t_{\\\\operatorname{E}}},}\\\\\\\\ \\\\end{aligned}', 'formula_region_id': 1, 'dt_polys': ([728.5045, 440.9215, 1224.0634, 570.8518],)}, {'rec_formula': '\\\\begin{aligned}{p(\\\\operatorname{l o g}_{10}}&amp;{{}M|\\\\operatorname{l o g}_{10}\\\\sigma)=\\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\epsilon_{0}}}\\\\\\\\ {}&amp;{{}\\\\times\\\\operatorname{e x p}\\\\left[-\\\\frac{1}{2}\\\\left(\\\\frac{\\\\operatorname{l o g}_{10}M-a_{\\\\bullet}-b_{\\\\bullet}\\\\operatorname{l o g}_{10}\\\\sigma}{\\\\epsilon_{0}}\\\\right)^{2}\\\\right].}\\\\\\\\ \\\\end{aligned}', 'formula_region_id': 2, 'dt_polys': ([722.9789, 1333.5085, 1257.1136, 1468.0432],)}, {'rec_formula': '\\\\psi_{0}(M)=\\\\int d\\\\sigma\\\\frac{p(\\\\operatorname{l o g}_{10}M|\\\\operatorname{l o g}_{10}\\\\sigma)}{M\\\\operatorname{l o g}(10)}\\\\frac{d n}{d\\\\sigma}(\\\\sigma),', 'formula_region_id': 3, 'dt_polys': ([756.4525, 1211.323, 1188.0428, 1268.2336],)}, {'rec_formula': '\\\\phi(L)\\\\equiv\\\\frac{d n}{d\\\\operatorname{l o g}_{10}L}=\\\\frac{\\\\phi_{*}}{(L/L_{*})^{\\\\gamma_{1}}+(L/L_{*})^{\\\\gamma_{2}}}.', 'formula_region_id': 4, 'dt_polys': ([777.51355, 207.87927, 1222.8966, 267.33014],)}, {'rec_formula': '\\\\begin{aligned}{\\\\rho_{\\\\operatorname{B H}}}&amp;{{}=\\\\int d M\\\\psi(M)M}\\\\\\\\ {}&amp;{{}=\\\\frac{1-\\\\epsilon_{r}}{\\\\epsilon_{r}c^{2}}\\\\int_{0}^{\\\\infty}d z\\\\frac{d t}{d z}\\\\int d\\\\operatorname{l o g}_{10}L\\\\phi(L,z)L,}\\\\\\\\ \\\\end{aligned}', 'formula_region_id': 5, 'dt_polys': ([153.89856, 924.2046, 601.0946, 1036.9038],)}, {'rec_formula': '\\\\frac{d n}{d\\\\sigma}d\\\\sigma=\\\\psi_{*}\\\\left(\\\\frac{\\\\sigma}{\\\\sigma_{*}}\\\\right)^{\\\\alpha}\\\\frac{e^{-(\\\\sigma/\\\\sigma_{*})^{\\\\beta}}}{\\\\Gamma(\\\\alpha/\\\\beta)}\\\\beta\\\\frac{d\\\\sigma}{\\\\sigma}.', 'formula_region_id': 6, 'dt_polys': ([810.86975, 1057.0771, 1175.101, 1117.6631],)}, {'rec_formula': '\\\\langle\\\\dot{M}(M,t)\\\\rangle\\\\psi(M,t)=\\\\frac{(1-\\\\epsilon_{r})}{\\\\epsilon_{r}c^{2}\\\\operatorname{l n}(10)}\\\\phi(L,t)\\\\frac{d L}{d M}.', 'formula_region_id': 7, 'dt_polys': ([165.26271, 557.8495, 598.1803, 614.35],)}, {'rec_formula': '\\\\frac{\\\\partial\\\\psi}{\\\\partial t}(M,t)+\\\\frac{(1-\\\\epsilon_{r})}{\\\\epsilon_{r}}\\\\frac{\\\\lambda^{2}c^{2}}{t_{\\\\operatorname{E}}^{2}\\\\operatorname{l n}(10)}\\\\left.\\\\frac{\\\\partial\\\\phi}{\\\\partial L}\\\\right|_{L=\\\\lambda M c^{2}/t_{\\\\operatorname{E}}}=0,', 'formula_region_id': 8, 'dt_polys': ([116.48187, 713.88416, 614.2181, 774.02576],)}, {'rec_formula': '\\\\operatorname{l o g}_{10}M=a_{\\\\bullet}+b_{\\\\bullet}\\\\operatorname{l o g}_{10}X.', 'formula_region_id': 9, 'dt_polys': ([852.90137, 908.64386, 1131.1882, 933.81793],)}, {'rec_formula': '\\\\phi(L,t)d\\\\operatorname{l o g}_{10}L=\\\\delta(M,t)\\\\psi(M,t)d M.', 'formula_region_id': 10, 'dt_polys': ([195.28397, 424.81024, 567.697, 451.1291],)}, {'rec_formula': '\\\\dot{M}\\\\:=\\\\:(1\\\\:-\\\\:\\\\epsilon_{r})\\\\dot{M}_{\\\\mathrm{a c c}}^{\\\\mathrm{~\\\\tiny~\\\\cdot~}}', 'formula_region_id': 11, 'dt_polys': ([96.00711, 235.49493, 295.43823, 265.60016],)}, {'rec_formula': 't_{E}=\\\\sigma_{T}c/4\\\\pi G m_{p}=4.5\\\\times10^{8}\\\\mathrm{y r}', 'formula_region_id': 12, 'dt_polys': ([166.60979, 129.20242, 511.65692, 156.29672],)}, {'rec_formula': 'M_{*}=L_{*}t_{E}/\\\\tilde{\\\\lambda}c^{2}', 'formula_region_id': 13, 'dt_polys': ([95.29655, 1320.3627, 264.93008, 1345.8473],)}, {'rec_formula': '\\\\phi(L,t)d\\\\operatorname{l o g}_{10}L', 'formula_region_id': 14, 'dt_polys': ([365.19897, 268.29675, 515.7938, 296.07013],)}, {'rec_formula': 'a_{\\\\bullet}=8.32\\\\pm0.05', 'formula_region_id': 15, 'dt_polys': ([1090.509, 1599.1382, 1276.6736, 1622.156],)}, {'rec_formula': '\\\\epsilon_{r}\\\\dot{M}_{\\\\mathrm{a c c}}', 'formula_region_id': 16, 'dt_polys': ([246.175, 161.22958, 314.3764, 186.40591],)}, {'rec_formula': '\\\\langle\\\\dot{M}(M,t)\\\\rangle=', 'formula_region_id': 17, 'dt_polys': ([538.45593, 478.09354, 661.8812, 508.50778],)}, {'rec_formula': '\\\\epsilon_{0}=0.38', 'formula_region_id': 18, 'dt_polys': ([916.51746, 1618.5188, 1009.62537, 1640.8206],)}, {'rec_formula': 'b_{\\\\bullet}=5.64\\\\dot{\\\\pm}\\\\dot{0.32}', 'formula_region_id': 19, 'dt_polys': ([694.8439, 1612.2507, 861.05334, 1635.9768],)}, {'rec_formula': '\\\\delta(M,t)\\\\dot{M}(M,t)', 'formula_region_id': 20, 'dt_polys': ([99.72007, 508.21167, 254.91953, 535.74744],)}, {'rec_formula': 'X=\\\\sigma/200\\\\mathrm{k m}\\\\mathrm{~s^{-1}~}', 'formula_region_id': 21, 'dt_polys': ([696.8011, 1561.4375, 899.79584, 1586.7349],)}, {'rec_formula': 'M-\\\\sigma', 'formula_region_id': 22, 'dt_polys': ([1117.0862, 1571.9763, 1191.502, 1594.742],)}, {'rec_formula': 'L_{*}', 'formula_region_id': 23, 'dt_polys': ([577.33484, 1274.4131, 602.5636, 1296.7021],)}, {'rec_formula': '\\\\phi(L,t)', 'formula_region_id': 24, 'dt_polys': ([175.28284, 349.82376, 241.24683, 376.6708],)}, {'rec_formula': '\\\\psi_{0}', 'formula_region_id': 25, 'dt_polys': ([773.06287, 595.202, 800.43884, 617.3812],)}, {'rec_formula': '\\\\mathrm{A^{\\\\prime\\\\prime}}', 'formula_region_id': 26, 'dt_polys': ([706.6776, 316.87082, 736.69714, 339.9352],)}, {'rec_formula': 'L_{*}', 'formula_region_id': 27, 'dt_polys': ([1263.9711, 314.65167, 1292.7728, 337.3896],)}, {'rec_formula': '\\\\phi_{*}', 'formula_region_id': 28, 'dt_polys': ([1219.2955, 316.599, 1243.9181, 339.71802],)}, {'rec_formula': '\\\\delta(M,t)', 'formula_region_id': 29, 'dt_polys': ([254.65729, 323.6553, 326.57758, 349.53494],)}, {'rec_formula': '\\\\phi(L)', 'formula_region_id': 30, 'dt_polys': ([255.8518, 1350.6472, 301.74304, 1375.5286],)}]}}\n</code></pre> <p>The explanation of the running result parameters can refer to the result interpretation in  2.2 Python Script Integration.</p> <p>The visualization results are saved under <code>save_path</code>, where the visualization result of formula recognition is as follows:</p> <p></p> <p> If you need to visualize the formula recognition pipeline, you need to run the following command to install the LaTeX rendering environment. Currently, visualization of the formula recognition pipeline only supports the Ubuntu environment, and other environments are not supported. For complex formulas, the LaTeX result may contain some advanced representations that may not be successfully displayed in environments such as Markdown:</p> <pre><code>sudo apt-get update\nsudo apt-get install texlive texlive-latex-base texlive-xetex latex-cjk-all texlive-latex-extra -y\n</code></pre> <p>Note: Due to the need to render each formula image during the formula recognition visualization process, the process takes a long time. Please be patient.</p>"},{"location":"en/version3.x/pipeline_usage/formula_recognition.html#22-python-script-integration","title":"2.2 Python Script Integration","text":"<p>Using the command line is a quick way to experience and check the results. Generally, in a project, you often need to integrate it through code. You can perform quick inference with just a few lines of code. The inference code is as follows:</p> <pre><code>from paddleocr import FormulaRecognitionPipeline\n\npipeline = FormulaRecognitionPipeline()\n# ocr = FormulaRecognitionPipeline(use_doc_orientation_classify=True) # Specify whether to use the document orientation classification model with use_doc_orientation_classify.\n# ocr = FormulaRecognitionPipeline(use_doc_unwarping=True) # Specify whether to use the text image unwarping module with use_doc_unwarping.\n# ocr = FormulaRecognitionPipeline(device=\"gpu\") # Specify the use of GPU for model inference with device.\noutput = pipeline.predict(\"./general_formula_recognition_001.png\")\nfor res in output:\n    res.print() ##  Print the structured output of the prediction\n    res.save_to_img(save_path=\"output\") ## Save the formula visualization result of the current image.\n    res.save_to_json(save_path=\"output\") ## Save the structured JSON result of the current image\n</code></pre> <p>In the above Python script, the following steps are executed:</p> <p>\uff081\uff09Instantiate the formula recognition pipeline object through <code>create_pipeline()</code>, with specific parameters as follows:</p> Parameter Description Type Default <code>doc_orientation_classify_model_name</code> The name of the document orientation classification model. If set to <code>None</code>, the default model in pipeline will be used. <code>str</code> <code>None</code> <code>doc_orientation_classify_model_dir</code> The directory path of the document orientation classification model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>doc_orientation_classify_batch_size</code> The batch size of the document orientation classification model. If set to <code>None</code>, the default batch size will be set to <code>1</code>. <code>int</code> <code>None</code> <code>doc_unwarping_model_name</code> The name of the text image unwarping model. If set to <code>None</code>, the default model in pipeline will be used. <code>str</code> <code>None</code> <code>doc_unwarping_model_dir</code> The directory path of the  text image unwarping model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>doc_unwarping_batch_size</code> The batch size of the text image unwarping model. If set to <code>None</code>, the default batch size will be set to <code>1</code>. <code>int</code> <code>None</code> <code>use_doc_orientation_classify</code> Whether to load and use the document orientation classification module. If set to <code>None</code>, the parameter will default to the value initialized in the pipeline, which is <code>True</code>. <code>bool</code> <code>None</code> <code>use_doc_unwarping</code> Whether to load and use the text image unwarping module. If set to <code>None</code>, the parameter will default to the value initialized in the pipeline, which is <code>True</code>. <code>bool</code> <code>None</code> <code>layout_detection_model_name</code> The name of the layout detection model. If set to <code>None</code>, the default model in pipeline will be used.  <code>str</code> <code>None</code> <code>layout_detection_model_dir</code> The directory path of the  layout detection model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>layout_threshold</code> Threshold for layout detection, used to filter out predictions with low confidence. <ul> <li>float: Such as 0.2, indicates filtering out all bounding boxes with a confidence score less than 0.2;</li> <li>Dictionary: With int keys representing <code>cls_id</code> and float values as thresholds. For example, <code>{0: 0.45, 2: 0.48, 7: 0.4}</code> indicates applying a threshold of 0.45 for class ID 0, 0.48 for class ID 2, and 0.4 for class ID 7;</li> <li>None: If set to <code>None</code>, the default is <code>0.5</code>.</li> </ul> <code>float|dict</code> <code>None</code> <code>layout_nms</code> Whether to use Non-Maximum Suppression (NMS) as post-processing for layout detection. If set to <code>None</code>, the parameter will default to the value initialized in the pipeline, which is set to <code>True</code> by default. <code>bool</code> <code>None</code> <code>layout_unclip_ratio</code> Expansion factor for the detection boxes of the layout region detection model. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>Tuple[float,float]: Expansion ratios in horizontal and vertical directions;</li> <li>dict: A dictionary with int keys representing <code>cls_id</code>, and tuple values, e.g., <code>{0: (1.1, 2.0)}</code> means width is expanded 1.1\u00d7 and height 2.0\u00d7 for class 0 boxes;</li> <li>None: If set to <code>None</code>, uses the pipeline default of <code>1.0</code>.</li> </ul> <code>float|Tuple[float,float]|dict</code> <code>None</code> <code>layout_merge_bboxes_mode</code> Filtering method for overlapping boxes in layout detection. <ul> <li>str: Options include <code>large</code>, <code>small</code>, and <code>union</code> to retain the larger box, smaller box, or both;</li> <li>dict: A dictionary with int keys representing <code>cls_id</code>, and str values, e.g., <code>{0: \"large\", 2: \"small\"}</code> means using different modes for different classes;</li> <li>None: If set to <code>None</code>, uses the pipeline default value <code>large</code>.</li> </ul> <code>str|dict</code> <code>None</code> <code>layout_detection_batch_size</code> The batch size for the layout region detection model. If set to <code>None</code>, the default batch size will be set to <code>1</code>. <code>int</code> <code>None</code> <code>use_layout_detection</code> Whether to load and use the layout detection module. If set to <code>None</code>, the parameter will default to the value initialized in the pipeline, which is <code>True</code>. <code>bool</code> <code>None</code> <code>formula_recognition_model_name</code> The name of the formula recognition model. If set to <code>None</code>, the default model from the pipeline will be used. <code>str</code> <code>None</code> <code>formula_recognition_model_dir</code> The directory path of the formula recognition model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>formula_recognition_batch_size</code> The batch size for the formula recognition model. If set to  <code>None</code>, the batch size will default to <code>1</code>. <code>int</code> <code>None</code> <code>device</code> The device used for inference. You can specify a particular card number: <ul> <li>CPU: e.g., <code>cpu</code> indicates using CPU for inference;</li> <li>GPU: e.g., <code>gpu:0</code> indicates using the 1st GPU for inference;</li> <li>NPU: e.g., <code>npu:0</code> indicates using the 1st NPU for inference;</li> <li>XPU: e.g., <code>xpu:0</code> indicates using the 1st XPU for inference;</li> <li>MLU: e.g., <code>mlu:0</code> indicates using the 1st MLU for inference;</li> <li>DCU: e.g., <code>dcu:0</code> indicates using the 1st DCU for inference;</li> <li>None: If set to <code>None</code>, the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.  <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to enable the high-performance inference plugin. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Compute precision, such as FP32 or FP16. <code>str</code> <code>\"fp32\"</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set.  <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> The number of threads to use when performing inference on the CPU. <code>int</code> <code>8</code> <code>paddlex_config</code> Path to PaddleX pipeline configuration file. <code>str</code> <code>None</code> <p>\uff082\uff09Call the <code>predict()</code> method of the formula recognition pipeline object to perform inference prediction. This method will return a list of results.</p> <p>Additionally, the pipeline also provides the <code>predict_iter()</code> method. Both methods are completely consistent in terms of parameter acceptance and result return. The difference is that <code>predict_iter()</code> returns a <code>generator</code>, which allows for step-by-step processing and retrieval of prediction results. This is suitable for handling large datasets or scenarios where memory saving is desired. You can choose to use either of these methods based on your actual needs.</p> <p>Here are the parameters of the <code>predict()</code> method and their descriptions:</p> Parameter Description Type Default Value <code>input</code> Data to be predicted, supporting multiple input types, required. <ul> <li>Python Var: Image data represented by <code>numpy.ndarray;</code></li> <li>str: Local path of image or PDF file, e.g., <code>/root/data/img.jpg</code>; URL link, e.g., network URL of image or PDF file: Example; Local directory, the directory should contain images to be predicted, e.g., local path: <code>/root/data/</code> (currently does not support prediction of PDF files in directories; PDF files must be specified with a specific file path);</li> <li>List: Elements of the list must be of the above types, e.g., <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\\\"/root/data/img1.jpg\\\", \\\"/root/data/img2.jpg\\\"]</code>, <code>[\\\"/root/data1\\\", \\\"/root/data2\\\"].</code></li> </ul> <code>Python Var|str|list</code> <code>use_layout_detection</code>  Whether to use the layout detection module during inference.  <code>bool</code> <code>None</code> <code>use_doc_orientation_classify</code>  Whether to use the document orientation classification module during inference. <code>bool</code> <code>None</code> <code>use_doc_unwarping</code> Whether to use the text image unwarping module during inference. <code>bool</code> <code>None</code> <code>layout_threshold</code> The parameters are the same as those used during instantiation. <code>float|dict</code> <code>None</code> <code>layout_nms</code> The parameters are the same as those used during instantiation. <code>bool</code> <code>None</code> <code>layout_unclip_ratio</code> The parameters are the same as those used during instantiation. <code>float|Tuple[float,float]|dict</code> <code>None</code> <code>layout_merge_bboxes_mode</code> The parameters are the same as those used during instantiation. <code>string</code> <code>None</code> <p>\uff083\uff09Process the prediction results, where the prediction result for each sample corresponds to a Result object, and supports operations such as printing, saving as an image, and saving as a <code>json</code> file:</p> Method Description Parameter Parameter Type Parameter Description Default Value <code>print()</code> Print results to terminal <code>format_json</code> <code>bool</code> Whether to format the output content using <code>JSON</code> indentation. <code>True</code> <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data, making it more readable. Effective only when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> retains the original characters. Effective only when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_json()</code> Save results as a JSON file <code>save_path</code> <code>str</code> Path to save the file. If it is a directory, the saved file will be named the same as the input file type. \u200b\u65e0\u200b <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data, making it more readable. Effective only when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> retains the original characters. Effective only when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_img()</code> Save results as an image file <code>save_path</code> <code>str</code> Path to save the file, supports directory or file path. \u200b\u65e0\u200b <ul> <li> <p>Calling the <code>print()</code> method will print the results to the terminal. The content printed to the terminal is explained as follows:</p> <ul> <li> <p><code>input_path</code>: <code>(str)</code> The input path of the image to be predicted.</p> </li> <li> <p><code>page_index</code>: <code>(Union[int, None])</code> If the input is a PDF file, this indicates the current page number of the PDF. Otherwise, it is <code>None</code></p> </li> <li> <p><code>model_settings</code>: <code>(Dict[str, bool])</code> The model parameters required for the pipeline configuration.</p> <ul> <li><code>use_doc_preprocessor</code>: <code>(bool)</code> Controls whether to enable the document preprocessing sub-pipeline.</li> <li><code>use_layout_detection</code>: <code>(bool)</code> Controls whether to enable the layout area detection module.</li> </ul> </li> <li> <p><code>doc_preprocessor_res</code>: <code>(Dict[str, Union[str, Dict[str, bool], int]])</code> The output result of the document preprocessing sub-pipeline. It exists only when <code>use_doc_preprocessor=True</code>.</p> <ul> <li><code>input_path</code>: <code>(Union[str, None])</code> The image path accepted by the image preprocessing sub-pipeline. When the input is a <code>numpy.ndarray</code>, it is saved as <code>None</code>.</li> <li><code>model_settings</code>: <code>(Dict)</code> The model configuration parameters of the preprocessing sub-pipeline.<ul> <li><code>use_doc_orientation_classify</code>: <code>(bool)</code> Controls whether to enable document orientation classification.</li> <li><code>use_doc_unwarping</code>: <code>(bool)</code> Controls whether to enable document distortion correction.</li> </ul> </li> <li><code>angle</code>: <code>(int)</code> The prediction result of document orientation classification. When enabled, it takes values from [0,1,2,3], corresponding to [0\u00b0,90\u00b0,180\u00b0,270\u00b0]; when disabled, it is -1.</li> </ul> </li> <li><code>layout_det_res</code>: <code>(Dict[str, List[Dict]])</code> The output result of the layout area detection module. It exists only when <code>use_layout_detection=True</code>.<ul> <li><code>input_path</code>: <code>(Union[str, None])</code> The image path accepted by the layout area detection module. When the input is a <code>numpy.ndarray</code>, it is saved as <code>None</code>.</li> <li><code>boxes</code>: <code>(List[Dict[int, str, float, List[float]]])</code> A list of layout area detection prediction results.<ul> <li><code>cls_id</code>: <code>(int)</code> The class ID predicted by layout area detection.</li> <li><code>label</code>: <code>(str)</code> The class label predicted by layout area detection.</li> <li><code>score</code>: <code>(float)</code> The confidence score of the predicted class.</li> <li><code>coordinate</code>: <code>(List[float])</code> The bounding box coordinates predicted by layout area detection, in the format [x_min, y_min, x_max, y_max], where (x_min, y_min) is the top-left corner and (x_max, y_max) is the bottom-right corner.</li> </ul> </li> </ul> </li> <li><code>formula_res_list</code>: <code>(List[Dict[str, int, List[float]]])</code> A list of formula recognition prediction results.<ul> <li><code>rec_formula</code>: <code>(str)</code> The LaTeX source code predicted by formula recognition.</li> <li><code>formula_region_id</code>: <code>(int)</code> The ID number predicted by formula recognition.</li> <li><code>dt_polys</code>: <code>(List[float])</code> The bounding box coordinates predicted by formula recognition, in the format [x_min, y_min, x_max, y_max], where (x_min, y_min) is the top-left corner and (x_max, y_max) is the bottom-right corner.</li> </ul> </li> </ul> </li> <li> <p>Calling the <code>save_to_json()</code> method will save the above content to the specified <code>save_path</code>. If a directory is specified, the saved path will be <code>save_path/{your_img_basename}_res.json</code>. If a file is specified, it will be saved directly to that file. Since JSON files do not support saving numpy arrays, <code>numpy.array</code> types will be converted to list format.</p> </li> <li> <p>Calling the <code>save_to_img()</code> method will save the visualization results to the specified <code>save_path</code>. If a directory is specified, the saved path will be <code>save_path/{your_img_basename}_formula_res_img.{your_img_extension}</code>. If a file is specified, it will be saved directly to that file. (The pipeline usually contains many result images, so it is not recommended to specify a specific file path directly, otherwise multiple images will be overwritten and only the last one will be retained.)</p> </li> <li> <p>In addition, you can also obtain the visualization image with results and the prediction results through attributes, as follows:</p> </li> </ul> Attribute Attribute Description <code>json</code> Get the prediction results in <code>json</code> format <code>img</code> Get the visualization image in <code>dict</code> format <ul> <li>The prediction result obtained from the <code>json</code> attribute is of the dict type, and its content is consistent with what is saved by calling the <code>save_to_json()</code> method.</li> <li>The prediction result returned by the  <code>img</code> attribute is a dictionary-type data. The keys are  <code>preprocessed_img</code>\u3001 <code>layout_det_res</code> and <code>formula_res_img</code>, and the corresponding values are three <code>Image.Image</code> objects: the first one is used to display the visualization of image preprocessing, the second one is for displaying the visualization of layout region detection, and the third one is for displaying the visualization of formula recognition. If the image preprocessing submodule is not used, the dictionary will not contain the <code>preprocessed_img</code> key. Similarly, if the layout region detection submodule is not used, the dictionary will not contain the <code>layout_det_res</code> key.</li> </ul>"},{"location":"en/version3.x/pipeline_usage/formula_recognition.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the formula recognition pipeline meets your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <p>If you need to integrate the formula recognition pipeline into your Python project, you can refer to the example code in  2.2 Python Script Integration.</p> <p>In addition, PaddleOCR also provides two other deployment methods, which are detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In real-world production environments, many applications have stringent standards for performance metrics of deployment strategies, particularly regarding response speed, to ensure efficient system operation and a smooth user experience. To address this, PaddleOCR offers high-performance inference capabilities designed to deeply optimize the performance of model inference and pre/post-processing, significantly accelerating the end-to-end process. For detailed information on the high-performance inference process, please refer to the High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Based Deployment:  Service-Based Deployment is a common deployment form in real-world production environments. By encapsulating inference capabilities as a service, clients can access these services via network requests to obtain inference results. For detailed instructions on Service-Based Deployment in pipelines, please refer to the Service-Based Deployment Guide.</p> <p>Below are the API references for basic service-based deployment and multi-language service invocation examples:</p> API Reference <p>For the main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>Both the request body and response body are JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the attributes of the response body are as follows:</li> </ul> Name Type Meaning <code>logId</code> <code>string</code> The UUID of the request. <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error message. Fixed as <code>\"Success\"</code>. <code>result</code> <code>object</code> The result of the operation. <ul> <li>When the request is not processed successfully, the attributes of the response body are as follows:</li> </ul> Name Type Meaning <code>logId</code> <code>string</code> The UUID of the request. <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error message. <p>The main operations provided by the service are as follows:</p> <ul> <li><code>infer</code></li> </ul> <p>Obtain the formula recognition results from images.</p> <p><code>POST /formula-recognition</code></p> <ul> <li>The attributes of the request body are as follows:</li> </ul> Name Type Meaning Required <code>file</code> <code>string</code> The URL of an image or PDF file accessible by the server, or the Base64-encoded content of the file. By default, for PDF files exceeding 10 pages, only the first 10 pages will be processed. To remove the page limit, please add the following configuration to the pipeline configuration file: <pre><code>Serving:\n  extra:\n    max_num_input_imgs: null\n</code></pre> Yes <code>fileType</code> <code>integer</code> | <code>null</code> The type of the file. <code>0</code> for PDF files, <code>1</code> for image files. If this attribute is missing, the file type will be inferred from the URL. No <code>useDocOrientationClassify</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>use_doc_orientation_classify</code> parameter of the pipeline object's <code>predict</code> method. No <code>useDocUnwarping</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>use_doc_unwarping</code> parameter of the pipeline object's <code>predict</code> method. No <code>useLayoutDetection</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>use_layout_detection</code> parameter of the pipeline object's <code>predict</code> method. No <code>layoutThreshold</code> <code>number</code> | <code>null</code> Please refer to the description of the <code>layout_threshold</code> parameter of the pipeline object's <code>predict</code> method. No <code>layoutNms</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>layout_nms</code> parameter of the pipeline object's <code>predict</code> method. No <code>layoutUnclipRatio</code> <code>number</code> | <code>array</code> | <code>null</code> Please refer to the description of the <code>layout_unclip_ratio</code> parameter of the pipeline object's <code>predict</code> method. No <code>layoutMergeBboxesMode</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>layout_merge_bboxes_mode</code> parameter of the pipeline object's <code>predict</code> method. No <ul> <li>When the request is processed successfully, the <code>result</code> in the response body has the following attributes:</li> </ul> \u200b\u540d\u79f0\u200b \u200b\u7c7b\u578b\u200b \u200b\u542b\u4e49\u200b <code>formulaRecResults</code> <code>object</code> The formula recognition results. The array length is 1 (for image input) or the actual number of document pages processed (for PDF input). For PDF input, each element in the array represents the result of each page actually processed in the PDF file. <code>dataInfo</code> <code>object</code> Information about the input data. <p>Each element in <code>formulaRecResults</code> is an <code>object</code> with the following attributes:</p> Name Type Meaning <code>prunedResult</code> <code>object</code> A simplified version of the <code>res</code> field in the JSON representation of the result generated by the pipeline object's <code>predict</code> method, excluding the <code>input_path</code> and the <code>page_index</code> fields. <code>outputImages</code> <code>object</code> | <code>null</code> See the description of the <code>img</code> attribute of the result of the pipeline prediction. The images are in JPEG format and are Base64-encoded. <code>inputImage</code> | <code>null</code> <code>string</code> The input image. The image is in JPEG format and is Base64-encoded. Multi-language Service Invocation Example Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/formula-recognition\"\nfile_path = \"./demo.jpg\"\n\nwith open(file_path, \"rb\") as file:\n    file_bytes = file.read()\n    file_data = base64.b64encode(file_bytes).decode(\"ascii\")\n\npayload = {\"file\": file_data, \"fileType\": 1}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nfor i, res in enumerate(result[\"formulaRecResults\"]):\n    print(res[\"prunedResult\"])\n    for img_name, img in res[\"outputImages\"].items():\n        img_path = f\"{img_name}_{i}.jpg\"\n        with open(img_path, \"wb\") as f:\n            f.write(base64.b64decode(img))\n        print(f\"Output image saved at {img_path}\")\n</code></pre> <p></p>"},{"location":"en/version3.x/pipeline_usage/formula_recognition.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the formula recognition pipeline do not meet your requirements in terms of accuracy or speed, you can try to fine-tune the existing models using your own domain-specific or application-specific data to improve the recognition performance of the formula recognition pipeline in your scenario.</p>"},{"location":"en/version3.x/pipeline_usage/formula_recognition.html#41-model-fine-tuning","title":"4.1 Model Fine-Tuning","text":"<p>Since the formula recognition pipeline consists of several modules, if the pipeline's performance is not satisfactory, the issue may arise from any one of these modules. You can analyze the poorly recognized images to determine which module is problematic and refer to the corresponding fine-tuning tutorial links in the table below for model fine-tuning.</p> Scenario Fine-Tuning Module Reference Link Formulas are missing Layout Detection Module Link Formula content is inaccurate Formula Recognition Module Link Whole-image rotation correction is inaccurate Document Image Orientation Classification Module Link Image distortion correction is inaccurate Text Image Correction Module Fine-tuning not supported"},{"location":"en/version3.x/pipeline_usage/formula_recognition.html#42-model-deployment","title":"4.2  Model Deployment","text":"<p>After you complete fine-tuning training using a private dataset, you can obtain a local model weight file. You can then use the fine-tuned model weights by specifying the local model save path through parameters or by customizing the pipeline configuration file.</p>"},{"location":"en/version3.x/pipeline_usage/formula_recognition.html#421-specify-the-local-model-path-through-parameters","title":"4.2.1 Specify the local model path through parameters","text":"<p>When initializing the pipeline object, specify the local model path through parameters. Take the usage of the weights after fine-tuning the text detection model as an example, as follows:</p> <p>Command line mode:</p> <pre><code># Specify the local model path via --formula_recognition_model_dir\npaddleocr formula_recognition_pipeline -i ./general_formula_recognition_001.png --formula_recognition_model_dir your_formula_recognition_model_path\n\n# PP-FormulaNet_plus-M model is used as the default formula recognition model. If you do not fine-tune this model, modify the model name by using --formula_recognition_model_name\npaddleocr formula_recognition_pipeline -i ./general_formula_recognition_001.png --formula_recognition_model_name PP-FormulaNet_plus-M --formula_recognition_model_dir your_ppformulanet_plus-m_formula_recognition_model_path\n</code></pre> <p>Script mode\uff1a</p> <pre><code>from paddleocr import FormulaRecognitionPipeline\n\n#  Specify the local model path via formula_recognition_model_dir\npipeline = FormulaRecognitionPipeline(formula_recognition_model_dir=\"./your_formula_recognition_model_path\")\noutput = pipeline.predict(\"./general_formula_recognition_001.png\")\nfor res in output:\n    res.print() ## Print the structured output of the prediction\n    res.save_to_img(save_path=\"output\") ## Save the formula visualization result of the current image.\n    res.save_to_json(save_path=\"output\") ## Save the structured JSON result of the current image\n\n# PP-FormulaNet_plus-M model is used as the default formula recognition model. If you do not fine-tune this model, modify the model name by using  formula_recognition_model_name\n# pipeline = FormulaRecognitionPipeline(formula_recognition_model_name=\"PP-FormulaNet_plus-M\", formula_recognition_model_dir=\"./your_ppformulanet_plus-m_formula_recognition_model_path\")\n</code></pre>"},{"location":"en/version3.x/pipeline_usage/formula_recognition.html#422-specify-the-local-model-path-through-the-configuration-file","title":"4.2.2 Specify the local model path through the configuration file","text":"<p>1.Obtain the pipeline configuration file</p> <p>Call the <code>export_paddlex_config_to_yaml</code> method of the Formula Recognition Pipeline object in PaddleOCR to export the current pipeline configuration as a YAML file:  </p> <pre><code>from paddleocr import FormulaRecognitionPipeline\n\npipeline = FormulaRecognitionPipeline()\npipeline.export_paddlex_config_to_yaml(\"FormulaRecognitionPipeline.yaml\")\n</code></pre> <p>2.Modify the Configuration File </p> <p>After obtaining the default pipeline configuration file, replace the paths of the default model weights with the local paths of your fine-tuned model weights. For example:  </p> <pre><code>......\nSubModules:\n  FormulaRecognition:\n    batch_size: 5\n    model_dir: null # Replace with the path to your fine-tuned formula recognition model weights  \n    model_name: PP-FormulaNet_plus-M # If the name of the fine-tuned model is different from the default model name, please modify it here as well\n    module_name: formula_recognition\n  LayoutDetection:\n    batch_size: 1\n    layout_merge_bboxes_mode: large\n    layout_nms: true\n    layout_unclip_ratio: 1.0\n    model_dir: null # Replace with the path to your fine-tuned layout detection model weights \n    model_name: PP-DocLayout_plus-L # If the name of the fine-tuned model is different from the default model name, please modify it here as well\n    module_name: layout_detection\n    threshold: 0.5\nSubPipelines:\n  DocPreprocessor:\n    SubModules:\n      DocOrientationClassify:\n        batch_size: 1\n        model_dir: null # Replace with the path to your fine-tuned document image orientation classification model weights \n        model_name: PP-LCNet_x1_0_doc_ori # If the name of the fine-tuned model is different from the default model name, please modify it here as well\n        module_name: doc_text_orientation\n      DocUnwarping:\n        batch_size: 1\n        model_dir: null \n        model_name: UVDoc \n        module_name: image_unwarping\n    pipeline_name: doc_preprocessor\n    use_doc_orientation_classify: true\n    use_doc_unwarping: true\npipeline_name: formula_recognition\nuse_doc_preprocessor: true\nuse_layout_detection: true\n......\n</code></pre> <p>The pipeline configuration file includes not only the parameters supported by the PaddleOCR CLI and Python API but also advanced configurations. For detailed instructions, refer to the PaddleX Pipeline Usage Overview and adjust the configurations as needed.  </p> <p>3.Load the Configuration File in CLI  </p> <p>After modifying the configuration file, specify its path using the <code>--paddlex_config</code> parameter in the command line. PaddleOCR will read the file and apply the configurations. Example:  </p> <p><pre><code>paddleocr formula_recognition_pipeline -i ./general_formula_recognition_001.png --paddlex_config FormulaRecognitionPipeline.yaml \n</code></pre> 4.Load the Configuration File in Python API  </p> <p>When initializing the pipeline object, pass the path of the PaddleX pipeline configuration file or a configuration dictionary via the <code>paddlex_config</code> parameter. PaddleOCR will read and apply the configurations. Example: </p> <pre><code>from paddleocr import  FormulaRecognitionPipeline\n\npipeline =  FormulaRecognitionPipeline(paddlex_config=\"FormulaRecognitionPipeline.yaml\")\noutput = pipeline.predict(\"./general_formula_recognition_001.png\")\nfor res in output:\n    res.print() ## Print the structured output of the prediction\n    res.save_to_img(save_path=\"output\") ## Save the formula visualization result of the current image.\n    res.save_to_json(save_path=\"output\") ## Save the structured JSON result of the current image\n</code></pre>"},{"location":"en/version3.x/pipeline_usage/pipeline_overview.html","title":"Pipeline Overview","text":"<p>A pipeline is a practical functional implementation composed of one or more modules. Through reasonable module combination and configuration, pipelines can meet the needs of complex application scenarios, such as technological applications like Optical Character Recognition (OCR). Pipelines not only demonstrate the integrated application of basic modules but also support capabilities such as high-performance inference and service-oriented deployment, providing users with higher development efficiency and broader application possibilities.</p>"},{"location":"en/version3.x/pipeline_usage/seal_recognition.html","title":"Seal Text Recognition Pipeline Usage Tutorial","text":""},{"location":"en/version3.x/pipeline_usage/seal_recognition.html#1-introduction-to-seal-text-recognition-pipeline","title":"1. Introduction to Seal Text Recognition Pipeline","text":"<p>Seal text recognition is a technology that automatically extracts and recognizes the content of seals from documents or images. The recognition of seal text is part of document processing and has many applications in various scenarios, such as contract comparison, warehouse entry and exit review, and invoice reimbursement review.</p> <p>The seal text recognition pipeline is used to recognize the text content of seals, extracting the text information from seal images and outputting it in text form. This pipeline integrates the industry-renowned end-to-end OCR system PP-OCRv4, supporting the detection and recognition of curved seal text. Additionally, this pipeline integrates an optional layout region localization module, which can accurately locate the layout position of the seal within the entire document. It also includes optional document image orientation correction and distortion correction functions. Based on this pipeline, millisecond-level accurate text content prediction can be achieved on a CPU. This pipeline also provides flexible service deployment methods, supporting the use of multiple programming languages on various hardware. Moreover, it offers custom development capabilities, allowing you to train and fine-tune on your own dataset based on this pipeline, and the trained model can be seamlessly integrated.</p> <p></p> <p>The seal text recognition pipeline includes a seal text detection module and a text recognition module, as well as optional layout detection module, document image orientation classification module, and text image correction module.</p> <ul> <li>Seal Text Detection Module</li> <li>Text Recognition Module</li> <li>Layout Detection Module (Optional)</li> <li>Document Image Orientation Classification Module (Optional)</li> <li>Text Image Unwarping Module (Optional)</li> </ul> <p>In this pipeline, you can choose the model to use based on the benchmark data below.</p> Layout Region Detection Module (Optional):  * Layout detection model, including 20 common categories: document title, paragraph title, text, page number, abstract, table of contents, references, footnotes, header, footer, algorithm, formula, formula number, image, table, figure and table title (figure title, table title, and chart title), seal, chart, sidebar text, and reference content ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Storage Size (M) Description PP-DocLayout_plus-LInference Model/Training Model 83.2 34.6244 / 10.3945 510.57 / -  126.01 M A higher precision layout region localization model based on RT-DETR-L trained on a self-built dataset including Chinese and English papers, multi-column magazines, newspapers, PPTs, contracts, books, exam papers, research reports, ancient books, Japanese documents, and vertical text documents   * Layout detection model, including 23 common categories: document title, paragraph title, text, page number, abstract, table of contents, references, footnotes, header, footer, algorithm, formula, formula number, image, chart title, table, table title, seal, chart title, chart, header image, footer image, sidebar text ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Storage Size (M) Description PP-DocLayout-LInference Model/Training Model 90.4 34.6244 / 10.3945 510.57 / - 123.76 M A high precision layout region localization model based on RT-DETR-L trained on a self-built dataset including Chinese and English papers, magazines, contracts, books, exam papers, and research reports PP-DocLayout-MInference Model/Training Model 75.2 13.3259 / 4.8685 44.0680 / 44.0680 22.578 A balanced model of accuracy and efficiency based on PicoDet-L trained on a self-built dataset including Chinese and English papers, magazines, contracts, books, exam papers, and research reports PP-DocLayout-SInference Model/Training Model 70.9 8.3008 / 2.3794 10.0623 / 9.9296 4.834 A highly efficient layout region localization model based on PicoDet-S trained on a self-built dataset including Chinese and English papers, magazines, contracts, books, exam papers, and research reports   &gt;\u2757 Listed above are the 4 core models that are the focus of the layout detection module, which supports a total of 13 full models, including multiple models with pre-defined different categories, among which 9 models include the seal category. Apart from the 3 core models mentioned above, the remaining models are as follows:   \ud83d\udc49Details of the Model List  * 3-class layout detection model, including table, image, seal ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Storage Size (M) Description PicoDet-S_layout_3clsInference Model/Training Model 88.2 8.99 / 2.22 16.11 / 8.73 4.8 A highly efficient layout region localization model based on the lightweight PicoDet-S model trained on a self-built dataset including Chinese and English papers, magazines, and research reports PicoDet-L_layout_3clsInference Model/Training Model 89.0 13.05 / 4.50 41.30 / 41.30 22.6 An efficiency-accuracy balanced layout region localization model based on PicoDet-L trained on a self-built dataset including Chinese and English papers, magazines, and research reports RT-DETR-H_layout_3clsInference Model/Training Model 95.8 114.93 / 27.71 947.56 / 947.56 470.1 A high precision layout region localization model based on RT-DETR-H trained on a self-built dataset including Chinese and English papers, magazines, and research reports   * 17-class region detection model, including 17 common layout categories: paragraph title, image, text, number, abstract, content, chart title, formula, table, table title, references, document title, footnote, header, algorithm, footer, seal ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Storage Size (M) Description PicoDet-S_layout_17clsInference Model/Training Model 87.4 9.11 / 2.12 15.42 / 9.12 4.8 A highly efficient layout region localization model based on the lightweight PicoDet-S model trained on a self-built dataset including Chinese and English papers, magazines, and research reports PicoDet-L_layout_17clsInference Model/Training Model 89.0 13.50 / 4.69 43.32 / 43.32 22.6 An efficiency-accuracy balanced layout region localization model based on PicoDet-L trained on a self-built dataset including Chinese and English papers, magazines, and research reports RT-DETR-H_layout_17clsInference Model/Training Model 98.3 115.29 / 104.09 995.27 / 995.27 470.2 A high precision layout region localization model based on RT-DETR-H trained on a self-built dataset including Chinese and English papers, magazines, and research reports Document Image Orientation Classification Module (Optional): ModelModel Download Link Top-1 Acc (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Storage Size (M) Description PP-LCNet_x1_0_doc_oriInference Model/Training Model 99.06 2.31 / 0.43 3.37 / 1.27 7 A document image classification model based on PP-LCNet_x1_0, containing four categories: 0 degrees, 90 degrees, 180 degrees, and 270 degrees Text Image Correction Module (Optional): ModelModel Download Link CER  Model Storage Size (M) Description UVDocInference Model/Training Model 0.179 30.3 M A high precision text image correction model Seal Text Detection Module: ModelModel Download Link Detection Hmean (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Storage Size (M) Description PP-OCRv4_server_seal_detInference Model/Training Model 98.40 74.75 / 67.72 382.55 / 382.55 109 PP-OCRv4 server-side seal text detection model, with higher accuracy, suitable for deployment on better servers PP-OCRv4_mobile_seal_detInference Model/Training Model 96.36 7.82 / 3.09 48.28 / 23.97 4.6 PP-OCRv4 mobile-side seal text detection model, with higher efficiency, suitable for deployment on the edge Text Recognition Module: ModelModel Download Link Recognition Avg Accuracy(%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Storage Size (M) Description PP-OCRv5_server_recInference Model/Training Model 86.38  8.45/2.36   122.69/122.69  81 M PP-OCRv5_rec is a new generation text recognition model. This model aims to efficiently and accurately support the recognition of four major languages: Simplified Chinese, Traditional Chinese, English, and Japanese, as well as complex text scenes like handwriting, vertical text, pinyin, and rare characters with a single model. It balances recognition effectiveness, inference speed, and model robustness, providing efficient and accurate technical support for document understanding in various scenarios. PP-OCRv5_mobile_recInference Model/Training Model 81.29  1.46/5.43   5.32/91.79  16 M PP-OCRv4_server_rec_docInference Model/Training Model 86.58 6.65 / 2.38 32.92 / 32.92 181 M PP-OCRv4_server_rec_doc is trained on a mix of more Chinese document data and PP-OCR training data based on PP-OCRv4_server_rec, enhancing recognition capabilities for some traditional Chinese characters, Japanese, and special characters, supporting over 15,000+ characters. Besides improving document-related text recognition, it also enhances general text recognition capabilities PP-OCRv4_mobile_recInference Model/Training Model 83.28 4.82 / 1.20 16.74 / 4.64 88 M PP-OCRv4 lightweight recognition model, with high inference efficiency, can be deployed on multiple hardware devices, including edge devices PP-OCRv4_server_rec Inference Model/Training Model 85.19  6.58 / 2.43 33.17 / 33.17 151 M PP-OCRv4 server-side model, with high inference accuracy, can be deployed on various servers en_PP-OCRv4_mobile_recInference Model/Training Model 70.39 4.81 / 0.75 16.10 / 5.31 66 M An ultra-lightweight English recognition model trained based on the PP-OCRv4 recognition model, supporting English and number recognition   &gt; \u2757 Listed above are the 6 core models that are the focus of the text recognition module, which supports a total of 20 full models, including multiple multi-language text recognition models, with the complete model list as follows:   \ud83d\udc49Details of the Model List  * PP-OCRv5 Multi-Scene Model ModelModel Download Link Chinese Recognition Avg Accuracy(%) English Recognition Avg Accuracy(%) Traditional Chinese Recognition Avg Accuracy(%) Japanese Recognition Avg Accuracy(%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Storage Size (M) Description PP-OCRv5_server_recInference Model/Training Model 86.38 64.70 93.29 60.35  1.46/5.43   5.32/91.79  81 M PP-OCRv5_rec is a new generation text recognition model. This model aims to efficiently and accurately support the recognition of four major languages: Simplified Chinese, Traditional Chinese, English, and Japanese, as well as complex text scenes like handwriting, vertical text, pinyin, and rare characters with a single model. It balances recognition effectiveness, inference speed, and model robustness, providing efficient and accurate technical support for document understanding in various scenarios. PP-OCRv5_mobile_recInference Model/Training Model 81.29 66.00 83.55 54.65  1.46/5.43   5.32/91.79  16 M   * Chinese Recognition Model ModelModel Download Link Recognition Avg Accuracy(%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Storage Size (M) Description PP-OCRv4_server_rec_docInference Model/Training Model 86.58 6.65 / 2.38 32.92 / 32.92 91 M PP-OCRv4_server_rec_doc is trained on a mix of more Chinese document data and PP-OCR training data based on PP-OCRv4_server_rec, enhancing recognition capabilities for some traditional Chinese characters, Japanese, and special characters, supporting over 15,000+ characters. Besides improving document-related text recognition, it also enhances general text recognition capabilities PP-OCRv4_mobile_recInference Model/Training Model 83.28 4.82 / 1.20 16.74 / 4.64 11 M PP-OCRv4 lightweight recognition model, with high inference efficiency, can be deployed on multiple hardware devices, including edge devices PP-OCRv4_server_rec Inference Model/Training Model 85.19  6.58 / 2.43 33.17 / 33.17 87 M PP-OCRv4 server-side model, with high inference accuracy, can be deployed on various servers PP-OCRv3_mobile_recInference Model/Training Model 75.43 5.87 / 1.19 9.07 / 4.28 11 M PP-OCRv3 lightweight recognition model, with high inference efficiency, can be deployed on multiple hardware devices, including edge devices ModelModel Download Link Recognition Avg Accuracy(%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Storage Size (M) Description ch_SVTRv2_recInference Model/Training Model 68.81 8.08 / 2.74 50.17 / 42.50 73.9 M  SVTRv2 is a server-side text recognition model developed by the OpenOCR team from Fudan University's Visual and Learning Lab (FVL), which won first place in the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition, improving the end-to-end recognition accuracy on the A leaderboard by 6% compared to PP-OCRv4.  ModelModel Download Link Recognition Avg Accuracy(%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Storage Size (M) Description ch_RepSVTR_recInference Model/Training Model 65.07 5.93 / 1.62 20.73 / 7.32 22.1 M RepSVTR text recognition model is a mobile-side text recognition model based on SVTRv2, which won first place in the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition, improving the end-to-end recognition accuracy on the B leaderboard by 2.5% compared to PP-OCRv4, with comparable inference speed.   * English Recognition Model ModelModel Download Link Recognition Avg Accuracy(%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Storage Size (M) Description en_PP-OCRv4_mobile_recInference Model/Training Model  70.39 4.81 / 0.75 16.10 / 5.31 6.8 M An ultra-lightweight English recognition model trained based on the PP-OCRv4 recognition model, supporting English and number recognition en_PP-OCRv3_mobile_recInference Model/Training Model 70.69 5.44 / 0.75 8.65 / 5.57 7.8 M  An ultra-lightweight English recognition model trained based on the PP-OCRv3 recognition model, supporting English and number recognition   * Multilingual Recognition Model ModelModel Download Link Recognition Avg Accuracy(%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Storage Size (M) Description korean_PP-OCRv3_mobile_recInference Model/Training Model 60.21 5.40 / 0.97 9.11 / 4.05 8.6 M An ultra-lightweight Korean recognition model trained based on the PP-OCRv3 recognition model, supporting Korean and number recognition japan_PP-OCRv3_mobile_recInference Model/Training Model 45.69 5.70 / 1.02 8.48 / 4.07 8.8 M  An ultra-lightweight Japanese recognition model trained based on the PP-OCRv3 recognition model, supporting Japanese and number recognition chinese_cht_PP-OCRv3_mobile_recInference Model/Training Model 82.06 5.90 / 1.28 9.28 / 4.34 9.7 M  An ultra-lightweight Traditional Chinese recognition model trained based on the PP-OCRv3 recognition model, supporting Traditional Chinese and number recognition te_PP-OCRv3_mobile_recInference Model/Training Model 95.88 5.42 / 0.82 8.10 / 6.91 7.8 M  An ultra-lightweight Telugu recognition model trained based on the PP-OCRv3 recognition model, supporting Telugu and number recognition ka_PP-OCRv3_mobile_recInference Model/Training Model 96.96 5.25 / 0.79 9.09 / 3.86 8.0 M  An ultra-lightweight Kannada recognition model trained based on the PP-OCRv3 recognition model, supporting Kannada and number recognition ta_PP-OCRv3_mobile_recInference Model/Training Model 76.83 5.23 / 0.75 10.13 / 4.30 8.0 M  An ultra-lightweight Tamil recognition model trained based on the PP-OCRv3 recognition model, supporting Tamil and number recognition latin_PP-OCRv3_mobile_recInference Model/Training Model 76.93 5.20 / 0.79 8.83 / 7.15 7.8 M An ultra-lightweight Latin recognition model trained based on the PP-OCRv3 recognition model, supporting Latin and number recognition arabic_PP-OCRv3_mobile_recInference Model/Training Model 73.55 5.35 / 0.79 8.80 / 4.56 7.8 M An ultra-lightweight Arabic letter recognition model trained based on the PP-OCRv3 recognition model, supporting Arabic letters and number recognition cyrillic_PP-OCRv3_mobile_recInference Model/Training Model 94.28 5.23 / 0.76 8.89 / 3.88 7.9 M   An ultra-lightweight Cyrillic letter recognition model trained based on the PP-OCRv3 recognition model, supporting Cyrillic letters and number recognition devanagari_PP-OCRv3_mobile_recInference Model/Training Model 96.44 5.22 / 0.79 8.56 / 4.06 7.9 M An ultra-lightweight Devanagari letter recognition model trained based on the PP-OCRv3 recognition model, supporting Devanagari letters and number recognition Test Environment Description: <ul> <li>Performance Test Environment <ul> <li>Test Dataset:               <ul> <li>Document Image Orientation Classification Model: Self-built internal dataset covering multiple scenarios such as documents and certificates, containing 1000 images.</li> <li>Text Image Correction Model: DocUNet.</li> <li>Layout Region Detection Model: PaddleOCR self-built layout region detection dataset, containing 500 common document type images such as Chinese and English papers, magazines, contracts, books, exam papers, and research reports.</li> <li>3-Class Layout Detection Model: PaddleOCR self-built layout region detection dataset, containing 1154 common document type images such as Chinese and English papers, magazines, and research reports.</li> <li>17-Class Region Detection Model: PaddleOCR self-built layout region detection dataset, containing 892 common document type images such as Chinese and English papers, magazines, and research reports.</li> <li>Text Detection Model: PaddleOCR self-built Chinese dataset covering multiple scenarios such as street scenes, web images, documents, and handwriting, where detection includes 500 images.</li> <li>Chinese Recognition Model: PaddleOCR self-built Chinese dataset covering multiple scenarios such as street scenes, web images, documents, and handwriting, where text recognition includes 11,000 images.</li> <li>ch_SVTRv2_rec: PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition A leaderboard evaluation set.</li> <li>ch_RepSVTR_rec: PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition B leaderboard evaluation set.</li> <li>English Recognition Model: Self-built internal English dataset.</li> <li>Multilingual Recognition Model: Self-built internal multilingual dataset.</li> <li>Text Line Orientation Classification Model: Self-built internal dataset covering multiple scenarios such as documents and certificates, containing 1000 images.</li> <li>Seal Text Detection Model: Self-built internal dataset containing 500 circular seal images.</li> </ul> </li> <li>Hardware Configuration: <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environment: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Description</li> </ul> Mode GPU Configuration CPU Configuration Acceleration Technology Combination Regular Mode FP32 Precision / No TRT Acceleration FP32 Precision / 8 Threads PaddleInference High-Performance Mode Optimal combination of prior precision type and acceleration strategy FP32 Precision / 8 Threads Select optimal prior backend (Paddle/OpenVINO/TRT, etc.) <p> If you are more concerned with model accuracy, please choose a model with higher accuracy. If you are more concerned with inference speed, please choose a model with faster inference speed. If you are more concerned with model storage size, please choose a model with smaller storage size.</p>"},{"location":"en/version3.x/pipeline_usage/seal_recognition.html#2-quick-start","title":"2. Quick Start","text":"<p>Before using the seal text recognition pipeline locally, please ensure that you have completed the installation of the wheel package according to the installation tutorial. Once the installation is complete, you can experience it locally via the command line or integrate it with Python.</p>"},{"location":"en/version3.x/pipeline_usage/seal_recognition.html#21-command-line-experience","title":"2.1 Command Line Experience","text":"<p>You can quickly experience the seal_recognition pipeline effect with a single command:</p> <pre><code>paddleocr seal_recognition -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/seal_text_det.png \\\n    --use_doc_orientation_classify False \\\n    --use_doc_unwarping False\n\n# Use --device to specify the use of GPU for model inference.\npaddleocr seal_recognition -i ./seal_text_det.png --device gpu\n</code></pre> The command line supports more parameter settings. Click to expand for detailed explanations of command line parameters. Parameter Description Parameter Type Default Value <code>input</code> Data to be predicted, required. Local path of image or PDF file, e.g., <code>/root/data/img.jpg</code>; URL link, e.g., network URL of image or PDF file: Example; Local directory, the directory should contain images to be predicted, e.g., local path: <code>/root/data/</code> (currently does not support prediction of PDF files in directories; PDF files must be specified with a specific file path).  <code>str</code> <code>save_path</code>  Specify the path to save the inference results file. If not set, the inference results will not be saved locally. <code>str</code> <code>doc_orientation_classify_model_name</code>   The name of the document orientation classification model. If not set, the default model in pipeline will be used. <code>str</code> <code>doc_orientation_classify_model_dir</code> The directory path of the document orientation classification model. If not set, the official model will be downloaded. <code>str</code> <code>doc_unwarping_model_name</code>  The name of the text image unwarping model. If not set, the default model in pipeline will be used. <code>str</code> <code>doc_unwarping_model_dir</code>  The directory path of the  text image unwarping model. If not set, the official model will be downloaded.  <code>str</code> <code>layout_detection_model_name</code>  The name of the layout detection model. If not set, the default model in pipeline will be used.  <code>str</code> <code>layout_detection_model_dir</code>  The directory path of the  layout detection model. If not set, the official model will be downloaded.  <code>str</code> <code>seal_text_detection_model_name</code> The name of the seal text detection model. If not set, the pipeline's default model will be used. <code>str</code> <code>seal_text_detection_model_dir</code> The directory path of the seal text detection model. If not set, the official model will be downloaded. <code>str</code> <code>text_recognition_model_name</code> Name of the text recognition model. If not set, the default pipeline model is used. <code>str</code> <code>text_recognition_model_dir</code> Directory path of the text recognition model. If not set, the official model is downloaded. <code>str</code> <code>text_recognition_batch_size</code> Batch size for the text recognition model. If not set, defaults to <code>1</code>. <code>int</code> <code>use_doc_orientation_classify</code> Whether to load and use document orientation classification module. If not set, defaults to pipeline initialization value (<code>True</code>). <code>bool</code> <code>use_doc_unwarping</code> Whether to load and use text image correction module. If not set, defaults to pipeline initialization value (<code>True</code>). <code>bool</code> <code>use_layout_detection</code>  Whether to load and use the layout detection module. If not set, the parameter will default to the value initialized in the pipeline, which is <code>True</code>. <code>bool</code> <code>layout_threshold</code> Score threshold for the layout model. Any value between <code>0-1</code>. If not set, the default value is used, which is <code>0.5</code>.  <code>float</code> <code>layout_nms</code> Whether to use Non-Maximum Suppression (NMS) as post-processing for layout detection. If not set, the parameter will default to the value initialized in the pipeline, which is set to <code>True</code> by default. <code>bool</code> <code>layout_unclip_ratio</code> Unclip ratio for detected boxes in layout detection model. Any float &gt; <code>0</code>. If not set, the default is <code>1.0</code>.  <code>float</code> <code>layout_merge_bboxes_mode</code> The merging mode for the detection boxes output by the model in layout region detection. <ul> <li>large: When set to \"large\", only the largest outer bounding box will be retained for overlapping bounding boxes, and the inner overlapping boxes will be removed;</li> <li>small: When set to \"small\", only the smallest inner bounding boxes will be retained for overlapping bounding boxes, and the outer overlapping boxes will be removed;</li> <li>union: No filtering of bounding boxes will be performed, and both inner and outer boxes will be retained;</li> </ul>If not set, the default is <code>large</code>.  <code>str</code> <code>seal_det_limit_side_len</code> Image side length limit for seal text detection. Any integer &gt; <code>0</code>. If not set, the default is <code>736</code>.  <code>int</code> <code>seal_det_limit_type</code> Limit type for image side in seal text detection. Supports <code>min</code> and <code>max</code>; <code>min</code> ensures shortest side \u2265 <code>det_limit_side_len</code>, <code>max</code> ensures longest side \u2264 <code>limit_side_len</code>. If not set, the default is <code>min</code>.  <code>str</code> <code>seal_det_thresh</code> Pixel threshold. Pixels with scores above this value in the probability map are considered text. any float &gt; <code>0</code>. If not set, the default is <code>0.2</code>.  <code>float</code> <code>seal_det_box_thresh</code> Box threshold. Boxes with average pixel scores above this value are considered text regions. any float &gt; <code>0</code>. If not set, the default is <code>0.6</code>.  <code>float</code> <code>seal_det_unclip_ratio</code> Expansion ratio for seal text detection. Higher value means larger expansion area. Any float &gt; <code>0</code>. If not set, the default is <code>0.5</code>.  <code>float</code> <code>seal_rec_score_thresh</code> Recognition score threshold. Text results above this value will be kept. Any float &gt; <code>0</code>. If not set, the default is <code>0.0</code> (no threshold).  <code>float</code> <code>device</code> The device used for inference. Support for specifying specific card numbers: <ul> <li>CPU: For example, <code>cpu</code> indicates using the CPU for inference.</li> <li>GPU: For example, <code>gpu:0</code> indicates using the first GPU for inference.</li> <li>NPU: For example, <code>npu:0</code> indicates using the first NPU for inference.</li> <li>XPU: For example, <code>xpu:0</code> indicates using the first XPU for inference.</li> <li>MLU: For example, <code>mlu:0</code> indicates using the first MLU for inference.</li> <li>DCU: For example, <code>dcu:0</code> indicates using the first DCU for inference.</li> </ul>If not set, the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.  <code>str</code> <code>enable_hpi</code> Whether to enable high-performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> The computational precision, such as fp32, fp16. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set. <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> The number of threads used for inference on the CPU. <code>int</code> <code>8</code> <code>paddlex_config</code> Path to PaddleX pipeline configuration file. <code>str</code> <p></p> <p>After running, the results will be printed to the terminal, as follows:</p> <p><pre><code>{'res': {'input_path': './seal_text_det.png', 'model_settings': {'use_doc_preprocessor': True, 'use_layout_detection': True}, 'doc_preprocessor_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_orientation_classify': False, 'use_doc_unwarping': False}, 'angle': -1}, 'layout_det_res': {'input_path': None, 'page_index': None, 'boxes': [{'cls_id': 16, 'label': 'seal', 'score': 0.975529670715332, 'coordinate': [6.191284, 0.16680908, 634.39325, 628.85345]}]}, 'seal_res_list': [{'input_path': None, 'page_index': None, 'model_settings': {'use_doc_preprocessor': False, 'use_textline_orientation': False}, 'dt_polys': [array([[320,  38],\n       ...,\n       [315,  38]]), array([[461, 347],\n       ...,\n       [456, 346]]), array([[439, 445],\n       ...,\n       [434, 444]]), array([[158, 468],\n       ...,\n       [154, 466]])], 'text_det_params': {'limit_side_len': 736, 'limit_type': 'min', 'thresh': 0.2, 'max_side_limit': 4000, 'box_thresh': 0.6, 'unclip_ratio': 0.5}, 'text_type': 'seal', 'textline_orientation_angles': array([-1, ..., -1]), 'text_rec_score_thresh': 0, 'rec_texts': ['\u200b\u5929\u6d25\u200b\u541b\u548c\u7f18\u200b\u5546\u8d38\u200b\u6709\u9650\u516c\u53f8\u200b', '\u200b\u53d1\u7968\u200b\u4e13\u7528\u7ae0\u200b', '\u200b\u5417\u200b\u7e41\u7269\u200b', '5263647368706'], 'rec_scores': array([0.99340463, ..., 0.9916274 ]), 'rec_polys': [array([[320,  38],\n       ...,\n       [315,  38]]), array([[461, 347],\n       ...,\n       [456, 346]]), array([[439, 445],\n       ...,\n       [434, 444]]), array([[158, 468],\n       ...,\n       [154, 466]])], 'rec_boxes': array([], dtype=float64)}]}}\n</code></pre> The visualized results are saved under <code>save_path</code>, and the visualized result of seal OCR is as follows:</p> <p></p>"},{"location":"en/version3.x/pipeline_usage/seal_recognition.html#22-python-script-integration","title":"2.2 Python Script Integration","text":"<ul> <li>The above command line is for quickly experiencing and viewing the effect. Generally, in a project, you often need to integrate through code. You can complete the quick inference of the pipeline with just a few lines of code. The inference code is as follows:</li> </ul> <pre><code>from paddleocr import SealRecognition\n\npipeline = SealRecognition(\n    use_doc_orientation_classify=False, # Set whether to use document orientation classification model\n    use_doc_unwarping=False, # Set whether to use document image unwarping module\n)\n# ocr = SealRecognition(device=\"gpu\") # Specify GPU for model inference\noutput = pipeline.predict(\"./seal_text_det.png\")\nfor res in output:\n    res.print() ## Print structured prediction results\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/\")\n</code></pre> <p>In the above Python script, the following steps were executed:</p> <p>(1) Instantiate a pipeline object for seal text recognition using the SealRecognition() class, with specific parameter descriptions as follows:</p> Parameter Description Type Default Value <code>doc_orientation_classify_model_name</code> Name of the document orientation classification model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>doc_orientation_classify_model_dir</code> Directory path of the document orientation classification model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>doc_unwarping_model_name</code> Name of the document unwarping model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>doc_unwarping_model_dir</code> Directory path of the document unwarping model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>layout_detection_model_name</code> Name of the layout detection model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>layout_detection_model_dir</code> Directory path of the layout detection model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>seal_text_detection_model_name</code> Name of the seal text detection model. If set to <code>None</code>, the default model will be used. <code>str</code> <code>seal_text_detection_model_dir</code> Directory of the seal text detection model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>text_recognition_model_name</code> Name of the text recognition model. If set to <code>None</code>, the pipeline default model is used. <code>str</code> <code>None</code> <code>text_recognition_model_dir</code> Directory path of the text recognition model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>text_recognition_batch_size</code> Batch size for the text recognition model. If set to <code>None</code>, the default batch size is <code>1</code>. <code>int</code> <code>None</code> <code>use_doc_orientation_classify</code> Whether to enable the document orientation classification module. If set to <code>None</code>, the default value is <code>True</code>. <code>bool</code> <code>None</code> <code>use_doc_unwarping</code> Whether to enable the document image unwarping module. If set to <code>None</code>, the default value is <code>True</code>. <code>bool</code> <code>None</code> <code>use_layout_detection</code> Whether to load and use the layout detection module. If set to <code>None</code>, the parameter will default to the value initialized in the pipeline, which is <code>True</code>. <code>bool</code> <code>None</code> <code>layout_threshold</code> Score threshold for the layout model. <ul> <li>float: Any float between <code>0-1</code>;</li> <li>dict: <code>{0:0.1}</code> where the key is the class ID and the value is the threshold for that class;</li> <li>None: If set to <code>None</code>, uses the pipeline default of <code>0.5</code>.</li> </ul> <code>float|dict</code> <code>None</code> <code>layout_nms</code> Whether to use Non-Maximum Suppression (NMS) as post-processing for layout detection. If set to <code>None</code>, the parameter will default to the value initialized in the pipeline, which is set to <code>True</code> by default. <code>bool</code> <code>None</code> <code>layout_unclip_ratio</code> Expansion ratio for the bounding boxes from the layout detection model. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>Tuple[float,float]: Expansion ratios in horizontal and vertical directions;</li> <li>dict: A dictionary with int keys representing <code>cls_id</code>, and tuple values, e.g., <code>{0: (1.1, 2.0)}</code> means width is expanded 1.1\u00d7 and height 2.0\u00d7 for class 0 boxes;</li> <li>None: If set to <code>None</code>, uses the pipeline default of <code>1.0</code>.</li> </ul> <code>float|Tuple[float,float]|dict</code> <code>None</code> <code>layout_merge_bboxes_mode</code> Filtering method for overlapping boxes in layout detection. <ul> <li>str: Options include <code>large</code>, <code>small</code>, and <code>union</code> to retain the larger box, smaller box, or both;</li> <li>dict: A dictionary with int keys representing <code>cls_id</code>, and str values, e.g., <code>{0: \"large\", 2: \"small\"}</code> means using different modes for different classes;</li> <li>None: If set to <code>None</code>, uses the pipeline default value <code>large</code>.</li> </ul> <code>str|dict</code> <code>None</code> <code>seal_det_limit_side_len</code> Image side length limit for seal text detection. <ul> <li>int: Any integer greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the default value is <code>736</code>.</li> </ul> <code>int</code> <code>None</code> <code>seal_det_limit_type</code> Limit type for seal text detection image side length. <ul> <li>str: Supports <code>min</code> and <code>max</code>. <code>min</code> ensures the shortest side is no less than <code>det_limit_side_len</code>, while <code>max</code> ensures the longest side is no greater than <code>limit_side_len</code>;</li> <li>None: If set to <code>None</code>, the default value is <code>min</code>.</li> </ul> <code>str</code> <code>None</code> <code>seal_det_thresh</code> Pixel threshold for detection. Pixels with scores greater than this value in the probability map are considered text pixels. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the default value is <code>0.2</code>.</li> </ul> <code>float</code> <code>None</code> <code>seal_det_box_thresh</code> Bounding box threshold. If the average score of all pixels inside a detection box exceeds this threshold, it is considered a text region. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the default value is <code>0.6</code>.</li> </ul> <code>float</code> <code>None</code> <code>seal_det_unclip_ratio</code> Expansion ratio for seal text detection. The larger the value, the larger the expanded area. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the default value is <code>0.5</code>.</li> </ul> <code>float</code> <code>None</code> <code>seal_rec_score_thresh</code> Score threshold for seal text recognition. Text results with scores above this threshold will be retained. <ul> <li>float: Any float greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the default value is <code>0.0</code> (no threshold).</li> </ul> <code>float</code> <code>None</code> <code>device</code> Device used for inference. Supports specifying device ID: <ul> <li>CPU: e.g., <code>cpu</code> means using CPU for inference;</li> <li>GPU: e.g., <code>gpu:0</code> means using GPU 0;</li> <li>NPU: e.g., <code>npu:0</code> means using NPU 0;</li> <li>XPU: e.g., <code>xpu:0</code> means using XPU 0;</li> <li>MLU: e.g., <code>mlu:0</code> means using MLU 0;</li> <li>DCU: e.g., <code>dcu:0</code> means using DCU 0;</li> <li>None: If set to <code>None</code>, the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.</li> </ul> <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to enable high-performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Computation precision, e.g., fp32, fp16. <code>str</code> <code>\"fp32\"</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set. <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads used for inference on CPU. <code>int</code> <code>8</code> <code>paddlex_config</code> Path to the PaddleX pipeline configuration file. <code>str</code> <code>None</code> <p>(2) Call the <code>predict()</code> method of the Seal Text Recognition pipeline object for inference prediction. This method will return a <code>generator</code>. Below are the parameters and their descriptions for the <code>predict()</code> method:</p> Parameter Parameter Description Parameter Type Default Value <code>input</code> Input data to be predicted. Required. Supports multiple types: <ul> <li>Python Var: Image data represented by <code>numpy.ndarray</code>;</li> <li>str: Local path of an image or PDF file, e.g., <code>/root/data/img.jpg</code>; URL link, e.g., the network URL of an image or PDF file: Example; Local directory, containing images to be predicted, e.g., <code>/root/data/</code> (currently does not support prediction of PDF files in directories; PDF files must be specified with an exact file path);</li> <li>List: Elements of the list must be of the above types, e.g., <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\\\"/root/data/img1.jpg\\\", \\\"/root/data/img2.jpg\\\"]</code>, <code>[\\\"/root/data1\\\", \\\"/root/data2\\\"]</code>.</li> </ul> <code>Python Var|str|list</code> <code>use_doc_orientation_classify</code> Whether to use the document orientation classification module during inference. <code>bool</code> <code>None</code> <code>use_doc_unwarping</code> Whether to use the text image correction module during inference. <code>bool</code> <code>None</code> <code>use_layout_detection</code>  Whether to use the layout detection module during inference.  <code>bool</code> <code>None</code> <code>layout_threshold</code> Same as the parameter during instantiation. <code>float|dict</code> <code>None</code> <code>layout_nms</code> Same as the parameter during instantiation. <code>bool</code> <code>None</code> <code>layout_unclip_ratio</code> Same as the parameter during instantiation. <code>float|Tuple[float,float]|dict</code> <code>None</code> <code>layout_merge_bboxes_mode</code> Same as the parameter during instantiation. <code>str|dict</code> <code>None</code> <code>seal_det_limit_side_len</code> Same as the parameter during instantiation. <code>int</code> <code>None</code> <code>seal_det_limit_type</code> Same as the parameter during instantiation. <code>str</code> <code>None</code> <code>seal_det_thresh</code> Same as the parameter during instantiation. <code>float</code> <code>None</code> <code>seal_det_box_thresh</code> Same as the parameter during instantiation. <code>float</code> <code>None</code> <code>seal_det_unclip_ratio</code> Same as the parameter during instantiation. <code>float</code> <code>None</code> <code>seal_rec_score_thresh</code> Same as the parameter during instantiation. <code>float</code> <code>None</code> <p>(3) Process the prediction results. The prediction result for each sample is of <code>dict</code> type and supports operations such as printing, saving as an image, and saving as a <code>json</code> file:</p> Method Description Parameter Parameter Type Parameter Description Default Value <code>print()</code> Print results to the terminal <code>format_json</code> <code>bool</code> Whether to format the output content using <code>JSON</code> indentation. <code>True</code> <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data for better readability, effective only when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> will retain the original characters, effective only when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_json()</code> Save results as a json file <code>save_path</code> <code>str</code> The file path to save the results. When it is a directory, the saved file name will be consistent with the input file type. None <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data for better readability, effective only when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> will retain the original characters, effective only when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_img()</code> Save results as an image file <code>save_path</code> <code>str</code> The file path to save the results, supports directory or file path. None <ul> <li> <p>Calling the <code>print()</code> method will print the results to the terminal, and the explanations of the printed content are as follows:</p> <ul> <li> <p><code>input_path</code>: <code>(str)</code> The input path of the image to be predicted.</p> </li> <li> <p><code>model_settings</code>: <code>(Dict[str, bool])</code> The model parameters required for pipeline configuration.</p> <ul> <li><code>use_doc_preprocessor</code>: <code>(bool)</code> Controls whether to enable the document preprocessing sub-pipeline.</li> <li><code>use_layout_detection</code>: <code>(bool)</code> Controls whether to enable the layout detection sub-module.</li> </ul> </li> <li> <p><code>layout_det_res</code>: <code>(Dict[str, Union[List[numpy.ndarray], List[float]]])</code> The output result of the layout detection sub-module. Only exists when <code>use_layout_detection=True</code>.</p> <ul> <li><code>input_path</code>: <code>(Union[str, None])</code> The image path accepted by the layout detection module. Saved as <code>None</code> when the input is a <code>numpy.ndarray</code>.</li> <li><code>page_index</code>: <code>(Union[int, None])</code> Indicates the current page number of the PDF if the input is a PDF file; otherwise, it is <code>None</code>.</li> <li><code>boxes</code>: <code>(List[Dict])</code> A list of detected layout seal regions, with each element containing the following fields:<ul> <li><code>cls_id</code>: <code>(int)</code> The class ID of the detected seal region.</li> <li><code>score</code>: <code>(float)</code> The confidence score of the detected region.</li> <li><code>coordinate</code>: <code>(List[float])</code> The coordinates of the four corners of the detection box, in the order of x1, y1, x2, y2, representing the x-coordinate of the top-left corner, the y-coordinate of the top-left corner, the x-coordinate of the bottom-right corner, and the y-coordinate of the bottom-right corner.</li> </ul> </li> </ul> </li> <li> <p><code>seal_res_list</code>: <code>List[Dict]</code> A list of seal text recognition results, with each element containing the following fields:</p> <ul> <li><code>input_path</code>: <code>(Union[str, None])</code> The image path accepted by the seal text recognition pipeline. Saved as <code>None</code> when the input is a <code>numpy.ndarray</code>.</li> <li><code>page_index</code>: <code>(Union[int, None])</code> Indicates the current page number of the PDF if the input is a PDF file; otherwise, it is <code>None</code>.</li> <li><code>model_settings</code>: <code>(Dict[str, bool])</code> The model configuration parameters for the seal text recognition pipeline.</li> <li><code>use_doc_preprocessor</code>: <code>(bool)</code> Controls whether to enable the document preprocessing sub-pipeline.</li> <li><code>use_textline_orientation</code>: <code>(bool)</code> Controls whether to enable the text line orientation classification sub-module.</li> </ul> </li> <li> <p><code>doc_preprocessor_res</code>: <code>(Dict[str, Union[str, Dict[str, bool], int]])</code> The output result of the document preprocessing sub-pipeline. Only exists when <code>use_doc_preprocessor=True</code>.</p> <ul> <li><code>input_path</code>: <code>(Union[str, None])</code> The image path accepted by the document preprocessing sub-pipeline. Saved as <code>None</code> when the input is a <code>numpy.ndarray</code>.</li> <li><code>model_settings</code>: <code>(Dict)</code> The model configuration parameters for the preprocessing sub-pipeline.<ul> <li><code>use_doc_orientation_classify</code>: <code>(bool)</code> Controls whether to enable document orientation classification.</li> <li><code>use_doc_unwarping</code>: <code>(bool)</code> Controls whether to enable document unwarping.</li> </ul> </li> <li><code>angle</code>: <code>(int)</code> The predicted result of document orientation classification. When enabled, it takes values [0, 1, 2, 3], corresponding to [0\u00b0, 90\u00b0, 180\u00b0, 270\u00b0]; when disabled, it is -1.</li> </ul> </li> <li> <p><code>dt_polys</code>: <code>(List[numpy.ndarray])</code> A list of polygon boxes for seal text detection. Each detection box is represented by a numpy array of multiple vertex coordinates, with the array shape being (n, 2).</p> </li> <li> <p><code>dt_scores</code>: <code>(List[float])</code> A list of confidence scores for text detection boxes.</p> </li> <li> <p><code>text_det_params</code>: <code>(Dict[str, Dict[str, int, float]])</code> Configuration parameters for the text detection module.</p> <ul> <li><code>limit_side_len</code>: <code>(int)</code> The side length limit value during image preprocessing.</li> <li><code>limit_type</code>: <code>(str)</code> The handling method for side length limits.</li> <li><code>thresh</code>: <code>(float)</code> The confidence threshold for text pixel classification.</li> <li><code>box_thresh</code>: <code>(float)</code> The confidence threshold for text detection boxes.</li> <li><code>unclip_ratio</code>: <code>(float)</code> The expansion ratio for text detection boxes.</li> <li><code>text_type</code>: <code>(str)</code> The type of seal text detection, currently fixed as \"seal\".</li> </ul> </li> <li> <p><code>text_rec_score_thresh</code>: <code>(float)</code> The filtering threshold for text recognition results.</p> </li> <li> <p><code>rec_texts</code>: <code>(List[str])</code> A list of text recognition results, containing only texts with confidence scores above <code>text_rec_score_thresh</code>.</p> </li> <li> <p><code>rec_scores</code>: <code>(List[float])</code> A list of confidence scores for text recognition, filtered by <code>text_rec_score_thresh</code>.</p> </li> <li> <p><code>rec_polys</code>: <code>(List[numpy.ndarray])</code> A list of text detection boxes filtered by confidence score, in the same format as <code>dt_polys</code>.</p> </li> <li> <p><code>rec_boxes</code>: <code>(numpy.ndarray)</code> An array of rectangular bounding boxes for detection boxes; the seal recognition pipeline returns an empty array.</p> </li> </ul> </li> <li> <p>Calling the <code>save_to_json()</code> method will save the above content to the specified <code>save_path</code>. If a directory is specified, the saved path will be <code>save_path/{your_img_basename}_res.json</code>. If a file is specified, it will be saved directly to that file. Since JSON files do not support saving numpy arrays, <code>numpy.array</code> types will be converted to list format.</p> </li> <li> <p>Calling the <code>save_to_img()</code> method will save the visualization results to the specified <code>save_path</code>. If a directory is specified, the saved path will be <code>save_path/{your_img_basename}_seal_res_region1.{your_img_extension}</code>. If a file is specified, it will be saved directly to that file. (The pipeline usually contains multiple result images, so it is not recommended to specify a specific file path directly, as multiple images will be overwritten, and only the last image will be retained.)</p> </li> <li> <p>Additionally, you can obtain visualized images with results and prediction results through attributes, as follows:</p> </li> </ul> Attribute Description <code>json</code> Get the prediction results in <code>json</code> format. <code>img</code> Get the visualization results in <code>dict</code> format. <ul> <li>The prediction results obtained through the <code>json</code> attribute are of dict type, with content consistent with what is saved by calling the <code>save_to_json()</code> method.</li> <li>The prediction results returned by the <code>img</code> attribute are of dict type. The keys are <code>layout_det_res</code>, <code>seal_res_region1</code>, and <code>preprocessed_img</code>, corresponding to three <code>Image.Image</code> objects: one for visualizing layout detection, one for visualizing seal text recognition results, and one for visualizing image preprocessing. If the image preprocessing sub-module is not used, <code>preprocessed_img</code> will not be included in the dictionary. If the layout region detection module is not used, <code>layout_det_res</code> will not be included.</li> </ul>"},{"location":"en/version3.x/pipeline_usage/seal_recognition.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <p>If you need to integrate the pipeline into your Python project, you can refer to the example code in 2.2 Python Script Method.</p> <p>In addition, PaddleOCR also provides three other deployment methods, which are detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In real-world production environments, many applications have stringent performance requirements for deployment strategies, especially in terms of response speed, to ensure efficient system operation and a smooth user experience. To address this, PaddleOCR offers high-performance inference capabilities aimed at deeply optimizing the performance of model inference and pre/post-processing, thereby significantly accelerating the end-to-end process. For detailed high-performance inference procedures, please refer to High-Performance Inference.</p> <p>\u2601\ufe0f Service Deployment: Service deployment is a common form of deployment in real-world production environments. By encapsulating inference functionality into a service, clients can access these services via network requests to obtain inference results. For detailed production service deployment procedures, please refer to Serving.</p> <p>Below are the API references for basic serving deployment and multi-language service invocation examples:</p> API Reference <p>For the main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the attributes of the response body are as follows:</li> </ul> Name Type Description <code>logId</code> <code>string</code> The UUID of the request. <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error message. Fixed as <code>\"Success\"</code>. <code>result</code> <code>object</code> The result of the operation. <ul> <li>When the request is not processed successfully, the attributes of the response body are as follows:</li> </ul> Name Type Description <code>logId</code> <code>string</code> The UUID of the request. <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error message. <p>The main operations provided by the service are as follows:</p> <ul> <li><code>infer</code></li> </ul> <p>Obtain the seal text recognition result.</p> <p><code>POST /seal-recognition</code></p> <ul> <li>The attributes of the request body are as follows:</li> </ul> Name Type Description Required <code>file</code> <code>string</code> The URL of an image or PDF file accessible by the server, or the Base64-encoded content of the file. By default, for PDF files exceeding 10 pages, only the content of the first 10 pages will be processed. To remove the page limit, please add the following configuration to the pipeline configuration file: <pre><code>Serving:\n  extra:\n    max_num_input_imgs: null\n</code></pre> Yes <code>fileType</code> <code>integer</code> | <code>null</code> The type of file. <code>0</code> indicates a PDF file, <code>1</code> indicates an image file. If this attribute is not present in the request body, the file type will be inferred from the URL. No <code>useDocOrientationClassify</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>use_doc_orientation_classify</code> parameter of the pipeline object's <code>predict</code> method. No <code>useDocUnwarping</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>use_doc_unwarping</code> parameter of the pipeline object's <code>predict</code> method. No <code>useLayoutDetection</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>use_layout_detection</code> parameter of the pipeline object's <code>predict</code> method. No <code>layoutThreshold</code> <code>number</code> | <code>null</code> Please refer to the description of the <code>layout_threshold</code> parameter of the pipeline object's <code>predict</code> method. No <code>layoutNms</code> <code>boolean</code> | <code>null</code> Please refer to the description of the <code>layout_nms</code> parameter of the pipeline object's <code>predict</code> method. No <code>layoutUnclipRatio</code> <code>number</code> | <code>array</code> | <code>null</code> Please refer to the description of the <code>layout_unclip_ratio</code> parameter of the pipeline object's <code>predict</code> method. No <code>layoutMergeBboxesMode</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>layout_merge_bboxes_mode</code> parameter of the pipeline object's <code>predict</code> method. No <code>sealDetLimitSideLen</code> <code>integer</code> | <code>null</code> Please refer to the description of the <code>seal_det_limit_side_len</code> parameter of the pipeline object's <code>predict</code> method. No <code>sealDetLimitType</code> <code>string</code> | <code>null</code> Please refer to the description of the <code>seal_det_limit_type</code> parameter of the pipeline object's <code>predict</code> method. No <code>sealDetThresh</code> <code>number</code> | <code>null</code> Please refer to the description of the <code>seal_det_thresh</code> parameter of the pipeline object's <code>predict</code> method. No <code>sealDetBoxThresh</code> <code>number</code> | <code>null</code> Please refer to the description of the <code>seal_det_box_thresh</code> parameter of the pipeline object's <code>predict</code> method. No <code>sealDetUnclipRatio</code> <code>number</code> | <code>null</code> Please refer to the description of the <code>seal_det_unclip_ratio</code> parameter of the pipeline object's <code>predict</code> method. No <code>sealRecScoreThresh</code> <code>number</code> | <code>null</code> Please refer to the description of the <code>seal_rec_score_thresh</code> parameter of the pipeline object's <code>predict</code> method. No <ul> <li>When the request is processed successfully, the <code>result</code> in the response body has the following properties:</li> </ul> Name Type Meaning <code>sealRecResults</code> <code>object</code> The seal text recognition result. The array length is 1 (for image input) or the actual number of document pages processed (for PDF input). For PDF input, each element in the array represents the result of each page actually processed in the PDF file. <code>dataInfo</code> <code>object</code> Information about the input data. <p>Each element in <code>sealRecResults</code> is an <code>object</code> with the following properties:</p> Name Type Meaning <code>prunedResult</code> <code>object</code> A simplified version of the <code>res</code> field in the JSON representation generated by the <code>predict</code> method of the production object, where the <code>input_path</code> and the <code>page_index</code> fields are removed. <code>outputImages</code> <code>object</code> | <code>null</code> See the description of the <code>img</code> attribute of the result of the pipeline prediction. The images are in JPEG format and encoded in Base64. <code>inputImage</code> <code>string</code> | <code>null</code> The input image. The image is in JPEG format and encoded in Base64. Multi-language Service Invocation Example Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/seal-recognition\"\nfile_path = \"./demo.jpg\"\n\nwith open(file_path, \"rb\") as file:\n    file_bytes = file.read()\n    file_data = base64.b64encode(file_bytes).decode(\"ascii\")\n\npayload = {\"file\": file_data, \"fileType\": 1}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nfor i, res in enumerate(result[\"sealRecResults\"]):\n    print(res[\"prunedResult\"])\n    for img_name, img in res[\"outputImages\"].items():\n        img_path = f\"{img_name}_{i}.jpg\"\n        with open(img_path, \"wb\") as f:\n            f.write(base64.b64decode(img))\n        print(f\"Output image saved at {img_path}\")\n</code></pre> <p></p>"},{"location":"en/version3.x/pipeline_usage/seal_recognition.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the seal text recognition pipeline do not meet your requirements in terms of accuracy or speed, you can try to fine-tune the existing models using your own domain-specific or application data to improve the recognition performance of the seal text recognition pipeline in your scenario.</p>"},{"location":"en/version3.x/pipeline_usage/seal_recognition.html#41-model-fine-tuning","title":"4.1 Model Fine-Tuning","text":"<p>Since the seal text recognition pipeline consists of several modules, if the pipeline's performance does not meet expectations, the issue may arise from any one of these modules. You can analyze images with poor recognition results to identify which module is problematic and refer to the corresponding fine-tuning tutorial links in the table below for model fine-tuning.</p> Scenario Fine-Tuning Module Fine-Tuning Reference Link Inaccurate or missing seal position detection Layout Detection Module Link Missing text detection Text Detection Module Link Inaccurate text content Text Recognition Module Link Inaccurate full-image rotation correction Document Image Orientation Classification Module Link Inaccurate image distortion correction Text Image Correction Module Not supported for fine-tuning"},{"location":"en/version3.x/pipeline_usage/seal_recognition.html#42-model-application","title":"4.2 Model Application","text":"<p>After you complete the fine-tuning training with a private dataset, you can obtain the local model weight files. You can then use the fine-tuned model weights by specifying the local model save path through parameters or by using a custom pipeline configuration file.</p>"},{"location":"en/version3.x/pipeline_usage/seal_recognition.html#421-specify-local-model-path-via-parameters","title":"4.2.1 Specify Local Model Path via Parameters","text":"<p>When initializing the pipeline object, specify the local model path through parameters. Taking the usage of fine-tuned weights for a text detection model as an example, the demonstration is as follows:</p> <p>Command line method:</p> <pre><code># Specify the local model path through --doc_orientation_classify_model_dir\npaddleocr seal_recognition -i ./seal_text_det.png --doc_orientation_classify_model_dir your_orientation_classify_model_path\n\n# By default, the PP-LCNet_x1_0_doc_ori model is used as the default text detection model. If the fine-tuned model is not this one, modify the model name with --text_detection_model_name\npaddleocr seal_recognition -i ./seal_text_det.png --doc_orientation_classify_model_name PP-LCNet_x1_0_doc_ori --doc_orientation_classify_model_dir your_orientation_classify_model_path\n</code></pre> <p>Script method:</p> <pre><code>from paddleocr import SealRecognition\n\n# Specify the local model path through doc_orientation_classify_model_dir\npipeline = SealRecognition(doc_orientation_classify_model_dir =\"./your_orientation_classify_model_path\")\n\n# By default, the PP-LCNet_x1_0_doc_ori model is used as the default text detection model. If the fine-tuned model is not this one, modify the model name with doc_orientation_classify_model_name\n# pipeline = SealRecognition(doc_orientation_classify_model_name=\"PP-LCNet_x1_0_doc_ori\", doc_orientation_classify_model_dir=\"./your_orientation_classify_model_path\")\n</code></pre>"},{"location":"en/version3.x/pipeline_usage/seal_recognition.html#422-specify-local-model-path-via-configuration-file","title":"4.2.2 Specify Local Model Path via Configuration File","text":"<ol> <li>Obtain pipeline Configuration File</li> </ol> <p>You can call the <code>export_paddlex_config_to_yaml</code> method of the general OCR pipeline object in PaddleOCR to export the current pipeline configuration to a YAML file:</p> <pre><code>from paddleocr import SealRecognition\n\npipeline = SealRecognition()\npipeline.export_paddlex_config_to_yaml(\"SealRecognition.yaml\")\n</code></pre> <ol> <li>Modify Configuration File</li> </ol> <p>After obtaining the default pipeline configuration file, replace the local path of the fine-tuned model weights in the corresponding position of the pipeline configuration file. For example:</p> <pre><code>......\nSubPipelines:\n  DocPreprocessor:\n    SubModules:\n      DocOrientationClassify:\n        model_dir: null  # Replace with the path of the fine-tuned document orientation classification model weights\n        model_name: PP-LCNet_x1_0_doc_ori # If the name of the fine-tuned model is different from the default model name, please also modify here\n        module_name: doc_text_orientation\n      DocUnwarping:\n        model_dir: null  # Replace with the path of the fine-tuned document unwarping model weights\n        model_name: UVDoc # If the name of the fine-tuned model is different from the default model name, please also modify here\n        module_name: image_unwarping\n    pipeline_name: doc_preprocessor\n    use_doc_orientation_classify: true\n    use_doc_unwarping: true\n......\n</code></pre> <p>The pipeline configuration file not only contains the parameters supported by the SealRecognition CLI and Python API but also allows for more advanced configurations. Detailed information can be found in the PaddleX Model pipeline Usage Overview, where you can find the corresponding pipeline usage tutorial and adjust various configurations as needed.</p> <ol> <li>Load pipeline Configuration File in CLI</li> </ol> <p>After modifying the configuration file, specify the path of the modified pipeline configuration file using the --paddlex_config parameter in the command line. PaddleOCR will read its contents as the pipeline configuration. Example:</p> <pre><code>paddleocr seal_recognition --paddlex_config SealRecognition.yaml ...\n</code></pre> <ol> <li>Load pipeline Configuration File in Python API</li> </ol> <p>When initializing the pipeline object, you can pass the PaddleX pipeline configuration file path or configuration dictionary through the paddlex_config parameter. PaddleOCR will read its contents as the pipeline configuration. Example:</p> <pre><code>from paddleocr import SealRecognition\n\npipeline = SealRecognition(paddlex_config=\"SealRecognition.yaml\")\n</code></pre>"},{"location":"en/version3.x/pipeline_usage/table_recognition_v2.html","title":"General Table Recognition V2 Pipeline Usage Tutorial","text":""},{"location":"en/version3.x/pipeline_usage/table_recognition_v2.html#1-introduction-to-general-table-recognition-v2-pipeline","title":"1. Introduction to General Table Recognition v2 pipeline","text":"<p>Table recognition is a technology that automatically identifies and extracts table content and its structure from documents or images. It is widely used in fields such as data entry, information retrieval, and document analysis. By using computer vision and machine learning algorithms, table recognition can convert complex table information into an editable format, making it easier for users to further process and analyze data.</p> <p>The General Table Recognition v2 pipeline (PP-TableMagic) is designed to tackle table recognition tasks, identifying tables in images and outputting them in HTML format. Unlike the original General Table Recognition pipeline, this version introduces two new modules: table classification and table cell detection. By adopting a multi-model pipeline combining \"table classification + table structure recognition + cell detection\", it achieves better end-to-end table recognition performance compared to the previous version. Based on this, the General Table Recognition v2 pipeline natively supports targeted model fine-tuning, allowing developers to customize it to varying degrees for satisfactory performance in different application scenarios. Furthermore, the General Table Recognition v2 pipeline also supports end-to-end table structure recognition models (e.g., SLANet, SLANet_plus, etc.) and allows independent configuration for wired and wireless table recognition methods, enabling developers to freely select and combine the best table recognition solutions.</p> <p>This pipeline is applicable in a variety of fields, including general, manufacturing, finance, and transportation. It also provides flexible service deployment options, supporting multiple programming languages on various hardware. Additionally, it offers capabilities for secondary development, allowing you to train and fine-tune your own datasets based on this pipeline, with the trained models seamlessly integrated.</p> <p></p> <p>The General Table Recognition Pipeline v2 includes the following 8 modules. Each module can be trained and inferred independently and contains multiple models. For detailed information, please click on the corresponding module to view the documentation.</p> <ul> <li>Table Structure Recognition Module</li> <li>Table Classification Module</li> <li>Table Cell Detection Module</li> <li>Text Detection Module</li> <li>Text Recognition Module</li> <li>Layout Region Detection Module (optional)</li> <li>Document Image Orientation Classification Module (optional)</li> <li>Text Image Unwarping Module (optional)</li> </ul> <p>In this pipeline, you can choose the models to use based on the benchmark data below.</p> Table Structure Recognition Module Models: ModelModel Download Link Accuracy (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Size (M) Description SLANetInference Model/Training Model 59.52 103.08 / 103.08 197.99 / 197.99 6.9 M SLANet is a table structure recognition model developed by Baidu PaddlePaddle's vision team. This model significantly improves the accuracy and inference speed of table structure recognition by using a CPU-friendly lightweight backbone network PP-LCNet, a high-low feature fusion module CSP-PAN, and a feature decoding module SLA Head that aligns structure and location information. SLANet_plusInference Model/Training Model 63.69 140.29 / 140.29 195.39 / 195.39 6.9 M SLANet_plus is an enhanced version of the SLANet table structure recognition model developed by Baidu PaddlePaddle's vision team. Compared to SLANet, SLANet_plus significantly improves the recognition capabilities for wireless tables and complex tables, while reducing the model's sensitivity to table positioning accuracy, allowing for accurate recognition even if the table is slightly misaligned. SLANeXt_wired Inference Model/Training Model 69.65 -- -- 351M The SLANeXt series is a new generation of table structure recognition models developed by Baidu PaddlePaddle's vision team. Compared to SLANet and SLANet_plus, SLANeXt focuses on recognizing table structures and has been trained with dedicated weights for recognizing wired and wireless tables, significantly enhancing recognition capabilities across both types, especially for wired tables. SLANeXt_wireless Inference Model/Training Model Table Classification Module Models: ModelModel Download Link Top1 Acc (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Size (M) PP-LCNet_x1_0_table_clsInference Model/Training Model 94.2 2.35 / 0.47 4.03 / 1.35 6.6M Table Cell Detection Module Models: ModelModel Download Link mAP (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Size (M) Description RT-DETR-L_wired_table_cell_det Inference Model/Training Model 82.7 35.00 / 10.45 495.51 / 495.51 124M RT-DETR is the first real-time end-to-end object detection model. The Baidu PaddlePaddle vision team based RT-DETR-L as the base model, completing pre-training on a self-built table cell detection dataset, achieving good performance in detecting both wired and wireless table cells. RT-DETR-L_wireless_table_cell_det Inference Model/Training Model Text Detection Module Models: ModelModel Download Link Detection Hmean (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Size (M) Description PP-OCRv5_server_detInference Model/Training Model 83.8 89.55 / 70.19 371.65 / 371.65 84.3 PP-OCRv5 server-side text detection model with higher accuracy, suitable for deployment on high-performance servers PP-OCRv5_mobile_detInference Model/Training Model 79.0 8.79 / 3.13 51.00 / 28.58 4.7 PP-OCRv5 mobile-side text detection model with higher efficiency, suitable for deployment on edge devices PP-OCRv4_server_detInference Model/Training Model 69.2 83.34 / 80.91 442.58 / 442.58 109 PP-OCRv4 server-side text detection model with higher accuracy, suitable for deployment on high-performance servers PP-OCRv4_mobile_detInference Model/Training Model 63.8 8.79 / 3.13 51.00 / 28.58 4.7 PP-OCRv4 mobile-side text detection model with higher efficiency, suitable for deployment on edge devices Text Recognition Module: ModelModel Download Links Recognition Avg Accuracy(%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-OCRv5_server_recInference Model/Pretrained Model 86.38  8.45/2.36   122.69/122.69  81 M PP-OCRv5_rec is a next-generation text recognition model. It aims to efficiently and accurately support the recognition of four major languages\u2014Simplified Chinese, Traditional Chinese, English, and Japanese\u2014as well as complex text scenarios such as handwriting, vertical text, pinyin, and rare characters using a single model. While maintaining recognition performance, it balances inference speed and model robustness, providing efficient and accurate technical support for document understanding in various scenarios. PP-OCRv5_mobile_recInference Model/Pretrained Model 81.29  1.46/5.43   5.32/91.79  16 M PP-OCRv4_server_rec_docInference Model/Pretrained Model 86.58 6.65 / 2.38 32.92 / 32.92 91 M PP-OCRv4_server_rec_doc is trained on a mixed dataset of more Chinese document data and PP-OCR training data, building upon PP-OCRv4_server_rec. It enhances the recognition capabilities for some Traditional Chinese characters, Japanese characters, and special symbols, supporting over 15,000 characters. In addition to improving document-related text recognition, it also enhances general text recognition capabilities. PP-OCRv4_mobile_recInference Model/Pretrained Model 83.28 4.82 / 1.20 16.74 / 4.64 11 M A lightweight recognition model of PP-OCRv4 with high inference efficiency, suitable for deployment on various hardware devices, including edge devices. PP-OCRv4_server_rec Inference Model/Pretrained Model 85.19  6.58 / 2.43 33.17 / 33.17 87 M The server-side model of PP-OCRv4, offering high inference accuracy and deployable on various servers. en_PP-OCRv4_mobile_recInference Model/Pretrained Model 70.39 4.81 / 0.75 16.10 / 5.31 7.3 M An ultra-lightweight English recognition model trained based on the PP-OCRv4 recognition model, supporting English and numeric character recognition.   &gt; \u2757 The above section lists the **6 core models** that are primarily supported by the text recognition module. In total, the module supports **20 comprehensive models**, including multiple multilingual text recognition models. Below is the complete list of models:   \ud83d\udc49Details of the Model List  * PP-OCRv5 Multi-Scenario Models ModelModel Download Links Avg Accuracy for Chinese Recognition (%) Avg Accuracy for English Recognition (%) Avg Accuracy for Traditional Chinese Recognition (%) Avg Accuracy for Japanese Recognition (%) GPU Inference Time (ms)[Normal Mode / High-Performance Mode] CPU Inference Time (ms)[Normal Mode / High-Performance Mode] Model Storage Size (M) Introduction PP-OCRv5_server_recInference Model/Pretrained Model 86.38 64.70 93.29 60.35  8.45/2.36   122.69/122.69  81 M PP-OCRv5_rec is a next-generation text recognition model. It aims to efficiently and accurately support the recognition of four major languages\u2014Simplified Chinese, Traditional Chinese, English, and Japanese\u2014as well as complex text scenarios such as handwriting, vertical text, pinyin, and rare characters using a single model. While maintaining recognition performance, it balances inference speed and model robustness, providing efficient and accurate technical support for document understanding in various scenarios. PP-OCRv5_mobile_recInference Model/Pretrained Model 81.29 66.00 83.55 54.65  1.46/5.43   5.32/91.79  16 M   * Chinese Recognition Models ModelModel Download Link Recognition Avg Accuracy (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Size (M) Description PP-OCRv4_server_rec_docInference Model/Training Model 86.58 6.65 / 2.38 32.92 / 32.92 181 M PP-OCRv4_server_rec_doc is based on PP-OCRv4_server_rec, trained with a mix of more Chinese document data and PP-OCR training data, increasing the recognition capabilities for some Traditional Chinese, Japanese, and special characters, supporting recognition of over 15,000 characters. In addition to improving the document-related text recognition capabilities, it also enhances general text recognition capabilities. PP-OCRv4_mobile_recInference Model/Training Model 83.28 4.82 / 1.20 16.74 / 4.64 88 M PP-OCRv4's lightweight recognition model has high inference efficiency and can be deployed on various hardware, including edge devices. PP-OCRv4_server_rec Inference Model/Training Model 85.19  6.58 / 2.43 33.17 / 33.17 151 M PP-OCRv4's server-side model has high inference accuracy and can be deployed on various servers. PP-OCRv3_mobile_recInference Model/Training Model 75.43 5.87 / 1.19 9.07 / 4.28 138 M PP-OCRv3's lightweight recognition model has high inference efficiency and can be deployed on various hardware, including edge devices. ModelModel Download Link Recognition Avg Accuracy (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Size (M) Description ch_SVTRv2_recInference Model/Training Model 68.81 8.08 / 2.74 50.17 / 42.50 126 M SVTRv2 is a server-side text recognition model developed by the OpenOCR team at Fudan University's Vision and Learning Laboratory (FVL). It won first place in the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition Task, achieving a 6% improvement in end-to-end recognition accuracy compared to PP-OCRv4. ModelModel Download Link Recognition Avg Accuracy (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Size (M) Description ch_RepSVTR_recInference Model/Training Model 65.07 5.93 / 1.62 20.73 / 7.32 70 M RepSVTR is a mobile text recognition model based on SVTRv2, which won first place in the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition Task, achieving a 2.5% improvement in end-to-end recognition accuracy compared to PP-OCRv4, while maintaining the same inference speed.   * English Recognition Models ModelModel Download Link Recognition Avg Accuracy (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Size (M) Description en_PP-OCRv4_mobile_recInference Model/Training Model  70.39 4.81 / 0.75 16.10 / 5.31 66 M This ultra-lightweight English recognition model is trained based on the PP-OCRv4 recognition model, supporting English and digit recognition. en_PP-OCRv3_mobile_recInference Model/Training Model 70.69 5.44 / 0.75 8.65 / 5.57 85 M  This ultra-lightweight English recognition model is trained based on the PP-OCRv3 recognition model, supporting English and digit recognition.   * Multilingual Recognition Models ModelModel Download Link Recognition Avg Accuracy (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Size (M) Description korean_PP-OCRv3_mobile_recInference Model/Training Model 60.21 5.40 / 0.97 9.11 / 4.05 114 M This ultra-lightweight Korean recognition model is trained based on the PP-OCRv3 recognition model, supporting Korean and digit recognition. japan_PP-OCRv3_mobile_recInference Model/Training Model 45.69 5.70 / 1.02 8.48 / 4.07 120 M  This ultra-lightweight Japanese recognition model is trained based on the PP-OCRv3 recognition model, supporting Japanese and digit recognition. chinese_cht_PP-OCRv3_mobile_recInference Model/Training Model 82.06 5.90 / 1.28 9.28 / 4.34 152 M  This ultra-lightweight Traditional Chinese recognition model is trained based on the PP-OCRv3 recognition model, supporting Traditional Chinese and digit recognition. te_PP-OCRv3_mobile_recInference Model/Training Model 95.88 5.42 / 0.82 8.10 / 6.91 85 M  This ultra-lightweight Telugu recognition model is trained based on the PP-OCRv3 recognition model, supporting Telugu and digit recognition. ka_PP-OCRv3_mobile_recInference Model/Training Model 96.96 5.25 / 0.79 9.09 / 3.86 85 M  This ultra-lightweight Kannada recognition model is trained based on the PP-OCRv3 recognition model, supporting Kannada and digit recognition. ta_PP-OCRv3_mobile_recInference Model/Training Model 76.83 5.23 / 0.75 10.13 / 4.30 85 M  This ultra-lightweight Tamil recognition model is trained based on the PP-OCRv3 recognition model, supporting Tamil and digit recognition. latin_PP-OCRv3_mobile_recInference Model/Training Model 76.93 5.20 / 0.79 8.83 / 7.15 85 M This ultra-lightweight Latin recognition model is trained based on the PP-OCRv3 recognition model, supporting Latin and digit recognition. arabic_PP-OCRv3_mobile_recInference Model/Training Model 73.55 5.35 / 0.79 8.80 / 4.56 85 M This ultra-lightweight Arabic alphabet recognition model is trained based on the PP-OCRv3 recognition model, supporting Arabic letters and digit recognition. cyrillic_PP-OCRv3_mobile_recInference Model/Training Model 94.28 5.23 / 0.76 8.89 / 3.88 85 M   This ultra-lightweight Slavic alphabet recognition model is trained based on the PP-OCRv3 recognition model, supporting Slavic letters and digit recognition. devanagari_PP-OCRv3_mobile_recInference Model/Training Model 96.44 5.22 / 0.79 8.56 / 4.06 85 M This ultra-lightweight Devanagari alphabet recognition model is trained based on the PP-OCRv3 recognition model, supporting Devanagari letters and digit recognition. Layout Region Detection Module Models: ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Size (M) Description PP-DocLayout_plus-LInference Model/Training Model 83.2 34.6244 / 10.3945 510.57 / -  126.01 M This higher-precision layout region localization model is trained based on RT-DETR-L on a self-built dataset that includes Chinese and English papers, multi-column magazines, newspapers, PPTs, contracts, books, exam papers, research reports, ancient texts, Japanese documents, and vertical text documents. PP-DocLayout-LInference Model/Training Model 90.4 34.6244 / 10.3945 510.57 / - 123.76 M This high-precision layout region localization model is trained based on RT-DETR-L on a self-built dataset that includes Chinese and English papers, magazines, contracts, books, exam papers, and research reports. PP-DocLayout-MInference Model/Training Model 75.2 13.3259 / 4.8685 44.0680 / 44.0680 22.578 This layout region localization model balances accuracy and efficiency, trained based on PicoDet-L on a self-built dataset that includes Chinese and English papers, magazines, contracts, books, exam papers, and research reports. PP-DocLayout-SInference Model/Training Model 70.9 8.3008 / 2.3794 10.0623 / 9.9296 4.834 This highly efficient layout region localization model is trained based on PicoDet-S on a self-built dataset that includes Chinese and English papers, magazines, contracts, books, exam papers, and research reports.   &gt; \u2757 The above lists the 4 core models that are key to the layout detection module. The module supports a total of 12 complete models, including multiple pre-defined models for different categories. The complete model list is as follows:  \ud83d\udc49 Model List Details * Table Layout Detection Models ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Size (M) Description PicoDet_layout_1x_tableInference Model/Training Model 97.5 8.02 / 3.09 23.70 / 20.41 7.4 M This high-efficiency layout region localization model is trained based on PicoDet-1x on a self-built dataset, capable of locating tables as one type of region.   * 3-Class Layout Detection Models, Including Tables, Images, and Stamps ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Size (M) Description PicoDet-S_layout_3clsInference Model/Training Model 88.2 8.99 / 2.22 16.11 / 8.73 4.8 This high-efficiency layout region localization model is trained using a self-built dataset of Chinese and English papers, magazines, and research reports based on the lightweight model PicoDet-S. PicoDet-L_layout_3clsInference Model/Training Model 89.0 13.05 / 4.50 41.30 / 41.30 22.6 This layout region localization model balances efficiency and accuracy, trained using a self-built dataset of Chinese and English papers, magazines, and research reports based on the model PicoDet-L. RT-DETR-H_layout_3clsInference Model/Training Model 95.8 114.93 / 27.71 947.56 / 947.56 470.1 This high-precision layout region localization model is trained using a self-built dataset of Chinese and English papers, magazines, and research reports based on the model RT-DETR-H.   * 5-Class English Document Region Detection Models, Including Text, Titles, Tables, Images, and Lists ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Size (M) Description PicoDet_layout_1xInference Model/Training Model 97.8 9.03 / 3.10 25.82 / 20.70 7.4 This high-efficiency English document layout region localization model is trained on the PubLayNet dataset.   * 17-Class Region Detection Models, Including 17 Common Layout Categories: Title, Image, Text, Number, Abstract, Content, Chart Title, Formula, Table, Table Title, References, Document Title, Footnote, Header, Algorithm, Footer, and Stamp ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Size (M) Description PicoDet-S_layout_17clsInference Model/Training Model 87.4 9.11 / 2.12 15.42 / 9.12 4.8 This high-efficiency layout region localization model is trained using a self-built dataset of Chinese and English papers, magazines, and research reports based on the lightweight model PicoDet-S. PicoDet-L_layout_17clsInference Model/Training Model 89.0 13.50 / 4.69 43.32 / 43.32 22.6 This layout region localization model balances efficiency and accuracy, trained using a self-built dataset of Chinese and English papers, magazines, and research reports based on the model PicoDet-L. RT-DETR-H_layout_17clsInference Model/Training Model 98.3 115.29 / 104.09 995.27 / 995.27 470.2 This high-precision layout region localization model is trained using a self-built dataset of Chinese and English papers, magazines, and research reports based on the model RT-DETR-H. Text Image Unwarping Module Models (Optional): ModelModel Download Link MS-SSIM (%) Model Size (M) Description UVDocInference Model/Training Model 54.40 30.3 M High-precision text image correction model. Document Image Orientation Classification Module Models (Optional): ModelModel Download Link Top-1 Acc (%) GPU Inference Time (ms)[Regular Mode / High-Performance Mode] CPU Inference Time (ms)[Regular Mode / High-Performance Mode] Model Size (M) Description PP-LCNet_x1_0_doc_oriInference Model/Training Model 99.06 2.31 / 0.43 3.37 / 1.27 7 Based on PP-LCNet_x1_0, this document image classification model includes four categories: 0 degrees, 90 degrees, 180 degrees, and 270 degrees. Testing Environment Information: <ul> <li>Performance Testing Environment <ul> <li>Test Dataset:               <ul> <li>Document Image Orientation Classification Model: A self-built dataset by PaddleX, covering multiple scenarios including identification documents and documents, containing 1,000 images.</li> <li>Layout Region Detection Model: A self-built layout region detection dataset by PaddleOCR, containing 500 common document type images, including Chinese and English papers, magazines, contracts, books, exam papers, and research reports.</li> <li>Table Layout Detection Model: A self-built layout table region detection dataset by PaddleOCR, containing 7,835 images with tables in Chinese and English papers.</li> <li>3-Class Layout Detection Model: A self-built layout region detection dataset by PaddleOCR, containing 1,154 common document type images including Chinese and English papers, magazines, and research reports.</li> <li>5-Class English Document Region Detection Model: The evaluation dataset from PubLayNet, containing 11,245 images of English documents.</li> <li>17-Class Region Detection Model: A self-built layout region detection dataset by PaddleOCR, containing 892 common document type images including Chinese and English papers, magazines, and research reports.</li> <li>Table Structure Recognition Model: A high-difficulty Chinese table recognition dataset built internally by PaddleX.</li> <li>Table Cell Detection Model: An evaluation set built internally by PaddleX.</li> <li>Table Classification Model: An evaluation set built internally by PaddleX.</li> <li>Text Detection Model: A Chinese dataset built by PaddleOCR, covering multiple scenarios including street scenes, web images, documents, and handwriting, with 500 images for detection.</li> <li>Chinese Recognition Model: A Chinese dataset built by PaddleOCR, covering multiple scenarios including street scenes, web images, documents, and handwriting, with 11,000 images for text recognition.</li> <li>ch_SVTRv2_rec: The evaluation set from PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition Task A-list.</li> <li>ch_RepSVTR_rec: The evaluation set from PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition Task B-list.</li> <li>English Recognition Model: An English dataset built internally by PaddleX.</li> <li>Multilingual Recognition Model: A multilingual dataset built internally by PaddleX.</li> </ul> </li> <li>Hardware Configuration: <ul> <li>GPU: NVIDIA Tesla T4</li> <li>CPU: Intel Xeon Gold 6271C @ 2.60GHz</li> <li>Other Environment: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2</li> </ul> </li> </ul> </li> <li>Inference Mode Information</li> </ul> Mode GPU Configuration CPU Configuration Acceleration Technology Combination Regular Mode FP32 Precision / No TRT Acceleration FP32 Precision / 8 Threads PaddleInference High-Performance Mode Optimal combination of prior precision type and acceleration strategy FP32 Precision / 8 Threads Optimal backend (Paddle/OpenVINO/TRT, etc.) selected based on prior knowledge <p> If you prioritize model accuracy, please choose models with higher accuracy; if you care more about inference speed, please select models with faster inference speeds; if you focus on model storage size, please choose models with smaller storage volumes.</p>"},{"location":"en/version3.x/pipeline_usage/table_recognition_v2.html#2-quick-start","title":"2. Quick Start","text":"<p>Before using the table structure recognition V2 pipeline locally, please ensure that you have completed the installation of the wheel package according to the installation guide. After installation, you can experience it locally using the command line or Python integration.</p>"},{"location":"en/version3.x/pipeline_usage/table_recognition_v2.html#21-command-line-experience","title":"2.1 Command Line Experience","text":"<p>A single command allows you to quickly experience the effects of the table_recognition_v2 pipeline:</p> <pre><code>paddleocr table_recognition_v2 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/table_recognition_v2.jpg\n\n# Specify whether to use the document orientation classification model with --use_doc_orientation_classify\npaddleocr table_recognition_v2 -i ./table_recognition_v2.jpg --use_doc_orientation_classify True\n\n# Specify whether to use the text image unwarping module with --use_doc_unwarping\npaddleocr table_recognition_v2 -i ./table_recognition_v2.jpg --use_doc_unwarping True\n\n# Specify the device to use GPU for model inference with --device\npaddleocr table_recognition_v2 -i ./table_recognition_v2.jpg --device gpu\n</code></pre> More command line parameters are supported. Click to expand for detailed descriptions of the command line parameters Parameter Description Type Default Value <code>input</code> Data to be predicted, required. Local path to image files or PDF files: <code>/root/data/img.jpg</code>; as URL links, such as network URLs for image files or PDF files: example; as local directories, the directory must contain images to be predicted, such as local path: <code>/root/data/</code> (currently, predictions do not support directories that contain PDF files; the PDF file must be specified to the specific file path).  <code>str</code> <code>save_path</code> Specify the path to save the inference result file. If not set, the inference result will not be saved locally. <code>str</code> <code>layout_detection_model_name</code> Name of the layout detection model. If not set, the default model of the pipeline will be used. <code>str</code> <code>layout_detection_model_dir</code> Directory path of the layout detection model. If not set, the official model will be downloaded. <code>str</code> <code>table_classification_model_name</code> Name of the table classification model. If not set, the default model of the pipeline will be used. <code>str</code> <code>table_classification_model_dir</code> Directory path of the table classification model. If not set, the official model will be downloaded. <code>str</code> <code>wired_table_structure_recognition_model_name</code> Name of the wired table structure recognition model. If not set, the default model of the pipeline will be used. <code>str</code> <code>wired_table_structure_recognition_model_dir</code> Directory path of the wired table structure recognition model. If not set, the official model will be downloaded. <code>str</code> <code>wireless_table_structure_recognition_model_name</code> Name of the wireless table structure recognition model. If not set, the default model of the pipeline will be used. <code>str</code> <code>wireless_table_structure_recognition_model_dir</code> Directory path of the wireless table structure recognition model. If not set, the official model will be downloaded. <code>str</code> <code>wired_table_cells_detection_model_name</code> Name of the wired table cell detection model. If not set, the default model of the pipeline will be used. <code>str</code> <code>wired_table_cells_detection_model_dir</code> Directory path of the wired table cell detection model. If not set, the official model will be downloaded. <code>str</code> <code>wireless_table_cells_detection_model_name</code> Name of the wireless table cell detection model. If not set, the default model of the pipeline will be used. <code>str</code> <code>wireless_table_cells_detection_model_dir</code> Directory path of the wireless table cell detection model. If not set, the official model will be downloaded. <code>str</code> <code>doc_orientation_classify_model_name</code> Name of the document orientation classification model. If not set, the default model of the pipeline will be used. <code>str</code> <code>doc_orientation_classify_model_dir</code> Directory path of the document orientation classification model. If not set, the official model will be downloaded. <code>str</code> <code>doc_unwarping_model_name</code> Name of the text image unwarping model. If not set, the default model of the pipeline will be used. <code>str</code> <code>doc_unwarping_model_dir</code> Directory path of the text image unwarping model. If not set, the official model will be downloaded. <code>str</code> <code>text_detection_model_name</code> Name of the text detection model. If not set, the default model of the pipeline will be used. <code>str</code> <code>text_detection_model_dir</code> Directory path of the text detection model. If not set, the official model will be downloaded. <code>str</code> <code>text_det_limit_side_len</code> Image side length limit for text detection. Any integer greater than <code>0</code>. If not set, the default value initialized by the pipeline will be used, initialized to <code>960</code>.  <code>int</code> <code>text_det_limit_type</code> Type of the image side length limit for text detection. Supports <code>min</code> and <code>max</code>. <code>min</code> ensures that the shortest side of the image is not less than <code>det_limit_side_len</code>, while <code>max</code> ensures that the longest side of the image is not greater than <code>limit_side_len</code>. If not set, the default value initialized by the pipeline will be used, initialized to <code>max</code>.  <code>str</code> <code>text_det_thresh</code> Detection pixel threshold. In the output probability map, only pixels with a score greater than this threshold will be considered text pixels. Any floating-point number greater than <code>0</code>. If not set, the default value initialized by the pipeline will be used, which is <code>0.3</code>.  <code>float</code> <code>text_det_box_thresh</code> Detection box threshold. When the average score of all pixels within the detection result box is greater than this threshold, the result is considered a text area. Any floating-point number greater than <code>0</code>. If not set, the default value initialized by the pipeline will be used, which is <code>0.6</code>.  <code>float</code> <code>text_det_unclip_ratio</code> Text detection expansion coefficient. This method expands the text area; the larger this value, the larger the expanded area. Any floating-point number greater than <code>0</code>. If not set, the default value initialized by the pipeline will be used, which is <code>2.0</code>.  <code>float</code> <code>text_recognition_model_name</code> Name of the text recognition model. If not set, the default model of the pipeline will be used. <code>str</code> <code>text_recognition_model_dir</code> Directory path of the text recognition model. If not set, the official model will be downloaded. <code>str</code> <code>text_recognition_batch_size</code> Batch size for the text recognition model. If not set, the default batch size will be set to <code>1</code>. <code>int</code> <code>text_rec_score_thresh</code> Text recognition threshold. Text results with a score greater than this threshold will be retained. Any floating-point number greater than <code>0</code>. If not set, the default value initialized by the pipeline will be used, which is <code>0.0</code>. That is, no threshold is set.  <code>float</code> <code>use_doc_orientation_classify</code> Whether to load and use the document orientation classification module. If not set, the default value initialized by the pipeline will be used, initialized to <code>True</code>. <code>bool</code> <code>use_doc_unwarping</code> Whether to load and use the text image unwarping module. If not set, the default value initialized by the pipeline will be used, initialized to <code>True</code>. <code>bool</code> <code>use_layout_detection</code> Whether to load and use the layout detection module. If not set, the default value initialized by the pipeline will be used, initialized to <code>True</code>. <code>bool</code> <code>use_ocr_model</code> Whether to load and use the OCR module. If not set, the default value initialized by the pipeline will be used, initialized to <code>True</code>. <code>bool</code> <code>device</code> The device used for inference. Supports specifying a specific card number: <ul> <li>CPU: For example, <code>cpu</code> indicates using CPU for inference;</li> <li>GPU: For example, <code>gpu:0</code> indicates using the first GPU for inference;</li> <li>NPU: For example, <code>npu:0</code> indicates using the first NPU for inference;</li> <li>XPU: For example, <code>xpu:0</code> indicates using the first XPU for inference;</li> <li>MLU: For example, <code>mlu:0</code> indicates using the first MLU for inference;</li> <li>DCU: For example, <code>dcu:0</code> indicates using the first DCU for inference;</li> </ul>If not set, the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.  <code>str</code> <code>enable_hpi</code> Whether to enable high-performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Computation precision, such as fp32, fp16. <code>str</code> <code>fp32</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set. <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads to use for inference on the CPU. <code>int</code> <code>8</code> <code>paddlex_config</code> Path to PaddleX pipeline configuration file. <code>str</code> <p></p> <p>To run inference on the example image, you can use the following command:</p> <pre><code>paddleocr table_recognition_v2 -i ./table_recognition_v2.jpg --use_doc_orientation_classify False --use_doc_unwarping False\n</code></pre> <p>The running results will be printed to the terminal. The default configuration of the table_recognition_v2 pipeline's running results is as follows:</p> <pre><code>{'res': {'input_path': 'table_recognition_v2.jpg', 'page_index': None, 'model_settings': {'use_doc_preprocessor': False, 'use_layout_detection': True, 'use_ocr_model': True}, 'layout_det_res': {'input_path': None, 'page_index': None, 'boxes': [{'cls_id': 8, 'label': 'table', 'score': 0.86655592918396, 'coordinate': [0.0125130415, 0.41920784, 1281.3737, 585.3884]}]}, 'overall_ocr_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_preprocessor': False, 'use_textline_orientation': False}, 'dt_polys': array([[[   9,   21],\n        ...,\n        [   9,   59]],\n\n       ...,\n\n       [[1046,  536],\n        ...,\n        [1046,  573]]], dtype=int16), 'text_det_params': {'limit_side_len': 960, 'limit_type': 'max', 'thresh': 0.3, 'box_thresh': 0.6, 'unclip_ratio': 2.0}, 'text_type': 'general', 'textline_orientation_angles': array([-1, ..., -1]), 'text_rec_score_thresh': 0, 'rec_texts': ['\u200b\u90e8\u95e8\u200b', '\u200b\u62a5\u9500\u200b\u4eba\u200b', '\u200b\u62a5\u9500\u200b\u4e8b\u7531\u200b', '\u200b\u6279\u51c6\u200b\u4eba\u200b\uff1a', '\u200b\u5355\u636e\u200b', '\u200b\u5f20\u200b', '\u200b\u5408\u8ba1\u200b\u91d1\u989d\u200b', '\u200b\u5143\u200b', '\u200b\u8f66\u8d39\u200b\u7968\u200b', '\u200b\u5176\u200b', '\u200b\u706b\u8f66\u200b\u8d39\u7968\u200b', '\u200b\u98de\u673a\u7968\u200b', '\u200b\u4e2d\u200b', '\u200b\u65c5\u200b\u4f4f\u5bbf\u8d39\u200b', '\u200b\u5176\u4ed6\u200b', '\u200b\u8865\u8d34\u200b'], 'rec_scores': array([0.99958128, ..., 0.99317062]), 'rec_polys': array([[[   9,   21],\n        ...,\n        [   9,   59]],\n\n       ...,\n\n       [[1046,  536],\n        ...,\n        [1046,  573]]], dtype=int16), 'rec_boxes': array([[   9, ...,   59],\n       ...,\n       [1046, ...,  573]], dtype=int16)}, 'table_res_list': [{'cell_box_list': [array([ 0.13052222, ..., 73.08310249]), array([104.43082511, ...,  73.27777413]), array([319.39041221, ...,  73.30439308]), array([424.2436837 , ...,  73.44736794]), array([580.75836265, ...,  73.24003914]), array([723.04370201, ...,  73.22717598]), array([984.67315757, ...,  73.20420387]), array([1.25130415e-02, ..., 5.85419208e+02]), array([984.37072837, ..., 137.02281502]), array([984.26586998, ..., 201.22290352]), array([984.24017417, ..., 585.30775765]), array([1039.90606773, ...,  265.44664314]), array([1039.69549644, ...,  329.30540779]), array([1039.66546714, ...,  393.57319954]), array([1039.5122689 , ...,  457.74644783]), array([1039.55535972, ...,  521.73030403]), array([1039.58612144, ...,  585.09468392])], 'pred_html': '&lt;html&gt;&lt;body&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;\u200b\u90e8\u95e8\u200b&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;\u200b\u62a5\u9500\u200b\u4eba\u200b&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;\u200b\u62a5\u9500\u200b\u4e8b\u7531\u200b&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td colspan=\"2\"&gt;\u200b\u6279\u51c6\u200b\u4eba\u200b\uff1a&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"6\" rowspan=\"8\"&gt;&lt;/td&gt;&lt;td colspan=\"2\"&gt;\u200b\u5355\u636e\u200b \u200b\u5f20\u200b&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=\"2\"&gt;\u200b\u5408\u8ba1\u200b\u91d1\u989d\u200b \u200b\u5143\u200b&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=\"6\"&gt;\u200b\u5176\u200b \u200b\u4e2d\u200b&lt;/td&gt;&lt;td&gt;\u200b\u8f66\u8d39\u200b\u7968\u200b&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;\u200b\u706b\u8f66\u200b\u8d39\u7968\u200b&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;\u200b\u98de\u673a\u7968\u200b&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;\u200b\u65c5\u200b\u4f4f\u5bbf\u8d39\u200b&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;\u200b\u5176\u4ed6\u200b&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;\u200b\u8865\u8d34\u200b&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;', 'table_ocr_pred': {'rec_polys': array([[[   9,   21],\n        ...,\n        [   9,   59]],\n\n       ...,\n\n       [[1046,  536],\n        ...,\n        [1046,  573]]], dtype=int16), 'rec_texts': ['\u200b\u90e8\u95e8\u200b', '\u200b\u62a5\u9500\u200b\u4eba\u200b', '\u200b\u62a5\u9500\u200b\u4e8b\u7531\u200b', '\u200b\u6279\u51c6\u200b\u4eba\u200b\uff1a', '\u200b\u5355\u636e\u200b', '\u200b\u5f20\u200b', '\u200b\u5408\u8ba1\u200b\u91d1\u989d\u200b', '\u200b\u5143\u200b', '\u200b\u8f66\u8d39\u200b\u7968\u200b', '\u200b\u5176\u200b', '\u200b\u706b\u8f66\u200b\u8d39\u7968\u200b', '\u200b\u98de\u673a\u7968\u200b', '\u200b\u4e2d\u200b', '\u200b\u65c5\u200b\u4f4f\u5bbf\u8d39\u200b', '\u200b\u5176\u4ed6\u200b', '\u200b\u8865\u8d34\u200b'], 'rec_scores': array([0.99958128, ..., 0.99317062]), 'rec_boxes': array([[   9, ...,   59],\n       ...,\n       [1046, ...,  573]], dtype=int16)}}]}}\n</code></pre> <p>The visualization results are saved under <code>save_path</code>, and the visualization results are as follows:</p> <p></p>"},{"location":"en/version3.x/pipeline_usage/table_recognition_v2.html#22-python-script-integration","title":"2.2 Python Script Integration","text":"<p>The command line method is designed for quick experience and viewing effects. Generally, in a project, it is often necessary to integrate through code. You can complete quick inference of the pipeline with just a few lines of code. The inference code is as follows:</p> <pre><code>from paddleocr import TableRecognitionPipelineV2\n\npipeline = TableRecognitionPipelineV2()\n# ocr = TableRecognitionPipelineV2(use_doc_orientation_classify=True) # Specify whether to use the document orientation classification model with use_doc_orientation_classify\n# ocr = TableRecognitionPipelineV2(use_doc_unwarping=True) # Specify whether to use the text image unwarping module with use_doc_unwarping\n# ocr = TableRecognitionPipelineV2(device=\"gpu\") # Specify the device to use GPU for model inference\noutput = pipeline.predict(\"./table_recognition_v2.jpg\")\nfor res in output:\n    res.print() ## Print the predicted structured output\n    res.save_to_img(\"./output/\")\n    res.save_to_xlsx(\"./output/\")\n    res.save_to_html(\"./output/\")\n    res.save_to_json(\"./output/\")\n</code></pre> <p>In the above Python script, the following steps are performed:</p> <p>(1) Instantiate the general table recognition V2 pipeline object using <code>TableRecognitionPipelineV2()</code>. The specific parameter descriptions are as follows:</p> Parameter Description Type Default Value <code>layout_detection_model_name</code> Name of the layout detection model. If set to <code>None</code>, the default model of the pipeline will be used. <code>str</code> <code>None</code> <code>layout_detection_model_dir</code> Directory path of the layout detection model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>table_classification_model_name</code> Name of the table classification model. If set to <code>None</code>, the default model of the pipeline will be used. <code>str</code> <code>None</code> <code>table_classification_model_dir</code> Directory path of the table classification model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>wired_table_structure_recognition_model_name</code> Name of the wired table structure recognition model. If set to <code>None</code>, the default model of the pipeline will be used. <code>str</code> <code>None</code> <code>wired_table_structure_recognition_model_dir</code> Directory path of the wired table structure recognition model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>wireless_table_structure_recognition_model_name</code> Name of the wireless table structure recognition model. If set to <code>None</code>, the default model of the pipeline will be used. <code>str</code> <code>None</code> <code>wireless_table_structure_recognition_model_dir</code> Directory path of the wireless table structure recognition model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>wired_table_cells_detection_model_name</code> Name of the wired table cell detection model. If set to <code>None</code>, the default model of the pipeline will be used. <code>str</code> <code>None</code> <code>wired_table_cells_detection_model_dir</code> Directory path of the wired table cell detection model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>wireless_table_cells_detection_model_name</code> Name of the wireless table cell detection model. If set to <code>None</code>, the default model of the pipeline will be used. <code>str</code> <code>None</code> <code>wireless_table_cells_detection_model_dir</code> Directory path of the wireless table cell detection model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>doc_orientation_classify_model_name</code> Name of the document orientation classification model. If set to <code>None</code>, the default model of the pipeline will be used. <code>str</code> <code>None</code> <code>doc_orientation_classify_model_dir</code> Directory path of the document orientation classification model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>doc_unwarping_model_name</code> Name of the text image unwarping model. If set to <code>None</code>, the default model of the pipeline will be used. <code>str</code> <code>None</code> <code>doc_unwarping_model_dir</code> Directory path of the text image unwarping model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>text_detection_model_name</code> Name of the text detection model. If set to <code>None</code>, the default model of the pipeline will be used. <code>str</code> <code>None</code> <code>text_detection_model_dir</code> Directory path of the text detection model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>text_det_limit_side_len</code> Image side length limit for text detection. <ul> <li>int: Any integer greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the default value initialized by the pipeline will be used, initialized to <code>960</code>.</li> </ul> <code>int</code> <code>None</code> <code>text_det_limit_type</code> Type of the image side length limit for text detection. <ul> <li>str: Supports <code>min</code> and <code>max</code>. <code>min</code> ensures that the shortest side of the image is not less than <code>det_limit_side_len</code>, while <code>max</code> ensures that the longest side of the image is not greater than <code>limit_side_len</code>;</li> <li>None: If set to <code>None</code>, the default value initialized by the pipeline will be used, initialized to <code>max</code>.</li> </ul> <code>str</code> <code>None</code> <code>text_det_thresh</code> Detection pixel threshold. In the output probability map, only pixels with a score greater than this threshold will be considered text pixels. <ul> <li>float: Any floating-point number greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the default value initialized by the pipeline will be used, which is <code>0.3</code>.</li> </ul> <code>float</code> <code>None</code> <code>text_det_box_thresh</code> Detection box threshold. When the average score of all pixels within the detection result box is greater than this threshold, the result is considered a text area. <ul> <li>float: Any floating-point number greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the default value initialized by the pipeline will be used, which is <code>0.6</code>.</li> </ul> <code>float</code> <code>None</code> <code>text_det_unclip_ratio</code> Text detection expansion coefficient. This method expands the text area; the larger this value, the larger the expanded area. <ul> <li>float: Any floating-point number greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the default value initialized by the pipeline will be used, which is <code>2.0</code>.</li> </ul> <code>float</code> <code>None</code> <code>text_recognition_model_name</code> Name of the text recognition model. If set to <code>None</code>, the default model of the pipeline will be used. <code>str</code> <code>None</code> <code>text_recognition_model_dir</code> Directory path of the text recognition model. If set to <code>None</code>, the official model will be downloaded. <code>str</code> <code>None</code> <code>text_recognition_batch_size</code> Batch size for the text recognition model. If set to <code>None</code>, the default batch size will be set to <code>1</code>. <code>int</code> <code>None</code> <code>text_rec_score_thresh</code> Text recognition threshold. Text results with a score greater than this threshold will be retained. <ul> <li>float: Any floating-point number greater than <code>0</code>;</li> <li>None: If set to <code>None</code>, the default value initialized by the pipeline will be used, which is <code>0.0</code>. That is, no threshold is set.  <code>float</code> <code>None</code> <code>use_doc_orientation_classify</code> Whether to load and use the document orientation classification module. If set to <code>None</code>, the default value initialized by the pipeline will be used, initialized to <code>True</code>. <code>bool</code> <code>None</code> <code>use_doc_unwarping</code> Whether to load and use the text image unwarping module. If set to <code>None</code>, the default value initialized by the pipeline will be used, initialized to <code>True</code>. <code>bool</code> <code>None</code> <code>use_layout_detection</code> Whether to load and use the layout detection module. If set to <code>None</code>, the default value initialized by the pipeline will be used, initialized to <code>True</code>. <code>bool</code> <code>None</code> <code>use_ocr_model</code> Whether to load and use the OCR module. If set to <code>None</code>, the default value initialized by the pipeline will be used, initialized to <code>True</code>. <code>bool</code> <code>None</code> <code>device</code> The device used for inference. Supports specifying a specific card number: <ul> <li>CPU: For example, <code>cpu</code> indicates using CPU for inference;</li> <li>GPU: For example, <code>gpu:0</code> indicates using the first GPU for inference;</li> <li>NPU: For example, <code>npu:0</code> indicates using the first NPU for inference;</li> <li>XPU: For example, <code>xpu:0</code> indicates using the first XPU for inference;</li> <li>MLU: For example, <code>mlu:0</code> indicates using the first MLU for inference;</li> <li>DCU: For example, <code>dcu:0</code> indicates using the first DCU for inference;</li> <li>None: If set to <code>None</code>, the pipeline initialized value for this parameter will be used. During initialization, the local GPU device 0 will be preferred; if unavailable, the CPU device will be used.</li> </ul> <code>str</code> <code>None</code> <code>enable_hpi</code> Whether to enable high-performance inference. <code>bool</code> <code>False</code> <code>use_tensorrt</code> Whether to use the Paddle Inference TensorRT subgraph engine. For Paddle with CUDA version 11.8, the compatible TensorRT version is 8.x (x&gt;=6), and it is recommended to install TensorRT 8.6.1.6. For Paddle with CUDA version 12.6, the compatible TensorRT version is 10.x (x&gt;=5), and it is recommended to install TensorRT 10.5.0.18.  <code>bool</code> <code>False</code> <code>precision</code> Computation precision, such as fp32, fp16. <code>str</code> <code>\"fp32\"</code> <code>enable_mkldnn</code> Whether to enable MKL-DNN acceleration for inference. If MKL-DNN is unavailable or the model does not support it, acceleration will not be used even if this flag is set. <code>bool</code> <code>True</code> <code>mkldnn_cache_capacity</code>  MKL-DNN cache capacity.  <code>int</code> <code>10</code> <code>cpu_threads</code> Number of threads to use for inference on the CPU. <code>int</code> <code>8</code> <code>paddlex_config</code> Path to PaddleX pipeline configuration file. <code>str</code> <code>None</code> <p>(2) Call the <code>predict()</code> method of the general table recognition V2 pipeline object to perform inference prediction, which returns a result list.</p> <p>Additionally, the pipeline also provides the <code>predict_iter()</code> method. Both methods accept the same parameters and return results in the same way; the difference is that <code>predict_iter()</code> returns a <code>generator</code>, allowing for gradual processing and retrieval of prediction results, suitable for handling large datasets or for scenarios where memory savings are desired. You can choose to use either method based on your actual needs.</p> <p>The parameters and descriptions of the <code>predict()</code> method are as follows:</p> Parameter Description Type Default Value <code>input</code> Data to be predicted, supports multiple input types, required. <ul> <li>Python Var: For example, image data represented as <code>numpy.ndarray</code>;</li> <li>str: Local path to image files or PDF files: <code>/root/data/img.jpg</code>; as URL links, such as network URLs for image files or PDF files: example; as local directories, the directory must contain images to be predicted, such as local path: <code>/root/data/</code> (currently, predictions do not support directories that contain PDF files; the PDF file must be specified to the specific file path);</li> <li>List: The elements of the list must be of the above types, such as <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code>.</li> </ul> <code>Python Var|str|list</code> <code>use_doc_orientation_classify</code> Whether to use the document orientation classification module during inference. <code>bool</code> <code>None</code> <code>use_doc_unwarping</code> Whether to use the text image unwarping module during inference. <code>bool</code> <code>None</code> <code>use_layout_detection</code> Whether to use the layout detection module during inference. <code>bool</code> <code>None</code> <code>use_ocr_model</code> Whether to use the <code>ocr</code> model during inference. <code>bool</code> <code>None</code> <code>text_det_limit_side_len</code> Same as the parameters during instantiation. <code>int</code> <code>None</code> <code>text_det_limit_type</code> Same as the parameters during instantiation. <code>str</code> <code>None</code> <code>text_det_thresh</code> Same as the parameters during instantiation. <code>float</code> <code>None</code> <code>text_det_box_thresh</code> Same as the parameters during instantiation. <code>float</code> <code>None</code> <code>text_det_unclip_ratio</code> Same as the parameters during instantiation. <code>float</code> <code>None</code> <code>text_rec_score_thresh</code> Same as the parameters during instantiation. <code>float</code> <code>None</code> <code>use_e2e_wired_table_rec_model</code> Whether to use the wired end-to-end table recognition mode during inference. <code>bool</code> <code>False</code> <code>use_e2e_wireless_table_rec_model</code> Whether to use the wireless end-to-end table recognition mode during inference. <code>bool</code> <code>False</code> <code>use_wired_table_cells_trans_to_html</code> Whether to use the wired table cell detection result direct-to-HTML mode during inference. If enabled, it directly constructs the HTML based on the geometric relationships of the wired table cell detection results. <code>bool</code> <code>False</code> <code>use_wireless_table_cells_trans_to_html</code> Whether to use the wireless table cell detection result direct-to-HTML mode during inference. If enabled, it directly constructs the HTML based on the geometric relationships of the wireless table cell detection results. <code>bool</code> <code>False</code> <code>use_table_orientation_classify</code> Whether to use the table orientation classification mode during inference. If enabled, it can correct the direction and correctly complete table recognition when the table in the image has 90/180/270-degree rotation. <code>bool</code> <code>True</code> <code>use_ocr_results_with_table_cells</code> Whether to use the cell-split OCR mode during inference. If enabled, it will split and re-recognize OCR detection results based on the cell prediction results to avoid missing text. <code>bool</code> <code>True</code> <p>(3) Process the prediction results. The prediction result for each sample is a corresponding Result object, which supports printing, saving as an image, saving as an <code>xlsx</code> file, saving as an <code>HTML</code> file, and saving as a <code>json</code> file:</p> Method Description Parameter Type Parameter Description Default Value <code>print()</code> Print results to the terminal <code>format_json</code> <code>bool</code> Whether to format the output content using <code>JSON</code> indentation. <code>True</code> <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data, making it more readable. Effective only when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> keeps the original characters. Effective only when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_json()</code> Save results as a json format file <code>save_path</code> <code>str</code> The path to save the file. When it is a directory, the saved file will be named the same as the input file type. None <code>indent</code> <code>int</code> Specify the indentation level to beautify the output <code>JSON</code> data, making it more readable. Effective only when <code>format_json</code> is <code>True</code>. 4 <code>ensure_ascii</code> <code>bool</code> Control whether to escape non-<code>ASCII</code> characters to <code>Unicode</code>. When set to <code>True</code>, all non-<code>ASCII</code> characters will be escaped; <code>False</code> keeps the original characters. Effective only when <code>format_json</code> is <code>True</code>. <code>False</code> <code>save_to_img()</code> Save results as an image format file <code>save_path</code> <code>str</code> The path to save the file, supporting directory or file path. None <code>save_to_xlsx()</code> Save results as an xlsx format file <code>save_path</code> <code>str</code> The path to save the file, supporting directory or file path. None <code>save_to_html()</code> Save results as an html format file <code>save_path</code> <code>str</code> The path to save the file, supporting directory or file path. None <ul> <li> <p>Calling the <code>print()</code> method will print the results to the terminal. The content printed to the terminal is explained as follows:</p> <ul> <li> <p><code>input_path</code>: <code>(str)</code> The input path of the image to be predicted.</p> </li> <li> <p><code>page_index</code>: <code>(Union[int, None])</code> If the input is a PDF file, this indicates which page of the PDF it is; otherwise, it is <code>None</code>.</p> </li> <li> <p><code>model_settings</code>: <code>(Dict[str, bool])</code> Configuration parameters required by the pipeline.</p> <ul> <li><code>use_doc_preprocessor</code>: <code>(bool)</code> Controls whether to enable the document preprocessing sub-pipeline.</li> <li><code>use_layout_detection</code>: <code>(bool)</code> Controls whether to enable the layout area detection sub-pipeline.</li> <li><code>use_ocr_model</code>: <code>(bool)</code> Controls whether to enable the OCR sub-pipeline.<ul> <li><code>layout_det_res</code>: <code>(Dict[str, Union[List[numpy.ndarray], List[float]]])</code> Output results of the layout detection sub-module. Only exists when <code>use_layout_detection=True</code>.</li> </ul> </li> <li><code>input_path</code>: <code>(Union[str, None])</code> The image path accepted by the layout detection area module, saved as <code>None</code> when the input is <code>numpy.ndarray</code>.</li> <li><code>page_index</code>: <code>(Union[int, None])</code> If the input is a PDF file, this indicates which page of the PDF it is; otherwise, it is <code>None</code>.</li> <li><code>boxes</code>: <code>(List[Dict])</code> List of detection boxes for the layout seal area. Each element in the list contains the following fields:<ul> <li><code>cls_id</code>: <code>(int)</code> The category ID of the detection box.</li> <li><code>score</code>: <code>(float)</code> The confidence of the detection box.</li> <li><code>coordinate</code>: <code>(List[float])</code> The coordinates of the four vertices of the detection box, in the order of x1, y1, x2, y2, indicating the x coordinate and y coordinate of the top left corner and the bottom right corner.</li> <li><code>doc_preprocessor_res</code>: <code>(Dict[str, Union[str, Dict[str, bool], int]])</code> Output results of the document preprocessing sub-pipeline. Only exists when <code>use_doc_preprocessor=True</code>.</li> </ul> </li> <li><code>input_path</code>: <code>(Union[str, None])</code> The image path accepted by the image preprocessing sub-pipeline, saved as <code>None</code> when the input is <code>numpy.ndarray</code>.</li> <li><code>model_settings</code>: <code>(Dict)</code> Configuration parameters for the preprocessing sub-pipeline.<ul> <li><code>use_doc_orientation_classify</code>: <code>(bool)</code> Controls whether to enable document orientation classification.</li> <li><code>use_doc_unwarping</code>: <code>(bool)</code> Controls whether to enable text image unwarping.</li> </ul> </li> <li><code>angle</code>: <code>(int)</code> Prediction result of the document orientation classification. When enabled, the values are [0,1,2,3], corresponding to [0\u00b0,90\u00b0,180\u00b0,270\u00b0]; when not enabled, it is -1.</li> </ul> </li> <li> <p><code>dt_polys</code>: <code>(List[numpy.ndarray])</code> List of polygon boxes for text detection. Each detection box is represented by a numpy array consisting of 4 vertex coordinates, with an array shape of (4, 2) and data type of int16.</p> </li> <li> <p><code>dt_scores</code>: <code>(List[float])</code> List of confidence scores for the text detection boxes.</p> </li> <li> <p><code>text_det_params</code>: <code>(Dict[str, Dict[str, int, float]])</code> Configuration parameters for the text detection module.</p> <ul> <li><code>limit_side_len</code>: <code>(int)</code> Side length limit value during image preprocessing.</li> <li><code>limit_type</code>: <code>(str)</code> Processing method for side length limits.</li> <li><code>thresh</code>: <code>(float)</code> Confidence threshold for classifying text pixels.</li> <li><code>box_thresh</code>: <code>(float)</code> Confidence threshold for text detection boxes.</li> <li><code>unclip_ratio</code>: <code>(float)</code> Expansion coefficient for text detection boxes.</li> <li><code>text_type</code>: <code>(str)</code> The type of text detection, currently fixed as \"general\".</li> </ul> </li> <li> <p><code>text_rec_score_thresh</code>: <code>(float)</code> Filtering threshold for text recognition results.</p> </li> <li> <p><code>rec_texts</code>: <code>(List[str])</code> List of text recognition results, including only the text with confidence exceeding <code>text_rec_score_thresh</code>.</p> </li> <li> <p><code>rec_scores</code>: <code>(List[float])</code> List of confidence scores for text recognition, filtered by <code>text_rec_score_thresh</code>.</p> </li> <li> <p><code>rec_polys</code>: <code>(List[numpy.ndarray])</code> List of text detection boxes that have been filtered by confidence, with the same format as <code>dt_polys</code>.</p> </li> <li> <p><code>rec_boxes</code>: <code>(numpy.ndarray)</code> Array of rectangular bounding boxes for detection boxes, with a shape of (n, 4) and dtype of int16. Each row represents the coordinates of a rectangular box as [x_min, y_min, x_max, y_max], where (x_min, y_min) is the top left corner coordinate, and (x_max, y_max) is the bottom right corner coordinate.</p> </li> </ul> </li> <li> <p>Calling the <code>save_to_json()</code> method will save the above content to the specified <code>save_path</code>. If a directory is specified, the saved path will be <code>save_path/{your_img_basename}_res.json</code>; if a file is specified, it will be saved directly to that file. Since json files do not support saving numpy arrays, the <code>numpy.array</code> types will be converted to list format.</p> </li> <li>Calling the <code>save_to_img()</code> method will save the visualization results to the specified <code>save_path</code>. If a directory is specified, the saved path will be <code>save_path/{your_img_basename}_ocr_res_img.{your_img_extension}</code>; if a file is specified, it will be saved directly to that file. (The pipeline usually contains many result images, so it is not recommended to specify a specific file path directly, as multiple images will be overwritten and only the last image will be retained.)</li> <li>Calling the <code>save_to_html()</code> method will save the above content to the specified <code>save_path</code>. If a directory is specified, the saved path will be <code>save_path/{your_img_basename}_table_1.html</code>; if a file is specified, it will be saved directly to that file. In the general table recognition V2 pipeline, the HTML format of the table in the image will be written to the specified HTML file.</li> <li> <p>Calling the <code>save_to_xlsx()</code> method will save the above content to the specified <code>save_path</code>. If a directory is specified, the saved path will be <code>save_path/{your_img_basename}_res.xlsx</code>; if a file is specified, it will be saved directly to that file. In the general table recognition V2 pipeline, the Excel format of the table in the image will be written to the specified xlsx file.</p> </li> <li> <p>Additionally, it also supports obtaining visualization images and prediction results through attributes, as follows:</p> </li> </ul> Attribute Description <code>json</code> Get the prediction results in <code>json</code> format <code>img</code> Get visualization images in <code>dict</code> format <ul> <li>The prediction result obtained by the <code>json</code> attribute is of dict type, and the relevant content is consistent with the content saved by calling the <code>save_to_json()</code> method.</li> <li>The prediction result returned by the <code>img</code> attribute is a dictionary type data. The keys are <code>table_res_img</code>, <code>ocr_res_img</code>, <code>layout_res_img</code>, and <code>preprocessed_img</code>, and the corresponding values are four <code>Image.Image</code> objects, in the order of: visualization image of the table recognition result, visualization image of the OCR result, visualization image of the layout area detection result, and visualization image of the image preprocessing. If a certain sub-module is not used, the corresponding result image will not be included in the dictionary.</li> </ul>"},{"location":"en/version3.x/pipeline_usage/table_recognition_v2.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the model can meet your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <p>If you need to apply the model directly in your Python project, you can refer to the example code in 2.2 Python Script Integration.</p> <p>Additionally, PaddleOCR provides two other deployment methods, which are described in detail below:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have strict performance criteria (especially response speed) for deployment strategies to ensure efficient system operation and smooth user experience. To address this, PaddleOCR provides high-performance inference capabilities aimed at optimizing model inference and preprocessing to significantly speed up the end-to-end process. For detailed high-performance inference procedures, please refer to High-Performance Inference.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common form of deployment in actual production environments. By encapsulating inference functions as services, clients can access these services via network requests to obtain inference results. For detailed service-oriented deployment procedures, please refer to Serving.</p> <p>Below is the API reference for basic service-oriented deployment and examples of multilingual service calls:</p> API Reference <p>The main operations provided by the service are as follows:</p> <ul> <li>HTTP request method is POST.</li> <li>Both request and response bodies are JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the properties of the response body are as follows:</li> </ul> Name Type Meaning <code>logId</code> <code>string</code> Request UUID. <code>errorCode</code> <code>integer</code> Error code. Fixed to <code>0</code>. <code>errorMsg</code> <code>string</code> Error description. Fixed to <code>\"Success\"</code>. <code>result</code> <code>object</code> Operation result. <ul> <li>When the request processing is unsuccessful, the properties of the response body are as follows:</li> </ul> Name Type Meaning <code>logId</code> <code>string</code> Request UUID. <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error description. <p>The main operation provided by the service is as follows:</p> <ul> <li><code>infer</code></li> </ul> <p>Locate and identify tables in the image.</p> <p><code>POST /table-recognition</code></p> <ul> <li>The properties of the request body are as follows:</li> </ul> Name Type Meaning Required <code>file</code> <code>string</code> URL of an accessible image file or PDF file on the server, or the Base64 encoded result of the content of the above types of files. By default, for PDF files with more than 10 pages, only the first 10 pages will be processed. To lift the page limit, please add the following configuration in the model configuration file: <pre><code>Serving:\n  extra:\n    max_num_input_imgs: null\n</code></pre> Yes <code>fileType</code> <code>integer</code> | <code>null</code> File type. <code>0</code> represents a PDF file, <code>1</code> represents an image file. If this property is not present in the request body, the file type will be inferred from the URL. No <code>useDocOrientationClassify</code> <code>boolean</code> | <code>null</code> Please refer to the <code>use_doc_orientation_classify</code> parameter description in the <code>predict</code> method of the model object. No <code>useDocUnwarping</code> <code>boolean</code> | <code>null</code> Please refer to the <code>use_doc_unwarping</code> parameter description in the <code>predict</code> method of the model object. No <code>useLayoutDetection</code> <code>boolean</code> | <code>null</code> Please refer to the <code>use_layout_detection</code> parameter description in the <code>predict</code> method of the model object. No <code>useOcrModel</code> <code>boolean</code> | <code>null</code> Please refer to the <code>use_ocr_model</code> parameter description in the <code>predict</code> method of the model object. No <code>textDetLimitSideLen</code> <code>integer</code> | <code>null</code> Please refer to the <code>text_det_limit_side_len</code> parameter description in the <code>predict</code> method of the model object. No <code>textDetLimitType</code> <code>string</code> | <code>null</code> Please refer to the <code>text_det_limit_type</code> parameter description in the <code>predict</code> method of the model object. No <code>textDetThresh</code> <code>number</code> | <code>null</code> Please refer to the <code>text_det_thresh</code> parameter description in the <code>predict</code> method of the model object. No <code>textDetBoxThresh</code> <code>number</code> | <code>null</code> Please refer to the <code>text_det_box_thresh</code> parameter description in the <code>predict</code> method of the model object. No <code>textDetUnclipRatio</code> <code>number</code> | <code>null</code> Please refer to the <code>text_det_unclip_ratio</code> parameter description in the <code>predict</code> method of the model object. No <code>textRecScoreThresh</code> <code>number</code> | <code>null</code> Please refer to the <code>text_rec_score_thresh</code> parameter description in the <code>predict</code> method of the model object. No <code>useTableCellsOcrResults</code> <code>boolean</code> Please refer to the <code>use_table_cells_ocr_results</code> parameter description in the <code>predict</code> method of the model object. No <code>useE2eWiredTableRecModel</code> <code>boolean</code> Please refer to the <code>use_e2e_wired_table_rec_model</code> parameter description in the <code>predict</code> method of the model object. No <code>useE2eWirelessTableRecModel</code> <code>boolean</code> Please refer to the <code>use_e2e_wireless_table_rec_model</code> parameter description in the <code>predict</code> method of the model object. No <ul> <li>When the request is processed successfully, the <code>result</code> in the response body has the following properties:</li> </ul> Name Type Meaning <code>tableRecResults</code> <code>object</code> Table recognition results. The length of the array is 1 (for image input) or the actual number of processed document pages (for PDF input). For PDF input, each element in the array represents the result of each processed page in the PDF file. <code>dataInfo</code> <code>object</code> Input data information. <p>Each element in <code>tableRecResults</code> is an <code>object</code> with the following properties:</p> Name Type Meaning <code>prunedResult</code> <code>object</code> A simplified version of the JSON representation of the result generated by the <code>predict</code> method of the model object, where the <code>input_path</code> and <code>page_index</code> fields are removed. <code>outputImages</code> <code>object</code> | <code>null</code> Refer to the <code>img</code> property description of the model prediction results. The images are in JPEG format and encoded in Base64. <code>inputImage</code> <code>string</code> | <code>null</code> Input image. The image is in JPEG format and encoded in Base64. Multilingual Service Call Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/table-recognition\"\nfile_path = \"./demo.jpg\"\n\nwith open(file_path, \"rb\") as file:\n    file_bytes = file.read()\n    file_data = base64.b64encode(file_bytes).decode(\"ascii\")\n\npayload = {\"file\": file_data, \"fileType\": 1}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nfor i, res in enumerate(result[\"tableRecResults\"]):\n    print(res[\"prunedResult\"])\n    for img_name, img in res[\"outputImages\"].items():\n        img_path = f\"{img_name}_{i}.jpg\"\n        with open(img_path, \"wb\") as f:\n            f.write(base64.b64decode(img))\n        print(f\"Output image saved at {img_path}\")\n</code></pre> <p></p>"},{"location":"en/version3.x/pipeline_usage/table_recognition_v2.html#4-secondary-development","title":"4. Secondary Development","text":"<p>If the default model weights provided by the General Table Recognition v2 model are not satisfactory in terms of accuracy or speed for your scenario, you can try further fine-tuning the existing model using your own domain-specific or application scenario data to improve the recognition performance of the General Table Recognition v2 model in your scenario.</p> <p>Since the General Table Recognition v2 model consists of several modules, if the model's performance is not as expected, it may be due to any of these modules. You can analyze images with poor recognition performance to determine which module is problematic and refer to the corresponding fine-tuning tutorial links in the following table for model fine-tuning.</p> Situation Fine-Tuning Module Fine-Tuning Reference Link Table classification error Table Classification Module Link Table cell location error Table Cell Detection Module Link Table structure recognition error Table Structure Recognition Module Link Failed to detect the area where the table is located Layout Area Detection Module Link Text detection missed Text Detection Module Link Incorrect text content Text Recognition Module Link Overall image rotation/table rotation correction is inaccurate Document Image Orientation Classification Module Link Image distortion correction is inaccurate Text Image Correction Module Fine-tuning not supported"},{"location":"en/version3.x/pipeline_usage/table_recognition_v2.html#42-model-application","title":"4.2 Model Application","text":"<p>When you complete fine-tuning with your private dataset, you\u2019ll obtain local model weight files. You can then use these fine-tuned model weights either by specifying the local model save path through parameters or by customizing the pipeline configuration file.</p>"},{"location":"en/version3.x/pipeline_usage/table_recognition_v2.html#421-specifying-local-model-path-through-parameters","title":"4.2.1 Specifying Local Model Path Through Parameters","text":"<p>When initializing the pipeline object, you can specify the local model path via parameters. Taking the use of fine-tuned weights for the SLANeXt_wired table structure recognition model as an example:</p> <p>Command line method:</p> <pre><code># Specify local model path via --wired_table_structure_recognition_model_dir\npaddleocr table_recognition_v2_pipeline -i ./table_recognition_v2.jpg --wired_table_structure_recognition_model_dir your_model_path\n</code></pre> <pre><code># If using SLANeXt_wired model as the default wired table structure recognition model, and if you fine-tuned a different model, modify the model name via --wired_table_structure_recognition_model_name\npaddleocr table_recognition_v2_pipeline -i ./table_recognition_v2.jpg --wired_table_structure_recognition_model_name SLANeXt_wired --wired_table_structure_recognition_model_dir your_model_path\n</code></pre> <p>Python script method:</p> <pre><code>from paddleocr import TableRecognitionPipelineV2\n\n# Specify local model path via wired_table_structure_recognition_model_dir\npipeline = TableRecognitionPipelineV2(wired_table_structure_recognition_model_dir=\"./your_model_path\")\n\n# By default, SLANeXt_wired is used as the default table recognition model. If you fine-tuned a different model, modify the model name via wired_table_structure_recognition_model_name\n# pipeline = PaddleOCR(wired_table_structure_recognition_model_name=\"SLANeXt_wired\", wired_table_structure_recognition_model_dir=\"./your_model_path\")\n</code></pre>"},{"location":"en/version3.x/pipeline_usage/table_recognition_v2.html#422-specifying-local-model-path-through-configuration-file","title":"4.2.2 Specifying Local Model Path Through Configuration File","text":"<p>1.Obtain the pipeline configuration file You can call the <code>export_paddlex_config_to_yaml method</code> of the TableRecognitionPipelineV2 object in PaddleOCR to export the current pipeline configuration to a YAML file:</p> <pre><code>from paddleocr import TableRecognitionPipelineV2\n\npipeline = TableRecognitionPipelineV2()\npipeline.export_paddlex_config_to_yaml(\"TableRecognitionPipelineV2.yaml\")\n</code></pre> <p>2.Modify the configuration file</p> <p>After obtaining the default pipeline configuration file, simply replace the corresponding paths in the pipeline configuration file with the local paths of your fine-tuned model weights. For example:</p> <pre><code>......\nSubModules:\n  LayoutDetection:\n    module_name: layout_detection\n    model_name: PicoDet_layout_1x_table\n    model_dir: null # Replace with the path to fine-tuned layout detection model weights\n\n  TableClassification:\n    module_name: table_classification\n    model_name: PP-LCNet_x1_0_table_cls\n    model_dir: null # Replace with the path to fine-tuned table classification model weights\n\n  WiredTableStructureRecognition:\n    module_name: table_structure_recognition\n    model_name: SLANeXt_wired\n    model_dir: null # Replace with the path to fine-tuned wired table structure recognition model weights\n\n  WirelessTableStructureRecognition:\n    module_name: table_structure_recognition\n    model_name: SLANeXt_wireless\n    model_dir: null # Replace with the path to fine-tuned wireless table structure recognition model weights\n\n  WiredTableCellsDetection:\n    module_name: table_cells_detection\n    model_name: RT-DETR-L_wired_table_cell_det\n    model_dir: null # Replace with the path to fine-tuned wired table cell detection model weights\n\n  WirelessTableCellsDetection:\n    module_name: table_cells_detection\n    model_name: RT-DETR-L_wireless_table_cell_det\n    model_dir: null # Replace with the path to fine-tuned wireless table cell detection model weights\n\nSubPipelines:\n  DocPreprocessor:\n    pipeline_name: doc_preprocessor\n    use_doc_orientation_classify: True\n    use_doc_unwarping: True\n    SubModules:\n      DocOrientationClassify:\n        module_name: doc_text_orientation\n        model_name: PP-LCNet_x1_0_doc_ori\n        model_dir: null # Replace with the path to fine-tuned document orientation classification model weights\n\n      DocUnwarping:\n        module_name: image_unwarping\n        model_name: UVDoc\n        model_dir: null\n\n  GeneralOCR:\n    pipeline_name: OCR\n    text_type: general\n    use_doc_preprocessor: False\n    use_textline_orientation: False\n    SubModules:\n      TextDetection:\n        module_name: text_detection\n        model_name: PP-OCRv5_server_det\n        model_dir: null # Replace with the path to fine-tuned text detection model weights\n        limit_side_len: 960\n        limit_type: max\n        max_side_limit: 4000\n        thresh: 0.3\n        box_thresh: 0.4\n        unclip_ratio: 1.5\n\n      TextRecognition:\n        module_name: text_recognition\n        model_name: PP-OCRv5_server_rec\n        model_dir: null # Replace with the path to fine-tuned text recognition model weights\n        batch_size: 1\n        score_thresh: 0\n......\n</code></pre> <p>The pipeline configuration file includes not only the parameters supported by PaddleOCR CLI and Python API but also allows for more advanced configurations. For detailed information, you can find the corresponding pipeline usage tutorial in Overview of PaddleX Model Pipeline Usage, and refer to the detailed instructions to adjust the configurations according to your needs.</p> <ol> <li>Loading the pipeline configuration file in CLI</li> </ol> <p>After completing the configuration file modifications, specify the path to the modified pipeline configuration file using the --paddlex_config parameter in the command line. PaddleOCR will read its contents as the pipeline configuration. For example:</p> <pre><code>paddleocr table_recognition_v2_pipeline --paddlex_config PaddleOCR.yaml ...\n</code></pre> <ol> <li>Loading the pipeline configuration file in Python API</li> </ol> <p>When initializing the pipeline object, you can pass the PaddleX pipeline configuration file path or configuration dictionary through the paddlex_config parameter. PaddleOCR will read its contents as the pipeline configuration. For example:</p> <pre><code>from paddleocr import TableRecognitionPipelineV2\n\npipeline = TableRecognitionPipelineV2(paddlex_config=\"TableRecognitionPipelineV2.yaml\")\n</code></pre>"},{"location":"en/version3.x/pipeline_usage/instructions/parallel_inference.html","title":"Parallel Inference in Pipeline","text":""},{"location":"en/version3.x/pipeline_usage/instructions/parallel_inference.html#specifying-multiple-inference-devices","title":"Specifying Multiple Inference Devices","text":"<p>For some pipelines, both the CLI and Python API of PaddleOCR support specifying multiple inference devices simultaneously. If multiple devices are specified, during pipeline initialization, an instance of the underlying pipeline class will be created on each device, and the received inputs will be processed using parallel inference. For example, for the document image preprocessing pipeline:</p> <pre><code>paddleocr doc_preprocessor \\\n  --input input_images/ \\\n  --device 'gpu:0,1,2,3' \\\n  --use_doc_orientation_classify True \\\n  --use_doc_unwarping True\n  --save_path ./output \\\n</code></pre> <pre><code>from paddleocr import DocPreprocessor\n\n\npipeline = DocPreprocessor(device=\"gpu:0,1,2,3\") \noutput = pipeline.predict(    \n    input=\"input_images/\",\n    use_doc_orientation_classify=True,\n    use_doc_unwarping=True)\n</code></pre> <p>Both examples above use 4 GPUs (numbered 0, 1, 2, 3) to perform parallel inference on the <code>doc_test_rotated.jpg</code> image.</p> <p>When specifying multiple devices, the inference interface remains consistent with that of single-device usage. Please refer to the production line usage tutorial to check whether a specific production line supports multiple inference devices.</p>"},{"location":"en/version3.x/pipeline_usage/instructions/parallel_inference.html#example-of-multi-process-parallel-inference","title":"Example of Multi-Process Parallel Inference","text":"<p>Beyond PaddleOCR's built-in multi-device parallel inference capability, users can also implement parallelism by wrapping PaddleOCR pipeline API calls themselves according to their specific scenario, with a view to achieving a better speedup. Below is an example of using Python multiprocessing to perform multi-GPU, multi-instance parallel processing on files in an input directory.</p> <p><pre><code>import argparse\nimport sys\nfrom multiprocessing import Manager, Process\nfrom pathlib import Path\nfrom queue import Empty\n\nimport paddleocr\n\n\ndef load_pipeline(class_name: str, device: str):\n    if not hasattr(paddleocr, class_name):\n        raise ValueError(f\"Class {class_name} not found in paddleocr module.\")\n    cls = getattr(paddleocr, class_name)\n    return cls(device=device)\n\n\ndef worker(pipeline_class_path, device, task_queue, batch_size, output_dir):\n    pipeline = load_pipeline(pipeline_class_path, device)\n\n    should_end = False\n    batch = []\n\n    while not should_end:\n        try:\n            input_path = task_queue.get_nowait()\n        except Empty:\n            should_end = True\n        else:\n            batch.append(input_path)\n\n        if batch and (len(batch) == batch_size or should_end):\n            try:\n                for result in pipeline.predict(batch):\n                    input_path = Path(result[\"input_path\"])\n                    if result.get(\"page_index\") is not None:\n                        output_path = f\"{input_path.stem}_{result['page_index']}.json\"\n                    else:\n                        output_path = f\"{input_path.stem}.json\"\n                    output_path = str(Path(output_dir, output_path))\n                    result.save_to_json(output_path)\n                    print(f\"Processed {repr(str(input_path))}\")\n            except Exception as e:\n                print(\n                    f\"Error processing {batch} on {repr(device)}: {e}\",\n                    file=sys.stderr\n                )\n            batch.clear()\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--pipeline\",\n        type=str,\n        required=True,\n        help=\"PaddleOCR pipeline, e.g. 'DocPreprocessor'.\",\n    )\n    parser.add_argument(\n        \"--input_dir\", type=str, required=True, help=\"Input directory.\"\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        required=True,\n        help=\"Specifies the devices for performing parallel inference.\",\n    )\n    parser.add_argument(\n        \"--output_dir\", type=str, default=\"output\", help=\"Output directory.\"\n    )\n    parser.add_argument(\n        \"--instances_per_device\",\n        type=int,\n        default=1,\n        help=\"Number of pipeline instances per device.\",\n    )\n    parser.add_argument(\n        \"--batch_size\",\n        type=int,\n        default=1,\n        help=\"Inference batch size for each pipeline instance.\",\n    )\n    parser.add_argument(\n        \"--input_glob_pattern\",\n        type=str,\n        default=\"*\",\n        help=\"Pattern to find the input files.\",\n    )\n    args = parser.parse_args()\n\n    input_dir = Path(args.input_dir)\n    if not input_dir.exists():\n        print(f\"The input directory does not exist: {input_dir}\", file=sys.stderr)\n        return 2\n    if not input_dir.is_dir():\n        print(f\"{repr(str(input_dir))} is not a directory.\", file=sys.stderr)\n        return 2\n\n    output_dir = Path(args.output_dir)\n    if output_dir.exists() and not output_dir.is_dir():\n        print(f\"{repr(str(output_dir))} is not a directory.\", file=sys.stderr)\n        return 2\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    from paddlex.utils.device import constr_device, parse_device\n\n    device_type, device_ids = parse_device(args.device)\n    if device_ids is None or len(device_ids) == 1:\n        print(\n            \"Please specify at least two devices for performing parallel inference.\",\n            file=sys.stderr,\n        )\n        return 2\n\n    if args.batch_size &lt;= 0:\n        print(\"Batch size must be greater than 0.\", file=sys.stderr)\n        return 2\n\n    with Manager() as manager:\n        task_queue = manager.Queue()\n        for img_path in input_dir.glob(args.input_glob_pattern):\n            task_queue.put(str(img_path))\n\n        processes = []\n        for device_id in device_ids:\n            for _ in range(args.instances_per_device):\n                device = constr_device(device_type, [device_id])\n                p = Process(\n                    target=worker,\n                    args=(\n                        args.pipeline,\n                        device,\n                        task_queue,\n                        args.batch_size,\n                        str(output_dir),\n                    ),\n                )\n                p.start()\n                processes.append(p)\n\n        for p in processes:\n            p.join()\n\n    print(\"All done\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n</code></pre> Assuming the script is saved as <code>infer_mp.py</code>, here are some example commands for running it: <pre><code># This is for the general layout analysis V3 pipeline, corresponding to `PPStructureV3`\n# For the exact value of the `--pipeline` parameter, please refer to the **script** import name of the pipeline\n# Other pipelines include `SealRecognition` for the seal text pipeline, and `DocUnderstanding` for the document understanding pipeline\n# Process all files in the `input_images` directory\n# Use GPUs 0, 1, 2, and 3, with 1 pipeline instance per GPU, and each instance processes 1 input file at a time\npython infer_mp.py \\\n    --pipeline PPStructureV3 \\\n    --input_dir input_images/ \\\n    --device 'gpu:0,1,2,3' \\\n    --output_dir output\n\n# General layout analysis V3 pipeline\n# Process all files with the `.jpg` suffix in the `input_images` directory\n# Use GPUs 0 and 2, with 2 pipeline instances per GPU, and each instance processes 4 input files at a time\npython infer_mp.py \\\n    --pipeline PPStructureV3 \\\n    --input_dir input_images/ \\\n    --device 'gpu:0,2' \\\n    --output_dir output \\\n    --instances_per_device 2 \\\n    --batch_size 4 \\\n    --input_glob_pattern '*.jpg'\n</code></pre></p>"}]}